{"2025-03-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.11630v1","updated":"2025-03-14T17:48:23Z","published":"2025-03-14T17:48:23Z","title":"The time scale of redundancy between prosody and linguistic context","summary":"  In spoken language, speakers transmit information not only using words, but\nalso via a rich array of non-verbal signals, which include prosody -- the\nauditory features of speech. However, previous studies have shown that prosodic\nfeatures exhibit significant redundancy with both past and future words. Here,\nwe examine the time scale of this relationship: How many words in the past (or\nfuture) contribute to predicting prosody? We find that this scale differs for\npast and future words. Prosody's redundancy with past words extends across\napproximately 3-8 words, whereas redundancy with future words is limited to\njust 1-2 words. These findings indicate that the prosody-future relationship\nreflects local word dependencies or short-scale processes such as next word\nprediction, while the prosody-past relationship unfolds over a longer time\nscale. The latter suggests that prosody serves to emphasize earlier information\nthat may be challenging for listeners to process given limited cognitive\nresources in real-time communication. Our results highlight the role of prosody\nin shaping efficient communication.\n","authors":["Tamar I. Regev","Chiebuka Ohams","Shaylee Xie","Lukas Wolf","Evelina Fedorenko","Alex Warstadt","Ethan Wilcox","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2503.11630v1.pdf","comment":"12 pages, 4 figures, recently submitted to ACL"},{"id":"http://arxiv.org/abs/2410.19150v2","updated":"2025-03-14T17:47:49Z","published":"2024-10-24T20:42:53Z","title":"A Test of Time: Predicting the Sustainable Success of Online\n  Collaboration in Wikipedia","summary":"  The Internet has significantly expanded the potential for global\ncollaboration, allowing millions of users to contribute to collective projects\nlike Wikipedia. While prior work has assessed the success of online\ncollaborations, most approaches are time-agnostic, evaluating success without\nconsidering its longevity. Research on the factors that ensure the long-term\npreservation of high-quality standards in online collaboration is scarce. In\nthis study, we address this gap. We propose a novel metric, `Sustainable\nSuccess,' which measures the ability of collaborative efforts to maintain their\nquality over time. Using Wikipedia as a case study, we introduce the\nSustainPedia dataset, which compiles data from over 40K Wikipedia articles,\nincluding each article's sustainable success label and more than 300\nexplanatory features such as edit history, user experience, and team\ncomposition. Using this dataset, we develop machine learning models to predict\nthe sustainable success of Wikipedia articles. Our best-performing model\nachieves a high AU-ROC score of 0.88 on average. Our analysis reveals important\ninsights. For example, we find that the longer an article takes to be\nrecognized as high-quality, the more likely it is to maintain that status over\ntime (i.e., be sustainable). Additionally, user experience emerged as the most\ncritical predictor of sustainability. Our analysis provides insights into\nbroader collective actions beyond Wikipedia (e.g., online activism,\ncrowdsourced open-source software), where the same social dynamics that drive\nsuccess on Wikipedia might play a role. We make all data and code used for this\nstudy publicly available for further research.\n","authors":["Abraham Israeli","David Jurgens","Daniel Romero"],"pdf_url":"https://arxiv.org/pdf/2410.19150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11614v1","updated":"2025-03-14T17:33:30Z","published":"2025-03-14T17:33:30Z","title":"Neutralizing Bias in LLM Reasoning using Entailment Graphs","summary":"  LLMs are often claimed to be capable of Natural Language Inference (NLI),\nwhich is widely regarded as a cornerstone of more complex forms of reasoning.\nHowever, recent works show that LLMs still suffer from hallucinations in NLI\ndue to attestation bias, where LLMs overly rely on propositional memory to\nbuild shortcuts. To solve the issue, we design an unsupervised framework to\nconstruct counterfactual reasoning data and fine-tune LLMs to reduce\nattestation bias. To measure bias reduction, we build bias-adversarial variants\nof NLI datasets with randomly replaced predicates in premises while keeping\nhypotheses unchanged. Extensive evaluations show that our framework can\nsignificantly reduce hallucinations from attestation bias. Then, we further\nevaluate LLMs fine-tuned with our framework on original NLI datasets and their\nbias-neutralized versions, where original entities are replaced with randomly\nsampled ones. Extensive results show that our framework consistently improves\ninferential performance on both original and bias-neutralized NLI datasets.\n","authors":["Liang Cheng","Tianyi Li","Zhaowei Wang","Tianyang Liu","Mark Steedman"],"pdf_url":"https://arxiv.org/pdf/2503.11614v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.03461v2","updated":"2025-03-14T17:27:00Z","published":"2024-10-04T14:21:27Z","title":"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation","summary":"  While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost.\n","authors":["Tobias Leemann","Periklis Petridis","Giuseppe Vietri","Dionysis Manousakas","Aaron Roth","Sergul Aydore"],"pdf_url":"https://arxiv.org/pdf/2410.03461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02603v2","updated":"2025-03-14T17:09:03Z","published":"2024-10-03T15:44:42Z","title":"Agents' Room: Narrative Generation through Multi-step Collaboration","summary":"  Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.\n","authors":["Fantine Huot","Reinald Kim Amplayo","Jennimaria Palomaki","Alice Shoshana Jakobovits","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.02603v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.11593v1","updated":"2025-03-14T17:02:45Z","published":"2025-03-14T17:02:45Z","title":"Do Construction Distributions Shape Formal Language Learning In German\n  BabyLMs?","summary":"  We analyze the influence of utterance-level construction distributions in\nGerman child-directed speech on the resulting formal linguistic competence and\nthe underlying learning trajectories for small language models trained on a\nnovel collection of developmentally plausible language data for German. We find\nthat trajectories are surprisingly robust for markedly different distributions\nof constructions in the training data, which have little effect on final\naccuracies and almost no effect on global learning trajectories. While syntax\nlearning benefits from more complex utterances, lexical learning culminates in\nbetter scores with more fragmentary data. We argue that LMs trained on\ndevelopmentally plausible data can contribute to debates on how rich or\nimpoverished linguistic stimuli actually are.\n","authors":["Bastian Bunzeck","Daniel Duran","Sina Zarrieß"],"pdf_url":"https://arxiv.org/pdf/2503.11593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01066v2","updated":"2025-03-14T16:57:12Z","published":"2025-03-03T00:14:34Z","title":"Alchemist: Towards the Design of Efficient Online Continual Learning\n  System","summary":"  Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.\n","authors":["Yuyang Huang","Yuhan Liu","Haryadi S. Gunawi","Beibin Li","Changho Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.01066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11586v1","updated":"2025-03-14T16:55:46Z","published":"2025-03-14T16:55:46Z","title":"Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs\n  using Semantic Space","summary":"  Large language models (LLMs) are used in chatbots or AI assistants to hold\nconversations with a human user. In such applications, the quality (e.g., user\nengagement, safety) of a conversation is important and can only be exactly\nknown at the end of the conversation. To maximize its expected quality,\nconversation planning reasons about the stochastic transitions within a\nconversation to select the optimal LLM response at each turn. Existing\nsimulation-based conversation planning algorithms typically select the optimal\nresponse by simulating future conversations with a large number of LLM queries\nat every turn. However, this process is extremely time-consuming and hence\nimpractical for real-time conversations. This paper presents a novel approach\ncalled Semantic space COnversation Planning with improved Efficiency (SCOPE)\nthat exploits the dense semantic representation of conversations to perform\nconversation planning efficiently. In particular, SCOPE models the stochastic\ntransitions in conversation semantics and their associated rewards to plan\nentirely within the semantic space. This allows us to select the optimal LLM\nresponse at every conversation turn without needing additional LLM queries for\nsimulation. As a result, SCOPE can perform conversation planning 70 times\nfaster than conventional simulation-based planning algorithms when applied to a\nwide variety of conversation starters and two reward functions seen in the real\nworld, yet achieving a higher reward within a practical planning budget. Our\ncode can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.\n","authors":["Zhiliang Chen","Xinyuan Niu","Chuan-Sheng Foo","Bryan Kian Hsiang Low"],"pdf_url":"https://arxiv.org/pdf/2503.11586v1.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2308.07462v3","updated":"2025-03-14T16:19:46Z","published":"2023-08-14T21:19:44Z","title":"Playing with words: Comparing the vocabulary and lexical diversity of\n  ChatGPT and humans","summary":"  The introduction of Artificial Intelligence (AI) generative language models\nsuch as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has\ntriggered a revolution that can transform how text is generated. This has many\nimplications, for example, as AI-generated text becomes a significant fraction\nof the text, would this have an effect on the language capabilities of readers\nand also on the training of newer AI tools? Would it affect the evolution of\nlanguages? Focusing on one specific aspect of the language: words; will the use\nof tools such as ChatGPT increase or reduce the vocabulary used or the lexical\nrichness? This has implications for words, as those not included in\nAI-generated content will tend to be less and less popular and may eventually\nbe lost. In this work, we perform an initial comparison of the vocabulary and\nlexical richness of ChatGPT and humans when performing the same tasks. In more\ndetail, two datasets containing the answers to different types of questions\nanswered by ChatGPT and humans, and a third dataset in which ChatGPT\nparaphrases sentences and questions are used. The analysis shows that ChatGPT\ntends to use fewer distinct words and lower lexical richness than humans. These\nresults are very preliminary and additional datasets and ChatGPT configurations\nhave to be evaluated to extract more general conclusions. Therefore, further\nresearch is needed to understand how the use of ChatGPT and more broadly\ngenerative AI tools will affect the vocabulary and lexical richness in\ndifferent types of text and languages.\n","authors":["Pedro Reviriego","Javier Conde","Elena Merino-Gómez","Gonzalo Martínez","José Alberto Hernández"],"pdf_url":"https://arxiv.org/pdf/2308.07462v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11519v1","updated":"2025-03-14T15:42:42Z","published":"2025-03-14T15:42:42Z","title":"Exploring Typographic Visual Prompts Injection Threats in Cross-Modality\n  Generation Models","summary":"  Current Cross-Modality Generation Models (GMs) demonstrate remarkable\ncapabilities in various generative tasks. Given the ubiquity and information\nrichness of vision modality inputs in real-world scenarios, Cross-vision,\nencompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), tasks\nhave attracted significant attention. Large Vision Language Models (LVLMs) and\nI2I GMs are employed to handle VLP and I2I tasks, respectively. Previous\nresearch indicates that printing typographic words into input images\nsignificantly induces LVLMs and I2I GMs to generate disruptive outputs\nsemantically related to those words. Additionally, visual prompts, as a more\nsophisticated form of typography, are also revealed to pose security risks to\nvarious applications of VLP tasks when injected into images. In this paper, we\ncomprehensively investigate the performance impact induced by Typographic\nVisual Prompt Injection (TVPI) in various LVLMs and I2I GMs. To better observe\nperformance modifications and characteristics of this threat, we also introduce\nthe TVPI Dataset. Through extensive explorations, we deepen the understanding\nof the underlying causes of the TVPI threat in various GMs and offer valuable\ninsights into its potential origins.\n","authors":["Hao Cheng","Erjia Xiao","Yichi Wang","Kaidi Xu","Mengshu Sun","Jindong Gu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2503.11519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11517v1","updated":"2025-03-14T15:41:45Z","published":"2025-03-14T15:41:45Z","title":"Prompt Injection Detection and Mitigation via AI Multi-Agent NLP\n  Frameworks","summary":"  Prompt injection constitutes a significant challenge for generative AI\nsystems by inducing unintended outputs. We introduce a multi-agent NLP\nframework specifically designed to address prompt injection vulnerabilities\nthrough layered detection and enforcement mechanisms. The framework\norchestrates specialized agents for generating responses, sanitizing outputs,\nand enforcing policy compliance. Evaluation on 500 engineered injection prompts\ndemonstrates a marked reduction in injection success and policy breaches. Novel\nmetrics, including Injection Success Rate (ISR), Policy Override Frequency\n(POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS),\nare proposed to derive a composite Total Injection Vulnerability Score (TIVS).\nThe system utilizes the OVON (Open Voice Network) framework for inter-agent\ncommunication via structured JSON messages, extending a previously established\nmulti-agent architecture from hallucination mitigation to address the unique\nchallenges of prompt injection.\n","authors":["Diego Gosmar","Deborah A. Dahl","Dario Gosmar"],"pdf_url":"https://arxiv.org/pdf/2503.11517v1.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.16869v3","updated":"2025-03-14T15:37:57Z","published":"2024-05-27T06:36:17Z","title":"Multiple Heads are Better than One: Mixture of Modality Knowledge\n  Experts for Entity Representation Learning","summary":"  Learning high-quality multi-modal entity representations is an important goal\nof multi-modal knowledge graph (MMKG) representation learning, which can\nenhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The\nmain challenge is to collaboratively model the structural information concealed\nin massive triples and the multi-modal features of the entities. Existing\nmethods focus on crafting elegant entity-wise multi-modal fusion strategies,\nyet they overlook the utilization of multi-perspective features concealed\nwithin the modalities under diverse relational contexts. To address this issue,\nwe introduce a novel framework with Mixture of Modality Knowledge experts\n(MoMoK for short) to learn adaptive multi-modal entity representations for\nbetter MMKGC. We design relation-guided modality knowledge experts to acquire\nrelation-aware modality embeddings and integrate the predictions from\nmulti-modalities to achieve joint decisions. Additionally, we disentangle the\nexperts by minimizing their mutual information. Experiments on four public MMKG\nbenchmarks demonstrate the outstanding performance of MoMoK under complex\nscenarios.\n","authors":["Yichi Zhang","Zhuo Chen","Lingbing Guo","Yajing Xu","Binbin Hu","Ziqi Liu","Wen Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.16869v3.pdf","comment":"ICLR 2025 Camera-ready Version. Code and data will be released at\n  https://github.com/zjukg/MoMoK"},{"id":"http://arxiv.org/abs/2503.11509v1","updated":"2025-03-14T15:29:58Z","published":"2025-03-14T15:29:58Z","title":"TikZero: Zero-Shot Text-Guided Graphics Program Synthesis","summary":"  With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.\n","authors":["Jonas Belouadi","Eddy Ilg","Margret Keuper","Hideki Tanaka","Masao Utiyama","Raj Dabre","Steffen Eger","Simone Paolo Ponzetto"],"pdf_url":"https://arxiv.org/pdf/2503.11509v1.pdf","comment":"Project page: https://github.com/potamides/DeTikZify"},{"id":"http://arxiv.org/abs/2406.17911v5","updated":"2025-03-14T14:44:32Z","published":"2024-06-25T19:52:01Z","title":"X-ray Made Simple: Lay Radiology Report Generation and Robust Evaluation","summary":"  Radiology Report Generation (RRG) has advanced considerably with the\ndevelopment of multimodal generative models. Despite the progress, the field\nstill faces significant challenges in evaluation, as existing metrics lack\nrobustness and fairness. We reveal that, RRG with high performance on existing\nlexical-based metrics (e.g. BLEU) might be more of a mirage - a model can get a\nhigh BLEU only by learning the template of reports. This has become a pressing\nissue for RRG due to the highly patternized nature of these reports. In\naddition, standard radiology reports are often highly technical. Helping\npatients understand these reports is crucial from a patient's perspective, yet\nthis has been largely overlooked in previous work. In this work, we\nun-intuitively approach these problems by proposing the Layman's RRG framework\nthat can systematically improve RRG with day-to-day language. Specifically, our\nframework first contributes a translated Layman's terms dataset. Building upon\nthe dataset, we then propose a semantics-based evaluation method, which is\neffective in mitigating the inflated numbers of BLEU and provides more robust\nevaluation. We show that training on the layman's terms dataset encourages\nmodels to focus on the semantics of the reports, as opposed to overfitting to\nlearning the report templates. Last, we reveal a promising scaling law between\nthe number of training examples and semantics gain provided by our dataset,\ncompared to the inverse pattern brought by the original formats.\n","authors":["Kun Zhao","Chenghao Xiao","Sixing Yan","William K. Cheung","Kai Ye","Noura Al Moubayed","Liang Zhan","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2406.17911v5.pdf","comment":"This paper has substantial data and conceptual changes since release\n  that go beyond simple updating the existing one. As a result, the authors\n  have changed and we need to re-coordinate and reach consensus. So we decide\n  to withdraw it"},{"id":"http://arxiv.org/abs/2502.17308v2","updated":"2025-03-14T14:32:01Z","published":"2025-02-24T16:43:05Z","title":"Implicit Word Reordering with Knowledge Distillation for Cross-Lingual\n  Dependency Parsing","summary":"  Word order difference between source and target languages is a major obstacle\nto cross-lingual transfer, especially in the dependency parsing task. Current\nworks are mostly based on order-agnostic models or word reordering to mitigate\nthis problem. However, such methods either do not leverage grammatical\ninformation naturally contained in word order or are computationally expensive\nas the permutation space grows exponentially with the sentence length.\nMoreover, the reordered source sentence with an unnatural word order may be a\nform of noising that harms the model learning. To this end, we propose an\nImplicit Word Reordering framework with Knowledge Distillation (IWR-KD). This\nframework is inspired by that deep networks are good at learning feature\nlinearization corresponding to meaningful data transformation, e.g. word\nreordering. To realize this idea, we introduce a knowledge distillation\nframework composed of a word-reordering teacher model and a dependency parsing\nstudent model. We verify our proposed method on Universal Dependency Treebanks\nacross 31 different languages and show it outperforms a series of competitors,\ntogether with experimental analysis to illustrate how our method works towards\ntraining a robust parser.\n","authors":["Zhuoran Li","Chunming Hu","Junfan Chen","Zhijun Chen","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17308v2.pdf","comment":"9 pages, 5 figures, 3 tables. Accepted by The 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2503.11444v1","updated":"2025-03-14T14:29:17Z","published":"2025-03-14T14:29:17Z","title":"Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment,\n  Distribution, and Discovery","summary":"  Autonomous LLM-based agents have emerged as a powerful paradigm for complex\ntask execution, yet the field lacks standardized tools for development,\ndeployment, distribution and discovery of agents. We present Cerebrum, an Agent\nSDK for AIOS that addresses this gap through three key components: (1) a\ncomprehensive SDK featuring a modular four-layer architecture for agent\ndevelopment, encompassing LLM, memory, storage, and tool management; (2) a\ncommunity-driven Agent Hub for sharing and discovering agents, complete with\nversion control and dependency management; (3) an interactive web interface for\ntesting and evaluating agents. The platform's effectiveness is demonstrated\nthrough implementations of various agent architectures, including Chain of\nThought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by\nproviding a unified framework that standardizes agent development while\nmaintaining flexibility for researchers and developers to innovate and\ndistribute their agents. The live website is at https://app.aios.foundation,\nthe code is at https://github.com/agiresearch/Cerebrum, and video is at\nhttps://app.aios.foundation/video-demo.\n","authors":["Balaji Rama","Kai Mei","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11444v1.pdf","comment":"Accepted to the 2025 Annual Conference of the North American Chapter\n  of the Association for Computational Linguistics (NAACL) - System\n  Demonstration Track"},{"id":"http://arxiv.org/abs/2503.11426v1","updated":"2025-03-14T14:14:05Z","published":"2025-03-14T14:14:05Z","title":"Text Compression for Efficient Language Generation","summary":"  We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.\n","authors":["David Gu","Peter Belcak","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2503.11426v1.pdf","comment":"accepted to NAACL SRW 2025"},{"id":"http://arxiv.org/abs/2502.11198v2","updated":"2025-03-14T14:13:50Z","published":"2025-02-16T16:59:10Z","title":"ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity\n  Recognition","summary":"  ANCHOLIK-NER is a linguistically diverse dataset for Named Entity Recognition\n(NER) in Bangla regional dialects, capturing variations across Sylhet,\nChittagong, Barishal, Noakhali, and Mymensingh. The dataset has around 17,405\nsentences, 3,481 sentences per region. The data was collected from two publicly\navailable datasets and through web scraping from various online newspapers,\narticles. To ensure high-quality annotations, the BIO tagging scheme was\nemployed, and professional annotators with expertise in regional dialects\ncarried out the labeling process. The dataset is structured into separate\nsubsets for each region and is available in CSV format. Each entry contains\ntextual data along with identified named entities and their corresponding\nannotations. Named entities are categorized into ten distinct classes: Person,\nLocation, Organization, Food, Animal, Colour, Role, Relation, Object, and\nMiscellaneous. This dataset serves as a valuable resource for developing and\nevaluating NER models for Bangla dialectal variations, contributing to regional\nlanguage processing and low-resource NLP applications. It can be utilized to\nenhance NER systems in Bangla dialects, improve regional language\nunderstanding, and support applications in machine translation, information\nretrieval, and conversational AI.\n","authors":["Bidyarthi Paul","Faika Fairuj Preotee","Shuvashis Sarker","Shamim Rahim Refat","Shifat Islam","Tashreef Muhammad","Mohammad Ashraful Hoque","Shahriar Manzoor"],"pdf_url":"https://arxiv.org/pdf/2502.11198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19209v3","updated":"2025-03-14T13:57:16Z","published":"2024-05-29T15:49:09Z","title":"VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos","summary":"  Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data.\n","authors":["Ziyang Wang","Shoubin Yu","Elias Stengel-Eskin","Jaehong Yoon","Feng Cheng","Gedas Bertasius","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2405.19209v3.pdf","comment":"CVPR 2025; First three authors contributed equally; Project page:\n  https://videotree2024.github.io/"},{"id":"http://arxiv.org/abs/2503.05788v2","updated":"2025-03-14T13:28:04Z","published":"2025-02-28T01:20:01Z","title":"Emergent Abilities in Large Language Models: A Survey","summary":"  Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight.\n","authors":["Leonardo Berti","Flavio Giorgi","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2503.05788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11384v1","updated":"2025-03-14T13:27:35Z","published":"2025-03-14T13:27:35Z","title":"Optimizing Large Language Models for Detecting Symptoms of Comorbid\n  Depression or Anxiety in Chronic Diseases: Insights from Patient Messages","summary":"  Patients with diabetes are at increased risk of comorbid depression or\nanxiety, complicating their management. This study evaluated the performance of\nlarge language models (LLMs) in detecting these symptoms from secure patient\nmessages. We applied multiple approaches, including engineered prompts,\nsystemic persona, temperature adjustments, and zero-shot and few-shot learning,\nto identify the best-performing model and enhance performance. Three out of\nfive LLMs demonstrated excellent performance (over 90% of F-1 and accuracy),\nwith Llama 3.1 405B achieving 93% in both F-1 and accuracy using a zero-shot\napproach. While LLMs showed promise in binary classification and handling\ncomplex metrics like Patient Health Questionnaire-4, inconsistencies in\nchallenging cases warrant further real-life assessment. The findings highlight\nthe potential of LLMs to assist in timely screening and referrals, providing\nvaluable empirical knowledge for real-world triage systems that could improve\nmental health care for patients with chronic diseases.\n","authors":["Jiyeong Kim","Stephen P. Ma","Michael L. Chen","Isaac R. Galatzer-Levy","John Torous","Peter J. van Roessel","Christopher Sharp","Michael A. Pfeffer","Carolyn I. Rodriguez","Eleni Linos","Jonathan H. Chen"],"pdf_url":"https://arxiv.org/pdf/2503.11384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11381v1","updated":"2025-03-14T13:25:41Z","published":"2025-03-14T13:25:41Z","title":"Modeling Subjectivity in Cognitive Appraisal with Language Models","summary":"  As the utilization of language models in interdisciplinary, human-centered\nstudies grow, the expectation of model capabilities continues to evolve. Beyond\nexcelling at conventional tasks, models are recently expected to perform well\non user-centric measurements involving confidence and human (dis)agreement --\nfactors that reflect subjective preferences. While modeling of subjectivity\nplays an essential role in cognitive science and has been extensively studied,\nit remains under-explored within the NLP community. In light of this gap, we\nexplore how language models can harness subjectivity by conducting\ncomprehensive experiments and analysis across various scenarios using both\nfine-tuned models and prompt-based large language models (LLMs). Our\nquantitative and qualitative experimental results indicate that existing\npost-hoc calibration approaches often fail to produce satisfactory results.\nHowever, our findings reveal that personality traits and demographical\ninformation are critical for measuring subjectivity. Furthermore, our in-depth\nanalysis offers valuable insights for future research and development in the\ninterdisciplinary studies of NLP and cognitive science.\n","authors":["Yuxiang Zhou","Hainiu Xu","Desmond C. Ong","Petr Slovak","Yulan He"],"pdf_url":"https://arxiv.org/pdf/2503.11381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11377v1","updated":"2025-03-14T13:22:09Z","published":"2025-03-14T13:22:09Z","title":"Advancing the Database of Cross-Linguistic Colexifications with New\n  Workflows and Data","summary":"  Lexical resources are crucial for cross-linguistic analysis and can provide\nnew insights into computational models for natural language learning. Here, we\npresent an advanced database for comparative studies of words with multiple\nmeanings, a phenomenon known as colexification. The new version includes\nimprovements in the handling, selection and presentation of the data. We\ncompare the new database with previous versions and find that our improvements\nprovide a more balanced sample covering more language families worldwide, with\nan enhanced data quality, given that all word forms are provided in phonetic\ntranscription. We conclude that the new Database of Cross-Linguistic\nColexifications has the potential to inspire exciting new studies that link\ncross-linguistic data to open questions in linguistic typology, historical\nlinguistics, psycholinguistics, and computational linguistics.\n","authors":["Annika Tjuka","Robert Forkel","Christoph Rzymski","Johann-Mattis List"],"pdf_url":"https://arxiv.org/pdf/2503.11377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11376v1","updated":"2025-03-14T13:21:59Z","published":"2025-03-14T13:21:59Z","title":"Annotating Scientific Uncertainty: A comprehensive model using\n  linguistic patterns and comparison with existing approaches","summary":"  UnScientify, a system designed to detect scientific uncertainty in scholarly\nfull text. The system utilizes a weakly supervised technique to identify\nverbally expressed uncertainty in scientific texts and their authorial\nreferences. The core methodology of UnScientify is based on a multi-faceted\npipeline that integrates span pattern matching, complex sentence analysis and\nauthor reference checking. This approach streamlines the labeling and\nannotation processes essential for identifying scientific uncertainty, covering\na variety of uncertainty expression types to support diverse applications\nincluding information retrieval, text mining and scientific document\nprocessing. The evaluation results highlight the trade-offs between modern\nlarge language models (LLMs) and the UnScientify system. UnScientify, which\nemploys more traditional techniques, achieved superior performance in the\nscientific uncertainty detection task, attaining an accuracy score of 0.808.\nThis finding underscores the continued relevance and efficiency of\nUnScientify's simple rule-based and pattern matching strategy for this specific\napplication. The results demonstrate that in scenarios where resource\nefficiency, interpretability, and domain-specific adaptability are critical,\ntraditional methods can still offer significant advantages.\n","authors":["Panggih Kusuma Ningrum","Philipp Mayr","Nina Smirnova","Iana Atanassova"],"pdf_url":"https://arxiv.org/pdf/2503.11376v1.pdf","comment":"Paper Accepted for Publication in the Journal of Informetrics (2025)"},{"id":"http://arxiv.org/abs/2502.12486v2","updated":"2025-03-14T13:13:13Z","published":"2025-02-18T03:15:55Z","title":"EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via\n  Reinforcement Learning","summary":"  Large Language Models (LLMs) have shown impressive reasoning capabilities in\nwell-defined problems with clear solutions, such as mathematics and coding.\nHowever, they still struggle with complex real-world scenarios like business\nnegotiations, which require strategic reasoning-an ability to navigate dynamic\nenvironments and align long-term goals amidst uncertainty. Existing methods for\nstrategic reasoning face challenges in adaptability, scalability, and\ntransferring strategies to new contexts. To address these issues, we propose\nexplicit policy optimization (EPO) for strategic reasoning, featuring an LLM\nthat provides strategies in open-ended action space and can be plugged into\narbitrary LLM agents to motivate goal-directed behavior. To improve\nadaptability and policy transferability, we train the strategic reasoning model\nvia multi-turn reinforcement learning (RL) using process rewards and iterative\nself-play, without supervised fine-tuning (SFT) as a preliminary step.\nExperiments across social and physical domains demonstrate EPO's ability of\nlong-term goal alignment through enhanced strategic reasoning, achieving\nstate-of-the-art performance on social dialogue and web navigation tasks. Our\nfindings reveal various collaborative reasoning mechanisms emergent in EPO and\nits effectiveness in generating novel strategies, underscoring its potential\nfor strategic reasoning in real-world applications.\n","authors":["Xiaoqian Liu","Ke Wang","Yongbin Li","Yuchuan Wu","Wentao Ma","Aobo Kong","Fei Huang","Jianbin Jiao","Junge Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12486v2.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.20941v3","updated":"2025-03-14T13:12:38Z","published":"2024-10-28T11:49:58Z","title":"Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation","summary":"  Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT\n","authors":["Yirong Sun","Dawei Zhu","Yanjun Chen","Erjia Xiao","Xinghao Chen","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.20941v3.pdf","comment":"Accepted at NAACL 2025 Student Research Workshop"},{"id":"http://arxiv.org/abs/2503.10267v2","updated":"2025-03-14T12:48:23Z","published":"2025-03-13T11:24:09Z","title":"An Expanded Massive Multilingual Dataset for High-Performance Language\n  Technologies","summary":"  Training state-of-the-art large language models requires vast amounts of\nclean and diverse textual data. However, building suitable multilingual\ndatasets remains a challenge. In this work, we present HPLT v2, a collection of\nhigh-quality multilingual monolingual and parallel corpora. The monolingual\nportion of the data contains 8T tokens covering 193 languages, while the\nparallel data contains 380M sentence pairs covering 51 languages. We document\nthe entire data pipeline and release the code to reproduce it. We provide\nextensive analysis of the quality and characteristics of our data. Finally, we\nevaluate the performance of language models and machine translation systems\ntrained on HPLT v2, demonstrating its value.\n","authors":["Laurie Burchell","Ona de Gibert","Nikolay Arefyev","Mikko Aulamo","Marta Bañón","Pinzhen Chen","Mariia Fedorova","Liane Guillou","Barry Haddow","Jan Hajič","Jindřich Helcl","Erik Henriksson","Mateusz Klimaszewski","Ville Komulainen","Andrey Kutuzov","Joona Kytöniemi","Veronika Laippala","Petter Mæhlum","Bhavitvya Malik","Farrokh Mehryary","Vladislav Mikhailov","Nikita Moghe","Amanda Myntti","Dayyán O'Brien","Stephan Oepen","Proyag Pal","Jousia Piha","Sampo Pyysalo","Gema Ramírez-Sánchez","David Samuel","Pavel Stepachev","Jörg Tiedemann","Dušan Variš","Tereza Vojtěchová","Jaume Zaragoza-Bernabeu"],"pdf_url":"https://arxiv.org/pdf/2503.10267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11348v1","updated":"2025-03-14T12:32:40Z","published":"2025-03-14T12:32:40Z","title":"RESPONSE: Benchmarking the Ability of Language Models to Undertake\n  Commonsense Reasoning in Crisis Situation","summary":"  An interesting class of commonsense reasoning problems arises when people are\nfaced with natural disasters. To investigate this topic, we present\n\\textsf{RESPONSE}, a human-curated dataset containing 1789 annotated instances\nfeaturing 6037 sets of questions designed to assess LLMs' commonsense reasoning\nin disaster situations across different time frames. The dataset includes\nproblem descriptions, missing resources, time-sensitive solutions, and their\njustifications, with a subset validated by environmental engineers. Through\nboth automatic metrics and human evaluation, we compare LLM-generated\nrecommendations against human responses. Our findings show that even\nstate-of-the-art models like GPT-4 achieve only 37\\% human-evaluated\ncorrectness for immediate response actions, highlighting significant room for\nimprovement in LLMs' ability for commonsense reasoning in crises.\n","authors":["Aissatou Diallo","Antonis Bikakis","Luke Dickens","Anthony Hunter","Rob Miller"],"pdf_url":"https://arxiv.org/pdf/2503.11348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11346v1","updated":"2025-03-14T12:23:45Z","published":"2025-03-14T12:23:45Z","title":"AIstorian lets AI be a historian: A KG-powered multi-agent system for\n  accurate biography generation","summary":"  Huawei has always been committed to exploring the AI application in\nhistorical research. Biography generation, as a specialized form of abstractive\nsummarization, plays a crucial role in historical research but faces unique\nchallenges that existing large language models (LLMs) struggle to address.\nThese challenges include maintaining stylistic adherence to historical writing\nconventions, ensuring factual fidelity, and handling fragmented information\nacross multiple documents. We present AIstorian, a novel end-to-end agentic\nsystem featured with a knowledge graph (KG)-powered retrieval-augmented\ngeneration (RAG) and anti-hallucination multi-agents. Specifically, AIstorian\nintroduces an in-context learning based chunking strategy and a KG-based index\nfor accurate and efficient reference retrieval. Meanwhile, AIstorian\norchestrates multi-agents to conduct on-the-fly hallucination detection and\nerror-type-aware correction. Additionally, to teach LLMs a certain language\nstyle, we finetune LLMs based on a two-step training approach combining data\naugmentation-enhanced supervised fine-tuning with stylistic preference\noptimization. Extensive experiments on a real-life historical Jinshi dataset\ndemonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and\na 47.6% reduction in hallucination rate compared to existing baselines. The\ndata and code are available at: https://github.com/ZJU-DAILY/AIstorian.\n","authors":["Fengyu Li","Yilin Li","Junhao Zhu","Lu Chen","Yanfei Zhang","Jia Zhou","Hui Zu","Jingwen Zhao","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2503.11346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15398v3","updated":"2025-03-14T12:22:12Z","published":"2025-01-26T04:37:27Z","title":"How Green are Neural Language Models? Analyzing Energy Consumption in\n  Text Summarization Fine-tuning","summary":"  Artificial intelligence systems significantly impact the environment,\nparticularly in natural language processing (NLP) tasks. These tasks often\nrequire extensive computational resources to train deep neural networks,\nincluding large-scale language models containing billions of parameters. This\nstudy analyzes the trade-offs between energy consumption and performance across\nthree neural language models: two pre-trained models (T5-base and BART-base),\nand one large language model (LLaMA-3-8B). These models were fine-tuned for the\ntext summarization task, focusing on generating research paper highlights that\nencapsulate the core themes of each paper. The carbon footprint associated with\nfine-tuning each model was measured, offering a comprehensive assessment of\ntheir environmental impact. It is observed that LLaMA-3-8B produces the largest\ncarbon footprint among the three models. A wide range of evaluation metrics,\nincluding ROUGE, METEOR, MoverScore, BERTScore, and SciBERTScore, were employed\nto assess the performance of the models on the given task. This research\nunderscores the importance of incorporating environmental considerations into\nthe design and implementation of neural language models and calls for the\nadvancement of energy-efficient AI methodologies.\n","authors":["Tohida Rehman","Debarshi Kumar Sanyal","Samiran Chattopadhyay"],"pdf_url":"https://arxiv.org/pdf/2501.15398v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16073v2","updated":"2025-03-14T12:21:37Z","published":"2025-01-27T14:21:34Z","title":"Challenging Assumptions in Learning Generic Text Style Embeddings","summary":"  Recent advancements in language representation learning primarily emphasize\nlanguage modeling for deriving meaningful representations, often neglecting\nstyle-specific considerations. This study addresses this gap by creating\ngeneric, sentence-level style embeddings crucial for style-centric tasks. Our\napproach is grounded on the premise that low-level text style changes can\ncompose any high-level style. We hypothesize that applying this concept to\nrepresentation learning enables the development of versatile text style\nembeddings. By fine-tuning a general-purpose text encoder using contrastive\nlearning and standard cross-entropy loss, we aim to capture these low-level\nstyle shifts, anticipating that they offer insights applicable to high-level\ntext styles. The outcomes prompt us to reconsider the underlying assumptions as\nthe results do not always show that the learned style representations capture\nhigh-level text styles.\n","authors":["Phil Ostheimer","Marius Kloft","Sophie Fellenz"],"pdf_url":"https://arxiv.org/pdf/2501.16073v2.pdf","comment":"Proceedings of the Sixth Workshop on Insights from Negative Results\n  in NLP at NAACL-HLT"},{"id":"http://arxiv.org/abs/2503.10351v2","updated":"2025-03-14T12:09:34Z","published":"2025-03-13T13:27:53Z","title":"New Trends for Modern Machine Translation with Large Reasoning Models","summary":"  Recent advances in Large Reasoning Models (LRMs), particularly those\nleveraging Chain-of-Thought reasoning (CoT), have opened brand new possibility\nfor Machine Translation (MT). This position paper argues that LRMs\nsubstantially transformed traditional neural MT as well as LLMs-based MT\nparadigms by reframing translation as a dynamic reasoning task that requires\ncontextual, cultural, and linguistic understanding and reasoning. We identify\nthree foundational shifts: 1) contextual coherence, where LRMs resolve\nambiguities and preserve discourse structure through explicit reasoning over\ncross-sentence and complex context or even lack of context; 2) cultural\nintentionality, enabling models to adapt outputs by inferring speaker intent,\naudience expectations, and socio-linguistic norms; 3) self-reflection, LRMs can\nperform self-reflection during the inference time to correct the potential\nerrors in translation especially extremely noisy cases, showing better\nrobustness compared to simply mapping X->Y translation. We explore various\nscenarios in translation including stylized translation, document-level\ntranslation and multimodal translation by showcasing empirical examples that\ndemonstrate the superiority of LRMs in translation. We also identify several\ninteresting phenomenons for LRMs for MT including auto-pivot translation as\nwell as the critical challenges such as over-localisation in translation and\ninference efficiency. In conclusion, we think that LRMs redefine translation\nsystems not merely as text converters but as multilingual cognitive agents\ncapable of reasoning about meaning beyond the text. This paradigm shift reminds\nus to think of problems in translation beyond traditional translation scenarios\nin a much broader context with LRMs - what we can achieve on top of it.\n","authors":["Sinuo Liu","Chenyang Lyu","Minghao Wu","Longyue Wang","Weihua Luo","Kaifu Zhang","Zifu Shang"],"pdf_url":"https://arxiv.org/pdf/2503.10351v2.pdf","comment":"arXiv admin note: text overlap with arXiv:1701.04715 by other authors"},{"id":"http://arxiv.org/abs/2503.11336v1","updated":"2025-03-14T12:05:06Z","published":"2025-03-14T12:05:06Z","title":"Rule-Guided Feedback: Enhancing Reasoning by Enforcing Rule Adherence in\n  Large Language Models","summary":"  In this paper, we introduce Rule-Guided Feedback (RGF), a framework designed\nto enhance Large Language Model (LLM) performance through structured rule\nadherence and strategic information seeking. RGF implements a teacher-student\nparadigm where rule-following is forced through established guidelines. Our\nframework employs a Teacher model that rigorously evaluates each student output\nagainst task-specific rules, providing constructive guidance rather than direct\nanswers when detecting deviations. This iterative feedback loop serves two\ncrucial purposes: maintaining solutions within defined constraints and\nencouraging proactive information seeking to resolve uncertainties. We evaluate\nRGF on diverse tasks including Checkmate-in-One puzzles, Sonnet Writing,\nPenguins-In-a-Table classification, GSM8k, and StrategyQA. Our findings suggest\nthat structured feedback mechanisms can significantly enhance LLMs' performance\nacross various domains.\n","authors":["Aissatou Diallo","Antonis Bikakis","Luke Dickens","Anthony Hunter","Rob Miller"],"pdf_url":"https://arxiv.org/pdf/2503.11336v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04284v3","updated":"2025-03-14T11:52:30Z","published":"2024-08-08T07:43:17Z","title":"LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection","summary":"  The ease of access to large language models (LLMs) has enabled a widespread\nof machine-generated texts, and now it is often hard to tell whether a piece of\ntext was human-written or machine-generated. This raises concerns about\npotential misuse, particularly within educational and academic domains. Thus,\nit is important to develop practical systems that can automate the process.\nHere, we present one such system, LLM-DetectAIve, designed for fine-grained\ndetection. Unlike most previous work on machine-generated text detection, which\nfocused on binary classification, LLM-DetectAIve supports four categories: (i)\nhuman-written, (ii) machine-generated, (iii) machine-written, then\nmachine-humanized, and (iv) human-written, then machine-polished. Category\n(iii) aims to detect attempts to obfuscate the fact that a text was\nmachine-generated, while category (iv) looks for cases where the LLM was used\nto polish a human-written text, which is typically acceptable in academic\nwriting, but not in education. Our experiments show that LLM-DetectAIve can\neffectively identify the above four categories, which makes it a potentially\nuseful tool in education, academia, and other domains.\n  LLM-DetectAIve is publicly accessible at\nhttps://github.com/mbzuai-nlp/LLM-DetectAIve. The video describing our system\nis available at https://youtu.be/E8eT_bE7k8c.\n","authors":["Mervat Abassy","Kareem Elozeiri","Alexander Aziz","Minh Ngoc Ta","Raj Vardhan Tomar","Bimarsha Adhikari","Saad El Dine Ahmed","Yuxia Wang","Osama Mohammed Afzal","Zhuohan Xie","Jonibek Mansurov","Ekaterina Artemova","Vladislav Mikhailov","Rui Xing","Jiahui Geng","Hasan Iqbal","Zain Muhammad Mujahid","Tarek Mahmoud","Akim Tsvigun","Alham Fikri Aji","Artem Shelmanov","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2408.04284v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11314v1","updated":"2025-03-14T11:30:37Z","published":"2025-03-14T11:30:37Z","title":"Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large\n  Language Models via Representation Engineering","summary":"  Recent advancements in long chain-of-thoughts(long CoTs) have significantly\nimproved the reasoning capabilities of large language models(LLMs). Existing\nwork finds that the capability of long CoT reasoning can be efficiently\nelicited by tuning on only a few examples and can easily transfer to other\ntasks. This motivates us to investigate whether long CoT reasoning is a general\ncapability for LLMs. In this work, we conduct an empirical analysis for this\nquestion from the perspective of representation. We find that LLMs do encode\nlong CoT reasoning as a general capability, with a clear distinction from\nvanilla CoTs. Furthermore, domain-specific representations are also required\nfor the effective transfer of long CoT reasoning. Inspired by these findings,\nwe propose GLoRE, a novel representation engineering method to unleash the\ngeneral long CoT reasoning capabilities of LLMs. Extensive experiments\ndemonstrate the effectiveness and efficiency of GLoRE in both in-domain and\ncross-domain scenarios.\n","authors":["Xinyu Tang","Xiaolei Wang","Zhihao Lv","Yingqian Min","Wayne Xin Zhao","Binbin Hu","Ziqi Liu","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11302v1","updated":"2025-03-14T11:11:03Z","published":"2025-03-14T11:11:03Z","title":"Are formal and functional linguistic mechanisms dissociated?","summary":"  Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2503.11302v1.pdf","comment":"35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation"},{"id":"http://arxiv.org/abs/2503.11301v1","updated":"2025-03-14T11:11:00Z","published":"2025-03-14T11:11:00Z","title":"GNNs as Predictors of Agentic Workflow Performances","summary":"  Agentic workflows invoked by Large Language Models (LLMs) have achieved\nremarkable success in handling complex tasks. However, optimizing such\nworkflows is costly and inefficient in real-world applications due to extensive\ninvocations of LLMs. To fill this gap, this position paper formulates agentic\nworkflows as computational graphs and advocates Graph Neural Networks (GNNs) as\nefficient predictors of agentic workflow performances, avoiding repeated LLM\ninvocations for evaluation. To empirically ground this position, we construct\nFLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic\nworkflow performances. With extensive experiments, we arrive at the following\nconclusion: GNNs are simple yet effective predictors. This conclusion supports\nnew applications of GNNs and a novel direction towards automating agentic\nworkflow optimization. All codes, models, and data are available at\nhttps://github.com/youngsoul0731/Flora-Bench.\n","authors":["Yuanshuo Zhang","Yuchen Hou","Bohan Tang","Shuo Chen","Muhan Zhang","Xiaowen Dong","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2503.11301v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.11299v1","updated":"2025-03-14T11:08:30Z","published":"2025-03-14T11:08:30Z","title":"BriLLM: Brain-inspired Large Language Model","summary":"  This paper reports the first brain-inspired large language model (BriLLM).\nThis is a non-Transformer, non-GPT, non-traditional machine learning\ninput-output controlled generative language model. The model is based on the\nSignal Fully-connected flowing (SiFu) definition on the directed graph in terms\nof the neural network, and has the interpretability of all nodes on the graph\nof the whole model, instead of the traditional machine learning model that only\nhas limited interpretability at the input and output ends. In the language\nmodel scenario, the token is defined as a node in the graph. A randomly shaped\nor user-defined signal flow flows between nodes on the principle of \"least\nresistance\" along paths. The next token or node to be predicted or generated is\nthe target of the signal flow. As a language model, BriLLM theoretically\nsupports infinitely long $n$-gram models when the model size is independent of\nthe input and predicted length of the model. The model's working signal flow\nprovides the possibility of recall activation and innate multi-modal support\nsimilar to the cognitive patterns of the human brain. At present, we released\nthe first BriLLM version in Chinese, with 4000 tokens, 32-dimensional node\nwidth, 16-token long sequence prediction ability, and language model prediction\nperformance comparable to GPT-1. More computing power will help us explore the\ninfinite possibilities depicted above.\n","authors":["Hai Zhao","Hongqiu Wu","Dongjie Yang","Anni Zou","Jiale Hong"],"pdf_url":"https://arxiv.org/pdf/2503.11299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11280v1","updated":"2025-03-14T10:39:27Z","published":"2025-03-14T10:39:27Z","title":"High-Dimensional Interlingual Representations of Large Language Models","summary":"  Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.\n","authors":["Bryan Wilie","Samuel Cahyawijaya","Junxian He","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2503.11280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08161v3","updated":"2025-03-14T10:09:13Z","published":"2025-03-11T08:26:37Z","title":"OASIS: Order-Augmented Strategy for Improved Code Search","summary":"  Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.\n","authors":["Zuchen Gao","Zizheng Zhan","Xianming Li","Erxin Yu","Haotian Zhang","Bin Chen","Yuqun Zhang","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2503.08161v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11256v1","updated":"2025-03-14T10:07:07Z","published":"2025-03-14T10:07:07Z","title":"Line of Duty: Evaluating LLM Self-Knowledge via Consistency in\n  Feasibility Boundaries","summary":"  As LLMs grow more powerful, their most profound achievement may be\nrecognising when to say \"I don't know\". Existing studies on LLM self-knowledge\nhave been largely constrained by human-defined notions of feasibility, often\nneglecting the reasons behind unanswerability by LLMs and failing to study\ndeficient types of self-knowledge. This study aims to obtain intrinsic insights\ninto different types of LLM self-knowledge with a novel methodology: allowing\nthem the flexibility to set their own feasibility boundaries and then analysing\nthe consistency of these limits. We find that even frontier models like GPT-4o\nand Mistral Large are not sure of their own capabilities more than 80% of the\ntime, highlighting a significant lack of trustworthiness in responses. Our\nanalysis of confidence balance in LLMs indicates that models swing between\noverconfidence and conservatism in feasibility boundaries depending on task\ncategories and that the most significant self-knowledge weaknesses lie in\ntemporal awareness and contextual understanding. These difficulties in\ncontextual comprehension additionally lead models to question their operational\nboundaries, resulting in considerable confusion within the self-knowledge of\nLLMs. We make our code and results available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Self_Knowledge_Eval\n","authors":["Sahil Kale","Vijaykant Nadadur"],"pdf_url":"https://arxiv.org/pdf/2503.11256v1.pdf","comment":"14 pages, 8 figures, Accepted to the 5th TrustNLP Workshop at NAACL\n  2025"},{"id":"http://arxiv.org/abs/2503.11251v1","updated":"2025-03-14T10:01:55Z","published":"2025-03-14T10:01:55Z","title":"Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven\n  Image-to-Video Generation Model","summary":"  We present Step-Video-TI2V, a state-of-the-art text-driven image-to-video\ngeneration model with 30B parameters, capable of generating videos up to 102\nframes based on both text and image inputs. We build Step-Video-TI2V-Eval as a\nnew benchmark for the text-driven image-to-video task and compare\nStep-Video-TI2V with open-source and commercial TI2V engines using this\ndataset. Experimental results demonstrate the state-of-the-art performance of\nStep-Video-TI2V in the image-to-video generation task. Both Step-Video-TI2V and\nStep-Video-TI2V-Eval are available at\nhttps://github.com/stepfun-ai/Step-Video-TI2V.\n","authors":["Haoyang Huang","Guoqing Ma","Nan Duan","Xing Chen","Changyi Wan","Ranchen Ming","Tianyu Wang","Bo Wang","Zhiying Lu","Aojie Li","Xianfang Zeng","Xinhao Zhang","Gang Yu","Yuhe Yin","Qiling Wu","Wen Sun","Kang An","Xin Han","Deshan Sun","Wei Ji","Bizhu Huang","Brian Li","Chenfei Wu","Guanzhe Huang","Huixin Xiong","Jiaxin He","Jianchang Wu","Jianlong Yuan","Jie Wu","Jiashuai Liu","Junjing Guo","Kaijun Tan","Liangyu Chen","Qiaohui Chen","Ran Sun","Shanshan Yuan","Shengming Yin","Sitong Liu","Wei Chen","Yaqi Dai","Yuchu Luo","Zheng Ge","Zhisheng Guan","Xiaoniu Song","Yu Zhou","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Yi Xiu","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.11251v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2503.11248v1","updated":"2025-03-14T10:00:03Z","published":"2025-03-14T10:00:03Z","title":"Reasoning-Grounded Natural Language Explanations for Language Models","summary":"  We propose a large language model explainability technique for obtaining\nfaithful natural language explanations by grounding the explanations in a\nreasoning process. When converted to a sequence of tokens, the outputs of the\nreasoning process can become part of the model context and later be decoded to\nnatural language as the model produces either the final answer or the\nexplanation. To improve the faithfulness of the explanations, we propose to use\na joint predict-explain approach, in which the answers and explanations are\ninferred directly from the reasoning sequence, without the explanations being\ndependent on the answers and vice versa. We demonstrate the plausibility of the\nproposed technique by achieving a high alignment between answers and\nexplanations in several problem domains, observing that language models often\nsimply copy the partial decisions from the reasoning sequence into the final\nanswers or explanations. Furthermore, we show that the proposed use of\nreasoning can also improve the quality of the answers.\n","authors":["Vojtech Cahlik","Rodrigo Alves","Pavel Kordik"],"pdf_url":"https://arxiv.org/pdf/2503.11248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03930v2","updated":"2025-03-14T09:44:42Z","published":"2024-06-06T10:16:43Z","title":"Culturally Aware and Adapted NLP: A Taxonomy and a Survey of the State\n  of the Art","summary":"  The surge of interest in \"culture\" in NLP has inspired much recent research,\nbut a shared understanding of \"culture\" remains unclear, making it difficult to\nevaluate progress in this emerging area. Drawing on prior research in NLP and\nrelated fields, we propose a fine-grained taxonomy of elements in culture that\ncan provide a systematic framework for analyzing and understanding research\nprogress. Using the taxonomy, we survey existing resources and methods for\nculturally aware and adapted NLP, providing an overview of the state of the art\nand the research gaps that still need to be filled.\n","authors":["Chen Cecilia Liu","Iryna Gurevych","Anna Korhonen"],"pdf_url":"https://arxiv.org/pdf/2406.03930v2.pdf","comment":"TACL; pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2503.11237v1","updated":"2025-03-14T09:42:07Z","published":"2025-03-14T09:42:07Z","title":"Collaboration is all you need: LLM Assisted Safe Code Translation","summary":"  This paper introduces UniTranslator, a visionary framework that re-imagines\ncode translation as a collaborative endeavor among multiple, compact LLMs. By\norchestrating the interaction of specialized agents, each focused on different\naspects of the translation process and grounded in a deep understanding of\nprogramming concepts, UniTranslator achieves a level of accuracy and efficiency\nthat rivals larger, monolithic models. Our preliminary evaluation demonstrates\nthe potential of UniTranslator to overcome the limitations of existing\napproaches and unlock the power of smaller LLMs for complex code translation\ntasks. We explore the effectiveness of this dynamic multi-agent paradigm in\nhandling diverse language pairs, including low-resource languages, and in\nmitigating common issues such as code artifacts and hallucinations through the\nuse of Natural Language Inference (NLI) grounding and iterative feedback\nmechanisms\n","authors":["Rabimba Karanjai","Sam Blackshear","Lei Xu","Weidong Shi"],"pdf_url":"https://arxiv.org/pdf/2503.11237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03309v5","updated":"2025-03-14T09:33:02Z","published":"2023-10-05T04:47:49Z","title":"Concise and Organized Perception Facilitates Reasoning in Large Language\n  Models","summary":"  Exploiting large language models (LLMs) to tackle reasoning has garnered\ngrowing attention. It still remains highly challenging to achieve satisfactory\nresults in complex logical problems, characterized by plenty of premises within\nthe context and requiring multi-hop reasoning. In particular, the reasoning\ncapabilities of LLMs are brittle to disorder and distractibility. In this work,\nwe first examine the mechanism from the perspective of information flow and\nreveal that LLMs confront difficulties akin to human-like cognitive biases when\ndealing with disordered and irrelevant content in reasoning tasks. However, in\ncontrast to LLMs, disordered and irrelevant content does not significantly\ndecrease human performance, as humans have a propensity to distill the most\nrelevant information and systematically organize their thoughts, aiding them in\nresponding to questions.Stem from that, we further propose a novel reasoning\napproach named Concise and Organized Perception (COP). COP carefully analyzes\nthe given statements to identify the most pertinent information while\neliminating redundancy efficiently. It then prompts the LLMs in a more\norganized form that adapts to the model's inference process. By perceiving\nconcise and organized context, the reasoning abilities of LLMs can be better\nelicited. Extensive experimental results on several popular logical benchmarks\n(ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark\n(DI-GSM) show that COP significantly outperforms previous state-of-the-art\nmethods.\n","authors":["Junjie Liu","Shaotian Yan","Chen Shen","Zhengdong Xiao","Liang Xie","Wenxiao Wang","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2310.03309v5.pdf","comment":"Accepted by NAACL2025 Findings"},{"id":"http://arxiv.org/abs/2503.11232v1","updated":"2025-03-14T09:31:01Z","published":"2025-03-14T09:31:01Z","title":"PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.\n","authors":["Ahmed Frikha","Muhammad Reza Ar Razi","Krishna Kanth Nakka","Ricardo Mendes","Xue Jiang","Xuebing Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.11232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11229v1","updated":"2025-03-14T09:26:07Z","published":"2025-03-14T09:26:07Z","title":"Exploring the Potential of Large Multimodal Models as Effective\n  Alternatives for Pronunciation Assessment","summary":"  Large Multimodal Models (LMMs) have demonstrated exceptional performance\nacross a wide range of domains. This paper explores their potential in\npronunciation assessment tasks, with a particular focus on evaluating the\ncapabilities of the Generative Pre-trained Transformer (GPT) model,\nspecifically GPT-4o. Our study investigates its ability to process speech and\naudio for pronunciation assessment across multiple levels of granularity and\ndimensions, with an emphasis on feedback generation and scoring. For our\nexperiments, we use the publicly available Speechocean762 dataset. The\nevaluation focuses on two key aspects: multi-level scoring and the practicality\nof the generated feedback. Scoring results are compared against the manual\nscores provided in the Speechocean762 dataset, while feedback quality is\nassessed using Large Language Models (LLMs). The findings highlight the\neffectiveness of integrating LMMs with traditional methods for pronunciation\nassessment, offering insights into the model's strengths and identifying areas\nfor further improvement.\n","authors":["Ke Wang","Lei He","Kun Liu","Yan Deng","Wenning Wei","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.11229v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2503.11224v1","updated":"2025-03-14T09:20:31Z","published":"2025-03-14T09:20:31Z","title":"Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models","summary":"  State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.\n","authors":["Xingtai Lv","Youbang Sun","Kaiyan Zhang","Shang Qu","Xuekai Zhu","Yuchen Fan","Yi Wu","Ermo Hua","Xinwei Long","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.11224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17174v2","updated":"2025-03-14T08:56:37Z","published":"2024-09-20T08:28:23Z","title":"CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal\n  Significance and Consistency","summary":"  Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal illusions between \\textit{a step of reasoning} and \\textit{corresponding\nstate transitions} are becoming a significant obstacle to advancing LLMs'\nreasoning capabilities, especially in long-range reasoning tasks. This paper\nproposes a non-chain-based reasoning framework for simultaneous consideration\nof causal significance and consistency, i.e., the Causal Significance and\nConsistency Enhancer (CSCE). We customize LLM's loss function utilizing\ntreatment effect assessments to enhance its reasoning ability from two aspects:\ncausal significance and consistency. This ensures that the model captures\nessential causal relationships and maintains robust and consistent performance\nacross various scenarios. Additionally, we transform the reasoning process from\nthe cascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.\n","authors":["Kangsheng Wang","Xiao Zhang","Zizheng Guo","Tianyu Hu","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2409.17174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10220v2","updated":"2025-03-14T08:44:13Z","published":"2025-03-13T10:01:07Z","title":"Assessing the validity of new paradigmatic complexity measures as\n  criterial features for proficiency in L2 writings in English","summary":"  This article addresses Second Language (L2) writing development through an\ninvestigation of new grammatical and structural complexity metrics. We explore\nthe paradigmatic production in learner English by linking language functions to\nspecific grammatical paradigms. Using the EFCAMDAT as a gold standard and a\ncorpus of French learners as an external test set, we employ a supervised\nlearning framework to operationalise and evaluate seven microsystems. We show\nthat learner levels are associated with the seven microsystems (MS). Using\nordinal regression modelling for evaluation, the results show that all MS are\nsignificant but yield a low impact if taken individually. However, their\ninfluence is shown to be impactful if taken as a group. These microsystems and\ntheir measurement method suggest that it is possible to use them as part of\nbroader-purpose CALL systems focused on proficiency assessment.\n","authors":["Cyriel Mallart","Andrew Simpkin","Nicolas Ballier","Paula Lissón","Rémi Venant","Jen-Yu Li","Bernardo Stearns","Thomas Gaillat"],"pdf_url":"https://arxiv.org/pdf/2503.10220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02465v2","updated":"2025-03-14T08:44:06Z","published":"2024-09-04T06:28:22Z","title":"DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels","summary":"  Recently, significant efforts have been devoted to enhancing the long-context\ncapabilities of Large Language Models (LLMs), particularly in long-context\nreasoning. To facilitate this research, we propose \\textbf{DetectiveQA}, a\ndataset specifically designed for narrative reasoning within long contexts. We\nleverage detective novels, averaging over 100k tokens, to create a dataset\ncontaining 1200 human-annotated questions in both Chinese and English, each\npaired with corresponding reference reasoning steps. Furthermore, we introduce\na step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning\nprocesses. We validate our approach and evaluate the mainstream LLMs, including\nGPT-4, Claude, and LLaMA, revealing persistent long-context reasoning\nchallenges and demonstrating their evidence-retrieval challenges. Our findings\noffer valuable insights into the study of long-context reasoning and lay the\nbase for more rigorous evaluations.\n","authors":["Zhe Xu","Jiasheng Ye","Xiaoran Liu","Xiangyang Liu","Tianxiang Sun","Zhigeng Liu","Qipeng Guo","Linlin Li","Qun Liu","Xuanjing Huang","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2409.02465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11197v1","updated":"2025-03-14T08:43:53Z","published":"2025-03-14T08:43:53Z","title":"Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering","summary":"  Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi/r1-aqa and https://huggingface.co/mispeech/r1-aqa.\n","authors":["Gang Li","Jizhong Liu","Heinrich Dinkel","Yadong Niu","Junbo Zhang","Jian Luan"],"pdf_url":"https://arxiv.org/pdf/2503.11197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11190v1","updated":"2025-03-14T08:34:28Z","published":"2025-03-14T08:34:28Z","title":"Cross-Modal Learning for Music-to-Music-Video Description Generation","summary":"  Music-to-music-video generation is a challenging task due to the intrinsic\ndifferences between the music and video modalities. The advent of powerful\ntext-to-video diffusion models has opened a promising pathway for music-video\n(MV) generation by first addressing the music-to-MV description task and\nsubsequently leveraging these models for video generation. In this study, we\nfocus on the MV description generation task and propose a comprehensive\npipeline encompassing training data construction and multimodal model\nfine-tuning. We fine-tune existing pre-trained multimodal models on our newly\nconstructed music-to-MV description dataset based on the Music4All dataset,\nwhich integrates both musical and visual information. Our experimental results\ndemonstrate that music representations can be effectively mapped to textual\ndomains, enabling the generation of meaningful MV description directly from\nmusic inputs. We also identify key components in the dataset construction\npipeline that critically impact the quality of MV description and highlight\nspecific musical attributes that warrant greater focus for improved MV\ndescription generation.\n","authors":["Zhuoyuan Mao","Mengjie Zhao","Qiyu Wu","Zhi Zhong","Wei-Hsiang Liao","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2503.11190v1.pdf","comment":"Accepted by RepL4NLP 2025 @ NAACL 2025"},{"id":"http://arxiv.org/abs/2503.11182v1","updated":"2025-03-14T08:30:09Z","published":"2025-03-14T08:30:09Z","title":"Palette of Language Models: A Solver for Controlled Text Generation","summary":"  Recent advancements in large language models have revolutionized text\ngeneration with their remarkable capabilities. These models can produce\ncontrolled texts that closely adhere to specific requirements when prompted\nappropriately. However, designing an optimal prompt to control multiple\nattributes simultaneously can be challenging. A common approach is to linearly\ncombine single-attribute models, but this strategy often overlooks attribute\noverlaps and can lead to conflicts. Therefore, we propose a novel combination\nstrategy inspired by the Law of Total Probability and Conditional Mutual\nInformation Minimization on generative language models. This method has been\nadapted for single-attribute control scenario and is termed the Palette of\nLanguage Models due to its theoretical linkage between attribute strength and\ngeneration style, akin to blending colors on an artist's palette. Moreover,\npositive correlation and attribute enhancement are advanced as theoretical\nproperties to guide a rational combination strategy design. We conduct\nexperiments on both single control and multiple control settings, and achieve\nsurpassing results.\n","authors":["Zhe Yang","Yi Huang","Yaqin Chen","Xiaoting Wu","Junlan Feng","Chao Deng"],"pdf_url":"https://arxiv.org/pdf/2503.11182v1.pdf","comment":"Accepted to NAACL 2025, Main, Long Paper"},{"id":"http://arxiv.org/abs/2503.11170v1","updated":"2025-03-14T08:16:02Z","published":"2025-03-14T08:16:02Z","title":"DeskVision: Large Scale Desktop Region Captioning for Advanced GUI\n  Agents","summary":"  The limitation of graphical user interface (GUI) data has been a significant\nbarrier to the development of GUI agents today, especially for the desktop /\ncomputer use scenarios. To address this, we propose an automated GUI data\ngeneration pipeline, AutoCaptioner, which generates data with rich descriptions\nwhile minimizing human effort. Using AutoCaptioner, we created a novel\nlarge-scale desktop GUI dataset, DeskVision, along with the largest desktop\ntest benchmark, DeskVision-Eval, which reflects daily usage and covers diverse\nsystems and UI elements, each with rich descriptions. With DeskVision, we train\na new GUI understanding model, GUIExplorer. Results show that GUIExplorer\nachieves state-of-the-art (SOTA) performance in understanding/grounding visual\nelements without the need for complex architectural designs. We further\nvalidated the effectiveness of the DeskVision dataset through ablation studies\non various large visual language models (LVLMs). We believe that AutoCaptioner\nand DeskVision will significantly advance the development of GUI agents, and\nwill open-source them for the community.\n","authors":["Yibin Xu","Liang Yang","Hao Chen","Hua Wang","Zhi Chen","Yaohua Tang"],"pdf_url":"https://arxiv.org/pdf/2503.11170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09214v2","updated":"2025-03-14T08:06:53Z","published":"2024-11-14T06:20:21Z","title":"HateGPT: Unleashing GPT-3.5 Turbo to Combat Hate Speech on X","summary":"  The widespread use of social media platforms like Twitter and Facebook has\nenabled people of all ages to share their thoughts and experiences, leading to\nan immense accumulation of user-generated content. However, alongside the\nbenefits, these platforms also face the challenge of managing hate speech and\noffensive content, which can undermine rational discourse and threaten\ndemocratic values. As a result, there is a growing need for automated methods\nto detect and mitigate such content, especially given the complexity of\nconversations that may require contextual analysis across multiple languages,\nincluding code-mixed languages like Hinglish, German-English, and Bangla. We\nparticipated in the English task where we have to classify English tweets into\ntwo categories namely Hate and Offensive and Non Hate-Offensive. In this work,\nwe experiment with state-of-the-art large language models like GPT-3.5 Turbo\nvia prompting to classify tweets into Hate and Offensive or Non Hate-Offensive.\nIn this study, we evaluate the performance of a classification model using\nMacro-F1 scores across three distinct runs. The Macro-F1 score, which balances\nprecision and recall across all classes, is used as the primary metric for\nmodel evaluation. The scores obtained are 0.756 for run 1, 0.751 for run 2, and\n0.754 for run 3, indicating a high level of performance with minimal variance\namong the runs. The results suggest that the model consistently performs well\nin terms of precision and recall, with run 1 showing the highest performance.\nThese findings highlight the robustness and reliability of the model across\ndifferent runs.\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2411.09214v2.pdf","comment":"There are errors in the results and conclusion section of the paper\n  due to which i want to withdraw"},{"id":"http://arxiv.org/abs/2503.11164v1","updated":"2025-03-14T08:05:49Z","published":"2025-03-14T08:05:49Z","title":"Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity","summary":"  N:M structured pruning is essential for large language models (LLMs) because\nit can remove less important network weights and reduce the memory and\ncomputation requirements. Existing pruning methods mainly focus on designing\nmetrics to measure the importance of network components to guide pruning. Apart\nfrom the impact of these metrics, we observe that different layers have\ndifferent sensitivities over the network performance. Thus, we propose an\nefficient method based on the trace of Fisher Information Matrix (FIM) to\nquantitatively measure and verify the different sensitivities across layers.\nBased on this, we propose Mixed Sparsity Pruning (MSP) which uses a\npruning-oriented evolutionary algorithm (EA) to determine the optimal sparsity\nlevels for different layers. To guarantee fast convergence and achieve\npromising performance, we utilize efficient FIM-inspired layer-wise sensitivity\nto initialize the population of EA. In addition, our MSP can work as a\nplug-and-play module, ready to be integrated into existing pruning methods.\nExtensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot\ntasks demonstrate our superior performance. In particular, in extreme pruning\nratio (e.g. 75%), our method significantly outperforms existing methods in\nterms of perplexity (PPL) by orders of magnitude (Figure 1).\n","authors":["Chi Xu","Gefei Zhang","Yantong Zhu","Luca Benini","Guosheng Hu","Yawei Li","Zhihong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04752v2","updated":"2025-03-14T08:04:15Z","published":"2024-11-07T14:41:01Z","title":"RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced\n  Code-Mixed Information Retrieval","summary":"  Code-mixing, the integration of lexical and grammatical elements from\nmultiple languages within a single sentence, is a widespread linguistic\nphenomenon, particularly prevalent in multilingual societies. In India, social\nmedia users frequently engage in code-mixed conversations using the Roman\nscript, especially among migrant communities who form online groups to share\nrelevant local information. This paper focuses on the challenges of extracting\nrelevant information from code-mixed conversations, specifically within Roman\ntransliterated Bengali mixed with English. This study presents a novel approach\nto address these challenges by developing a mechanism to automatically identify\nthe most relevant answers from code-mixed conversations. We have experimented\nwith a dataset comprising of queries and documents from Facebook, and Query\nRelevance files (QRels) to aid in this task. Our results demonstrate the\neffectiveness of our approach in extracting pertinent information from complex,\ncode-mixed digital conversations, contributing to the broader field of natural\nlanguage processing in multilingual and informal text environments. We use\nGPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant\ndocuments to frame a mathematical model which helps to detect relevant\ndocuments corresponding to a query.\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2411.04752v2.pdf","comment":"There are errors in the results and conclusion section of the paper\n  due to which i want to withdraw"},{"id":"http://arxiv.org/abs/2503.11154v1","updated":"2025-03-14T07:46:33Z","published":"2025-03-14T07:46:33Z","title":"Don't Take Things Out of Context: Attention Intervention for Enhancing\n  Chain-of-Thought Reasoning in Large Language Models","summary":"  Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning\ncapabilities of large language models (LLMs), functioning as a whole to guide\nthese models in generating reasoning steps toward final answers. However, we\nobserve that isolated segments, words, or tokens within CoT demonstrations can\nunexpectedly disrupt the generation process of LLMs. The model may overly\nconcentrate on certain local information present in the demonstration,\nintroducing irrelevant noise into the reasoning process and potentially leading\nto incorrect answers. In this paper, we investigate the underlying mechanism of\nCoT through dynamically tracing and manipulating the inner workings of LLMs at\neach output step, which demonstrates that tokens exhibiting specific attention\ncharacteristics are more likely to induce the model to take things out of\ncontext; these tokens directly attend to the hidden states tied with\nprediction, without substantial integration of non-local information. Building\nupon these insights, we propose a Few-shot Attention Intervention method (FAI)\nthat dynamically analyzes the attention patterns of demonstrations to\naccurately identify these tokens and subsequently make targeted adjustments to\nthe attention weights to effectively suppress their distracting effect on LLMs.\nComprehensive experiments across multiple benchmarks demonstrate consistent\nimprovements over baseline methods, with a remarkable 5.91% improvement on the\nAQuA dataset, further highlighting the effectiveness of FAI.\n","authors":["Shaotian Yan","Chen Shen","Wenxiao Wang","Liang Xie","Junjie Liu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2503.11154v1.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2503.11144v1","updated":"2025-03-14T07:22:07Z","published":"2025-03-14T07:22:07Z","title":"MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling","summary":"  Large-scale pre-training of deep models, followed by fine-tuning them, has\nbecome the cornerstone of natural language processing (NLP). The prevalence of\ndata coupled with computational resources has led to large models with a\nconsiderable number of parameters. While the massive size of these models has\nled to remarkable success in many NLP tasks, a detriment is the expense\nrequired to retrain all the base model's parameters for the adaptation to each\ntask or domain. Parameter Efficient Fine-Tuning (PEFT) provides an effective\nsolution for this challenge by minimizing the number of parameters required to\nbe fine-tuned while maintaining the quality of the model. While existing\nmethods have achieved impressive results, they mainly focus on adapting a\nsubset of parameters, weight reparameterization, and prompt engineering. In\nthis paper, we study layers as extractors of different types of linguistic\ninformation that are valuable when used in conjunction. We then propose the\nMixture of Layer Experts (MoLEx), a novel sparse mixture of experts (SMoE)\nwhose experts are layers in the pre-trained model. It performs a conditional\ncomputation of a mixture of layers during fine-tuning to provide the model with\nmore structural knowledge about the data. By providing an avenue for\ninformation exchange between layers, MoLEx enables the model to make a more\nwell-informed prediction for the downstream task, leading to better fine-tuning\nresults with the same number of effective parameters. As experts can be\nprocessed in parallel, MoLEx introduces minimal additional computational\noverhead. We empirically corroborate the advantages of MoLEx when combined with\npopular PEFT baseline methods on a variety of downstream fine-tuning tasks,\nincluding the popular GLUE benchmark as well as the End-to-End Challenge (E2E).\nThe code is publicly available at https://github.com/rachtsy/molex.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.11144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11132v1","updated":"2025-03-14T06:49:37Z","published":"2025-03-14T06:49:37Z","title":"X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression","summary":"  Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.\n","authors":["Guihong Li","Mehdi Rezagholizadeh","Mingyu Yang","Vikram Appia","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2503.11132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11118v1","updated":"2025-03-14T06:29:51Z","published":"2025-03-14T06:29:51Z","title":"UMB@PerAnsSumm 2025: Enhancing Perspective-Aware Summarization with\n  Prompt Optimization and Supervised Fine-Tuning","summary":"  We present our approach to the PerAnsSumm Shared Task, which involves\nperspective span identification and perspective-aware summarization in\ncommunity question-answering (CQA) threads. For span identification, we adopt\nensemble learning that integrates three transformer models through averaging to\nexploit individual model strengths, achieving an 82.91% F1-score on test data.\nFor summarization, we design a suite of Chain-of-Thought (CoT) prompting\nstrategies that incorporate keyphrases and guide information to structure\nsummary generation into manageable steps. To further enhance summary quality,\nwe apply prompt optimization using the DSPy framework and supervised\nfine-tuning (SFT) on Llama-3 to adapt the model to domain-specific data.\nExperimental results on validation and test sets show that structured prompts\nwith keyphrases and guidance improve summaries aligned with references, while\nthe combination of prompt optimization and fine-tuning together yields\nsignificant improvement in both relevance and factuality evaluation metrics.\n","authors":["Kristin Qi","Youxiang Zhu","Xiaohui Liang"],"pdf_url":"https://arxiv.org/pdf/2503.11118v1.pdf","comment":"CL4HEALTH NAACL: Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics"},{"id":"http://arxiv.org/abs/2503.11116v1","updated":"2025-03-14T06:28:22Z","published":"2025-03-14T06:28:22Z","title":"Trust in Disinformation Narratives: a Trust in the News Experiment","summary":"  Understanding why people trust or distrust one another, institutions, or\ninformation is a complex task that has led scholars from various fields of\nstudy to employ diverse epistemological and methodological approaches. Despite\nthe challenges, it is generally agreed that the antecedents of trust (and\ndistrust) encompass a multitude of emotional and cognitive factors, including a\ngeneral disposition to trust and an assessment of trustworthiness factors. In\nan era marked by increasing political polarization, cultural backlash,\nwidespread disinformation and fake news, and the use of AI software to produce\nnews content, the need to study trust in the news has gained significant\ntraction. This study presents the findings of a trust in the news experiment\ndesigned in collaboration with Spanish and UK journalists, fact-checkers, and\nthe CardiffNLP Natural Language Processing research group. The purpose of this\nexperiment, conducted in June 2023, was to examine the extent to which people\ntrust a set of fake news articles based on previously identified disinformation\nnarratives related to gender, climate change, and COVID-19. The online\nexperiment participants (801 in Spain and 800 in the UK) were asked to read\nthree fake news items and rate their level of trust on a scale from 1 (not\ntrue) to 8 (true). The pieces used a combination of factors, including stance\n(favourable, neutral, or against the narrative), presence of toxic expressions,\nclickbait titles, and sources of information to test which elements influenced\npeople's responses the most. Half of the pieces were produced by humans and the\nother half by ChatGPT. The results show that the topic of news articles,\nstance, people's age, gender, and political ideologies significantly affected\ntheir levels of trust in the news, while the authorship (humans or ChatGPT)\ndoes not have a significant impact.\n","authors":["Hanbyul Song","Miguel F. Santos Silva","Jaume Suau","Luis Espinosa-Anke"],"pdf_url":"https://arxiv.org/pdf/2503.11116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00138v3","updated":"2025-03-14T06:03:20Z","published":"2024-08-29T17:58:38Z","title":"PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in\n  Action","summary":"  As language models (LMs) are widely utilized in personalized communication\nscenarios (e.g., sending emails, writing social media posts) and endowed with a\ncertain level of agency, ensuring they act in accordance with the contextual\nprivacy norms becomes increasingly critical. However, quantifying the privacy\nnorm awareness of LMs and the emerging privacy risk in LM-mediated\ncommunication is challenging due to (1) the contextual and long-tailed nature\nof privacy-sensitive cases, and (2) the lack of evaluation approaches that\ncapture realistic application scenarios. To address these challenges, we\npropose PrivacyLens, a novel framework designed to extend privacy-sensitive\nseeds into expressive vignettes and further into agent trajectories, enabling\nmulti-level evaluation of privacy leakage in LM agents' actions. We instantiate\nPrivacyLens with a collection of privacy norms grounded in privacy literature\nand crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM\nperformance in answering probing questions and their actual behavior when\nexecuting user instructions in an agent setup. State-of-the-art LMs, like GPT-4\nand Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even\nwhen prompted with privacy-enhancing instructions. We also demonstrate the\ndynamic nature of PrivacyLens by extending each seed into multiple trajectories\nto red-team LM privacy leakage risk. Dataset and code are available at\nhttps://github.com/SALT-NLP/PrivacyLens.\n","authors":["Yijia Shao","Tianshi Li","Weiyan Shi","Yanchen Liu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.00138v3.pdf","comment":"NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2503.11108v1","updated":"2025-03-14T06:01:42Z","published":"2025-03-14T06:01:42Z","title":"Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers","summary":"  The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.\n","authors":["Yifang Chen","Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2503.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09501v2","updated":"2025-03-14T05:33:47Z","published":"2025-03-12T16:05:31Z","title":"ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning","summary":"  Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.\n","authors":["Ziyu Wan","Yunxiang Li","Yan Song","Hanjing Wang","Linyi Yang","Mark Schmidt","Jun Wang","Weinan Zhang","Shuyue Hu","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2503.09501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11084v1","updated":"2025-03-14T04:51:36Z","published":"2025-03-14T04:51:36Z","title":"Semantic and Contextual Modeling for Malicious Comment Detection with\n  BERT-BiLSTM","summary":"  This study aims to develop an efficient and accurate model for detecting\nmalicious comments, addressing the increasingly severe issue of false and\nharmful content on social media platforms. We propose a deep learning model\nthat combines BERT and BiLSTM. The BERT model, through pre-training, captures\ndeep semantic features of text, while the BiLSTM network excels at processing\nsequential data and can further model the contextual dependencies of text.\nExperimental results on the Jigsaw Unintended Bias in Toxicity Classification\ndataset demonstrate that the BERT+BiLSTM model achieves superior performance in\nmalicious comment detection tasks, with a precision of 0.94, recall of 0.93,\nand accuracy of 0.94. This surpasses other models, including standalone BERT,\nTextCNN, TextRNN, and traditional machine learning algorithms using TF-IDF\nfeatures. These results confirm the superiority of the BERT+BiLSTM model in\nhandling imbalanced data and capturing deep semantic features of malicious\ncomments, providing an effective technical means for social media content\nmoderation and online environment purification.\n","authors":["Zhou Fang","Hanlu Zhang","Jacky He","Zhen Qi","Hongye Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.11084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11080v1","updated":"2025-03-14T04:45:46Z","published":"2025-03-14T04:45:46Z","title":"Joint Training And Decoding for Multilingual End-to-End Simultaneous\n  Speech Translation","summary":"  Recent studies on end-to-end speech translation(ST) have facilitated the\nexploration of multilingual end-to-end ST and end-to-end simultaneous ST. In\nthis paper, we investigate end-to-end simultaneous speech translation in a\none-to-many multilingual setting which is closer to applications in real\nscenarios. We explore a separate decoder architecture and a unified\narchitecture for joint synchronous training in this scenario. To further\nexplore knowledge transfer across languages, we propose an asynchronous\ntraining strategy on the proposed unified decoder architecture. A multi-way\naligned multilingual end-to-end ST dataset was curated as a benchmark testbed\nto evaluate our methods. Experimental results demonstrate the effectiveness of\nour models on the collected dataset. Our codes and data are available at:\nhttps://github.com/XiaoMi/TED-MMST.\n","authors":["Wuwei Huang","Renren Jin","Wen Zhang","Jian Luan","Bin Wang","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.11080v1.pdf","comment":"ICASSP 2023"},{"id":"http://arxiv.org/abs/2406.12718v3","updated":"2025-03-14T04:38:44Z","published":"2024-06-18T15:38:41Z","title":"Mitigating Object Hallucinations in Large Vision-Language Models with\n  Assembly of Global and Local Attention","summary":"  Despite great success across various multimodal tasks, Large Vision-Language\nModels (LVLMs) often encounter object hallucinations with generated textual\nresponses being inconsistent with the actual objects in images. We examine\ndifferent LVLMs and pinpoint that one root cause of object hallucinations lies\nwith deficient attention on discriminative image features. Specifically, LVLMs\noften predominantly attend to prompt-irrelevant global features instead of\nprompt-relevant local features, undermining their visual grounding capacity and\nleading to object hallucinations. We propose Assembly of Global and Local\nAttention (AGLA), a training-free and plug-and-play approach that mitigates\nhallucinations by assembling global features for response generation and local\nfeatures for visual discrimination simultaneously. Specifically, we introduce\nan image-prompt matching scheme that captures prompt-relevant local features\nfrom images, leading to an augmented view of the input image where\nprompt-relevant content is highlighted while irrelevant distractions are\nsuppressed. Hallucinations can thus be mitigated with a calibrated logit\ndistribution that is from generative global features of the original image and\ndiscriminative local features of the augmented image. Extensive experiments\nshow the superiority of AGLA in LVLM hallucination mitigation, demonstrating\nits wide applicability across both discriminative and generative tasks. Our\ncode is available at https://github.com/Lackel/AGLA.\n","authors":["Wenbin An","Feng Tian","Sicong Leng","Jiahao Nie","Haonan Lin","QianYing Wang","Ping Chen","Xiaoqin Zhang","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2406.12718v3.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11074v1","updated":"2025-03-14T04:34:31Z","published":"2025-03-14T04:34:31Z","title":"Large Reasoning Models in Agent Scenarios: Exploring the Necessity of\n  Reasoning Capabilities","summary":"  The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward\nadvanced computational reasoning. Yet, this progress disrupts traditional agent\nframeworks, traditionally anchored by execution-oriented Large Language Models\n(LLMs). To explore this transformation, we propose the LaRMA framework,\nencompassing nine tasks across Tool Usage, Plan Design, and Problem Solving,\nassessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs\n(e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass\nLLMs in reasoning-intensive tasks like Plan Design, leveraging iterative\nreflection for superior outcomes; LLMs excel in execution-driven tasks such as\nTool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing\nLLMs as actors with LRMs as reflectors, optimize agent performance by blending\nexecution speed with reasoning depth; and LRMs' enhanced reasoning incurs\nhigher computational costs, prolonged processing, and behavioral challenges,\nincluding overthinking and fact-ignoring tendencies. This study fosters deeper\ninquiry into LRMs' balance of deep thinking and overthinking, laying a critical\nfoundation for future agent design advancements.\n","authors":["Xueyang Zhou","Guiyao Tie","Guowen Zhang","Weidong Wang","Zhigang Zuo","Di Wu","Duanfeng Chu","Pan Zhou","Lichao Sun","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2503.11074v1.pdf","comment":"71 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2405.03279v4","updated":"2025-03-14T03:56:58Z","published":"2024-05-06T08:52:11Z","title":"Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous\n  Prompt Learning","summary":"  Model editing aims to correct outdated or erroneous knowledge in large\nlanguage models (LLMs) without the need for costly retraining. Lifelong model\nediting is the most challenging task that caters to the continuous editing\nrequirements of LLMs. Prior works primarily focus on single or batch editing;\nnevertheless, these methods fall short in lifelong editing scenarios due to\ncatastrophic knowledge forgetting and the degradation of model performance.\nAlthough retrieval-based methods alleviate these issues, they are impeded by\nslow and cumbersome processes of integrating the retrieved knowledge into the\nmodel. In this work, we introduce RECIPE, a RetriEval-augmented ContInuous\nPrompt lEarning method, to boost editing efficacy and inference efficiency in\nlifelong learning. RECIPE first converts knowledge statements into short and\ninformative continuous prompts, prefixed to the LLM's input query embedding, to\nefficiently refine the response grounded on the knowledge. It further\nintegrates the Knowledge Sentinel (KS) that acts as an intermediary to\ncalculate a dynamic threshold, determining whether the retrieval repository\ncontains relevant knowledge. Our retriever and prompt encoder are jointly\ntrained to achieve editing properties, i.e., reliability, generality, and\nlocality. In our experiments, RECIPE is assessed extensively across multiple\nLLMs and editing datasets, where it achieves superior editing performance.\nRECIPE also demonstrates its capability to maintain the overall performance of\nLLMs alongside showcasing fast editing and inference speed.\n","authors":["Qizhou Chen","Taolin Zhang","Xiaofeng He","Dongyang Li","Chengyu Wang","Longtao Huang","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2405.03279v4.pdf","comment":"EMNLP 2024 main"},{"id":"http://arxiv.org/abs/2411.15432v2","updated":"2025-03-14T03:47:02Z","published":"2024-11-23T03:19:40Z","title":"Lifelong Knowledge Editing for Vision Language Models with Low-Rank\n  Mixture-of-Experts","summary":"  Model editing aims to correct inaccurate knowledge, update outdated\ninformation, and incorporate new data into Large Language Models (LLMs) without\nthe need for retraining. This task poses challenges in lifelong scenarios where\nedits must be continuously applied for real-world applications. While some\neditors demonstrate strong robustness for lifelong editing in pure LLMs, Vision\nLLMs (VLLMs), which incorporate an additional vision modality, are not directly\nadaptable to existing LLM editors. In this paper, we propose LiveEdit, a\nLIfelong Vision language modEl Edit to bridge the gap between lifelong LLM\nediting and VLLMs. We begin by training an editing expert generator to\nindependently produce low-rank experts for each editing instance, with the goal\nof correcting the relevant responses of the VLLM. A hard filtering mechanism is\ndeveloped to utilize visual semantic knowledge, thereby coarsely eliminating\nvisually irrelevant experts for input queries during the inference stage of the\npost-edited model. Finally, to integrate visually relevant experts, we\nintroduce a soft routing mechanism based on textual semantic relevance to\nachieve multi-expert fusion. For evaluation, we establish a benchmark for\nlifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers\nsignificant advantages in lifelong VLLM editing scenarios. Further experiments\nvalidate the rationality and effectiveness of each module design in LiveEdit.\n","authors":["Qizhou Chen","Chengyu Wang","Dakan Wang","Taolin Zhang","Wangyue Li","Xiaofeng He"],"pdf_url":"https://arxiv.org/pdf/2411.15432v2.pdf","comment":"CVPR 2025 Accepted"},{"id":"http://arxiv.org/abs/2409.03277v3","updated":"2025-03-14T03:19:00Z","published":"2024-09-05T06:41:02Z","title":"ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart\n  Understanding","summary":"  Automatic chart understanding is crucial for content comprehension and\ndocument parsing. Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable capabilities in chart understanding through domain-specific\nalignment and fine-tuning. However, current MLLMs still struggle to provide\nfaithful data and reliable analysis only based on charts. To address it, we\npropose ChartMoE, which employs the Mixture of Expert (MoE) architecture to\nreplace the traditional linear projector to bridge the modality gap.\nSpecifically, we train several linear connectors through distinct alignment\ntasks, which are utilized as the foundational initialization parameters for\ndifferent experts. Additionally, we introduce ChartMoE-Align, a dataset with\nnearly 1 million chart-table-JSON-code quadruples to conduct three alignment\ntasks (chart-table/JSON/code). Combined with the vanilla connector, we\ninitialize different experts diversely and adopt high-quality knowledge\nlearning to further refine the MoE connector and LLM parameters. Extensive\nexperiments demonstrate the effectiveness of the MoE connector and our\ninitialization strategy, e.g., ChartMoE improves the accuracy of the previous\nstate-of-the-art from 80.48\\% to 84.64\\% on the ChartQA benchmark.\n","authors":["Zhengzhuo Xu","Bowen Qu","Yiyan Qi","Sinan Du","Chengjin Xu","Chun Yuan","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2409.03277v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20592v2","updated":"2025-03-14T02:48:03Z","published":"2025-02-27T23:34:47Z","title":"Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document\n  Processing","summary":"  Recent advances in test-time scaling have shown promising results in\nimproving Large Language Models (LLMs) performance through strategic\ncomputation allocation during inference. While this approach has demonstrated\nstrong performance improvements in logical and mathematical reasoning tasks,\nits application to natural language generation (NLG), especially summarization,\nhas yet to be explored. Multi-Document Summarization (MDS) is a challenging\ntask that focuses on extracting and synthesizing useful information from\nmultiple lengthy documents. Unlike reasoning tasks, MDS requires a more nuanced\napproach to prompt design and ensemble, as there is no \"best\" prompt to satisfy\ndiverse summarization requirements. To address this, we propose a novel\nframework that leverages inference-time scaling for this task. Precisely, we\ntake prompt ensemble approach by leveraging various prompt to first generate\ncandidate summaries and then ensemble them with an aggregator to produce a\nrefined summary. We also introduce two new evaluation metrics:\nConsistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (ACU) score,\nto enhance LLM's contextual understanding while mitigating its positional bias.\nExtensive experiments demonstrate the effectiveness of our approach in\nimproving summary quality while identifying and analyzing the scaling\nboundaries in summarization tasks.\n","authors":["Juntai Cao","Xiang Zhang","Raymond Li","Chuyuan Li","Shafiq Joty","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.20592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10997v1","updated":"2025-03-14T01:45:38Z","published":"2025-03-14T01:45:38Z","title":"RONA: Pragmatically Diverse Image Captioning with Coherence Relations","summary":"  Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally\ngenerate diverse image captions by employing syntactic and semantic variations\nto describe image components. However, human-written captions prioritize\nconveying a central message alongside visual descriptions using pragmatic cues.\nTo enhance pragmatic diversity, it is essential to explore alternative ways of\ncommunicating these messages in conjunction with visual content. To address\nthis challenge, we propose RONA, a novel prompting strategy for Multi-modal\nLarge Language Models (MLLM) that leverages Coherence Relations as an axis for\nvariation. We demonstrate that RONA generates captions with better overall\ndiversity and ground-truth alignment, compared to MLLM baselines across\nmultiple domains. Our code is available at: https://github.com/aashish2000/RONA\n","authors":["Aashish Anantha Ramakrishnan","Aadarsh Anantha Ramakrishnan","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2503.10997v1.pdf","comment":"To appear in the NAACL Fourth Workshop on Intelligent and Interactive\n  Writing Assistants (In2Writing), Albuquerque, New Mexico, May 2025,\n  https://in2writing.glitch.me"},{"id":"http://arxiv.org/abs/2503.10996v1","updated":"2025-03-14T01:45:00Z","published":"2025-03-14T01:45:00Z","title":"Taming Knowledge Conflicts in Language Models","summary":"  Language Models (LMs) often encounter knowledge conflicts when parametric\nmemory contradicts contextual knowledge. Previous works attribute this conflict\nto the interplay between \"memory heads\" and \"context heads\", attention heads\nassumed to promote either memory or context exclusively. In this study, we go\nbeyond this fundamental assumption by uncovering a critical phenomenon we term\nthe \"superposition of contextual information and parametric memory\", where\nhighly influential attention heads could simultaneously contribute to both\nmemory and context. Building upon this insight, we propose Just Run Twice\n(JUICE), a test-time attention intervention method that steers LMs toward\neither parametric beliefs or contextual knowledge without requiring\nfine-tuning. JUICE identifies a set of reliable attention heads and leverages a\ndual-run approach to mitigate the superposition effects. Extensive experiments\nacross 11 datasets and 6 model architectures demonstrate that JUICE sets the\nnew state-of-the-art performance and robust generalization, achieving\nsignificant and consistent improvement across different domains under various\nconflict types. Finally, we theoretically analyze knowledge conflict and the\nsuperposition of contextual information and parametric memory in attention\nheads, which further elucidates the effectiveness of JUICE in these settings.\n","authors":["Gaotang Li","Yuzhong Chen","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2503.10996v1.pdf","comment":"30 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.09837v2","updated":"2025-03-14T01:44:17Z","published":"2025-03-12T20:58:16Z","title":"On the Limitations of Vision-Language Models in Understanding Image\n  Transforms","summary":"  Vision Language Models (VLMs) have demonstrated significant potential in\nvarious downstream tasks, including Image/Video Generation, Visual Question\nAnswering, Multimodal Chatbots, and Video Understanding. However, these models\noften struggle with basic image transformations. This paper investigates the\nimage-level understanding of VLMs, specifically CLIP by OpenAI and SigLIP by\nGoogle. Our findings reveal that these models lack comprehension of multiple\nimage-level augmentations. To facilitate this study, we created an augmented\nversion of the Flickr8k dataset, pairing each image with a detailed description\nof the applied transformation. We further explore how this deficiency impacts\ndownstream tasks, particularly in image editing, and evaluate the performance\nof state-of-the-art Image2Image models on simple transformations.\n","authors":["Ahmad Mustafa Anis","Hasnain Ali","Saquib Sarfraz"],"pdf_url":"https://arxiv.org/pdf/2503.09837v2.pdf","comment":"8 pages, 15 images"},{"id":"http://arxiv.org/abs/2503.10995v1","updated":"2025-03-14T01:41:16Z","published":"2025-03-14T01:41:16Z","title":"TigerLLM -- A Family of Bangla Large Language Models","summary":"  The development of Large Language Models (LLMs) remains heavily skewed\ntowards English and a few other high-resource languages. This linguistic\ndisparity is particularly evident for Bangla - the 5th most spoken language. A\nfew initiatives attempted to create open-source Bangla LLMs with performance\nstill behind high-resource languages and limited reproducibility. To address\nthis gap, we introduce TigerLLM - a family of Bangla LLMs. Our results\ndemonstrate that these models surpass all open-source alternatives and also\noutperform larger proprietary models like GPT3.5 across standard benchmarks,\nestablishing TigerLLM as the new baseline for future Bangla language modeling.\n","authors":["Nishat Raihan","Marcos Zampieri"],"pdf_url":"https://arxiv.org/pdf/2503.10995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10061v2","updated":"2025-03-14T01:39:39Z","published":"2025-03-13T05:21:22Z","title":"Compute Optimal Scaling of Skills: Knowledge vs Reasoning","summary":"  Scaling laws are a critical component of the LLM development pipeline, most\nfamously as a way to forecast training decisions such as 'compute-optimally'\ntrading-off parameter count and dataset size, alongside a more recent growing\nlist of other crucial decisions. In this work, we ask whether compute-optimal\nscaling behaviour can be skill-dependent. In particular, we examine knowledge\nand reasoning-based skills such as knowledge-based QA and code generation, and\nwe answer this question in the affirmative: scaling laws are skill-dependent.\nNext, to understand whether skill-dependent scaling is an artefact of the\npretraining datamix, we conduct an extensive ablation of different datamixes\nand find that, also when correcting for datamix differences, knowledge and code\nexhibit fundamental differences in scaling behaviour. We conclude with an\nanalysis of how our findings relate to standard compute-optimal scaling using a\nvalidation set, and find that a misspecified validation set can impact\ncompute-optimal parameter count by nearly 50%, depending on its skill\ncomposition.\n","authors":["Nicholas Roberts","Niladri Chatterji","Sharan Narang","Mike Lewis","Dieuwke Hupkes"],"pdf_url":"https://arxiv.org/pdf/2503.10061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12988v2","updated":"2025-03-14T01:34:01Z","published":"2024-01-16T13:54:43Z","title":"Few-Shot Learning for Mental Disorder Detection: A Continuous\n  Multi-Prompt Engineering Approach with Medical Knowledge Injection","summary":"  This study harnesses state-of-the-art AI technology for detecting mental\ndisorders through user-generated textual content. Existing studies typically\nrely on fully supervised machine learning, which presents challenges such as\nthe labor-intensive manual process of annotating extensive training data for\neach research problem and the need to design specialized deep learning\narchitectures for each task. We propose a novel method to address these\nchallenges by leveraging large language models and continuous multi-prompt\nengineering, which offers two key advantages: (1) developing personalized\nprompts that capture each user's unique characteristics and (2) integrating\nstructured medical knowledge into prompts to provide context for disease\ndetection and facilitate predictive modeling. We evaluate our method using\nthree widely prevalent mental disorders as research cases. Our method\nsignificantly outperforms existing methods, including feature engineering,\narchitecture engineering, and discrete prompt engineering. Meanwhile, our\napproach demonstrates success in few-shot learning, i.e., requiring only a\nminimal number of training examples. Moreover, our method can be generalized to\nother rare mental disorder detection tasks with few positive labels. In\naddition to its technical contributions, our method has the potential to\nenhance the well-being of individuals with mental disorders and offer a\ncost-effective, accessible alternative for stakeholders beyond traditional\nmental disorder screening methods.\n","authors":["Haoxin Liu","Wenli Zhang","Jiaheng Xie","Buomsoo Kim","Zhu Zhang","Yidong Chai","Sudha Ram"],"pdf_url":"https://arxiv.org/pdf/2401.12988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09016v2","updated":"2025-03-14T01:26:57Z","published":"2024-10-11T17:30:28Z","title":"Parameter-Efficient Fine-Tuning of State Space Models","summary":"  Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have become\npowerful tools for language modeling, offering high performance and linear\nscalability with sequence length. However, the application of\nparameter-efficient fine-tuning (PEFT) methods to SSM-based models remains\nunderexplored. We start by investigating two fundamental questions on existing\nPEFT methods: (i) How do they perform on SSM-based models? (ii) Which\nparameters should they target for optimal results? Our analysis shows that LoRA\nand its variants consistently outperform all other PEFT methods. While LoRA is\neffective for linear projection matrices, it fails on SSM modules-yet still\noutperforms other methods applicable to SSMs, indicating their limitations.\nThis underscores the need for a specialized SSM tuning approach. To address\nthis, we propose Sparse Dimension Tuning (SDT), a PEFT method tailored for SSM\nmodules. Combining SDT for SSMs with LoRA for linear projection matrices, we\nachieve state-of-the-art performance across extensive experiments.\n","authors":["Kevin Galim","Wonjun Kang","Yuchen Zeng","Hyung Il Koo","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2410.09016v2.pdf","comment":"Code is available at https://github.com/furiosa-ai/ssm-peft"},{"id":"http://arxiv.org/abs/2503.10968v1","updated":"2025-03-14T00:26:00Z","published":"2025-03-14T00:26:00Z","title":"Combinatorial Optimization for All: Using LLMs to Aid Non-Experts in\n  Improving Optimization Algorithms","summary":"  Large Language Models (LLMs) have shown notable potential in code generation\nfor optimization algorithms, unlocking exciting new opportunities. This paper\nexamines how LLMs, rather than creating algorithms from scratch, can improve\nexisting ones without the need for specialized expertise. To explore this\npotential, we selected 10 baseline optimization algorithms from various domains\n(metaheuristics, reinforcement learning, deterministic, and exact methods) to\nsolve the classic Travelling Salesman Problem. The results show that our simple\nmethodology often results in LLM-generated algorithm variants that improve over\nthe baseline algorithms in terms of solution quality, reduction in\ncomputational time, and simplification of code complexity, all without\nrequiring specialized optimization knowledge or advanced algorithmic\nimplementation skills.\n","authors":["Camilo Chacón Sartori","Christian Blum"],"pdf_url":"https://arxiv.org/pdf/2503.10968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10965v1","updated":"2025-03-14T00:21:15Z","published":"2025-03-14T00:21:15Z","title":"Auditing language models for hidden objectives","summary":"  We study the feasibility of conducting alignment audits: investigations into\nwhether models have undesired objectives. As a testbed, we train a language\nmodel with a hidden objective. Our training pipeline first teaches the model\nabout exploitable errors in RLHF reward models (RMs), then trains the model to\nexploit some of these errors. We verify via out-of-distribution evaluations\nthat the model generalizes to exhibit whatever behaviors it believes RMs rate\nhighly, including ones not reinforced during training. We leverage this model\nto study alignment audits in two ways. First, we conduct a blind auditing game\nwhere four teams, unaware of the model's hidden objective or training,\ninvestigate it for concerning behaviors and their causes. Three teams\nsuccessfully uncovered the model's hidden objective using techniques including\ninterpretability with sparse autoencoders (SAEs), behavioral attacks, and\ntraining data analysis. Second, we conduct an unblinded follow-up study of\neight techniques for auditing the model, analyzing their strengths and\nlimitations. Overall, our work provides a concrete example of using alignment\naudits to discover a model's hidden objective and proposes a methodology for\npracticing and validating progress in alignment auditing.\n","authors":["Samuel Marks","Johannes Treutlein","Trenton Bricken","Jack Lindsey","Jonathan Marcus","Siddharth Mishra-Sharma","Daniel Ziegler","Emmanuel Ameisen","Joshua Batson","Tim Belonax","Samuel R. Bowman","Shan Carter","Brian Chen","Hoagy Cunningham","Carson Denison","Florian Dietz","Satvik Golechha","Akbir Khan","Jan Kirchner","Jan Leike","Austin Meek","Kei Nishimura-Gasparian","Euan Ong","Christopher Olah","Adam Pearce","Fabien Roger","Jeanne Salle","Andy Shih","Meg Tong","Drake Thomas","Kelley Rivoire","Adam Jermyn","Monte MacDiarmid","Tom Henighan","Evan Hubinger"],"pdf_url":"https://arxiv.org/pdf/2503.10965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05786v2","updated":"2025-03-14T00:18:36Z","published":"2025-02-27T07:04:19Z","title":"FedMentalCare: Towards Privacy-Preserving Fine-Tuned LLMs to Analyze\n  Mental Health Status Using Federated Learning Framework","summary":"  With the increasing prevalence of mental health conditions worldwide,\nAI-powered chatbots and conversational agents have emerged as accessible tools\nto support mental health. However, deploying Large Language Models (LLMs) in\nmental healthcare applications raises significant privacy concerns, especially\nregarding regulations like HIPAA and GDPR. In this work, we propose\nFedMentalCare, a privacy-preserving framework that leverages Federated Learning\n(FL) combined with Low-Rank Adaptation (LoRA) to fine-tune LLMs for mental\nhealth analysis. We investigate the performance impact of varying client data\nvolumes and model architectures (e.g., MobileBERT and MiniLM) in FL\nenvironments. Our framework demonstrates a scalable, privacy-aware approach for\ndeploying LLMs in real-world mental healthcare scenarios, addressing data\nsecurity and computational efficiency challenges.\n","authors":["S M Sarwar"],"pdf_url":"https://arxiv.org/pdf/2503.05786v2.pdf","comment":"9 pages, 3 figures, 2 tables and 2 algorithms"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.11652v1","updated":"2025-03-14T17:59:54Z","published":"2025-03-14T17:59:54Z","title":"Bring Your Rear Cameras for Egocentric 3D Human Pose Estimation","summary":"  Egocentric 3D human pose estimation has been actively studied using cameras\ninstalled in front of a head-mounted device (HMD). While frontal placement is\nthe optimal and the only option for some tasks, such as hand tracking, it\nremains unclear if the same holds for full-body tracking due to self-occlusion\nand limited field-of-view coverage. Notably, even the state-of-the-art methods\noften fail to estimate accurate 3D poses in many scenarios, such as when HMD\nusers tilt their heads upward (a common motion in human activities). A key\nlimitation of existing HMD designs is their neglect of the back of the body,\ndespite its potential to provide crucial 3D reconstruction cues. Hence, this\npaper investigates the usefulness of rear cameras in the HMD design for\nfull-body tracking. We also show that simply adding rear views to the frontal\ninputs is not optimal for existing methods due to their dependence on\nindividual 2D joint detectors without effective multi-view integration. To\naddress this issue, we propose a new transformer-based method that refines 2D\njoint heatmap estimation with multi-view information and heatmap uncertainty,\nthereby improving 3D pose tracking. Moreover, we introduce two new large-scale\ndatasets, Ego4View-Syn and Ego4View-RW, for a rear-view evaluation. Our\nexperiments show that the new camera configurations with back views provide\nsuperior support for 3D pose tracking compared to only frontal placements. The\nproposed method achieves significant improvement over the current state of the\nart (>10% on MPJPE). We will release the source code, trained models, and new\ndatasets on our project page https://4dqv.mpi-inf.mpg.de/EgoRear/.\n","authors":["Hiroyasu Akada","Jian Wang","Vladislav Golyanik","Christian Theobalt"],"pdf_url":"https://arxiv.org/pdf/2503.11652v1.pdf","comment":"Project page: https://4dqv.mpi-inf.mpg.de/EgoRear/"},{"id":"http://arxiv.org/abs/2503.11651v1","updated":"2025-03-14T17:59:47Z","published":"2025-03-14T17:59:47Z","title":"VGGT: Visual Geometry Grounded Transformer","summary":"  We present VGGT, a feed-forward neural network that directly infers all key\n3D attributes of a scene, including camera parameters, point maps, depth maps,\nand 3D point tracks, from one, a few, or hundreds of its views. This approach\nis a step forward in 3D computer vision, where models have typically been\nconstrained to and specialized for single tasks. It is also simple and\nefficient, reconstructing images in under one second, and still outperforming\nalternatives that require post-processing with visual geometry optimization\ntechniques. The network achieves state-of-the-art results in multiple 3D tasks,\nincluding camera parameter estimation, multi-view depth estimation, dense point\ncloud reconstruction, and 3D point tracking. We also show that using pretrained\nVGGT as a feature backbone significantly enhances downstream tasks, such as\nnon-rigid point tracking and feed-forward novel view synthesis. Code and models\nare publicly available at https://github.com/facebookresearch/vggt.\n","authors":["Jianyuan Wang","Minghao Chen","Nikita Karaev","Andrea Vedaldi","Christian Rupprecht","David Novotny"],"pdf_url":"https://arxiv.org/pdf/2503.11651v1.pdf","comment":"CVPR 2025, Project Page: https://vgg-t.github.io/"},{"id":"http://arxiv.org/abs/2503.11650v1","updated":"2025-03-14T17:59:41Z","published":"2025-03-14T17:59:41Z","title":"Centaur: Robust End-to-End Autonomous Driving with Test-Time Training","summary":"  How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels.\n","authors":["Chonghao Sima","Kashyap Chitta","Zhiding Yu","Shiyi Lan","Ping Luo","Andreas Geiger","Hongyang Li","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2503.11650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11647v1","updated":"2025-03-14T17:59:31Z","published":"2025-03-14T17:59:31Z","title":"ReCamMaster: Camera-Controlled Generative Rendering from A Single Video","summary":"  Camera control has been actively studied in text or image conditioned video\ngeneration tasks. However, altering camera trajectories of a given video\nremains under-explored, despite its importance in the field of video creation.\nIt is non-trivial due to the extra constraints of maintaining multiple-frame\nappearance and dynamic synchronization. To address this, we present\nReCamMaster, a camera-controlled generative video re-rendering framework that\nreproduces the dynamic scene of an input video at novel camera trajectories.\nThe core innovation lies in harnessing the generative capabilities of\npre-trained text-to-video models through a simple yet powerful video\nconditioning mechanism -- its capability often overlooked in current research.\nTo overcome the scarcity of qualified training data, we construct a\ncomprehensive multi-camera synchronized video dataset using Unreal Engine 5,\nwhich is carefully curated to follow real-world filming characteristics,\ncovering diverse scenes and camera movements. It helps the model generalize to\nin-the-wild videos. Lastly, we further improve the robustness to diverse inputs\nthrough a meticulously designed training strategy. Extensive experiments tell\nthat our method substantially outperforms existing state-of-the-art approaches\nand strong baselines. Our method also finds promising applications in video\nstabilization, super-resolution, and outpainting. Project page:\nhttps://jianhongbai.github.io/ReCamMaster/\n","authors":["Jianhong Bai","Menghan Xia","Xiao Fu","Xintao Wang","Lianrui Mu","Jinwen Cao","Zuozhu Liu","Haoji Hu","Xiang Bai","Pengfei Wan","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11647v1.pdf","comment":"Project page: https://jianhongbai.github.io/ReCamMaster/"},{"id":"http://arxiv.org/abs/2411.17686v3","updated":"2025-03-14T17:56:09Z","published":"2024-11-26T18:53:51Z","title":"Filter, Correlate, Compress: Training-Free Token Reduction for MLLM\n  Acceleration","summary":"  The quadratic complexity of Multimodal Large Language Models (MLLMs) with\nrespect to sequence length poses significant computational and memory\nchallenges, hindering their real-world deployment. While existing training-free\ntoken reduction methods aim to address these inefficiencies, how to precisely\nidentify redundant visual tokens and recover the essential information from the\ndiscarded tokens remain unclear. In this paper, we propose a\n''filter-correlate-compress'' framework that decomposes the token reduction\ninto three stages: filtering redundant tokens, correlating discarded\ninformation to preserved tokens, and compressing tokens to minimize redundancy.\nFollowing the framework, we propose a solution FiCoCo to identify limitations\nin single redundancy assessment, propose adaptive strategies to retain critical\ninformation from discarded tokens, and mitigate semantic dilution during token\nfusion. Two specialized variants, FiCoCo-V (for vision encoders) and FiCoCo-L\n(for LLM decoders), further optimize efficiency across MLLM architectures.\nExtensive experiments demonstrate that FiCoCo achieves up to 5.7x/14.7x FLOPs\nreduction with 92.8%/93.6% performance retention on LLaVA-1.5-7B/LLaVA-NeXT-7B.\nOur methods consistently outperform state-of-the-art training-free approaches,\nshowcasing effectiveness and generalizability across model architectures,\nsizes, and tasks without requiring retraining. Our project page is at\nhttps://ficoco-accelerate.github.io/.\n","authors":["Yuhang Han","Xuyang Liu","Zihan Zhang","Pengxiang Ding","Donglin Wang","Honggang Chen","Qingsen Yan","Siteng Huang"],"pdf_url":"https://arxiv.org/pdf/2411.17686v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11633v1","updated":"2025-03-14T17:52:06Z","published":"2025-03-14T17:52:06Z","title":"Seeing and Seeing Through the Glass: Real and Synthetic Data for\n  Multi-Layer Depth Estimation","summary":"  Transparent objects are common in daily life, and understanding their\nmulti-layer depth information -- perceiving both the transparent surface and\nthe objects behind it -- is crucial for real-world applications that interact\nwith transparent materials. In this paper, we introduce LayeredDepth, the first\ndataset with multi-layer depth annotations, including a real-world benchmark\nand a synthetic data generator, to support the task of multi-layer depth\nestimation. Our real-world benchmark consists of 1,500 images from diverse\nscenes, and evaluating state-of-the-art depth estimation methods on it reveals\nthat they struggle with transparent objects. The synthetic data generator is\nfully procedural and capable of providing training data for this task with an\nunlimited variety of objects and scene compositions. Using this generator, we\ncreate a synthetic dataset with 15,300 images. Baseline models training solely\non this synthetic dataset produce good cross-domain multi-layer depth\nestimation. Fine-tuning state-of-the-art single-layer depth models on it\nsubstantially improves their performance on transparent objects, with\nquadruplet accuracy on our benchmark increased from 55.14% to 75.20%. All\nimages and validation annotations are available under CC0 at\nhttps://layereddepth.cs.princeton.edu.\n","authors":["Hongyu Wen","Yiming Zuo","Venkat Subramanian","Patrick Chen","Jia Deng"],"pdf_url":"https://arxiv.org/pdf/2503.11633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11629v1","updated":"2025-03-14T17:48:06Z","published":"2025-03-14T17:48:06Z","title":"TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing","summary":"  We introduce TreeMeshGPT, an autoregressive Transformer designed to generate\nhigh-quality artistic meshes aligned with input point clouds. Instead of the\nconventional next-token prediction in autoregressive Transformer, we propose a\nnovel Autoregressive Tree Sequencing where the next input token is retrieved\nfrom a dynamically growing tree structure that is built upon the triangle\nadjacency of faces within the mesh. Our sequencing enables the mesh to extend\nlocally from the last generated triangular face at each step, and therefore\nreduces training difficulty and improves mesh quality. Our approach represents\neach triangular face with two tokens, achieving a compression rate of\napproximately 22% compared to the naive face tokenization. This efficient\ntokenization enables our model to generate highly detailed artistic meshes with\nstrong point cloud conditioning, surpassing previous methods in both capacity\nand fidelity. Furthermore, our method generates mesh with strong normal\norientation constraints, minimizing flipped normals commonly encountered in\nprevious methods. Our experiments show that TreeMeshGPT enhances the mesh\ngeneration quality with refined details and normal orientation consistency.\n","authors":["Stefan Lionar","Jiabin Liang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2503.11629v1.pdf","comment":"CVPR 2025. Code: https://github.com/sail-sg/TreeMeshGPT"},{"id":"http://arxiv.org/abs/2503.11609v1","updated":"2025-03-14T17:24:01Z","published":"2025-03-14T17:24:01Z","title":"Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages","summary":"  An old-school recipe for training a classifier is to (i) learn a good feature\nextractor and (ii) optimize a linear layer atop. When only a handful of samples\nare available per category, as in Few-Shot Adaptation (FSA), data are\ninsufficient to fit a large number of parameters, rendering the above\nimpractical. This is especially true with large pre-trained Vision-Language\nModels (VLMs), which motivated successful research at the intersection of\nParameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by\nanalyzing the learning dynamics of PEFT techniques when trained on few-shot\ndata from only a subset of categories, referred to as the ``base'' classes. We\nshow that such dynamics naturally splits into two distinct phases: (i)\ntask-level feature extraction and (ii) specialization to the available\nconcepts. To accommodate this dynamic, we then depart from prompt- or\nadapter-based methods and tackle FSA differently. Specifically, given a fixed\ncomputational budget, we split it to (i) learn a task-specific feature\nextractor via PEFT and (ii) train a linear classifier on top. We call this\nscheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established\nmethods, our scheme enables a novel form of selective inference at a category\nlevel, i.e., at test time, only novel categories are embedded by the adapted\ntext encoder, while embeddings of base categories are available within the\nclassifier. Results with fixed hyperparameters across two settings, three\nbackbones, and eleven datasets, show that 2SFS matches or surpasses the\nstate-of-the-art, while established methods degrade significantly across\nsettings.\n","authors":["Matteo Farina","Massimiliano Mancini","Giovanni Iacca","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.11609v1.pdf","comment":"Camera-ready version for CVPR 2025 (w/ SuppMat, 23 pages)"},{"id":"http://arxiv.org/abs/2503.11601v1","updated":"2025-03-14T17:15:26Z","published":"2025-03-14T17:15:26Z","title":"Advancing 3D Gaussian Splatting Editing with Complementary and Consensus\n  Information","summary":"  We present a novel framework for enhancing the visual fidelity and\nconsistency of text-guided 3D Gaussian Splatting (3DGS) editing. Existing\nediting approaches face two critical challenges: inconsistent geometric\nreconstructions across multiple viewpoints, particularly in challenging camera\npositions, and ineffective utilization of depth information during image\nmanipulation, resulting in over-texture artifacts and degraded object\nboundaries. To address these limitations, we introduce: 1) A complementary\ninformation mutual learning network that enhances depth map estimation from\n3DGS, enabling precise depth-conditioned 3D editing while preserving geometric\nstructures. 2) A wavelet consensus attention mechanism that effectively aligns\nlatent codes during the diffusion denoising process, ensuring multi-view\nconsistency in the edited results. Through extensive experimentation, our\nmethod demonstrates superior performance in rendering quality and view\nconsistency compared to state-of-the-art approaches. The results validate our\nframework as an effective solution for text-guided editing of 3D scenes.\n","authors":["Xuanqi Zhang","Jieun Lee","Chris Joslin","Wonsook Lee"],"pdf_url":"https://arxiv.org/pdf/2503.11601v1.pdf","comment":"7 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.11591v1","updated":"2025-03-14T17:01:17Z","published":"2025-03-14T17:01:17Z","title":"Pathology Image Compression with Pre-trained Autoencoders","summary":"  The growing volume of high-resolution Whole Slide Images in digital\nhistopathology poses significant storage, transmission, and computational\nefficiency challenges. Standard compression methods, such as JPEG, reduce file\nsizes but often fail to preserve fine-grained phenotypic details critical for\ndownstream tasks. In this work, we repurpose autoencoders (AEs) designed for\nLatent Diffusion Models as an efficient learned compression framework for\npathology images. We systematically benchmark three AE models with varying\ncompression levels and evaluate their reconstruction ability using pathology\nfoundation models. We introduce a fine-tuning strategy to further enhance\nreconstruction fidelity that optimizes a pathology-specific learned perceptual\nmetric. We validate our approach on downstream tasks, including segmentation,\npatch classification, and multiple instance learning, showing that replacing\nimages with AE-compressed reconstructions leads to minimal performance\ndegradation. Additionally, we propose a K-means clustering-based quantization\nmethod for AE latents, improving storage efficiency while maintaining\nreconstruction quality. We provide the weights of the fine-tuned autoencoders\nat\nhttps://huggingface.co/collections/StonyBrook-CVLab/pathology-fine-tuned-aes-67d45f223a659ff2e3402dd0.\n","authors":["Srikar Yellapragada","Alexandros Graikos","Kostas Triaridis","Zilinghan Li","Tarak Nath Nandi","Ravi K Madduri","Prateek Prasanna","Joel Saltz","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2503.11591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00096v2","updated":"2025-03-14T16:52:55Z","published":"2023-09-29T19:09:27Z","title":"Towards Few-Call Model Stealing via Active Self-Paced Knowledge\n  Distillation and Diffusion-Based Image Generation","summary":"  Diffusion models showcase strong capabilities in image synthesis, being used\nin many computer vision tasks with great success. To this end, we propose to\nexplore a new use case, namely to copy black-box classification models without\nhaving access to the original training data, the architecture, and the weights\nof the model, i.e. the model is only exposed through an inference API. More\nspecifically, we can only observe the (soft or hard) labels for some image\nsamples passed as input to the model. Furthermore, we consider an additional\nconstraint limiting the number of model calls, mostly focusing our research on\nfew-call model stealing. In order to solve the model extraction task given the\napplied restrictions, we propose the following framework. As training data, we\ncreate a synthetic data set (called proxy data set) by leveraging the ability\nof diffusion models to generate realistic and diverse images. Given a maximum\nnumber of allowed API calls, we pass the respective number of samples through\nthe black-box model to collect labels. Finally, we distill the knowledge of the\nblack-box teacher (attacked model) into a student model (copy of the attacked\nmodel), harnessing both labeled and unlabeled data generated by the diffusion\nmodel. We employ a novel active self-paced learning framework to make the most\nof the proxy data during distillation. Our empirical results on three data sets\nconfirm the superiority of our framework over four state-of-the-art methods in\nthe few-call model extraction scenario. We release our code for free\nnon-commercial use at https://github.com/vladhondru25/model-stealing.\n","authors":["Vlad Hondru","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2310.00096v2.pdf","comment":"Accepted in Artificial Intelligence Review"},{"id":"http://arxiv.org/abs/2503.11579v1","updated":"2025-03-14T16:45:23Z","published":"2025-03-14T16:45:23Z","title":"Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers","summary":"  State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640$\\times$360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks.\n","authors":["Weiming Ren","Wentao Ma","Huan Yang","Cong Wei","Ge Zhang","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2503.11579v1.pdf","comment":"Project Page: https://tiger-ai-lab.github.io/Vamba/"},{"id":"http://arxiv.org/abs/2503.11576v1","updated":"2025-03-14T16:44:14Z","published":"2025-03-14T16:44:14Z","title":"SmolDocling: An ultra-compact vision-language model for end-to-end\n  multi-modal document conversion","summary":"  We introduce SmolDocling, an ultra-compact vision-language model targeting\nend-to-end document conversion. Our model comprehensively processes entire\npages by generating DocTags, a new universal markup format that captures all\npage elements in their full context with location. Unlike existing approaches\nthat rely on large foundational models, or ensemble solutions that rely on\nhandcrafted pipelines of multiple specialized models, SmolDocling offers an\nend-to-end conversion for accurately capturing content, structure and spatial\nlocation of document elements in a 256M parameters vision-language model.\nSmolDocling exhibits robust performance in correctly reproducing document\nfeatures such as code listings, tables, equations, charts, lists, and more\nacross a diverse range of document types including business documents, academic\npapers, technical reports, patents, and forms -- significantly extending beyond\nthe commonly observed focus on scientific papers. Additionally, we contribute\nnovel publicly sourced datasets for charts, tables, equations, and code\nrecognition. Experimental results demonstrate that SmolDocling competes with\nother Vision Language Models that are up to 27 times larger in size, while\nreducing computational requirements substantially. The model is currently\navailable, datasets will be publicly available soon.\n","authors":["Ahmed Nassar","Andres Marafioti","Matteo Omenetti","Maksym Lysak","Nikolaos Livathinos","Christoph Auer","Lucas Morin","Rafael Teixeira de Lima","Yusik Kim","A. Said Gurbuz","Michele Dolfi","Miquel Farré","Peter W. J. Staar"],"pdf_url":"https://arxiv.org/pdf/2503.11576v1.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.11571v1","updated":"2025-03-14T16:39:15Z","published":"2025-03-14T16:39:15Z","title":"RASA: Replace Anyone, Say Anything -- A Training-Free Framework for\n  Audio-Driven and Universal Portrait Video Editing","summary":"  Portrait video editing focuses on modifying specific attributes of portrait\nvideos, guided by audio or video streams. Previous methods typically either\nconcentrate on lip-region reenactment or require training specialized models to\nextract keypoints for motion transfer to a new identity. In this paper, we\nintroduce a training-free universal portrait video editing framework that\nprovides a versatile and adaptable editing strategy. This framework supports\nportrait appearance editing conditioned on the changed first reference frame,\nas well as lip editing conditioned on varied speech, or a combination of both.\nIt is based on a Unified Animation Control (UAC) mechanism with source\ninversion latents to edit the entire portrait, including visual-driven shape\ncontrol, audio-driven speaking control, and inter-frame temporal control.\nFurthermore, our method can be adapted to different scenarios by adjusting the\ninitial reference frame, enabling detailed editing of portrait videos with\nspecific head rotations and facial expressions. This comprehensive approach\nensures a holistic and flexible solution for portrait video editing. The\nexperimental results show that our model can achieve more accurate and\nsynchronized lip movements for the lip editing task, as well as more flexible\nmotion transfer for the appearance editing task. Demo is available at\nhttps://alice01010101.github.io/RASA/.\n","authors":["Tianrui Pan","Lin Liu","Jie Liu","Xiaopeng Zhang","Jie Tang","Gangshan Wu","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2503.11571v1.pdf","comment":"Demo is available at https://alice01010101.github.io/RASA/"},{"id":"http://arxiv.org/abs/2410.02086v2","updated":"2025-03-14T16:36:53Z","published":"2024-10-02T23:19:23Z","title":"Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations","summary":"  A unified representation space in multi-modal learning is essential for\neffectively integrating diverse data sources, such as text, images, and audio,\nto enhance efficiency and performance across various downstream tasks. Recent\nbinding methods, such as ImageBind (Girdhar et al., 2023), typically rely on a\nsingle, fixed anchor modality for aligning multi-modal data. We mathematically\nanalyze these fixed anchor binding method and uncover significant limitations:\n(1) over-reliance on the choice of the anchor modality, (2) inadequate capture\nof intra-modal information, and (3) failure to account for cross-modal\ncorrelation among non-anchored modalities. To address these issues, we propose\nthe need for adaptive anchor binding methods, exemplified by our framework\nCentroBind. The proposed method uses adaptively adjustable centroid-based\nanchors generated from all available modalities, leading to a balanced and rich\nrepresentation space. We theoretically demonstrate that our approach captures\nthree critical properties of multi-modal learning -- intra-modal learning,\ninter-modal learning, and multi-modal alignment -- while constructing a unified\nrepresentation that spans all modalities. Experiments on both synthetic and\nreal-world datasets show that adaptive anchor methods such as CentroBind\nconsistently outperform fixed anchor binding methods, verifying our analysis.\n","authors":["Minoh Jeong","Min Namgung","Zae Myung Kim","Dongyeop Kang","Yao-Yi Chiang","Alfred Hero"],"pdf_url":"https://arxiv.org/pdf/2410.02086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.04517v2","updated":"2025-03-14T16:34:24Z","published":"2022-09-09T20:39:22Z","title":"Affinity-VAE: incorporating prior knowledge in representation learning\n  from scientific images","summary":"  Learning compact and interpretable representations of data is a critical\nchallenge in scientific image analysis. Here, we introduce Affinity-VAE, a\ngenerative model that enables us to impose our scientific intuition about the\nsimilarity of instances in the dataset on the learned representation during\ntraining. We demonstrate the utility of the approach in the scientific domain\nof cryo-electron tomography (cryo-ET) where a significant current challenge is\nto identify similar molecules within a noisy and low contrast tomographic image\nvolume. This task is distinct from classification in that, at inference time,\nit is unknown whether an instance is part of the training set or not. We\ntrained affinity-VAE using prior knowledge of protein structure to inform the\nlatent space. Our model is able to create rotationally-invariant,\nmorphologically homogeneous clusters in the latent representation, with\nimproved cluster separation compared to other approaches. It achieves\ncompetitive performance on protein classification with the added benefit of\ndisentangling object pose, structural similarity and an interpretable latent\nrepresentation. In the context of cryo-ET data, affinity-VAE captures the\norientation of identified proteins in 3D which can be used as a prior for\nsubsequent scientific experiments. Extracting physical principles from a\ntrained network is of significant importance in scientific imaging where a\nground truth training set is not always feasible.\n","authors":["Marjan Famili","Jola Mirecka","Camila Rangel Smith","Anna Kotańska","Nikolai Juraschko","Beatriz Costa-Gomes","Colin M. Palmer","Jeyan Thiyagalingam","Tom Burnley","Mark Basham","Alan R. Lowe"],"pdf_url":"https://arxiv.org/pdf/2209.04517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11565v1","updated":"2025-03-14T16:33:48Z","published":"2025-03-14T16:33:48Z","title":"Disentangled Object-Centric Image Representation for Robotic\n  Manipulation","summary":"  Learning robotic manipulation skills from vision is a promising approach for\ndeveloping robotics applications that can generalize broadly to real-world\nscenarios. As such, many approaches to enable this vision have been explored\nwith fruitful results. Particularly, object-centric representation methods have\nbeen shown to provide better inductive biases for skill learning, leading to\nimproved performance and generalization. Nonetheless, we show that\nobject-centric methods can struggle to learn simple manipulation skills in\nmulti-object environments. Thus, we propose DOCIR, an object-centric framework\nthat introduces a disentangled representation for objects of interest,\nobstacles, and robot embodiment. We show that this approach leads to\nstate-of-the-art performance for learning pick and place skills from visual\ninputs in multi-object environments and generalizes at test time to changing\nobjects of interest and distractors in the scene. Furthermore, we show its\nefficacy both in simulation and zero-shot transfer to the real world.\n","authors":["David Emukpere","Romain Deffayet","Bingbing Wu","Romain Brégier","Michael Niemaz","Jean-Luc Meunier","Denys Proux","Jean-Michel Renders","Seungsu Kim"],"pdf_url":"https://arxiv.org/pdf/2503.11565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11557v1","updated":"2025-03-14T16:26:11Z","published":"2025-03-14T16:26:11Z","title":"VERIFY: A Benchmark of Visual Explanation and Reasoning for\n  Investigating Multimodal Reasoning Fidelity","summary":"  Visual reasoning is central to human cognition, enabling individuals to\ninterpret and abstractly understand their environment. Although recent\nMultimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance across language and vision-language tasks, existing benchmarks\nprimarily measure recognition-based skills and inadequately assess true visual\nreasoning capabilities. To bridge this critical gap, we introduce VERIFY, a\nbenchmark explicitly designed to isolate and rigorously evaluate the visual\nreasoning capabilities of state-of-the-art MLLMs. VERIFY compels models to\nreason primarily from visual information, providing minimal textual context to\nreduce reliance on domain-specific knowledge and linguistic biases. Each\nproblem is accompanied by a human-annotated reasoning path, making it the first\nto provide in-depth evaluation of model decision-making processes.\nAdditionally, we propose novel metrics that assess visual reasoning fidelity\nbeyond mere accuracy, highlighting critical imbalances in current model\nreasoning patterns. Our comprehensive benchmarking of leading MLLMs uncovers\nsignificant limitations, underscoring the need for a balanced and holistic\napproach to both perception and reasoning. For more teaser and testing, visit\nour project page (https://verify-eqh.pages.dev/).\n","authors":["Jing Bi","Junjia Guo","Susan Liang","Guangyu Sun","Luchuan Song","Yunlong Tang","Jinxi He","Jiarui Wu","Ali Vosoughi","Chen Chen","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2503.11557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11549v1","updated":"2025-03-14T16:12:23Z","published":"2025-03-14T16:12:23Z","title":"Similarity-Aware Token Pruning: Your VLM but Faster","summary":"  The computational demands of Vision Transformers (ViTs) and Vision-Language\nModels (VLMs) remain a significant challenge due to the quadratic complexity of\nself-attention. While token pruning offers a promising solution, existing\nmethods often introduce training overhead or fail to adapt dynamically across\nlayers. We present SAINT, a training-free token pruning framework that\nleverages token similarity and a graph-based formulation to dynamically\noptimize pruning rates and redundancy thresholds. Through systematic analysis,\nwe identify a universal three-stage token evolution process\n(aligner-explorer-aggregator) in transformers, enabling aggressive pruning in\nearly stages without sacrificing critical information. For ViTs, SAINT doubles\nthe throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on\nImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply\nSAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's\ntokens by 75%, achieving latency comparable to LLaVA-7B with less than 1%\nperformance loss across benchmarks. Our work establishes a unified, practical\nframework for efficient inference in ViTs and VLMs.\n","authors":["Ahmadreza Jeddi","Negin Baghbanzadeh","Elham Dolatabadi","Babak Taati"],"pdf_url":"https://arxiv.org/pdf/2503.11549v1.pdf","comment":"15 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2411.18092v2","updated":"2025-03-14T16:12:10Z","published":"2024-11-27T07:04:00Z","title":"Training Noise Token Pruning","summary":"  In the present work we present Training Noise Token (TNT) Pruning for vision\ntransformers. Our method relaxes the discrete token dropping condition to\ncontinuous additive noise, providing smooth optimization in training, while\nretaining discrete dropping computational gains in deployment settings. We\nprovide theoretical connections to Rate-Distortion literature, and empirical\nevaluations on the ImageNet dataset using ViT and DeiT architectures\ndemonstrating TNT's advantages over previous pruning methods.\n","authors":["Mingxing Rao","Bohan Jiang","Daniel Moyer"],"pdf_url":"https://arxiv.org/pdf/2411.18092v2.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.11544v1","updated":"2025-03-14T16:10:21Z","published":"2025-03-14T16:10:21Z","title":"AugGen: Synthetic Augmentation Can Improve Discriminative Models","summary":"  The increasing dependence on large-scale datasets in machine learning\nintroduces significant privacy and ethical challenges. Synthetic data\ngeneration offers a promising solution; however, most current methods rely on\nexternal datasets or pre-trained models, which add complexity and escalate\nresource demands. In this work, we introduce a novel self-contained synthetic\naugmentation technique that strategically samples from a conditional generative\nmodel trained exclusively on the target dataset. This approach eliminates the\nneed for auxiliary data sources. Applied to face recognition datasets, our\nmethod achieves 1--12\\% performance improvements on the IJB-C and IJB-B\nbenchmarks. It outperforms models trained solely on real data and exceeds the\nperformance of state-of-the-art synthetic data generation baselines. Notably,\nthese enhancements often surpass those achieved through architectural\nimprovements, underscoring the significant impact of synthetic augmentation in\ndata-scarce environments. These findings demonstrate that carefully integrated\nsynthetic data not only addresses privacy and resource constraints but also\nsubstantially boosts model performance. Project page\nhttps://parsa-ra.github.io/auggen\n","authors":["Parsa Rahimi","Damien Teney","Sebastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2503.11544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11538v1","updated":"2025-03-14T16:04:10Z","published":"2025-03-14T16:04:10Z","title":"FLASHμ: Fast Localizing And Sizing of Holographic Microparticles","summary":"  Reconstructing the 3D location and size of microparticles from diffraction\nimages - holograms - is a computationally expensive inverse problem that has\ntraditionally been solved using physics-based reconstruction methods. More\nrecently, researchers have used machine learning methods to speed up the\nprocess. However, for small particles in large sample volumes the performance\nof these methods falls short of standard physics-based reconstruction methods.\nHere we designed a two-stage neural network architecture, FLASH$\\mu$, to detect\nsmall particles (6-100$\\mu$m) from holograms with large sample depths up to\n20cm. Trained only on synthetic data with added physical noise, our method\nreliably detects particles of at least 9$\\mu$m diameter in real holograms,\ncomparable to the standard reconstruction-based approaches while operating on\nsmaller crops, at quarter of the original resolution and providing roughly a\n600-fold speedup. In addition to introducing a novel approach to a non-local\nobject detection or signal demixing problem, our work could enable low-cost,\nreal-time holographic imaging setups.\n","authors":["Ayush Paliwal","Oliver Schlenczek","Birte Thiede","Manuel Santos Pereira","Katja Stieger","Eberhard Bodenschatz","Gholamhossein Bagheri","Alexander Ecker"],"pdf_url":"https://arxiv.org/pdf/2503.11538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21705v2","updated":"2025-03-14T15:55:43Z","published":"2024-10-29T03:41:47Z","title":"AdaptGCD: Multi-Expert Adapter Tuning for Generalized Category Discovery","summary":"  Different from the traditional semi-supervised learning paradigm that is\nconstrained by the close-world assumption, Generalized Category Discovery (GCD)\npresumes that the unlabeled dataset contains new categories not appearing in\nthe labeled set, and aims to not only classify old categories but also discover\nnew categories in the unlabeled data. Existing studies on GCD typically devote\nto transferring the general knowledge from the self-supervised pretrained model\nto the target GCD task via some fine-tuning strategies, such as partial tuning\nand prompt learning. Nevertheless, these fine-tuning methods fail to make a\nsound balance between the generalization capacity of pretrained backbone and\nthe adaptability to the GCD task. To fill this gap, in this paper, we propose a\nnovel adapter-tuning-based method named AdaptGCD, which is the first work to\nintroduce the adapter tuning into the GCD task and provides some key insights\nexpected to enlighten future research. Furthermore, considering the discrepancy\nof supervision information between the old and new classes, a multi-expert\nadapter structure equipped with a route assignment constraint is elaborately\ndevised, such that the data from old and new classes are separated into\ndifferent expert groups. Extensive experiments are conducted on 7 widely-used\ndatasets. The remarkable improvements in performance highlight the\neffectiveness of our proposals.\n","authors":["Yuxun Qu","Yongqiang Tang","Chenyang Zhang","Wensheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11519v1","updated":"2025-03-14T15:42:42Z","published":"2025-03-14T15:42:42Z","title":"Exploring Typographic Visual Prompts Injection Threats in Cross-Modality\n  Generation Models","summary":"  Current Cross-Modality Generation Models (GMs) demonstrate remarkable\ncapabilities in various generative tasks. Given the ubiquity and information\nrichness of vision modality inputs in real-world scenarios, Cross-vision,\nencompassing Vision-Language Perception (VLP) and Image-to-Image (I2I), tasks\nhave attracted significant attention. Large Vision Language Models (LVLMs) and\nI2I GMs are employed to handle VLP and I2I tasks, respectively. Previous\nresearch indicates that printing typographic words into input images\nsignificantly induces LVLMs and I2I GMs to generate disruptive outputs\nsemantically related to those words. Additionally, visual prompts, as a more\nsophisticated form of typography, are also revealed to pose security risks to\nvarious applications of VLP tasks when injected into images. In this paper, we\ncomprehensively investigate the performance impact induced by Typographic\nVisual Prompt Injection (TVPI) in various LVLMs and I2I GMs. To better observe\nperformance modifications and characteristics of this threat, we also introduce\nthe TVPI Dataset. Through extensive explorations, we deepen the understanding\nof the underlying causes of the TVPI threat in various GMs and offer valuable\ninsights into its potential origins.\n","authors":["Hao Cheng","Erjia Xiao","Yichi Wang","Kaidi Xu","Mengshu Sun","Jindong Gu","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2503.11519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11513v1","updated":"2025-03-14T15:36:39Z","published":"2025-03-14T15:36:39Z","title":"HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation\n  with Autoregressive Large Language Models","summary":"  Text-to-video generation poses significant challenges due to the inherent\ncomplexity of video data, which spans both temporal and spatial dimensions. It\nintroduces additional redundancy, abrupt variations, and a domain gap between\nlanguage and vision tokens while generation. Addressing these challenges\nrequires an effective video tokenizer that can efficiently encode video data\nwhile preserving essential semantic and spatiotemporal information, serving as\na critical bridge between text and vision. Inspired by the observation in\nVQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for\ntext-to-video generation with hierarchical tokenizers. It utilizes a 3D causal\nVAE with a multi-layer discrete token framework, encoding video content into\nhierarchically structured codebooks. Higher layers capture semantic information\nwith higher compression, while lower layers focus on fine-grained\nspatiotemporal details, striking a balance between compression efficiency and\nreconstruction quality. Our approach efficiently encodes longer video sequences\n(e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately\n70\\% compared to baseline tokenizers, while maintaining competitive\nreconstruction quality. We explore the trade-offs between compression and\nreconstruction, while emphasizing the advantages of high-compressed semantic\ntokens in text-to-video tasks. HiTVideo aims to address the potential\nlimitations of existing video tokenizers in text-to-video generation tasks,\nstriving for higher compression ratios and simplify LLMs modeling under\nlanguage guidance, offering a scalable and promising framework for advancing\ntext to video generation. Demo page:\nhttps://ziqinzhou66.github.io/project/HiTVideo.\n","authors":["Ziqin Zhou","Yifan Yang","Yuqing Yang","Tianyu He","Houwen Peng","Kai Qiu","Qi Dai","Lili Qiu","Chong Luo","Lingqiao Liu"],"pdf_url":"https://arxiv.org/pdf/2503.11513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11511v1","updated":"2025-03-14T15:34:37Z","published":"2025-03-14T15:34:37Z","title":"Alzheimer's Disease Classification Using Retinal OCT: TransnetOCT and\n  Swin Transformer Models","summary":"  Retinal optical coherence tomography (OCT) images are the biomarkers for\nneurodegenerative diseases, which are rising in prevalence. Early detection of\nAlzheimer's disease using retinal OCT is a primary challenging task. This work\nutilizes advanced deep learning techniques to classify retinal OCT images of\nsubjects with Alzheimer's disease (AD) and healthy controls (CO). The goal is\nto enhance diagnostic capabilities through efficient image analysis. In the\nproposed model, Raw OCT images have been preprocessed with ImageJ and given to\nvarious deep-learning models to evaluate the accuracy. The best classification\narchitecture is TransNetOCT, which has an average accuracy of 98.18% for input\nOCT images and 98.91% for segmented OCT images for five-fold cross-validation\ncompared to other models, and the Swin Transformer model has achieved an\naccuracy of 93.54%. The evaluation accuracy metric demonstrated TransNetOCT and\nSwin transformer models capability to classify AD and CO subjects reliably,\ncontributing to the potential for improved diagnostic processes in clinical\nsettings.\n","authors":["Siva Manohar Reddy Kesu","Neelam Sinha","Hariharan Ramasangu","Thomas Gregor Issac"],"pdf_url":"https://arxiv.org/pdf/2503.11511v1.pdf","comment":"18 pages, 25 figures"},{"id":"http://arxiv.org/abs/2410.15959v4","updated":"2025-03-14T15:30:07Z","published":"2024-10-21T12:43:54Z","title":"Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy","summary":"  While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io/\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Haonan Duan","Hengjun Pu","Ronglei Tong","Chengyang Zhao","Xizhou Zhu","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v4.pdf","comment":"Preprint; https://robodita.github.io/"},{"id":"http://arxiv.org/abs/2503.11509v1","updated":"2025-03-14T15:29:58Z","published":"2025-03-14T15:29:58Z","title":"TikZero: Zero-Shot Text-Guided Graphics Program Synthesis","summary":"  With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.\n","authors":["Jonas Belouadi","Eddy Ilg","Margret Keuper","Hideki Tanaka","Masao Utiyama","Raj Dabre","Steffen Eger","Simone Paolo Ponzetto"],"pdf_url":"https://arxiv.org/pdf/2503.11509v1.pdf","comment":"Project page: https://github.com/potamides/DeTikZify"},{"id":"http://arxiv.org/abs/2503.11498v1","updated":"2025-03-14T15:26:02Z","published":"2025-03-14T15:26:02Z","title":"Cloud2BIM: An open-source automatic pipeline for efficient conversion of\n  large-scale point clouds into IFC format","summary":"  Building Information Modeling (BIM) is an essential component in the\nsustainable reconstruction and revitalization of ageing structures. However,\nmodel creation usually relies on laborious manual transformation of the\nunstructured point cloud data provided by laser scans or photogrammetry. This\npaper presents Cloud2BIM, an open-source software tool designed to automate the\nconversion of point clouds into BIM models compliant with the Industry\nFoundation Classes (IFC) standard. Cloud2BIM integrates advanced algorithms for\nwall and slab segmentation, opening detection, and room zoning based on real\nwall surfaces, resulting in a comprehensive and fully automated workflow.\nUnlike existing tools, it avoids computationally- and calibration-intensive\ntechniques such as RANSAC, supports non-orthogonal geometries, and provides\nunprecedented processing speed-achieving results up to seven times faster than\nfastest competing solutions. Systematic validation using benchmark datasets\nconfirms that Cloud2BIM is an easy-to-use, efficient, and scalable solution for\ngenerating accurate BIM models, capable of converting extensive point cloud\ndatasets for entire buildings into IFC format with minimal user input.\n","authors":["Slávek Zbirovský","Václav Nežerka"],"pdf_url":"https://arxiv.org/pdf/2503.11498v1.pdf","comment":"42 pages, 18 figures"},{"id":"http://arxiv.org/abs/2503.11496v1","updated":"2025-03-14T15:21:54Z","published":"2025-03-14T15:21:54Z","title":"Cognitive Disentanglement for Referring Multi-Object Tracking","summary":"  As a significant application of multi-source information fusion in\nintelligent transportation perception systems, Referring Multi-Object Tracking\n(RMOT) involves localizing and tracking specific objects in video sequences\nbased on language references. However, existing RMOT approaches often treat\nlanguage descriptions as holistic embeddings and struggle to effectively\nintegrate the rich semantic information contained in language expressions with\nvisual features. This limitation is especially apparent in complex scenes\nrequiring comprehensive understanding of both static object attributes and\nspatial motion information. In this paper, we propose a Cognitive\nDisentanglement for Referring Multi-Object Tracking (CDRMT) framework that\naddresses these challenges. It adapts the \"what\" and \"where\" pathways from\nhuman visual processing system to RMOT tasks. Specifically, our framework\ncomprises three collaborative components: (1)The Bidirectional Interactive\nFusion module first establishes cross-modal connections while preserving\nmodality-specific characteristics; (2) Building upon this foundation, the\nProgressive Semantic-Decoupled Query Learning mechanism hierarchically injects\ncomplementary information into object queries, progressively refining object\nunderstanding from coarse to fine-grained semantic levels; (3) Finally, the\nStructural Consensus Constraint enforces bidirectional semantic consistency\nbetween visual features and language descriptions, ensuring that tracked\nobjects faithfully reflect the referring expression. Extensive experiments on\ndifferent benchmark datasets demonstrate that CDRMT achieves substantial\nimprovements over state-of-the-art methods, with average gains of 6.0% in HOTA\nscore on Refer-KITTI and 3.2% on Refer-KITTI-V2. Our approach advances the\nstate-of-the-art in RMOT while simultaneously providing new insights into\nmulti-source information fusion.\n","authors":["Shaofeng Liang","Runwei Guan","Wangwang Lian","Daizong Liu","Xiaolou Sun","Dongming Wu","Yutao Yue","Weiping Ding","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.11496v1.pdf","comment":"24 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.11495v1","updated":"2025-03-14T15:21:44Z","published":"2025-03-14T15:21:44Z","title":"V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning","summary":"  Human processes video reasoning in a sequential spatio-temporal reasoning\nlogic, we first identify the relevant frames (\"when\") and then analyse the\nspatial relationships (\"where\") between key objects, and finally leverage these\nrelationships to draw inferences (\"what\"). However, can Video Large Language\nModels (Video-LLMs) also \"reason through a sequential spatio-temporal logic\" in\nvideos? Existing Video-LLM benchmarks primarily focus on assessing object\npresence, neglecting relational reasoning. Consequently, it is difficult to\nmeasure whether a model truly comprehends object interactions (actions/events)\nin videos or merely relies on pre-trained \"memory\" of co-occurrences as biases\nin generating answers. In this work, we introduce a Video Spatio-Temporal\nReasoning (V-STaR) benchmark to address these shortcomings. The key idea is to\ndecompose video understanding into a Reverse Spatio-Temporal Reasoning (RSTR)\ntask that simultaneously evaluates what objects are present, when events occur,\nand where they are located while capturing the underlying Chain-of-thought\n(CoT) logic. To support this evaluation, we construct a dataset to elicit the\nspatial-temporal reasoning process of Video-LLMs. It contains coarse-to-fine\nCoT questions generated by a semi-automated GPT-4-powered pipeline, embedding\nexplicit reasoning chains to mimic human cognition. Experiments from 14\nVideo-LLMs on our V-STaR reveal significant gaps between current Video-LLMs and\nthe needs for robust and consistent spatio-temporal reasoning.\n","authors":["Zixu Cheng","Jian Hu","Ziquan Liu","Chenyang Si","Wei Li","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2503.11495v1.pdf","comment":"A benchmark for Video Spatio-Temporal Reasoning"},{"id":"http://arxiv.org/abs/2503.11481v1","updated":"2025-03-14T15:06:12Z","published":"2025-03-14T15:06:12Z","title":"T2I-FineEval: Fine-Grained Compositional Metric for Text-to-Image\n  Evaluation","summary":"  Although recent text-to-image generative models have achieved impressive\nperformance, they still often struggle with capturing the compositional\ncomplexities of prompts including attribute binding, and spatial relationships\nbetween different entities. This misalignment is not revealed by common\nevaluation metrics such as CLIPScore. Recent works have proposed evaluation\nmetrics that utilize Visual Question Answering (VQA) by decomposing prompts\ninto questions about the generated image for more robust compositional\nevaluation. Although these methods align better with human evaluations, they\nstill fail to fully cover the compositionality within the image. To address\nthis, we propose a novel metric that breaks down images into components, and\ntexts into fine-grained questions about the generated image for evaluation. Our\nmethod outperforms previous state-of-the-art metrics, demonstrating its\neffectiveness in evaluating text-to-image generative models. Code is available\nat https://github.com/hadi-hosseini/ T2I-FineEval.\n","authors":["Seyed Mohammad Hadi Hosseini","Amir Mohammad Izadi","Ali Abdollahi","Armin Saghafian","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2503.11481v1.pdf","comment":"Accepted at ECCV 2024 Workshop EVAL-FoMo"},{"id":"http://arxiv.org/abs/2502.20292v2","updated":"2025-03-14T15:01:37Z","published":"2025-02-27T17:17:43Z","title":"Visual Adaptive Prompting for Compositional Zero-Shot Learning","summary":"  Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nlearning joint representations of visual and textual data, making them powerful\ntools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires\nmodels to generalize to novel combinations of visual primitives-such as\nattributes and objects-that were not explicitly encountered during training.\nRecent works in prompting for CZSL have focused on modifying inputs for the\ntext encoder, often using static prompts that do not change across varying\nvisual contexts. However, these approaches struggle to fully capture varying\nvisual contexts, as they focus on text adaptation rather than leveraging visual\nfeatures for compositional reasoning. To address this, we propose Visual\nAdaptive Prompting System (VAPS) that leverages a learnable visual prompt\nrepository and similarity-based retrieval mechanism within the framework of\nVLMs to bridge the gap between semantic and visual features. Our method\nintroduces a dynamic visual prompt repository mechanism that selects the most\nrelevant attribute and object prompts based on the visual features of the\nimage. Our proposed system includes a visual prompt adapter that encourages the\nmodel to learn a more generalizable embedding space. Experiments on three CZSL\nbenchmarks, across both closed and open-world scenarios, demonstrate\nstate-of-the-art results.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2502.20292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11465v1","updated":"2025-03-14T14:50:58Z","published":"2025-03-14T14:50:58Z","title":"Remote Photoplethysmography in Real-World and Extreme Lighting Scenarios","summary":"  Physiological activities can be manifested by the sensitive changes in facial\nimaging. While they are barely observable to our eyes, computer vision manners\ncan, and the derived remote photoplethysmography (rPPG) has shown considerable\npromise. However, existing studies mainly rely on spatial skin recognition and\ntemporal rhythmic interactions, so they focus on identifying explicit features\nunder ideal light conditions, but perform poorly in-the-wild with intricate\nobstacles and extreme illumination exposure. In this paper, we propose an\nend-to-end video transformer model for rPPG. It strives to eliminate complex\nand unknown external time-varying interferences, whether they are sufficient to\noccupy subtle biosignal amplitudes or exist as periodic perturbations that\nhinder network training. In the specific implementation, we utilize global\ninterference sharing, subject background reference, and self-supervised\ndisentanglement to eliminate interference, and further guide learning based on\nspatiotemporal filtering, reconstruction guidance, and frequency domain and\nbiological prior constraints to achieve effective rPPG. To the best of our\nknowledge, this is the first robust rPPG model for real outdoor scenarios based\non natural face videos, and is lightweight to deploy. Extensive experiments\nshow the competitiveness and performance of our model in rPPG prediction across\ndatasets and scenes.\n","authors":["Hang Shao","Lei Luo","Jianjun Qian","Mengkai Yan","Shuo Chen","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2503.11465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10604v2","updated":"2025-03-14T14:32:09Z","published":"2024-10-14T15:12:16Z","title":"Multi-modal Vision Pre-training for Medical Image Analysis","summary":"  Self-supervised learning has greatly facilitated medical image analysis by\nsuppressing the training data requirement for real-world applications. Current\nparadigms predominantly rely on self-supervision within uni-modal image data,\nthereby neglecting the inter-modal correlations essential for effective\nlearning of cross-modal image representations. This limitation is particularly\nsignificant for naturally grouped multi-modal data, e.g., multi-parametric MRI\nscans for a patient undergoing various functional imaging protocols in the same\nstudy. To bridge this gap, we conduct a novel multi-modal image pre-training\nwith three proxy tasks to facilitate the learning of cross-modality\nrepresentations and correlations using multi-modal brain MRI scans (over 2.4\nmillion images in 16,022 scans of 3,755 patients), i.e., cross-modal image\nreconstruction, modality-aware contrastive learning, and modality template\ndistillation. To demonstrate the generalizability of our pre-trained model, we\nconduct extensive experiments on various benchmarks with ten downstream tasks.\nThe superior performance of our method is reported in comparison to\nstate-of-the-art pre-training methods, with Dice Score improvement of\n0.28\\%-14.47\\% across six segmentation benchmarks and a consistent accuracy\nboost of 0.65\\%-18.07\\% in four individual image classification tasks.\n","authors":["Shaohao Rui","Lingzhi Chen","Zhenyu Tang","Lilong Wang","Mianxin Liu","Shaoting Zhang","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11439v1","updated":"2025-03-14T14:27:24Z","published":"2025-03-14T14:27:24Z","title":"COIN: Confidence Score-Guided Distillation for Annotation-Free Cell\n  Segmentation","summary":"  Cell instance segmentation (CIS) is crucial for identifying individual cell\nmorphologies in histopathological images, providing valuable insights for\nbiological and medical research. While unsupervised CIS (UCIS) models aim to\nreduce the heavy reliance on labor-intensive image annotations, they fail to\naccurately capture cell boundaries, causing missed detections and poor\nperformance. Recognizing the absence of error-free instances as a key\nlimitation, we present COIN (COnfidence score-guided INstance distillation), a\nnovel annotation-free framework with three key steps: (1) Increasing the\nsensitivity for the presence of error-free instances via unsupervised semantic\nsegmentation with optimal transport, leveraging its ability to discriminate\nspatially minor instances, (2) Instance-level confidence scoring to measure the\nconsistency between model prediction and refined mask and identify highly\nconfident instances, offering an alternative to ground truth annotations, and\n(3) Progressive expansion of confidence with recursive self-distillation.\nExtensive experiments across six datasets show COIN outperforming existing UCIS\nmethods, even surpassing semi- and weakly-supervised approaches across all\nmetrics on the MoNuSeg and TNBC datasets. The code is available at\nhttps://github.com/shjo-april/COIN.\n","authors":["Sanghyun Jo","Seo Jin Lee","Seungwoo Lee","Seohyung Hong","Hyungseok Seo","Kyungsu Kim"],"pdf_url":"https://arxiv.org/pdf/2503.11439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07496v2","updated":"2025-03-14T14:22:02Z","published":"2025-01-13T17:14:25Z","title":"Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal\n  Violence Detection Method","summary":"  Weakly supervised violence detection refers to the technique of training\nmodels to identify violent segments in videos using only video-level labels.\nAmong these approaches, multimodal violence detection, which integrates\nmodalities such as audio and optical flow, holds great potential. Existing\nmethods in this domain primarily focus on designing multimodal fusion models to\naddress modality discrepancies. In contrast, we take a different approach;\nleveraging the inherent discrepancies across modalities in violence event\nrepresentation to propose a novel multimodal semantic feature alignment method.\nThis method sparsely maps the semantic features of local, transient, and less\ninformative modalities ( such as audio and optical flow ) into the more\ninformative RGB semantic feature space. Through an iterative process, the\nmethod identifies the suitable no-zero feature matching subspace and aligns the\nmodality-specific event representations based on this subspace, enabling the\nfull exploitation of information from all modalities during the subsequent\nmodality fusion stage. Building on this, we design a new weakly supervised\nviolence detection framework that consists of unimodal multiple-instance\nlearning for extracting unimodal semantic features, multimodal alignment,\nmultimodal fusion, and final detection. Experimental results on benchmark\ndatasets demonstrate the effectiveness of our method, achieving an average\nprecision (AP) of 86.07% on the XD-Violence dataset. Our code is available at\nhttps://github.com/xjpp2016/MAVD.\n","authors":["Wenping Jin","Li Zhu","Jing Sun"],"pdf_url":"https://arxiv.org/pdf/2501.07496v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11423v1","updated":"2025-03-14T14:09:31Z","published":"2025-03-14T14:09:31Z","title":"TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object\n  Interaction for Generalizable Robotic Manipulation","summary":"  We address key limitations in existing datasets and models for task-oriented\nhand-object interaction video generation, a critical approach of generating\nvideo demonstrations for robotic imitation learning. Current datasets, such as\nEgo4D, often suffer from inconsistent view perspectives and misaligned\ninteractions, leading to reduced video quality and limiting their applicability\nfor precise imitation learning tasks. Towards this end, we introduce TASTE-Rob\n-- a pioneering large-scale dataset of 100,856 ego-centric hand-object\ninteraction videos. Each video is meticulously aligned with language\ninstructions and recorded from a consistent camera viewpoint to ensure\ninteraction clarity. By fine-tuning a Video Diffusion Model (VDM) on TASTE-Rob,\nwe achieve realistic object interactions, though we observed occasional\ninconsistencies in hand grasping postures. To enhance realism, we introduce a\nthree-stage pose-refinement pipeline that improves hand posture accuracy in\ngenerated videos. Our curated dataset, coupled with the specialized\npose-refinement framework, provides notable performance gains in generating\nhigh-quality, task-oriented hand-object interaction videos, resulting in\nachieving superior generalizable robotic manipulation. The TASTE-Rob dataset\nwill be made publicly available upon publication to foster further advancements\nin the field.\n","authors":["Hongxiang Zhao","Xingchen Liu","Mutian Xu","Yiming Hao","Weikai Chen","Xiaoguang Han"],"pdf_url":"https://arxiv.org/pdf/2503.11423v1.pdf","comment":"Conference on Computer Vision and Pattern Recognition 2025"},{"id":"http://arxiv.org/abs/2503.11420v1","updated":"2025-03-14T14:03:33Z","published":"2025-03-14T14:03:33Z","title":"AQUA-SLAM: Tightly-Coupled Underwater Acoustic-Visual-Inertial SLAM with\n  Sensor Calibration","summary":"  Underwater environments pose significant challenges for visual Simultaneous\nLocalization and Mapping (SLAM) systems due to limited visibility, inadequate\nillumination, and sporadic loss of structural features in images. Addressing\nthese challenges, this paper introduces a novel, tightly-coupled\nAcoustic-Visual-Inertial SLAM approach, termed AQUA-SLAM, to fuse a Doppler\nVelocity Log (DVL), a stereo camera, and an Inertial Measurement Unit (IMU)\nwithin a graph optimization framework. Moreover, we propose an efficient sensor\ncalibration technique, encompassing multi-sensor extrinsic calibration (among\nthe DVL, camera and IMU) and DVL transducer misalignment calibration, with a\nfast linear approximation procedure for real-time online execution. The\nproposed methods are extensively evaluated in a tank environment with ground\ntruth, and validated for offshore applications in the North Sea. The results\ndemonstrate that our method surpasses current state-of-the-art underwater and\nvisual-inertial SLAM systems in terms of localization accuracy and robustness.\nThe proposed system will be made open-source for the community.\n","authors":["Shida Xu","Kaicheng Zhang","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19209v3","updated":"2025-03-14T13:57:16Z","published":"2024-05-29T15:49:09Z","title":"VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on\n  Long Videos","summary":"  Long-form video understanding is complicated by the high redundancy of video\ndata and the abundance of query-irrelevant information. To tackle these\nchallenges, we propose VideoTree, a training-free framework which builds a\nquery-adaptive and hierarchical video representation for LLM reasoning over\nlong-form videos. First, VideoTree extracts query-relevant information from the\ninput video through an iterative process, progressively refining the selection\nof keyframes based on their relevance to the query. Furthermore, VideoTree\nleverages the inherent hierarchical structure of long video data, which is\noften overlooked by existing LLM-based methods. Specifically, we incorporate\nmulti-granularity information into a tree-based representation, allowing\nVideoTree to extract query-relevant details from long videos in a\ncoarse-to-fine manner. This enables the model to effectively handle a wide\nrange of video queries with varying levels of detail. Finally, VideoTree\naggregates the hierarchical query-relevant information within the tree\nstructure and feeds it into an LLM reasoning model to answer the query. Our\nexperiments show that our method improves both reasoning accuracy and\nefficiency. Specifically, VideoTree outperforms existing training-free\napproaches on EgoSchema and NExT-QA with less inference time, achieving 61.1%\nand 75.6% accuracy on the test set without additional video-specific training.\nMoreover, on the long split of Video-MME (average 44 minutes), VideoTree\nachieves better performance than GPT-4V and many other MLLMs that were\nextensively trained on video data.\n","authors":["Ziyang Wang","Shoubin Yu","Elias Stengel-Eskin","Jaehong Yoon","Feng Cheng","Gedas Bertasius","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2405.19209v3.pdf","comment":"CVPR 2025; First three authors contributed equally; Project page:\n  https://videotree2024.github.io/"},{"id":"http://arxiv.org/abs/2503.10422v2","updated":"2025-03-14T13:56:52Z","published":"2025-03-13T14:43:03Z","title":"Category Prompt Mamba Network for Nuclei Segmentation and Classification","summary":"  Nuclei segmentation and classification provide an essential basis for tumor\nimmune microenvironment analysis. The previous nuclei segmentation and\nclassification models require splitting large images into smaller patches for\ntraining, leading to two significant issues. First, nuclei at the borders of\nadjacent patches often misalign during inference. Second, this patch-based\napproach significantly increases the model's training and inference time.\nRecently, Mamba has garnered attention for its ability to model large-scale\nimages with linear time complexity and low memory consumption. It offers a\npromising solution for training nuclei segmentation and classification models\non full-sized images. However, the Mamba orientation-based scanning method\nlacks account for category-specific features, resulting in sub-optimal\nperformance in scenarios with imbalanced class distributions. To address these\nchallenges, this paper introduces a novel scanning strategy based on category\nprobability sorting, which independently ranks and scans features for each\ncategory according to confidence from high to low. This approach enhances the\nfeature representation of uncertain samples and mitigates the issues caused by\nimbalanced distributions. Extensive experiments conducted on four public\ndatasets demonstrate that our method outperforms state-of-the-art approaches,\ndelivering superior performance in nuclei segmentation and classification\ntasks.\n","authors":["Ye Zhang","Zijie Fang","Yifeng Wang","Lingbo Zhang","Xianchao Guan","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.10422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11412v1","updated":"2025-03-14T13:54:10Z","published":"2025-03-14T13:54:10Z","title":"MTV-Inpaint: Multi-Task Long Video Inpainting","summary":"  Video inpainting involves modifying local regions within a video, ensuring\nspatial and temporal consistency. Most existing methods focus primarily on\nscene completion (i.e., filling missing regions) and lack the capability to\ninsert new objects into a scene in a controllable manner. Fortunately, recent\nadvancements in text-to-video (T2V) diffusion models pave the way for\ntext-guided video inpainting. However, directly adapting T2V models for\ninpainting remains limited in unifying completion and insertion tasks, lacks\ninput controllability, and struggles with long videos, thereby restricting\ntheir applicability and flexibility. To address these challenges, we propose\nMTV-Inpaint, a unified multi-task video inpainting framework capable of\nhandling both traditional scene completion and novel object insertion tasks. To\nunify these distinct tasks, we design a dual-branch spatial attention mechanism\nin the T2V diffusion U-Net, enabling seamless integration of scene completion\nand object insertion within a single framework. In addition to textual\nguidance, MTV-Inpaint supports multimodal control by integrating various image\ninpainting models through our proposed image-to-video (I2V) inpainting mode.\nAdditionally, we propose a two-stage pipeline that combines keyframe inpainting\nwith in-between frame propagation, enabling MTV-Inpaint to effectively handle\nlong videos with hundreds of frames. Extensive experiments demonstrate that\nMTV-Inpaint achieves state-of-the-art performance in both scene completion and\nobject insertion tasks. Furthermore, it demonstrates versatility in derived\napplications such as multi-modal inpainting, object editing, removal, image\nobject brush, and the ability to handle long videos. Project page:\nhttps://mtv-inpaint.github.io/.\n","authors":["Shiyuan Yang","Zheng Gu","Liang Hou","Xin Tao","Pengfei Wan","Xiaodong Chen","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2503.11412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11409v1","updated":"2025-03-14T13:51:52Z","published":"2025-03-14T13:51:52Z","title":"LuSeg: Efficient Negative and Positive Obstacles Segmentation via\n  Contrast-Driven Multi-Modal Feature Fusion on the Lunar","summary":"  As lunar exploration missions grow increasingly complex, ensuring safe and\nautonomous rover-based surface exploration has become one of the key challenges\nin lunar exploration tasks. In this work, we have developed a lunar surface\nsimulation system called the Lunar Exploration Simulator System (LESS) and the\nLunarSeg dataset, which provides RGB-D data for lunar obstacle segmentation\nthat includes both positive and negative obstacles. Additionally, we propose a\nnovel two-stage segmentation network called LuSeg. Through contrastive\nlearning, it enforces semantic consistency between the RGB encoder from Stage I\nand the depth encoder from Stage II. Experimental results on our proposed\nLunarSeg dataset and additional public real-world NPO road obstacle dataset\ndemonstrate that LuSeg achieves state-of-the-art segmentation performance for\nboth positive and negative obstacles while maintaining a high inference speed\nof approximately 57\\,Hz. We have released the implementation of our LESS\nsystem, LunarSeg dataset, and the code of LuSeg\nat:https://github.com/nubot-nudt/LuSeg.\n","authors":["Shuaifeng Jiao","Zhiwen Zeng","Zhuoqun Su","Xieyuanli Chen","Zongtan Zhou","Huimin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.11409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11404v1","updated":"2025-03-14T13:45:46Z","published":"2025-03-14T13:45:46Z","title":"Towards A Correct Usage of Cryptography in Semantic Watermarks for\n  Diffusion Models","summary":"  Semantic watermarking methods enable the direct integration of watermarks\ninto the generation process of latent diffusion models by only modifying the\ninitial latent noise. One line of approaches building on Gaussian Shading\nrelies on cryptographic primitives to steer the sampling process of the latent\nnoise. However, we identify several issues in the usage of cryptographic\ntechniques in Gaussian Shading, particularly in its proof of lossless\nperformance and key management, causing ambiguity in follow-up works, too. In\nthis work, we therefore revisit the cryptographic primitives for semantic\nwatermarking. We introduce a novel, general proof of lossless performance based\non IND\\$-CPA security for semantic watermarks. We then discuss the\nconfiguration of the cryptographic primitives in semantic watermarks with\nrespect to security, efficiency, and generation quality.\n","authors":["Jonas Thietke","Andreas Müller","Denis Lukovnikov","Asja Fischer","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2503.11404v1.pdf","comment":"8 pages, 3 figures, WMark@ICLR"},{"id":"http://arxiv.org/abs/2503.11400v1","updated":"2025-03-14T13:43:26Z","published":"2025-03-14T13:43:26Z","title":"A Framework for a Capability-driven Evaluation of Scenario Understanding\n  for Multimodal Large Language Models in Autonomous Driving","summary":"  Multimodal large language models (MLLMs) hold the potential to enhance\nautonomous driving by combining domain-independent world knowledge with\ncontext-specific language guidance. Their integration into autonomous driving\nsystems shows promising results in isolated proof-of-concept applications,\nwhile their performance is evaluated on selective singular aspects of\nperception, reasoning, or planning. To leverage their full potential a\nsystematic framework for evaluating MLLMs in the context of autonomous driving\nis required. This paper proposes a holistic framework for a capability-driven\nevaluation of MLLMs in autonomous driving. The framework structures scenario\nunderstanding along the four core capability dimensions semantic, spatial,\ntemporal, and physical. They are derived from the general requirements of\nautonomous driving systems, human driver cognition, and language-based\nreasoning. It further organises the domain into context layers, processing\nmodalities, and downstream tasks such as language-based interaction and\ndecision-making. To illustrate the framework's applicability, two exemplary\ntraffic scenarios are analysed, grounding the proposed dimensions in realistic\ndriving situations. The framework provides a foundation for the structured\nevaluation of MLLMs' potential for scenario understanding in autonomous\ndriving.\n","authors":["Tin Stribor Sohn","Philipp Reis","Maximilian Dillitzer","Johannes Bach","Jason J. Corso","Eric Sax"],"pdf_url":"https://arxiv.org/pdf/2503.11400v1.pdf","comment":"Submitted to IEEE IAVVC 2025, Under Review"},{"id":"http://arxiv.org/abs/2503.02537v2","updated":"2025-03-14T13:40:17Z","published":"2025-03-04T12:03:26Z","title":"RectifiedHR: Enable Efficient High-Resolution Image Generation via\n  Energy Rectification","summary":"  Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an straightforward and efficient solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we introduce average latent energy analysis and discover\nthat an improved classifier-free guidance hyperparameter can significantly\nenhance generation performance. Our method is entirely training-free and boasts\na simple implementation logic and efficient performance. Through extensive\ncomparisons with numerous baseline methods, our RectifiedHR demonstrates\nsuperior effectiveness and efficiency.\n","authors":["Zhen Yang","Guibao Shen","Liang Hou","Mushui Liu","Luozhou Wang","Xin Tao","Pengfei Wan","Di Zhang","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02537v2.pdf","comment":"Project Page: https://zhenyangcs.github.io/RectifiedHR-Diffusion/"},{"id":"http://arxiv.org/abs/2312.04584v3","updated":"2025-03-14T13:36:51Z","published":"2023-12-03T09:12:14Z","title":"Towards Sample-specific Backdoor Attack with Clean Labels via Attribute\n  Trigger","summary":"  Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and\nmalicious methods since they can easily circumvent most of the current backdoor\ndefenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due\nto their poisoned-label nature, where users can discover anomalies if they\ncheck the image-label relationship. In particular, we demonstrate that it is\nineffective to directly generalize existing SSBAs to their clean-label variants\nby poisoning samples solely from the target class. We reveal that it is\nprimarily due to two reasons, including \\textbf{(1)} the `antagonistic effects'\nof ground-truth features and \\textbf{(2)} the learning difficulty of\nsample-specific features. Accordingly, trigger-related features of existing\nSSBAs cannot be effectively learned under the clean-label setting due to their\nmild trigger intensity required for ensuring stealthiness. We argue that the\nintensity constraint of existing SSBAs is mostly because their trigger patterns\nare `content-irrelevant' and therefore act as `noises' for both humans and\nDNNs. Motivated by this understanding, we propose to exploit content-relevant\nfeatures, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design\nclean-label SSBAs. This new attack paradigm is dubbed backdoor attack with\nattribute trigger (BAAT). Extensive experiments are conducted on benchmark\ndatasets, which verify the effectiveness of our BAAT and its resistance to\nexisting defenses.\n","authors":["Mingyan Zhu","Yiming Li","Junfeng Guo","Tao Wei","Shu-Tao Xia","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2312.04584v3.pdf","comment":"This paper is accepted by IEEE Transactions on Dependable and Secure\n  Computing (TDSC), 2025. The first two authors contributed equally to this\n  work. 14 pages"},{"id":"http://arxiv.org/abs/2503.11392v1","updated":"2025-03-14T13:36:13Z","published":"2025-03-14T13:36:13Z","title":"Watch and Learn: Leveraging Expert Knowledge and Language for Surgical\n  Video Understanding","summary":"  Automated surgical workflow analysis is crucial for education, research, and\nclinical decision-making, but the lack of annotated datasets hinders the\ndevelopment of accurate and comprehensive workflow analysis solutions. We\nintroduce a novel approach for addressing the sparsity and heterogeneity of\nannotated training data inspired by the human learning procedure of watching\nexperts and understanding their explanations. Our method leverages a\nvideo-language model trained on alignment, denoising, and generative tasks to\nlearn short-term spatio-temporal and multimodal representations. A\ntask-specific temporal model is then used to capture relationships across\nentire videos. To achieve comprehensive video-language understanding in the\nsurgical domain, we introduce a data collection and filtering strategy to\nconstruct a large-scale pretraining dataset from educational YouTube videos. We\nthen utilize parameter-efficient fine-tuning by projecting downstream task\nannotations from publicly available surgical datasets into the language domain.\nExtensive experiments in two surgical domains demonstrate the effectiveness of\nour approach, with performance improvements of up to 7% in phase segmentation\ntasks, 8% in zero-shot phase segmentation, and comparable capabilities to\nfully-supervised models in few-shot settings. Harnessing our model's\ncapabilities for long-range temporal localization and text generation, we\npresent the first comprehensive solution for dense video captioning (DVC) of\nsurgical videos, addressing this task despite the absence of existing DVC\ndatasets in the surgical domain. We introduce a novel approach to surgical\nworkflow understanding that leverages video-language pretraining, large-scale\nvideo pretraining, and optimized fine-tuning. Our method improves performance\nover state-of-the-art techniques and enables new downstream tasks for surgical\nvideo understanding.\n","authors":["David Gastager","Ghazal Ghazaei","Constantin Patsch"],"pdf_url":"https://arxiv.org/pdf/2503.11392v1.pdf","comment":"14 pages main manuscript with 3 figures; 6 pages supplementary\n  material with 3 figures. To be presented at International Conference on\n  Information Processing in Computer-Assisted Interventions (IPCAI 2025). To be\n  published in International Journal of Computer Assisted Radiology and Surgery\n  (IJCARS)"},{"id":"http://arxiv.org/abs/2503.11389v1","updated":"2025-03-14T13:33:22Z","published":"2025-03-14T13:33:22Z","title":"Deepfake Detection of Face Images based on a Convolutional Neural\n  Network","summary":"  Fake News and especially deepfakes (generated, non-real image or video\ncontent) have become a serious topic over the last years. With the emergence of\nmachine learning algorithms it is now easier than ever before to generate such\nfake content, even for private persons. This issue of generated fake images is\nespecially critical in the context of politics and public figures. We want to\naddress this conflict by building a model based on a Convolutions Neural\nNetwork in order to detect such generated and fake images showing human\nportraits. As a basis, we use a pre-trained ResNet-50 model due to its\neffectiveness in terms of classifying images. We then adopted the base model to\nour task of classifying a single image as authentic/real or fake by adding an\nfully connected output layer containing a single neuron indicating the\nauthenticity of an image. We applied fine tuning and transfer learning to\ndevelop the model and improve its parameters. For the training process we\ncollected the image data set \"Diverse Face Fake Dataset\" containing a wide\nrange of different image manipulation methods and also diversity in terms of\nfaces visible on the images. With our final model we reached the following\noutstanding performance metrics: precision = 0.98, recall 0.96, F1-Score = 0.97\nand an area-under-curve = 0.99.\n","authors":["Lukas Kroiß","Johannes Reschke"],"pdf_url":"https://arxiv.org/pdf/2503.11389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11372v1","updated":"2025-03-14T13:17:43Z","published":"2025-03-14T13:17:43Z","title":"BEVDiffLoc: End-to-End LiDAR Global Localization in BEV View based on\n  Diffusion Model","summary":"  Localization is one of the core parts of modern robotics. Classic\nlocalization methods typically follow the retrieve-then-register paradigm,\nachieving remarkable success. Recently, the emergence of end-to-end\nlocalization approaches has offered distinct advantages, including a\nstreamlined system architecture and the elimination of the need to store\nextensive map data. Although these methods have demonstrated promising results,\ncurrent end-to-end localization approaches still face limitations in robustness\nand accuracy. Bird's-Eye-View (BEV) image is one of the most widely adopted\ndata representations in autonomous driving. It significantly reduces data\ncomplexity while preserving spatial structure and scale consistency, making it\nan ideal representation for localization tasks. However, research on BEV-based\nend-to-end localization remains notably insufficient. To fill this gap, we\npropose BEVDiffLoc, a novel framework that formulates LiDAR localization as a\nconditional generation of poses. Leveraging the properties of BEV, we first\nintroduce a specific data augmentation method to significantly enhance the\ndiversity of input data. Then, the Maximum Feature Aggregation Module and\nVision Transformer are employed to learn robust features while maintaining\nrobustness against significant rotational view variations. Finally, we\nincorporate a diffusion model that iteratively refines the learned features to\nrecover the absolute pose. Extensive experiments on the Oxford Radar RobotCar\nand NCLT datasets demonstrate that BEVDiffLoc outperforms the baseline methods.\nOur code is available at https://github.com/nubot-nudt/BEVDiffLoc.\n","authors":["Ziyue Wang","Chenghao Shi","Neng Wang","Qinghua Yu","Xieyuanli Chen","Huimin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.11372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11371v1","updated":"2025-03-14T13:15:54Z","published":"2025-03-14T13:15:54Z","title":"EMoTive: Event-guided Trajectory Modeling for 3D Motion Estimation","summary":"  Visual 3D motion estimation aims to infer the motion of 2D pixels in 3D space\nbased on visual cues. The key challenge arises from depth variation induced\nspatio-temporal motion inconsistencies, disrupting the assumptions of local\nspatial or temporal motion smoothness in previous motion estimation frameworks.\nIn contrast, event cameras offer new possibilities for 3D motion estimation\nthrough continuous adaptive pixel-level responses to scene changes. This paper\npresents EMoTive, a novel event-based framework that models spatio-temporal\ntrajectories via event-guided non-uniform parametric curves, effectively\ncharacterizing locally heterogeneous spatio-temporal motion. Specifically, we\nfirst introduce Event Kymograph - an event projection method that leverages a\ncontinuous temporal projection kernel and decouples spatial observations to\nencode fine-grained temporal evolution explicitly. For motion representation,\nwe introduce a density-aware adaptation mechanism to fuse spatial and temporal\nfeatures under event guidance, coupled with a non-uniform rational curve\nparameterization framework to adaptively model heterogeneous trajectories. The\nfinal 3D motion estimation is achieved through multi-temporal sampling of\nparametric trajectories, yielding optical flow and depth motion fields. To\nfacilitate evaluation, we introduce CarlaEvent3D, a multi-dynamic synthetic\ndataset for comprehensive validation. Extensive experiments on both this\ndataset and a real-world benchmark demonstrate the effectiveness of the\nproposed method.\n","authors":["Zengyu Wan","Wei Zhai","Yang Cao","Zhengjun Zha"],"pdf_url":"https://arxiv.org/pdf/2503.11371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10637v2","updated":"2025-03-14T13:11:59Z","published":"2025-03-13T17:59:56Z","title":"Distilling Diversity and Control in Diffusion Models","summary":"  Distilled diffusion models suffer from a critical limitation: reduced sample\ndiversity compared to their base counterparts. In this work, we uncover that\ndespite this diversity loss, distilled models retain the fundamental concept\nrepresentations of base models. We demonstrate control distillation - where\ncontrol mechanisms like Concept Sliders and LoRAs trained on base models can be\nseamlessly transferred to distilled models and vice-versa, effectively\ndistilling control without any retraining. This preservation of\nrepresentational structure prompted our investigation into the mechanisms of\ndiversity collapse during distillation. To understand how distillation affects\ndiversity, we introduce Diffusion Target (DT) Visualization, an analysis and\ndebugging tool that reveals how models predict final outputs at intermediate\nsteps. Through DT-Visualization, we identify generation artifacts,\ninconsistencies, and demonstrate that initial diffusion timesteps\ndisproportionately determine output diversity, while later steps primarily\nrefine details. Based on these insights, we introduce diversity distillation -\na hybrid inference approach that strategically employs the base model for only\nthe first critical timestep before transitioning to the efficient distilled\nmodel. Our experiments demonstrate that this simple modification not only\nrestores the diversity capabilities from base to distilled models but\nsurprisingly exceeds it, while maintaining nearly the computational efficiency\nof distilled inference, all without requiring additional training or model\nmodifications. Our code and data are available at\nhttps://distillation.baulab.info\n","authors":["Rohit Gandikota","David Bau"],"pdf_url":"https://arxiv.org/pdf/2503.10637v2.pdf","comment":"Project Page: https://distillation.baulab.info"},{"id":"http://arxiv.org/abs/2411.12523v3","updated":"2025-03-14T13:11:28Z","published":"2024-11-19T14:13:25Z","title":"Data Pruning in Generative Diffusion Models","summary":"  Data pruning is the problem of identifying a core subset that is most\nbeneficial to training and discarding the remainder. While pruning strategies\nare well studied for discriminative models like those used in classification,\nlittle research has gone into their application to generative models.\nGenerative models aim to estimate the underlying distribution of the data, so\npresumably they should benefit from larger datasets. In this work we aim to\nshed light on the accuracy of this statement, specifically answer the question\nof whether data pruning for generative diffusion models could have a positive\nimpact. Contrary to intuition, we show that eliminating redundant or noisy data\nin large datasets is beneficial particularly when done strategically. We\nexperiment with several pruning methods including recent-state-of-art methods,\nand evaluate over CelebA-HQ and ImageNet datasets. We demonstrate that a simple\nclustering method outperforms other sophisticated and computationally demanding\nmethods. We further exhibit how we can leverage clustering to balance skewed\ndatasets in an unsupervised manner to allow fair sampling for underrepresented\npopulations in the data distribution, which is a crucial problem in generative\nmodels.\n","authors":["Rania Briq","Jiangtao Wang","Stefan Kesselheim"],"pdf_url":"https://arxiv.org/pdf/2411.12523v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11368v1","updated":"2025-03-14T13:11:19Z","published":"2025-03-14T13:11:19Z","title":"PBR3DGen: A VLM-guided Mesh Generation with High-quality PBR Texture","summary":"  Generating high-quality physically based rendering (PBR) materials is\nimportant to achieve realistic rendering in the downstream tasks, yet it\nremains challenging due to the intertwined effects of materials and lighting.\nWhile existing methods have made breakthroughs by incorporating material\ndecomposition in the 3D generation pipeline, they tend to bake highlights into\nalbedo and ignore spatially varying properties of metallicity and roughness. In\nthis work, we present PBR3DGen, a two-stage mesh generation method with\nhigh-quality PBR materials that integrates the novel multi-view PBR material\nestimation model and a 3D PBR mesh reconstruction model. Specifically, PBR3DGen\nleverages vision language models (VLM) to guide multi-view diffusion, precisely\ncapturing the spatial distribution and inherent attributes of\nreflective-metalness material. Additionally, we incorporate view-dependent\nillumination-aware conditions as pixel-aware priors to enhance spatially\nvarying material properties. Furthermore, our reconstruction model reconstructs\nhigh-quality mesh with PBR materials. Experimental results demonstrate that\nPBR3DGen significantly outperforms existing methods, achieving new\nstate-of-the-art results for PBR estimation and mesh generation. More results\nand visualization can be found on our project page:\nhttps://pbr3dgen1218.github.io/.\n","authors":["Xiaokang Wei","Bowen Zhang","Xianghui Yang","Yuxuan Wang","Chunchao Guo","Xi Zhao","Yan Luximon"],"pdf_url":"https://arxiv.org/pdf/2503.11368v1.pdf","comment":"Homepage: https://pbr3dgen1218.github.io/"},{"id":"http://arxiv.org/abs/2501.08983v2","updated":"2025-03-14T12:54:19Z","published":"2025-01-15T17:59:56Z","title":"Compositional Generative Model of Unbounded 4D Cities","summary":"  3D scene generation has garnered growing attention in recent years and has\nmade significant progress. Generating 4D cities is more challenging than 3D\nscenes due to the presence of structurally complex, visually diverse objects\nlike buildings and vehicles, and heightened human sensitivity to distortions in\nurban environments. To tackle these issues, we propose CityDreamer4D, a\ncompositional generative model specifically tailored for generating unbounded\n4D cities. Our main insights are 1) 4D city generation should separate dynamic\nobjects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)\nall objects in the 4D scene should be composed of different types of neural\nfields for buildings, vehicles, and background stuff. Specifically, we propose\nTraffic Scenario Generator and Unbounded Layout Generator to produce dynamic\ntraffic scenarios and static city layouts using a highly compact BEV\nrepresentation. Objects in 4D cities are generated by combining stuff-oriented\nand instance-oriented neural fields for background stuff, buildings, and\nvehicles. To suit the distinct characteristics of background stuff and\ninstances, the neural fields employ customized generative hash grids and\nperiodic positional embeddings as scene parameterizations. Furthermore, we\noffer a comprehensive suite of datasets for city generation, including OSM,\nGoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world\ncity layouts, while the Google Earth and CityTopia datasets deliver\nlarge-scale, high-quality city imagery complete with 3D instance annotations.\nLeveraging its compositional design, CityDreamer4D supports a range of\ndownstream applications, such as instance editing, city stylization, and urban\nsimulation, while delivering state-of-the-art performance in generating\nrealistic 4D cities.\n","authors":["Haozhe Xie","Zhaoxi Chen","Fangzhou Hong","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2501.08983v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11360v1","updated":"2025-03-14T12:53:37Z","published":"2025-03-14T12:53:37Z","title":"PARIC: Probabilistic Attention Regularization for Language Guided Image\n  Classification from Pre-trained Vison Language Models","summary":"  Language-guided attention frameworks have significantly enhanced both\ninterpretability and performance in image classification; however, the reliance\non deterministic embeddings from pre-trained vision-language foundation models\nto generate reference attention maps frequently overlooks the intrinsic\nmultivaluedness and ill-posed characteristics of cross-modal mappings. To\naddress these limitations, we introduce PARIC, a probabilistic framework for\nguiding visual attention via language specifications. Our approach enables\npre-trained vision-language models to generate probabilistic reference\nattention maps, which align textual and visual modalities more effectively\nwhile incorporating uncertainty estimates, as compared to their deterministic\ncounterparts. Experiments on benchmark test problems demonstrate that PARIC\nenhances prediction accuracy, mitigates bias, ensures consistent predictions,\nand improves robustness across various datasets.\n","authors":["Mayank Nautiyal","Stela Arranz Gheorghe","Kristiana Stefa","Li Ju","Ida-Maria Sintorn","Prashant Singh"],"pdf_url":"https://arxiv.org/pdf/2503.11360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11352v1","updated":"2025-03-14T12:40:43Z","published":"2025-03-14T12:40:43Z","title":"Enhancing Hand Palm Motion Gesture Recognition by Eliminating Reference\n  Frame Bias via Frame-Invariant Similarity Measures","summary":"  The ability of robots to recognize human gestures facilitates a natural and\naccessible human-robot collaboration. However, most work in gesture recognition\nremains rooted in reference frame-dependent representations. This poses a\nchallenge when reference frames vary due to different work cell layouts,\nimprecise frame calibrations, or other environmental changes. This paper\ninvestigated the use of invariant trajectory descriptors for robust hand palm\nmotion gesture recognition under reference frame changes. First, a novel\ndataset of recorded Hand Palm Motion (HPM) gestures is introduced. The motion\ngestures in this dataset were specifically designed to be distinguishable\nwithout dependence on specific reference frames or directional cues.\nAfterwards, multiple invariant trajectory descriptor approaches were\nbenchmarked to assess how their performances generalize to this novel HPM\ndataset. After this offline benchmarking, the best scoring approach is\nvalidated for online recognition by developing a real-time Proof of Concept\n(PoC). In this PoC, hand palm motion gestures were used to control the\nreal-time movement of a manipulator arm. The PoC demonstrated a high\nrecognition reliability in real-time operation, achieving an $F_1$-score of\n92.3%. This work demonstrates the effectiveness of the invariant descriptor\napproach as a standalone solution. Moreover, we believe that the invariant\ndescriptor approach can also be utilized within other state-of-the-art pattern\nrecognition and learning systems to improve their robustness against reference\nframe variations.\n","authors":["Arno Verduyn","Maxim Vochten","Joris De Schutter"],"pdf_url":"https://arxiv.org/pdf/2503.11352v1.pdf","comment":"8 pages, 4 figures, this work has been submitted as a conference\n  paper for consideration in the 2025 IEEE International Conference on\n  Automation Science and Engineering (CASE), the content in this preprint is\n  identical to the version submitted for peer review"},{"id":"http://arxiv.org/abs/2403.03551v4","updated":"2025-03-14T12:30:28Z","published":"2024-03-06T08:51:09Z","title":"Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers","summary":"  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n","authors":["Tim Selig","Thomas März","Martin Storath","Andreas Weinmann"],"pdf_url":"https://arxiv.org/pdf/2403.03551v4.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.02063v2","updated":"2025-03-14T12:29:29Z","published":"2025-03-03T21:27:38Z","title":"V$^2$Dial: Unification of Video and Visual Dialog via Multimodal Experts","summary":"  We present V$^2$Dial - a novel expert-based model specifically geared towards\nsimultaneously handling image and video input data for multimodal\nconversational tasks. Current multimodal models primarily focus on simpler\ntasks (e.g., VQA, VideoQA, video-text retrieval) and often neglect the more\nchallenging conversational counterparts, such as video and visual/image dialog.\nMoreover, works on both conversational tasks evolved separately from each other\ndespite their apparent similarities limiting their applicability potential. To\nthis end, we propose to unify both tasks using a single model that for the\nfirst time jointly learns the spatial and temporal features of images and\nvideos by routing them through dedicated experts and aligns them using matching\nand contrastive learning techniques. Furthermore, we systemically study the\ndomain shift between the two tasks by investigating whether and to what extent\nthese seemingly related tasks can mutually benefit from their respective\ntraining data. Extensive evaluations on the widely used video and visual dialog\ndatasets of AVSD and VisDial show that our model achieves new state-of-the-art\nresults across four benchmarks both in zero-shot and fine-tuning settings.\n","authors":["Adnen Abdessaied","Anna Rohrbach","Marcus Rohrbach","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2503.02063v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2310.14560v2","updated":"2025-03-14T12:25:32Z","published":"2023-10-23T04:24:31Z","title":"Polyhedral Surface: Self-supervised Point Cloud Reconstruction Based on\n  Polyhedral Surface","summary":"  Point cloud reconstruction from raw point cloud has been an important topic\nin computer graphics for decades, especially due to its high demand in modeling\nand rendering applications. An important way to solve this problem is\nestablishing a local geometry to fit the local curve. However, previous methods\nbuild either a local plane or polynomial curve. Local plane brings the loss of\nsharp feature and the boundary artefacts on open surface. Polynomial curve is\nhard to combine with neural network due to the local coordinate consistent\nproblem. To address this, we propose a novel polyhedral surface to represent\nlocal surface. This method provides more flexible to represent sharp feature\nand surface boundary on open surface. It does not require any local coordinate\nsystem, which is important when introducing neural networks. Specifically, we\nuse normals to construct the polyhedral surface, including both dihedral and\ntrihedral surfaces using 2 and 3 normals, respectively. Our method achieves\nstate-of-the-art results on three commonly used datasets (ShapeNetCore, ABC,\nand ScanNet). Code will be released upon acceptance.\n","authors":["Hui Tian","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2310.14560v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15277v2","updated":"2025-03-14T12:22:49Z","published":"2024-11-22T15:21:38Z","title":"Foundation Cures Personalization: Improving Personalized Models' Prompt\n  Consistency via Hidden Foundation Knowledge","summary":"  Facial personalization faces challenges to maintain identity fidelity without\ndisrupting the foundation model's prompt consistency. The mainstream\npersonalization models employ identity embedding to integrate identity\ninformation within the cross-attention mechanisms of UNet. However, our\npreliminary experimental findings reveal that identity embeddings compromise\nthe effectiveness of other tokens in the prompt, thereby limiting high prompt\nconsistency and controllability. Moreover, by deactivating identity embedding,\npersonalization models still demonstrate the underlying foundation models'\nability to control facial attributes precisely. It suggests that such\nfoundation models' knowledge can be leveraged to \\textbf{cure} the ill-aligned\nprompt consistency of personalization models. Building upon these insights, we\npropose \\textbf{FreeCure}, a framework that improves the prompt consistency of\npersonalization models with their latent foundation models' knowledge. First,\nby setting a dual inference paradigm with/without identity embedding, we\nidentify attributes (\\textit{e.g.}, hair, accessories, etc.) for enhancements.\nSecond, we introduce a novel foundation-aware self-attention module, coupled\nwith an inversion-based process to bring well-aligned attribute information to\nthe personalization process. Our approach is \\textbf{training-free}, and can\neffectively enhance a wide array of facial attributes in a non-intrusive\nmanner; and it can be seamlessly integrated into existing popular\npersonalization models, without harming their well-trained modules. FreeCure\nhas demonstrated significant improvements in prompt consistency across a\ndiverse set of state-of-the-art facial personalization models while maintaining\nthe integrity of original identity fidelity. The project page is available\n\\href{https://github.com/YIYANGCAI/freecure-project-page}{here}.\n","authors":["Yiyang Cai","Zhengkai Jiang","Yulong Liu","Chunyang Jiang","Wei Xue","Wenhan Luo","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2411.15277v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2503.11345v1","updated":"2025-03-14T12:21:26Z","published":"2025-03-14T12:21:26Z","title":"EgoSplat: Open-Vocabulary Egocentric Scene Understanding with Language\n  Embedded 3D Gaussian Splatting","summary":"  Egocentric scenes exhibit frequent occlusions, varied viewpoints, and dynamic\ninteractions compared to typical scene understanding tasks. Occlusions and\nvaried viewpoints can lead to multi-view semantic inconsistencies, while\ndynamic objects may act as transient distractors, introducing artifacts into\nsemantic feature modeling. To address these challenges, we propose EgoSplat, a\nlanguage-embedded 3D Gaussian Splatting framework for open-vocabulary\negocentric scene understanding. A multi-view consistent instance feature\naggregation method is designed to leverage the segmentation and tracking\ncapabilities of SAM2 to selectively aggregate complementary features across\nviews for each instance, ensuring precise semantic representation of scenes.\nAdditionally, an instance-aware spatial-temporal transient prediction module is\nconstructed to improve spatial integrity and temporal continuity in predictions\nby incorporating spatial-temporal associations across multi-view instances,\neffectively reducing artifacts in the semantic reconstruction of egocentric\nscenes. EgoSplat achieves state-of-the-art performance in both localization and\nsegmentation tasks on two datasets, outperforming existing methods with a 8.2%\nimprovement in localization accuracy and a 3.7% improvement in segmentation\nmIoU on the ADT dataset, and setting a new benchmark in open-vocabulary\negocentric scene understanding. The code will be made publicly available.\n","authors":["Di Li","Jie Feng","Jiahao Chen","Weisheng Dong","Guanbin Li","Guangming Shi","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2503.11345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11343v1","updated":"2025-03-14T12:18:33Z","published":"2025-03-14T12:18:33Z","title":"FG-DFPN: Flow Guided Deformable Frame Prediction Network","summary":"  Video frame prediction remains a fundamental challenge in computer vision\nwith direct implications for autonomous systems, video compression, and media\nsynthesis. We present FG-DFPN, a novel architecture that harnesses the synergy\nbetween optical flow estimation and deformable convolutions to model complex\nspatio-temporal dynamics. By guiding deformable sampling with motion cues, our\napproach addresses the limitations of fixed-kernel networks when handling\ndiverse motion patterns. The multi-scale design enables FG-DFPN to\nsimultaneously capture global scene transformations and local object movements\nwith remarkable precision. Our experiments demonstrate that FG-DFPN achieves\nstate-of-the-art performance on eight diverse MPEG test sequences,\noutperforming existing methods by 1dB PSNR while maintaining competitive\ninference speeds. The integration of motion cues with adaptive geometric\ntransformations makes FG-DFPN a promising solution for next-generation video\nprocessing systems that require high-fidelity temporal predictions. The model\nand instructions to reproduce our results will be released at:\nhttps://github.com/KUIS-AI-Tekalp-Research Group/frame-prediction\n","authors":["M. Akın Yılmaz","Ahmet Bilican","A. Murat Tekalp"],"pdf_url":"https://arxiv.org/pdf/2503.11343v1.pdf","comment":"Submitted to 33th European Signal Processing Conference (EUSIPCO)\n  2025"},{"id":"http://arxiv.org/abs/2503.11342v1","updated":"2025-03-14T12:18:11Z","published":"2025-03-14T12:18:11Z","title":"Road Rage Reasoning with Vision-language Models (VLMs): Task Definition\n  and Evaluation Dataset","summary":"  Road rage, triggered by driving-related stimuli such as traffic congestion\nand aggressive driving, poses a significant threat to road safety. Previous\nresearch on road rage regulation has primarily focused on response suppression,\nlacking proactive prevention capabilities. With the advent of Vision-Language\nModels (VLMs), it has become possible to reason about trigger events visually\nand then engage in dialog-based comforting before drivers' anger escalates. To\nthis end, we propose the road rage reasoning task, along with a finely\nannotated test dataset and evaluation metrics, to assess the capabilities of\ncurrent mainstream VLMs in scene understanding, event recognition, and road\nrage reasoning. The results indicate that current VLMs exhibit significant\nshortcomings in scene understanding within the visual modality, as well as in\ncomprehending the spatial relationships between objects in the textual\nmodality. Improving VLMs' performance in these areas will greatly benefit\ndownstream tasks like antecedent-focused road rage regulation.\n","authors":["Yibing Weng","Yu Gu","Fuji Ren"],"pdf_url":"https://arxiv.org/pdf/2503.11342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11341v1","updated":"2025-03-14T12:15:20Z","published":"2025-03-14T12:15:20Z","title":"Self-Supervised Pretraining for Fine-Grained Plankton Recognition","summary":"  Plankton recognition is an important computer vision problem due to\nplankton's essential role in ocean food webs and carbon capture, highlighting\nthe need for species-level monitoring. However, this task is challenging due to\nits fine-grained nature and dataset shifts caused by different imaging\ninstruments and varying species distributions. As new plankton image datasets\nare collected at an increasing pace, there is a need for general plankton\nrecognition models that require minimal expert effort for data labeling. In\nthis work, we study large-scale self-supervised pretraining for fine-grained\nplankton recognition. We first employ masked autoencoding and a large volume of\ndiverse plankton image data to pretrain a general-purpose plankton image\nencoder. Then we utilize fine-tuning to obtain accurate plankton recognition\nmodels for new datasets with a very limited number of labeled training images.\nOur experiments show that self-supervised pretraining with diverse plankton\ndata clearly increases plankton recognition accuracy compared to standard\nImageNet pretraining when the amount of training data is limited. Moreover, the\naccuracy can be further improved when unlabeled target data is available and\nutilized during the pretraining.\n","authors":["Joona Kareinen","Tuomas Eerola","Kaisa Kraft","Lasse Lensu","Sanna Suikkanen","Heikki Kälviäinen"],"pdf_url":"https://arxiv.org/pdf/2503.11341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11335v1","updated":"2025-03-14T12:03:29Z","published":"2025-03-14T12:03:29Z","title":"APLA: A Simple Adaptation Method for Vision Transformers","summary":"  Existing adaptation techniques typically require architectural modifications\nor added parameters, leading to high computational costs and complexity. We\nintroduce Attention Projection Layer Adaptation (APLA), a simple approach to\nadapt vision transformers (ViTs) without altering the architecture or adding\nparameters. Through a systematic analysis, we find that the layer immediately\nafter the attention mechanism is crucial for adaptation. By updating only this\nprojection layer, or even just a random subset of this layer's weights, APLA\nachieves state-of-the-art performance while reducing GPU memory usage by up to\n52.63% and training time by up to 43.0%, with no extra cost at inference.\nAcross 46 datasets covering a variety of tasks including scene classification,\nmedical imaging, satellite imaging, and fine-grained classification, APLA\nconsistently outperforms 17 other leading adaptation methods, including full\nfine-tuning, on classification, segmentation, and detection tasks. The code is\navailable at https://github.com/MoeinSorkhei/APLA.\n","authors":["Moein Sorkhei","Emir Konuk","Kevin Smith","Christos Matsoukas"],"pdf_url":"https://arxiv.org/pdf/2503.11335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11332v1","updated":"2025-03-14T12:00:22Z","published":"2025-03-14T12:00:22Z","title":"Advancements in Real-Time Oncology Diagnosis: Harnessing AI and Image\n  Fusion Techniques","summary":"  Real-time computer-aided diagnosis using artificial intelligence (AI), with\nimages, can help oncologists diagnose cancer with high accuracy and in an early\nphase. We reviewed real-time AI-based analyzed images for decision-making in\ndifferent cancer types. This paper provides insights into the present and\nfuture potential of real-time imaging and image fusion. It explores various\nreal-time techniques, encompassing technical solutions, AI-based imaging, and\nimage fusion diagnosis across multiple anatomical areas, and electromagnetic\nneedle tracking. To provide a thorough overview, this paper discusses\nultrasound image fusion, real-time in vivo cancer diagnosis with different\nspectroscopic techniques, different real-time optical imaging-based cancer\ndiagnosis techniques, elastography-based cancer diagnosis, cervical cancer\ndetection using neuromorphic architectures, different fluorescence image-based\ncancer diagnosis techniques, and hyperspectral imaging-based cancer diagnosis.\nWe close by offering a more futuristic overview to solve existing problems in\nreal-time image-based cancer diagnosis.\n","authors":["Leila Bagheriye","Johan Kwisthout"],"pdf_url":"https://arxiv.org/pdf/2503.11332v1.pdf","comment":"This paper is under review"},{"id":"http://arxiv.org/abs/2503.11331v1","updated":"2025-03-14T11:59:23Z","published":"2025-03-14T11:59:23Z","title":"Cardiomyopathy Diagnosis Model from Endomyocardial Biopsy Specimens:\n  Appropriate Feature Space and Class Boundary in Small Sample Size Data","summary":"  As the number of patients with heart failure increases, machine learning (ML)\nhas garnered attention in cardiomyopathy diagnosis, driven by the shortage of\npathologists. However, endomyocardial biopsy specimens are often small sample\nsize and require techniques such as feature extraction and dimensionality\nreduction. This study aims to determine whether texture features are effective\nfor feature extraction in the pathological diagnosis of cardiomyopathy.\nFurthermore, model designs that contribute toward improving generalization\nperformance are examined by applying feature selection (FS) and dimensional\ncompression (DC) to several ML models. The obtained results were verified by\nvisualizing the inter-class distribution differences and conducting statistical\nhypothesis testing based on texture features. Additionally, they were evaluated\nusing predictive performance across different model designs with varying\ncombinations of FS and DC (applied or not) and decision boundaries. The\nobtained results confirmed that texture features may be effective for the\npathological diagnosis of cardiomyopathy. Moreover, when the ratio of features\nto the sample size is high, a multi-step process involving FS and DC improved\nthe generalization performance, with the linear kernel support vector machine\nachieving the best results. This process was demonstrated to be potentially\neffective for models with reduced complexity, regardless of whether the\ndecision boundaries were linear, curved, perpendicular, or parallel to the\naxes. These findings are expected to facilitate the development of an effective\ncardiomyopathy diagnostic model for its rapid adoption in medical practice.\n","authors":["Masaya Mori","Yuto Omae","Yutaka Koyama","Kazuyuki Hara","Jun Toyotani","Yasuo Okumura","Hiroyuki Hao"],"pdf_url":"https://arxiv.org/pdf/2503.11331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11329v1","updated":"2025-03-14T11:57:34Z","published":"2025-03-14T11:57:34Z","title":"Colour Morphological Distance Ordering based on the Log-Exp-Supremum","summary":"  Mathematical morphology, a field within image processing, includes various\nfilters that either highlight, modify, or eliminate certain information in\nimages based on an application's needs. Key operations in these filters are\ndilation and erosion, which determine the supremum or infimum for each pixel\nwith respect to an order of the tonal values over a subset of the image\nsurrounding the pixel. This subset is formed by a structuring element at the\nspecified pixel, which weighs the tonal values. Unlike grey-scale morphology,\nwhere tonal order is clearly defined, colour morphology lacks a definitive\ntotal order. As no method fully meets all desired properties for colour,\nbecause of this difficulty, some limitations are always present.\n  This paper shows how to combine the theory of the log-exp-supremum of colour\nmatrices that employs the Loewner semi-order with a well-known colour distance\napproach in the form of a pre-ordering. The log-exp-supremum will therefore\nserve as the reference colour for determining the colour distance. To the\nresulting pre-ordering with respect to these distance values, we add a\nlexicographic cascade to ensure a total order and a unique result. The\nobjective of this approach is to identify the original colour within the\nstructuring element that most closely resembles a supremum, which fulfils a\nnumber of desired properties. Consequently, this approach avoids the\nfalse-colour problem. The behaviour of the introduced operators is illustrated\nby application examples of dilation and closing for synthetic and natural\nimages.\n","authors":["Marvin Kahra","Michael Breuß"],"pdf_url":"https://arxiv.org/pdf/2503.11329v1.pdf","comment":"13 pages, 13 figures, submitted to SSVM 2025"},{"id":"http://arxiv.org/abs/2503.11328v1","updated":"2025-03-14T11:56:37Z","published":"2025-03-14T11:56:37Z","title":"TransiT: Transient Transformer for Non-line-of-sight Videography","summary":"  High quality and high speed videography using Non-Line-of-Sight (NLOS)\nimaging benefit autonomous navigation, collision prevention, and post-disaster\nsearch and rescue tasks. Current solutions have to balance between the frame\nrate and image quality. High frame rates, for example, can be achieved by\nreducing either per-point scanning time or scanning density, but at the cost of\nlowering the information density at individual frames. Fast scanning process\nfurther reduces the signal-to-noise ratio and different scanning systems\nexhibit different distortion characteristics. In this work, we design and\nemploy a new Transient Transformer architecture called TransiT to achieve\nreal-time NLOS recovery under fast scans. TransiT directly compresses the\ntemporal dimension of input transients to extract features, reducing\ncomputation costs and meeting high frame rate requirements. It further adopts a\nfeature fusion mechanism as well as employs a spatial-temporal Transformer to\nhelp capture features of NLOS transient videos. Moreover, TransiT applies\ntransfer learning to bridge the gap between synthetic and real-measured data.\nIn real experiments, TransiT manages to reconstruct from sparse transients of\n$16 \\times 16$ measured at an exposure time of 0.4 ms per point to NLOS videos\nat a $64 \\times 64$ resolution at 10 frames per second. We will make our code\nand dataset available to the community.\n","authors":["Ruiqian Li","Siyuan Shen","Suan Xia","Ziheng Wang","Xingyue Peng","Chengxuan Song","Yingsheng Zhu","Tao Wu","Shiying Li","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2503.11328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19097v4","updated":"2025-03-14T11:56:29Z","published":"2024-05-29T14:01:40Z","title":"A study of why we need to reassess full reference image quality\n  assessment with medical images","summary":"  Image quality assessment (IQA) is indispensable in clinical practice to\nensure high standards, as well as in the development stage of machine learning\nalgorithms that operate on medical images. The popular full reference (FR) IQA\nmeasures PSNR and SSIM are known and tested for working successfully in many\nnatural imaging tasks, but discrepancies in medical scenarios have been\nreported in the literature, highlighting the gap between development and actual\nclinical application. Such inconsistencies are not surprising, as medical\nimages have very different properties than natural images, and PSNR and SSIM\nhave neither been targeted nor properly tested for medical images. This may\ncause unforeseen problems in clinical applications due to wrong judgment of\nnovel methods. This paper provides a structured and comprehensive overview of\nexamples where PSNR and SSIM prove to be unsuitable for the assessment of novel\nalgorithms using different kinds of medical images, including real-world MRI,\nCT, OCT, X-Ray, digital pathology and photoacoustic imaging data. Therefore,\nimprovement is urgently needed in particular in this era of AI to increase\nreliability and explainability in machine learning for medical imaging and\nbeyond. Lastly, we will provide ideas for future research as well as suggesting\nguidelines for the usage of FR-IQA measures applied to medical images.\n","authors":["Anna Breger","Ander Biguri","Malena Sabaté Landman","Ian Selby","Nicole Amberg","Elisabeth Brunner","Janek Gröhl","Sepideh Hatamikia","Clemens Karner","Lipeng Ning","Sören Dittmer","Michael Roberts","AIX-COVNET Collaboration","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2405.19097v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11324v1","updated":"2025-03-14T11:45:10Z","published":"2025-03-14T11:45:10Z","title":"Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative\n  Watermarking","summary":"  With the success of autoregressive learning in large language models, it has\nbecome a dominant approach for text-to-image generation, offering high\nefficiency and visual quality. However, invisible watermarking for visual\nautoregressive (VAR) models remains underexplored, despite its importance in\nmisuse prevention. Existing watermarking methods, designed for diffusion\nmodels, often struggle to adapt to the sequential nature of VAR models. To\nbridge this gap, we propose Safe-VAR, the first watermarking framework\nspecifically designed for autoregressive text-to-image generation. Our study\nreveals that the timing of watermark injection significantly impacts generation\nquality, and watermarks of different complexities exhibit varying optimal\ninjection times. Motivated by this observation, we propose an Adaptive Scale\nInteraction Module, which dynamically determines the optimal watermark\nembedding strategy based on the watermark information and the visual\ncharacteristics of the generated image. This ensures watermark robustness while\nminimizing its impact on image quality. Furthermore, we introduce a Cross-Scale\nFusion mechanism, which integrates mixture of both heads and experts to\neffectively fuse multi-resolution features and handle complex interactions\nbetween image content and watermark patterns. Experimental results demonstrate\nthat Safe-VAR achieves state-of-the-art performance, significantly surpassing\nexisting counterparts regarding image quality, watermarking fidelity, and\nrobustness against perturbations. Moreover, our method exhibits strong\ngeneralization to an out-of-domain watermark dataset QR Codes.\n","authors":["Ziyi Wang","Songbai Tan","Gang Xu","Xuerui Qiu","Hongbin Xu","Xin Meng","Ming Li","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2503.11324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11321v1","updated":"2025-03-14T11:41:33Z","published":"2025-03-14T11:41:33Z","title":"Leveraging Diffusion Knowledge for Generative Image Compression with\n  Fractal Frequency-Aware Band Learning","summary":"  By optimizing the rate-distortion-realism trade-off, generative image\ncompression approaches produce detailed, realistic images instead of the only\nsharp-looking reconstructions produced by rate-distortion-optimized models. In\nthis paper, we propose a novel deep learning-based generative image compression\nmethod injected with diffusion knowledge, obtaining the capacity to recover\nmore realistic textures in practical scenarios. Efforts are made from three\nperspectives to navigate the rate-distortion-realism trade-off in the\ngenerative image compression task. First, recognizing the strong connection\nbetween image texture and frequency-domain characteristics, we design a Fractal\nFrequency-Aware Band Image Compression (FFAB-IC) network to effectively capture\nthe directional frequency components inherent in natural images. This network\nintegrates commonly used fractal band feature operations within a neural\nnon-linear mapping design, enhancing its ability to retain essential given\ninformation and filter out unnecessary details. Then, to improve the visual\nquality of image reconstruction under limited bandwidth, we integrate diffusion\nknowledge into the encoder and implement diffusion iterations into the decoder\nprocess, thus effectively recovering lost texture details. Finally, to fully\nleverage the spatial and frequency intensity information, we incorporate\nfrequency- and content-aware regularization terms to regularize the training of\nthe generative image compression network. Extensive experiments in quantitative\nand qualitative evaluations demonstrate the superiority of the proposed method,\nadvancing the boundaries of achievable distortion-realism pairs, i.e., our\nmethod achieves better distortions at high realism and better realism at low\ndistortion than ever before.\n","authors":["Lingyu Zhu","Xiangrui Zeng","Bolin Chen","Peilin Chen","Yung-Hui Li","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11318v1","updated":"2025-03-14T11:35:36Z","published":"2025-03-14T11:35:36Z","title":"Open-Set Plankton Recognition","summary":"  This paper considers open-set recognition (OSR) of plankton images. Plankton\ninclude a diverse range of microscopic aquatic organisms that have an important\nrole in marine ecosystems as primary producers and as a base of food webs.\nGiven their sensitivity to environmental changes, fluctuations in plankton\npopulations offer valuable information about oceans' health and climate change\nmotivating their monitoring. Modern automatic plankton imaging devices enable\nthe collection of large-scale plankton image datasets, facilitating\nspecies-level analysis. Plankton species recognition can be seen as an image\nclassification task and is typically solved using deep learning-based image\nrecognition models. However, data collection in real aquatic environments\nresults in imaging devices capturing a variety of non-plankton particles and\nplankton species not present in the training set. This creates a challenging\nfine-grained OSR problem, characterized by subtle differences between\ntaxonomically close plankton species. We address this challenge by conducting\nextensive experiments on three OSR approaches using both phyto- and zooplankton\nimages analyzing also on the effect of the rejection thresholds for OSR. The\nresults demonstrate that high OSR accuracy can be obtained promoting the use of\nthese methods in operational plankton research. We have made the data publicly\navailable to the research community.\n","authors":["Joona Kareinen","Annaliina Skyttä","Tuomas Eerola","Kaisa Kraft","Lasse Lensu","Sanna Suikkanen","Maiju Lehtiniemi","Heikki Kälviäinen"],"pdf_url":"https://arxiv.org/pdf/2503.11318v1.pdf","comment":"ECCV 2024, OOD-CV workshop paper"},{"id":"http://arxiv.org/abs/2502.15363v2","updated":"2025-03-14T11:33:44Z","published":"2025-02-21T10:22:08Z","title":"M2LADS Demo: A System for Generating Multimodal Learning Analytics\n  Dashboards","summary":"  We present a demonstration of a web-based system called M2LADS (\"System for\nGenerating Multimodal Learning Analytics Dashboards\"), designed to integrate,\nsynchronize, visualize, and analyze multimodal data recorded during\ncomputer-based learning sessions with biosensors. This system presents a range\nof biometric and behavioral data on web-based dashboards, providing detailed\ninsights into various physiological and activity-based metrics. The multimodal\ndata visualized include electroencephalogram (EEG) data for assessing attention\nand brain activity, heart rate metrics, eye-tracking data to measure visual\nattention, webcam video recordings, and activity logs of the monitored tasks.\nM2LADS aims to assist data scientists in two key ways: (1) by providing a\ncomprehensive view of participants' experiences, displaying all data\ncategorized by the activities in which participants are engaged, and (2) by\nsynchronizing all biosignals and videos, facilitating easier data relabeling if\nany activity information contains errors.\n","authors":["Alvaro Becerra","Roberto Daza","Ruth Cobos","Aythami Morales","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2502.15363v2.pdf","comment":"Published in the Workshop on Innovation and Responsibility in\n  AI-Supported Education (iRAISE25) at AAAI 2025"},{"id":"http://arxiv.org/abs/2403.17064v2","updated":"2025-03-14T11:33:08Z","published":"2024-03-25T18:00:42Z","title":"Continuous, Subject-Specific Attribute Control in T2I Models by\n  Identifying Semantic Directions","summary":"  Recent advances in text-to-image (T2I) diffusion models have significantly\nimproved the quality of generated images. However, providing efficient control\nover individual subjects, particularly the attributes characterizing them,\nremains a key challenge. While existing methods have introduced mechanisms to\nmodulate attribute expression, they typically provide either detailed,\nobject-specific localization of such a modification or full-scale fine-grained,\nnuanced control of attributes. No current approach offers both simultaneously,\nresulting in a gap when trying to achieve precise continuous and\nsubject-specific attribute modulation in image generation. In this work, we\ndemonstrate that token-level directions exist within commonly used CLIP text\nembeddings that enable fine-grained, subject-specific control of high-level\nattributes in T2I models. We introduce two methods to identify these\ndirections: a simple, optimization-free technique and a learning-based approach\nthat utilizes the T2I model to characterize semantic concepts more\nspecifically. Our methods allow the augmentation of the prompt text input,\nenabling fine-grained control over multiple attributes of individual subjects\nsimultaneously, without requiring any modifications to the diffusion model\nitself. This approach offers a unified solution that fills the gap between\nglobal and localized control, providing competitive flexibility and precision\nin text-guided image generation. Project page:\nhttps://compvis.github.io/attribute-control. Code is available at\nhttps://github.com/CompVis/attribute-control.\n","authors":["Stefan Andreas Baumann","Felix Krause","Michael Neumayr","Nick Stracke","Melvin Sevi","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.17064v2.pdf","comment":"CVPR 2025. Project page: https://compvis.github.io/attribute-control"},{"id":"http://arxiv.org/abs/2502.19691v2","updated":"2025-03-14T11:32:24Z","published":"2025-02-27T02:02:58Z","title":"Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set\n  Annotation: An Energy-Based Approach","summary":"  Active learning (AL), which iteratively queries the most informative examples\nfrom a large pool of unlabeled candidates for model training, faces significant\nchallenges in the presence of open-set classes. Existing methods either\nprioritize query examples likely to belong to known classes, indicating low\nepistemic uncertainty (EU), or focus on querying those with highly uncertain\npredictions, reflecting high aleatoric uncertainty (AU). However, they both\nyield suboptimal performance, as low EU corresponds to limited useful\ninformation, and closed-set AU metrics for unknown class examples are less\nmeaningful. In this paper, we propose an Energy-based Active Open-set\nAnnotation (EAOA) framework, which effectively integrates EU and AU to achieve\nsuperior performance. EAOA features a $(C+1)$-class detector and a target\nclassifier, incorporating an energy-based EU measure and a margin-based energy\nloss designed for the detector, alongside an energy-based AU measure for the\ntarget classifier. Another crucial component is the target-driven adaptive\nsampling strategy. It first forms a smaller candidate set with low EU scores to\nensure closed-set properties, making AU metrics meaningful. Subsequently,\nexamples with high AU scores are queried to form the final query set, with the\ncandidate set size adjusted adaptively. Extensive experiments show that EAOA\nachieves state-of-the-art performance while maintaining high query precision\nand low training overhead. The code is available at\nhttps://github.com/chenchenzong/EAOA.\n","authors":["Chen-Chen Zong","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19691v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11315v1","updated":"2025-03-14T11:31:30Z","published":"2025-03-14T11:31:30Z","title":"MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens","summary":"  Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%.\n","authors":["Jeong Hun Yeo","Hyeongseop Rha","Se Jin Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2503.11315v1.pdf","comment":"The code and models are available\n  https://github.com/JeongHun0716/MMS-LLaMA"},{"id":"http://arxiv.org/abs/2407.07041v2","updated":"2025-03-14T11:31:15Z","published":"2024-07-09T17:03:57Z","title":"Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack","summary":"  The vast accessibility of Synthetic Aperture Radar (SAR) images through\nonline portals has propelled the research across various fields. This\nwidespread use and easy availability have unfortunately made SAR data\nsusceptible to malicious alterations, such as local editing applied to the\nimages for inserting or covering the presence of sensitive targets.\nVulnerability is further emphasized by the fact that most SAR products, despite\ntheir original complex nature, are often released as amplitude-only\ninformation, allowing even inexperienced attackers to edit and easily alter the\npixel content. To contrast malicious manipulations, in the last years the\nforensic community has begun to dig into the SAR manipulation issue, proposing\ndetectors that effectively localize the tampering traces in amplitude images.\nNonetheless, in this paper we demonstrate that an expert practitioner can\nexploit the complex nature of SAR data to obscure any signs of manipulation\nwithin a locally altered amplitude image. We refer to this approach as a\ncounter-forensic attack. To achieve the concealment of manipulation traces, the\nattacker can simulate a re-acquisition of the manipulated scene by the SAR\nsystem that initially generated the pristine image. In doing so, the attacker\ncan obscure any evidence of manipulation, making it appear as if the image was\nlegitimately produced by the system. This attack has unique features that make\nit both highly generalizable and relatively easy to apply. First, it is a\nblack-box attack, meaning it is not designed to deceive a specific forensic\ndetector. Furthermore, it does not require a training phase and is not based on\nadversarial operations. We assess the effectiveness of the proposed\ncounter-forensic approach across diverse scenarios, examining various\nmanipulation operations.\n","authors":["Sara Mandelli","Edoardo Daniele Cannas","Paolo Bestagini","Stefano Tebaldini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2407.07041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03859v2","updated":"2025-03-14T11:16:30Z","published":"2024-12-05T04:09:47Z","title":"CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative\n  Layout-to-Image Generation","summary":"  Diffusion models have been recognized for their ability to generate images\nthat are not only visually appealing but also of high artistic quality. As a\nresult, Layout-to-Image (L2I) generation has been proposed to leverage\nregion-specific positions and descriptions to enable more precise and\ncontrollable generation. However, previous methods primarily focus on\nUNet-based models (e.g., SD1.5 and SDXL), and limited effort has explored\nMultimodal Diffusion Transformers (MM-DiTs), which have demonstrated powerful\nimage generation capabilities. Enabling MM-DiT for layout-to-image generation\nseems straightforward but is challenging due to the complexity of how layout is\nintroduced, integrated, and balanced among multiple modalities. To this end, we\nexplore various network variants to efficiently incorporate layout guidance\ninto MM-DiT, and ultimately present SiamLayout. To Inherit the advantages of\nMM-DiT, we use a separate set of network weights to process the layout,\ntreating it as equally important as the image and text modalities. Meanwhile,\nto alleviate the competition among modalities, we decouple the image-layout\ninteraction into a siamese branch alongside the image-text one and fuse them in\nthe later stage. Moreover, we contribute a large-scale layout dataset, named\nLayoutSAM, which includes 2.7 million image-text pairs and 10.7 million\nentities. Each entity is annotated with a bounding box and a detailed\ndescription. We further construct the LayoutSAM-Eval benchmark as a\ncomprehensive tool for evaluating the L2I generation quality. Finally, we\nintroduce the Layout Designer, which taps into the potential of large language\nmodels in layout planning, transforming them into experts in layout generation\nand optimization. Our code, model, and dataset will be available at\nhttps://creatilayout.github.io.\n","authors":["Hui Zhang","Dexiang Hong","Yitong Wang","Jie Shao","Xinglong Wu","Zuxuan Wu","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.03859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11297v1","updated":"2025-03-14T11:06:49Z","published":"2025-03-14T11:06:49Z","title":"GMG: A Video Prediction Method Based on Global Focus and Motion Guided","summary":"  Recent years, weather forecasting has gained significant attention. However,\naccurately predicting weather remains a challenge due to the rapid variability\nof meteorological data and potential teleconnections. Current spatiotemporal\nforecasting models primarily rely on convolution operations or sliding windows\nfor feature extraction. These methods are limited by the size of the\nconvolutional kernel or sliding window, making it difficult to capture and\nidentify potential teleconnection features in meteorological data.\nAdditionally, weather data often involve non-rigid bodies, whose motion\nprocesses are accompanied by unpredictable deformations, further complicating\nthe forecasting task. In this paper, we propose the GMG model to address these\ntwo core challenges. The Global Focus Module, a key component of our model,\nenhances the global receptive field, while the Motion Guided Module adapts to\nthe growth or dissipation processes of non-rigid bodies. Through extensive\nevaluations, our method demonstrates competitive performance across various\ncomplex tasks, providing a novel approach to improving the predictive accuracy\nof complex spatiotemporal data.\n","authors":["Yuhao Du","Hui Liu","Haoxiang Peng","Xinyuan Chen","Chenrong Wu","Jiankai Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11290v1","updated":"2025-03-14T10:55:56Z","published":"2025-03-14T10:55:56Z","title":"EmoAgent: Multi-Agent Collaboration of Plan, Edit, and Critic, for\n  Affective Image Manipulation","summary":"  Affective Image Manipulation (AIM) aims to alter an image's emotional impact\nby adjusting multiple visual elements to evoke specific feelings.Effective AIM\nis inherently complex, necessitating a collaborative approach that involves\nidentifying semantic cues within source images, manipulating these elements to\nelicit desired emotional responses, and verifying that the combined adjustments\nsuccessfully evoke the target emotion.To address these challenges, we introduce\nEmoAgent, the first multi-agent collaboration framework for AIM. By emulating\nthe cognitive behaviors of a human painter, EmoAgent incorporates three\nspecialized agents responsible for planning, editing, and critical evaluation.\nFurthermore, we develop an emotion-factor knowledge retriever, a\ndecision-making tree space, and a tool library to enhance EmoAgent's\neffectiveness in handling AIM. Experiments demonstrate that the proposed\nmulti-agent framework outperforms existing methods, offering more reasonable\nand effective emotional expression.\n","authors":["Qi Mao","Haobo Hu","Yujie He","Difei Gao","Haokun Chen","Libiao Jin"],"pdf_url":"https://arxiv.org/pdf/2503.11290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10157v3","updated":"2025-03-14T10:35:13Z","published":"2025-01-17T12:42:30Z","title":"Structure-guided Deep Multi-View Clustering","summary":"  Deep multi-view clustering seeks to utilize the abundant information from\nmultiple views to improve clustering performance. However, most of the existing\nclustering methods often neglect to fully mine multi-view structural\ninformation and fail to explore the distribution of multi-view data, limiting\nclustering performance. To address these limitations, we propose a\nstructure-guided deep multi-view clustering model. Specifically, we introduce a\npositive sample selection strategy based on neighborhood relationships, coupled\nwith a corresponding loss function. This strategy constructs multi-view nearest\nneighbor graphs to dynamically redefine positive sample pairs, enabling the\nmining of local structural information within multi-view data and enhancing the\nreliability of positive sample selection. Additionally, we introduce a Gaussian\ndistribution model to uncover latent structural information and introduce a\nloss function to reduce discrepancies between view embeddings. These two\nstrategies explore multi-view structural information and data distribution from\ndifferent perspectives, enhancing consistency across views and increasing\nintra-cluster compactness. Experimental evaluations demonstrate the efficacy of\nour method, showing significant improvements in clustering performance on\nmultiple benchmark datasets compared to state-of-the-art multi-view clustering\napproaches.\n","authors":["Jinrong Cui","Xiaohuang Wu","Haitao Zhang","Chongjie Dong","Jie Wen"],"pdf_url":"https://arxiv.org/pdf/2501.10157v3.pdf","comment":"We have found that our paper has many imperfections and incorrect\n  formulas and derivations, and we insist on retracting the manuscript in order\n  to avoid misleading readers"},{"id":"http://arxiv.org/abs/2503.11269v1","updated":"2025-03-14T10:25:54Z","published":"2025-03-14T10:25:54Z","title":"Prof. Robot: Differentiable Robot Rendering Without Static and\n  Self-Collisions","summary":"  Differentiable rendering has gained significant attention in the field of\nrobotics, with differentiable robot rendering emerging as an effective paradigm\nfor learning robotic actions from image-space supervision. However, the lack of\nphysical world perception in this approach may lead to potential collisions\nduring action optimization. In this work, we introduce a novel improvement on\nprevious efforts by incorporating physical awareness of collisions through the\nlearning of a neural robotic collision classifier. This enables the\noptimization of actions that avoid collisions with static, non-interactable\nenvironments as well as the robot itself. To facilitate effective gradient\noptimization with the classifier, we identify the underlying issue and propose\nleveraging Eikonal regularization to ensure consistent gradients for\noptimization. Our solution can be seamlessly integrated into existing\ndifferentiable robot rendering frameworks, utilizing gradients for optimization\nand providing a foundation for future applications of differentiable rendering\nin robotics with improved reliability of interactions with the physical world.\nBoth qualitative and quantitative experiments demonstrate the necessity and\neffectiveness of our method compared to previous solutions.\n","authors":["Quanyuan Ruan","Jiabao Lei","Wenhao Yuan","Yanglin Zhang","Dekun Lu","Guiliang Liu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2503.11269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06171v2","updated":"2025-03-14T10:23:06Z","published":"2024-12-09T03:05:34Z","title":"Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any\n  Granularity","summary":"  How can we enable models to comprehend video anomalies occurring over varying\ntemporal scales and contexts? Traditional Video Anomaly Understanding (VAU)\nmethods focus on frame-level anomaly prediction, often missing the\ninterpretability of complex and diverse real-world anomalies. Recent multimodal\napproaches leverage visual and textual data but lack hierarchical annotations\nthat capture both short-term and long-term anomalies. To address this\nchallenge, we introduce HIVAU-70k, a large-scale benchmark for hierarchical\nvideo anomaly understanding across any granularity. We develop a semi-automated\nannotation engine that efficiently scales high-quality annotations by combining\nmanual video segmentation with recursive free-text annotation using large\nlanguage models (LLMs). This results in over 70,000 multi-granular annotations\norganized at clip-level, event-level, and video-level segments. For efficient\nanomaly detection in long videos, we propose the Anomaly-focused Temporal\nSampler (ATS). ATS integrates an anomaly scorer with a density-aware sampler to\nadaptively select frames based on anomaly scores, ensuring that the multimodal\nLLM concentrates on anomaly-rich regions, which significantly enhances both\nefficiency and accuracy. Extensive experiments demonstrate that our\nhierarchical instruction data markedly improves anomaly comprehension. The\nintegrated ATS and visual-language model outperform traditional methods in\nprocessing long videos. Our benchmark and model are publicly available at\nhttps://github.com/pipixin321/HolmesVAU.\n","authors":["Huaxin Zhang","Xiaohao Xu","Xiang Wang","Jialong Zuo","Xiaonan Huang","Changxin Gao","Shanjun Zhang","Li Yu","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2412.06171v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.11266v1","updated":"2025-03-14T10:22:26Z","published":"2025-03-14T10:22:26Z","title":"CyclePose -- Leveraging Cycle-Consistency for Annotation-Free Nuclei\n  Segmentation in Fluorescence Microscopy","summary":"  In recent years, numerous neural network architectures specifically designed\nfor the instance segmentation of nuclei in microscopic images have been\nreleased. These models embed nuclei-specific priors to outperform generic\narchitectures like U-Nets; however, they require large annotated datasets,\nwhich are often not available. Generative models (GANs, diffusion models) have\nbeen used to compensate for this by synthesizing training data. These two-stage\napproaches are computationally expensive, as first a generative model and then\na segmentation model has to be trained. We propose CyclePose, a hybrid\nframework integrating synthetic data generation and segmentation training.\nCyclePose builds on a CycleGAN architecture, which allows unpaired translation\nbetween microscopy images and segmentation masks. We embed a segmentation model\ninto CycleGAN and leverage a cycle consistency loss for self-supervision.\nWithout annotated data, CyclePose outperforms other weakly or unsupervised\nmethods on two public datasets. Code is available at\nhttps://github.com/jonasutz/CyclePose\n","authors":["Jonas Utz","Stefan Vocht","Anne Tjorven Buessen","Dennis Possart","Fabian Wagner","Mareike Thies","Mingxuan Gu","Stefan Uderhardt","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2503.11266v1.pdf","comment":"under review for MICCAI 2025"},{"id":"http://arxiv.org/abs/2501.10283v3","updated":"2025-03-14T10:19:59Z","published":"2025-01-17T16:26:24Z","title":"GauSTAR: Gaussian Surface Tracking and Reconstruction","summary":"  3D Gaussian Splatting techniques have enabled efficient photo-realistic\nrendering of static scenes. Recent works have extended these approaches to\nsupport surface reconstruction and tracking. However, tracking dynamic surfaces\nwith 3D Gaussians remains challenging due to complex topology changes, such as\nsurfaces appearing, disappearing, or splitting. To address these challenges, we\npropose GauSTAR, a novel method that achieves photo-realistic rendering,\naccurate surface reconstruction, and reliable 3D tracking for general dynamic\nscenes with changing topology. Given multi-view captures as input, GauSTAR\nbinds Gaussians to mesh faces to represent dynamic objects. For surfaces with\nconsistent topology, GauSTAR maintains the mesh topology and tracks the meshes\nusing Gaussians. For regions where topology changes, GauSTAR adaptively unbinds\nGaussians from the mesh, enabling accurate registration and generation of new\nsurfaces based on these optimized Gaussians. Additionally, we introduce a\nsurface-based scene flow method that provides robust initialization for\ntracking between frames. Experiments demonstrate that our method effectively\ntracks and reconstructs dynamic surfaces, enabling a range of applications. Our\nproject page with the code release is available at\nhttps://eth-ait.github.io/GauSTAR/.\n","authors":["Chengwei Zheng","Lixin Xue","Juan Zarate","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2501.10283v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11265v1","updated":"2025-03-14T10:19:24Z","published":"2025-03-14T10:19:24Z","title":"DynRsl-VLM: Enhancing Autonomous Driving Perception with Dynamic\n  Resolution Vision-Language Models","summary":"  Visual Question Answering (VQA) models, which fall under the category of\nvision-language models, conventionally execute multiple downsampling processes\non image inputs to strike a balance between computational efficiency and model\nperformance. Although this approach aids in concentrating on salient features\nand diminishing computational burden, it incurs the loss of vital detailed\ninformation, a drawback that is particularly damaging in end-to-end autonomous\ndriving scenarios. Downsampling can lead to an inadequate capture of distant or\nsmall objects such as pedestrians, road signs, or obstacles, all of which are\ncrucial for safe navigation. This loss of features negatively impacts an\nautonomous driving system's capacity to accurately perceive the environment,\npotentially escalating the risk of accidents. To tackle this problem, we put\nforward the Dynamic Resolution Vision Language Model (DynRsl-VLM). DynRsl-VLM\nincorporates a dynamic resolution image input processing approach that captures\nall entity feature information within an image while ensuring that the image\ninput remains computationally tractable for the Vision Transformer (ViT).\nMoreover, we devise a novel image-text alignment module to replace the\nQ-Former, enabling simple and efficient alignment with text when dealing with\ndynamic resolution image inputs. Our method enhances the environmental\nperception capabilities of autonomous driving systems without overstepping\ncomputational constraints.\n","authors":["Xirui Zhou","Lianlei Shan","Xiaolin Gui"],"pdf_url":"https://arxiv.org/pdf/2503.11265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11262v1","updated":"2025-03-14T10:16:54Z","published":"2025-03-14T10:16:54Z","title":"Noise Synthesis for Low-Light Image Denoising with Diffusion Models","summary":"  Low-light photography produces images with low signal-to-noise ratios due to\nlimited photons. In such conditions, common approximations like the Gaussian\nnoise model fall short, and many denoising techniques fail to remove noise\neffectively. Although deep-learning methods perform well, they require large\ndatasets of paired images that are impractical to acquire. As a remedy,\nsynthesizing realistic low-light noise has gained significant attention. In\nthis paper, we investigate the ability of diffusion models to capture the\ncomplex distribution of low-light noise. We show that a naive application of\nconventional diffusion models is inadequate for this task and propose three key\nadaptations that enable high-precision noise generation without calibration or\npost-processing: a two-branch architecture to better model signal-dependent and\nsignal-independent noise, the incorporation of positional information to\ncapture fixed-pattern noise, and a tailored diffusion noise schedule.\nConsequently, our model enables the generation of large datasets for training\nlow-light denoising networks, leading to state-of-the-art performance. Through\ncomprehensive analysis, including statistical evaluation and noise\ndecomposition, we provide deeper insights into the characteristics of the\ngenerated data.\n","authors":["Liying Lu","Raphaël Achddou","Sabine Süsstrunk"],"pdf_url":"https://arxiv.org/pdf/2503.11262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03021v4","updated":"2025-03-14T10:07:40Z","published":"2024-12-04T04:24:15Z","title":"PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm","summary":"  Video Virtual Try-on aims to seamlessly transfer a reference garment onto a\ntarget person in a video while preserving both visual fidelity and temporal\ncoherence. Existing methods typically rely on inpainting masks to define the\ntry-on area, enabling accurate garment transfer for simple scenes (e.g.,\nin-shop videos). However, these mask-based approaches struggle with complex\nreal-world scenarios, as overly large and inconsistent masks often destroy\nspatial-temporal information, leading to distorted results. Mask-free methods\nalleviate this issue but face challenges in accurately determining the try-on\narea, especially for videos with dynamic body movements. To address these\nlimitations, we propose PEMF-VTO, a novel Point-Enhanced Mask-Free Video\nVirtual Try-On framework that leverages sparse point alignments to explicitly\nguide garment transfer. Our key innovation is the introduction of\npoint-enhanced guidance, which provides flexible and reliable control over both\nspatial-level garment transfer and temporal-level video coherence.\nSpecifically, we design a Point-Enhanced Transformer (PET) with two core\ncomponents: Point-Enhanced Spatial Attention (PSA), which uses frame-cloth\npoint alignments to precisely guide garment transfer, and Point-Enhanced\nTemporal Attention (PTA), which leverages frame-frame point correspondences to\nenhance temporal coherence and ensure smooth transitions across frames.\nExtensive experiments demonstrate that our PEMF-VTO outperforms\nstate-of-the-art methods, generating more natural, coherent, and visually\nappealing try-on videos, particularly for challenging in-the-wild scenarios.\nThe link to our paper's homepage is https://pemf-vto.github.io/.\n","authors":["Tianyu Chang","Xiaohao Chen","Zhichao Wei","Xuanpu Zhang","Qing-Guo Chen","Weihua Luo","Peipei Song","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2412.03021v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11251v1","updated":"2025-03-14T10:01:55Z","published":"2025-03-14T10:01:55Z","title":"Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven\n  Image-to-Video Generation Model","summary":"  We present Step-Video-TI2V, a state-of-the-art text-driven image-to-video\ngeneration model with 30B parameters, capable of generating videos up to 102\nframes based on both text and image inputs. We build Step-Video-TI2V-Eval as a\nnew benchmark for the text-driven image-to-video task and compare\nStep-Video-TI2V with open-source and commercial TI2V engines using this\ndataset. Experimental results demonstrate the state-of-the-art performance of\nStep-Video-TI2V in the image-to-video generation task. Both Step-Video-TI2V and\nStep-Video-TI2V-Eval are available at\nhttps://github.com/stepfun-ai/Step-Video-TI2V.\n","authors":["Haoyang Huang","Guoqing Ma","Nan Duan","Xing Chen","Changyi Wan","Ranchen Ming","Tianyu Wang","Bo Wang","Zhiying Lu","Aojie Li","Xianfang Zeng","Xinhao Zhang","Gang Yu","Yuhe Yin","Qiling Wu","Wen Sun","Kang An","Xin Han","Deshan Sun","Wei Ji","Bizhu Huang","Brian Li","Chenfei Wu","Guanzhe Huang","Huixin Xiong","Jiaxin He","Jianchang Wu","Jianlong Yuan","Jie Wu","Jiashuai Liu","Junjing Guo","Kaijun Tan","Liangyu Chen","Qiaohui Chen","Ran Sun","Shanshan Yuan","Shengming Yin","Sitong Liu","Wei Chen","Yaqi Dai","Yuchu Luo","Zheng Ge","Zhisheng Guan","Xiaoniu Song","Yu Zhou","Binxing Jiao","Jiansheng Chen","Jing Li","Shuchang Zhou","Xiangyu Zhang","Yi Xiu","Yibo Zhu","Heung-Yeung Shum","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.11251v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2503.02332v2","updated":"2025-03-14T10:00:48Z","published":"2025-03-04T06:45:10Z","title":"COMMA: Coordinate-aware Modulated Mamba Network for 3D Dispersed Vessel\n  Segmentation","summary":"  Accurate segmentation of 3D vascular structures is essential for various\nmedical imaging applications. The dispersed nature of vascular structures leads\nto inherent spatial uncertainty and necessitates location awareness, yet most\ncurrent 3D medical segmentation models rely on the patch-wise training strategy\nthat usually loses this spatial context. In this study, we introduce the\nCoordinate-aware Modulated Mamba Network (COMMA) and contribute a manually\nlabeled dataset of 570 cases, the largest publicly available 3D vessel dataset\nto date. COMMA leverages both entire and cropped patch data through global and\nlocal branches, ensuring robust and efficient spatial location awareness.\nSpecifically, COMMA employs a channel-compressed Mamba (ccMamba) block to\nencode entire image data, capturing long-range dependencies while optimizing\ncomputational costs. Additionally, we propose a coordinate-aware modulated\n(CaM) block to enhance interactions between the global and local branches,\nallowing the local branch to better perceive spatial information. We evaluate\nCOMMA on six datasets, covering two imaging modalities and five types of\nvascular tissues. The results demonstrate COMMA's superior performance compared\nto state-of-the-art methods with computational efficiency, especially in\nsegmenting small vessels. Ablation studies further highlight the importance of\nour proposed modules and spatial information. The code and data will be open\nsource at https://github.com/shigen-StoneRoot/COMMA.\n","authors":["Gen Shi","Hui Zhang","Jie Tian"],"pdf_url":"https://arxiv.org/pdf/2503.02332v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11247v1","updated":"2025-03-14T09:56:13Z","published":"2025-03-14T09:56:13Z","title":"Breaking Shallow Limits: Task-Driven Pixel Fusion for Gap-free RGBT\n  Tracking","summary":"  Current RGBT tracking methods often overlook the impact of fusion location on\nmitigating modality gap, which is key factor to effective tracking. Our\nanalysis reveals that shallower fusion yields smaller distribution gap.\nHowever, the limited discriminative power of shallow networks hard to\ndistinguish task-relevant information from noise, limiting the potential of\npixel-level fusion. To break shallow limits, we propose a novel\n\\textbf{T}ask-driven \\textbf{P}ixel-level \\textbf{F}usion network, named\n\\textbf{TPF}, which unveils the power of pixel-level fusion in RGBT tracking\nthrough a progressive learning framework. In particular, we design a\nlightweight Pixel-level Fusion Adapter (PFA) that exploits Mamba's linear\ncomplexity to ensure real-time, low-latency RGBT tracking. To enhance the\nfusion capabilities of the PFA, our task-driven progressive learning framework\nfirst utilizes adaptive multi-expert distillation to inherits fusion knowledge\nfrom state-of-the-art image fusion models, establishing robust initialization,\nand then employs a decoupled representation learning scheme to achieve\ntask-relevant information fusion. Moreover, to overcome appearance variations\nbetween the initial template and search frames, we presents a nearest-neighbor\ndynamic template updating scheme, which selects the most reliable frame closest\nto the current search frame as the dynamic template. Extensive experiments\ndemonstrate that TPF significantly outperforms existing most of advanced\ntrackers on four public RGBT tracking datasets. The code will be released upon\nacceptance.\n","authors":["Andong Lu","Yuanzhi Guo","Wanyu Wang","Chenglong Li","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2503.11247v1.pdf","comment":"In peer review"},{"id":"http://arxiv.org/abs/2503.10603v2","updated":"2025-03-14T09:55:43Z","published":"2025-03-13T17:46:16Z","title":"Dual-Stage Cross-Modal Network with Dynamic Feature Fusion for Emotional\n  Mimicry Intensity Estimation","summary":"  Emotional Mimicry Intensity (EMI) estimation serves as a critical technology\nfor understanding human social behavior and enhancing human-computer\ninteraction experiences, where the core challenge lies in dynamic correlation\nmodeling and robust fusion of multimodal temporal signals. To address the\nlimitations of existing methods in insufficient exploitation of modal\nsynergistic effects, noise sensitivity, and limited fine-grained alignment\ncapabilities, this paper proposes a dual-stage cross-modal alignment framework.\nFirst, we construct vision-text and audio-text contrastive learning networks\nbased on an improved CLIP architecture, achieving preliminary alignment in the\nfeature space through modality-decoupled pre-training. Subsequently, we design\na temporal-aware dynamic fusion module that combines Temporal Convolutional\nNetworks (TCN) and gated bidirectional LSTM to respectively capture the\nmacro-evolution patterns of facial expressions and local dynamics of acoustic\nfeatures. Innovatively, we introduce a quality-guided modality fusion strategy\nthat enables modality compensation under occlusion and noisy scenarios through\ndifferentiable weight allocation. Experimental results on the Hume-Vidmimic2\ndataset demonstrate that our method achieves an average Pearson correlation\ncoefficient of 0.35 across six emotion dimensions, outperforming the best\nbaseline by 40\\%. Ablation studies further validate the effectiveness of the\ndual-stage training strategy and dynamic fusion mechanism, providing a novel\ntechnical pathway for fine-grained emotion analysis in open environments.\n","authors":["Jun Yu","Lingsi Zhu","Yanjun Chi","Yunxiang Zhang","Yang Zheng","Yongqi Wang","Xilong Lu"],"pdf_url":"https://arxiv.org/pdf/2503.10603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06664v2","updated":"2025-03-14T09:54:17Z","published":"2024-10-09T08:19:25Z","title":"Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning","summary":"  Diffusion models are trained by learning a sequence of models that reverse\neach step of noise corruption. Typically, the model parameters are fully shared\nacross multiple timesteps to enhance training efficiency. However, since the\ndenoising tasks differ at each timestep, the gradients computed at different\ntimesteps may conflict, potentially degrading the overall performance of image\ngeneration. To solve this issue, this work proposes a\n\\textbf{De}couple-then-\\textbf{Me}rge (\\textbf{DeMe}) framework, which begins\nwith a pretrained model and finetunes separate models tailored to specific\ntimesteps. We introduce several improved techniques during the finetuning stage\nto promote effective knowledge sharing while minimizing training interference\nacross timesteps. Finally, after finetuning, these separate models can be\nmerged into a single model in the parameter space, ensuring efficient and\npractical inference. Experimental results show significant generation quality\nimprovements upon 6 benchmarks including Stable Diffusion on COCO30K,\nImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10.\nCode is available at \\href{https://github.com/MqLeet/DeMe}{GitHub}.\n","authors":["Qianli Ma","Xuefei Ning","Dongrui Liu","Li Niu","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.06664v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.11245v1","updated":"2025-03-14T09:52:54Z","published":"2025-03-14T09:52:54Z","title":"L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban\n  Scenes via Remote Sensing Imagery","summary":"  We tackle the challenge of LiDAR-based place recognition, which traditionally\ndepends on costly and time-consuming prior 3D maps. To overcome this, we first\nconstruct XA-L&RSI dataset, which encompasses approximately $110,000$ remote\nsensing submaps and $13,000$ LiDAR point cloud submaps captured in urban\nscenes, and propose a novel method, L2RSI, for cross-view LiDAR place\nrecognition using high-resolution Remote Sensing Imagery. This approach enables\nlarge-scale localization capabilities at a reduced cost by leveraging readily\navailable overhead images as map proxies. L2RSI addresses the dual challenges\nof cross-view and cross-modal place recognition by learning feature alignment\nbetween point cloud submaps and remote sensing submaps in the semantic domain.\nAdditionally, we introduce a novel probability propagation method based on a\ndynamic Gaussian mixture model to refine position predictions, effectively\nleveraging temporal and spatial information. This approach enables large-scale\nretrieval and cross-scene generalization without fine-tuning. Extensive\nexperiments on XA-L&RSI demonstrate that, within a $100km^2$ retrieval range,\nL2RSI accurately localizes $95.08\\%$ of point cloud submaps within a $30m$\nradius for top-$1$ retrieved location. We provide a video to more vividly\ndisplay the place recognition results of L2RSI at\nhttps://shizw695.github.io/L2RSI/.\n","authors":["Ziwei Shi","Xiaoran Zhang","Yan Xia","Yu Zang","Siqi Shen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05111v2","updated":"2025-03-14T09:52:11Z","published":"2024-10-07T15:07:56Z","title":"LiDAR-GS:Real-time LiDAR Re-Simulation using Gaussian Splatting","summary":"  We present LiDAR-GS, a Gaussian Splatting (GS) method for real-time,\nhigh-fidelity re-simulation of LiDAR scans in public urban road scenes. Recent\nGS methods proposed for cameras have achieved significant advancements in\nreal-time rendering beyond Neural Radiance Fields (NeRF). However, applying GS\nrepresentation to LiDAR, an active 3D sensor type, poses several challenges\nthat must be addressed to preserve high accuracy and unique characteristics.\nSpecifically, LiDAR-GS designs a differentiable laser beam splatting, using\nrange-view representation for precise surface splatting by projecting lasers\nonto micro cross-sections, effectively eliminating artifacts associated with\nlocal affine approximations. Furthermore, LiDAR-GS leverages Neural Gaussian\nRepresentation, which further integrate view-dependent clues, to represent key\nLiDAR properties that are influenced by the incident direction and external\nfactors. Combining these practices with some essential adaptations, e.g.,\ndynamic instances decomposition, LiDAR-GS succeeds in simultaneously\nre-simulating depth, intensity, and ray-drop channels, achieving\nstate-of-the-art results in both rendering frame rate and quality on publically\navailable large scene datasets when compared with the methods using explicit\nmesh or implicit NeRF. Our source code is publicly available at\nhttps://www.github.com/cqf7419/LiDAR-GS.\n","authors":["Qifeng Chen","Sheng Yang","Sicong Du","Tao Tang","Peng Chen","Yuchi Huo"],"pdf_url":"https://arxiv.org/pdf/2410.05111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05609v2","updated":"2025-03-14T09:51:44Z","published":"2024-11-08T14:52:42Z","title":"A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis","summary":"  The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and an off-the-shelf\nLarge Language Model (LLM) to generate disease diagnoses based on the predicted\nconcepts. Furthermore, our approach supports test-time human intervention,\nenabling corrections to predicted concepts, which improves final diagnoses and\nenhances transparency in decision-making. We validate our approach on three\nskin lesion datasets, demonstrating that it outperforms traditional CBMs and\nstate-of-the-art explainable methods, all without requiring any training and\nutilizing only a few annotated examples. The code is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.\n","authors":["Cristiano Patrício","Luís F. Teixeira","João C. Neves"],"pdf_url":"https://arxiv.org/pdf/2411.05609v2.pdf","comment":"Published in the Computational and Structural Biotechnology Journal"},{"id":"http://arxiv.org/abs/2503.11241v1","updated":"2025-03-14T09:46:05Z","published":"2025-03-14T09:46:05Z","title":"Compound Expression Recognition via Large Vision-Language Models","summary":"  Compound Expression Recognition (CER) is crucial for understanding human\nemotions and improving human-computer interaction. However, CER faces\nchallenges due to the complexity of facial expressions and the difficulty of\ncapturing subtle emotional cues. To address these issues, we propose a novel\napproach leveraging Large Vision-Language Models (LVLMs). Our method employs a\ntwo-stage fine-tuning process: first, pre-trained LVLMs are fine-tuned on basic\nfacial expressions to establish foundational patterns; second, the model is\nfurther optimized on a compound-expression dataset to refine visual-language\nfeature interactions. Our approach achieves advanced accuracy on the RAF-DB\ndataset and demonstrates strong zero-shot generalization on the C-EXPR-DB\ndataset, showcasing its potential for real-world applications in emotion\nanalysis and human-computer interaction.\n","authors":["Jun Yu","Xilong Lu"],"pdf_url":"https://arxiv.org/pdf/2503.11241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11240v1","updated":"2025-03-14T09:45:19Z","published":"2025-03-14T09:45:19Z","title":"Towards Better Alignment: Training Diffusion Models with Reinforcement\n  Learning Against Sparse Rewards","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration. However, their practical applications are hindered by the\nmisalignment between generated images and corresponding text prompts. To tackle\nthis issue, reinforcement learning (RL) has been considered for diffusion model\nfine-tuning. Yet, RL's effectiveness is limited by the challenge of sparse\nreward, where feedback is only available at the end of the generation process.\nThis makes it difficult to identify which actions during the denoising process\ncontribute positively to the final generated image, potentially leading to\nineffective or unnecessary denoising policies. To this end, this paper presents\na novel RL-based framework that addresses the sparse reward problem when\ntraining diffusion models. Our framework, named $\\text{B}^2\\text{-DiffuRL}$,\nemploys two strategies: \\textbf{B}ackward progressive training and\n\\textbf{B}ranch-based sampling. For one thing, backward progressive training\nfocuses initially on the final timesteps of denoising process and gradually\nextends the training interval to earlier timesteps, easing the learning\ndifficulty from sparse rewards. For another, we perform branch-based sampling\nfor each training interval. By comparing the samples within the same branch, we\ncan identify how much the policies of the current training interval contribute\nto the final image, which helps to learn effective policies instead of\nunnecessary ones. $\\text{B}^2\\text{-DiffuRL}$ is compatible with existing\noptimization algorithms. Extensive experiments demonstrate the effectiveness of\n$\\text{B}^2\\text{-DiffuRL}$ in improving prompt-image alignment and maintaining\ndiversity in generated images. The code for this work is available.\n","authors":["Zijing Hu","Fengda Zhang","Long Chen","Kun Kuang","Jiahui Li","Kaifeng Gao","Jun Xiao","Xin Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.11240v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11231v1","updated":"2025-03-14T09:29:55Z","published":"2025-03-14T09:29:55Z","title":"Deep Lossless Image Compression via Masked Sampling and Coarse-to-Fine\n  Auto-Regression","summary":"  Learning-based lossless image compression employs pixel-based or\nsubimage-based auto-regression for probability estimation, which achieves\ndesirable performances. However, the existing works only consider context\ndependencies in one direction, namely, those symbols that appear before the\ncurrent symbol in raster order. We believe that the dependencies between the\ncurrent and future symbols should be further considered. In this work, we\npropose a deep lossless image compression via masked sampling and\ncoarse-to-fine auto-regression. It combines lossy reconstruction and\nprogressive residual compression, which fuses contexts from various directions\nand is more consistent with human perception. Specifically,\n  the residuals are decomposed via $T$ iterative masked sampling, and each\nsampling consists of three steps: 1) probability estimation, 2) mask\ncomputation, and 3) arithmetic coding. The iterative process progressively\nrefines our prediction and gradually presents a real image. Extensive\nexperimental results show that compared with the existing traditional and\nlearned lossless compression, our method achieves comparable compression\nperformance on extensive datasets with competitive coding speed and more\nflexibility.\n","authors":["Tiantian Li","Qunbing Xia","Yue Li","Ruixiao Guo","Gaobo Yang"],"pdf_url":"https://arxiv.org/pdf/2503.11231v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.08215v2","updated":"2025-03-14T09:24:22Z","published":"2024-03-13T03:24:36Z","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual\n  Semantic Segmentation for Autonomous Driving","summary":"  Despite the impressive performance achieved by data-fusion networks with\nduplex encoders for visual semantic segmentation, they become ineffective when\nspatial geometric data are not available. Implicitly infusing the spatial\ngeometric prior knowledge acquired by a data-fusion teacher network into a\nsingle-modal student network is a practical, albeit less explored research\navenue. This article delves into this topic and resorts to knowledge\ndistillation approaches to address this problem. We introduce the Learning to\nInfuse ''X'' (LIX) framework, with novel contributions in both logit\ndistillation and feature distillation aspects. We present a mathematical proof\nthat underscores the limitation of using a single, fixed weight in decoupled\nknowledge distillation and introduce a logit-wise dynamic weight controller as\na solution to this issue. Furthermore, we develop an adaptively-recalibrated\nfeature distillation algorithm, including two novel techniques: feature\nrecalibration via kernel regression and in-depth feature consistency\nquantification via centered kernel alignment. Extensive experiments conducted\nwith intermediate-fusion and late-fusion networks across various public\ndatasets provide both quantitative and qualitative evaluations, demonstrating\nthe superior performance of our LIX framework when compared to other\nstate-of-the-art approaches.\n","authors":["Sicen Guo","Ziwei Long","Zhiyuan Wu","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2403.08215v2.pdf","comment":"13 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.11226v1","updated":"2025-03-14T09:22:51Z","published":"2025-03-14T09:22:51Z","title":"Non Line-of-Sight Optical Wireless Communication using Neuromorphic\n  Cameras","summary":"  Neuromorphic or event cameras, inspired by biological vision systems, capture\nchanges in illumination with high temporal resolution and efficiency, producing\nstreams of events rather than traditional images. In this paper, we explore the\nuse of neuromorphic cameras for passive optical wireless communication (OWC),\nleveraging their asynchronous detection of illumination changes to decode data\ntransmitted through reflections of light from objects. We propose a novel\nsystem that utilizes neuromorphic cameras for passive visible light\ncommunication (VLC), extending the concept to Non Line-of-Sight (NLoS)\nscenarios through passive reflections from everyday objects. Our experiments\ndemonstrate the feasibility and advantages of using neuromorphic cameras for\nVLC, characterizing the performance of various modulation schemes, including\ntraditional On-Off Keying (OOK) and advanced N-pulse modulation. We introduce\nan adaptive N-pulse modulation scheme that dynamically adjusts encoding based\non the packet's bit composition, achieving higher data rates and robustness in\ndifferent scenarios. Our results show that lighter-colored, glossy objects are\nbetter for NLoS communication, while larger objects and those with matte\nfinishes experience higher error rates due to multipath reflections.\n","authors":["Abbaas Alif Mohamed Nishar","Alireza Marefat","Ashwin Ashok"],"pdf_url":"https://arxiv.org/pdf/2503.11226v1.pdf","comment":"Accepted to be Presented at THE 22ND INTERNATIONAL CONFERENCE ON\n  EMBEDDED WIRELESS SYSTEMS AND NETWORKS"},{"id":"http://arxiv.org/abs/2503.10312v2","updated":"2025-03-14T09:12:38Z","published":"2025-03-13T12:46:23Z","title":"An Ensemble-Based Two-Step Framework for Classification of Pap Smear\n  Cell Images","summary":"  Early detection of cervical cancer is crucial for improving patient outcomes\nand reducing mortality by identifying precancerous lesions as soon as possible.\nAs a result, the use of pap smear screening has significantly increased,\nleading to a growing demand for automated tools that can assist cytologists\nmanaging their rising workload. To address this, the Pap Smear Cell\nClassification Challenge (PS3C) has been organized in association with ISBI in\n2025. This project aims to promote the development of automated tools for pap\nsmear images classification. The analyzed images are grouped into four\ncategories: healthy, unhealthy, both, and rubbish images which are considered\nas unsuitable for diagnosis. In this work, we propose a two-stage ensemble\napproach: first, a neural network determines whether an image is rubbish or\nnot. If not, a second neural network classifies the image as containing a\nhealthy cell, an unhealthy cell, or both.\n","authors":["Theo Di Piazza","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2503.10312v2.pdf","comment":"7 pages, 3 figures, Grand Challenge paper accepted at ISBI 2025"},{"id":"http://arxiv.org/abs/2503.11221v1","updated":"2025-03-14T09:12:03Z","published":"2025-03-14T09:12:03Z","title":"Toward Generalized Image Quality Assessment: Relaxing the Perfect\n  Reference Quality Assumption","summary":"  Full-reference image quality assessment (FR-IQA) generally assumes that\nreference images are of perfect quality. However, this assumption is flawed due\nto the sensor and optical limitations of modern imaging systems. Moreover,\nrecent generative enhancement methods are capable of producing images of higher\nquality than their original. All of these challenge the effectiveness and\napplicability of current FR-IQA models. To relax the assumption of perfect\nreference image quality, we build a large-scale IQA database, namely DiffIQA,\ncontaining approximately 180,000 images generated by a diffusion-based image\nenhancer with adjustable hyper-parameters. Each image is annotated by human\nsubjects as either worse, similar, or better quality compared to its reference.\nBuilding on this, we present a generalized FR-IQA model, namely Adaptive\nFidelity-Naturalness Evaluator (A-FINE), to accurately assess and adaptively\ncombine the fidelity and naturalness of a test image. A-FINE aligns well with\nstandard FR-IQA when the reference image is much more natural than the test\nimage. We demonstrate by extensive experiments that A-FINE surpasses standard\nFR-IQA models on well-established IQA datasets and our newly created DiffIQA.\nTo further validate A-FINE, we additionally construct a super-resolution IQA\nbenchmark (SRIQA-Bench), encompassing test images derived from ten\nstate-of-the-art SR methods with reliable human quality annotations. Tests on\nSRIQA-Bench re-affirm the advantages of A-FINE. The code and dataset are\navailable at https://tianhewu.github.io/A-FINE-page.github.io/.\n","authors":["Du Chen","Tianhe Wu","Kede Ma","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11221v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11219v1","updated":"2025-03-14T09:10:45Z","published":"2025-03-14T09:10:45Z","title":"MEET: A Million-Scale Dataset for Fine-Grained Geospatial Scene\n  Classification with Zoom-Free Remote Sensing Imagery","summary":"  Accurate fine-grained geospatial scene classification using remote sensing\nimagery is essential for a wide range of applications. However, existing\napproaches often rely on manually zooming remote sensing images at different\nscales to create typical scene samples. This approach fails to adequately\nsupport the fixed-resolution image interpretation requirements in real-world\nscenarios. To address this limitation, we introduce the Million-scale\nfinE-grained geospatial scEne classification dataseT (MEET), which contains\nover 1.03 million zoom-free remote sensing scene samples, manually annotated\ninto 80 fine-grained categories. In MEET, each scene sample follows a\nscene-inscene layout, where the central scene serves as the reference, and\nauxiliary scenes provide crucial spatial context for finegrained\nclassification. Moreover, to tackle the emerging challenge of scene-in-scene\nclassification, we present the Context-Aware Transformer (CAT), a model\nspecifically designed for this task, which adaptively fuses spatial context to\naccurately classify the scene samples. CAT adaptively fuses spatial context to\naccurately classify the scene samples by learning attentional features that\ncapture the relationships between the center and auxiliary scenes. Based on\nMEET, we establish a comprehensive benchmark for fine-grained geospatial scene\nclassification, evaluating CAT against 11 competitive baselines. The results\ndemonstrate that CAT significantly outperforms these baselines, achieving a\n1.88% higher balanced accuracy (BA) with the Swin-Large backbone, and a notable\n7.87% improvement with the Swin-Huge backbone. Further experiments validate the\neffectiveness of each module in CAT and show the practical applicability of CAT\nin the urban functional zone mapping. The source code and dataset will be\npublicly available at https://jerrywyn.github.io/project/MEET.html.\n","authors":["Yansheng Li","Yuning Wu","Gong Cheng","Chao Tao","Bo Dang","Yu Wang","Jiahao Zhang","Chuge Zhang","Yiting Liu","Xu Tang","Jiayi Ma","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05029v2","updated":"2025-03-14T09:10:28Z","published":"2023-11-08T21:25:27Z","title":"S$^3$AD: Semi-supervised Small Apple Detection in Orchard Environments","summary":"  Crop detection is integral for precision agriculture applications such as\nautomated yield estimation or fruit picking. However, crop detection, e.g.,\napple detection in orchard environments remains challenging due to a lack of\nlarge-scale datasets and the small relative size of the crops in the image. In\nthis work, we address these challenges by reformulating the apple detection\ntask in a semi-supervised manner. To this end, we provide the large,\nhigh-resolution dataset MAD comprising 105 labeled images with 14,667 annotated\napple instances and 4,440 unlabeled images. Utilizing this dataset, we also\npropose a novel Semi-Supervised Small Apple Detection system S$^3$AD based on\ncontextual attention and selective tiling to improve the challenging detection\nof small apples, while limiting the computational overhead. We conduct an\nextensive evaluation on MAD and the MSU dataset, showing that S$^3$AD\nsubstantially outperforms strong fully-supervised baselines, including several\nsmall object detection systems, by up to $14.9\\%$. Additionally, we exploit the\ndetailed annotations of our dataset w.r.t. apple properties to analyze the\ninfluence of relative size or level of occlusion on the results of various\nsystems, quantifying current challenges.\n","authors":["Robert Johanson","Christian Wilms","Ole Johannsen","Simone Frintrop"],"pdf_url":"https://arxiv.org/pdf/2311.05029v2.pdf","comment":"Accepted at WACV 2024. The code and the dataset MAD are available at\n  http://www.inf.uni-hamburg.de/mad"},{"id":"http://arxiv.org/abs/2503.11218v1","updated":"2025-03-14T09:09:43Z","published":"2025-03-14T09:09:43Z","title":"Towards General Multimodal Visual Tracking","summary":"  Existing multimodal tracking studies focus on bi-modal scenarios such as\nRGB-Thermal, RGB-Event, and RGB-Language. Although promising tracking\nperformance is achieved through leveraging complementary cues from different\nsources, it remains challenging in complex scenes due to the limitations of\nbi-modal scenarios. In this work, we introduce a general multimodal visual\ntracking task that fully exploits the advantages of four modalities, including\nRGB, thermal infrared, event, and language, for robust tracking under\nchallenging conditions. To provide a comprehensive evaluation platform for\ngeneral multimodal visual tracking, we construct QuadTrack600, a large-scale,\nhigh-quality benchmark comprising 600 video sequences (totaling 384.7K\nhigh-resolution (640x480) frame groups). In each frame group, all four\nmodalities are spatially aligned and meticulously annotated with bounding\nboxes, while 21 sequence-level challenge attributes are provided for detailed\nperformance analysis. Despite quad-modal data provides richer information, the\ndifferences in information quantity among modalities and the computational\nburden from four modalities are two challenging issues in fusing four\nmodalities. To handle these issues, we propose a novel approach called\nQuadFusion, which incorporates an efficient Multiscale Fusion Mamba with four\ndifferent scanning scales to achieve sufficient interactions of the four\nmodalities while overcoming the exponential computational burden, for general\nmultimodal visual tracking. Extensive experiments on the QuadTrack600 dataset\nand three bi-modal tracking datasets, including LasHeR, VisEvent, and TNL2K,\nvalidate the effectiveness of our QuadFusion.\n","authors":["Andong Lu","Mai Wen","Jinhu Wang","Yuanzhi Guo","Chenglong Li","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2503.11218v1.pdf","comment":"In peer review"},{"id":"http://arxiv.org/abs/2503.11213v1","updated":"2025-03-14T09:03:25Z","published":"2025-03-14T09:03:25Z","title":"Simulating Dual-Pixel Images From Ray Tracing For Depth Estimation","summary":"  Many studies utilize dual-pixel (DP) sensor phase characteristics for various\napplications, such as depth estimation and deblurring. However, since the DP\nimage features are entirely determined by the camera hardware, DP-depth paired\ndatasets are very scarce, especially when performing depth estimation on\ncustomized cameras. To overcome this, studies simulate DP images using ideal\noptical system models. However, these simulations often violate real optical\npropagation laws,leading to poor generalization to real DP data. To address\nthis, we investigate the domain gap between simulated and real DP data, and\npropose solutions using the Simulating DP images from ray tracing (Sdirt)\nscheme. The Sdirt generates realistic DP images via ray tracing and integrates\nthem into the depth estimation training pipeline. Experimental results show\nthat models trained with Sdirt-simulated images generalize better to real DP\ndata.\n","authors":["Fengchen He","Dayang Zhao","Hao Xu","Tingwei Quan","Shaoqun Zeng"],"pdf_url":"https://arxiv.org/pdf/2503.11213v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11205v1","updated":"2025-03-14T08:49:52Z","published":"2025-03-14T08:49:52Z","title":"LLaVA-MLB: Mitigating and Leveraging Attention Bias for Training-Free\n  Video LLMs","summary":"  Training-free video large language models (LLMs) leverage pretrained Image\nLLMs to process video content without the need for further training. A key\nchallenge in such approaches is the difficulty of retaining essential visual\nand temporal information, constrained by the token limits in Image LLMs. To\naddress this, we propose a two-stage method for selecting query-relevant tokens\nbased on the LLM attention scores: compressing the video sequence and then\nexpanding the sequence. However, during the compression stage, Image LLMs often\nexhibit a positional attention bias in video sequences, where attention is\noverly concentrated on later frames, causing early-frame information to be\nunderutilized. To alleviate this attention bias during sequence compression, we\npropose Gridded Attention Pooling for preserving spatiotemporal structure.\nAdditionally, we introduce Visual Summarization Tail to effectively utilize\nthis bias, facilitating overall video understanding during sequence expansion.\nIn this way, our method effectively Mitigates and Leverages attention Bias\n(LLaVA-MLB), enabling the frozen Image LLM for detailed video understanding.\nExperiments on several benchmarks demonstrate that our approach outperforms\nstate-of-the-art methods, achieving superior performance in both efficiency and\naccuracy. Our code will be released.\n","authors":["Leqi Shen","Tao He","Guoqiang Gong","Fan Yang","Yifeng Zhang","Pengzhang Liu","Sicheng Zhao","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2503.11205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12928v3","updated":"2025-03-14T08:48:51Z","published":"2024-08-23T09:14:58Z","title":"ParGo: Bridging Vision-Language with Partial and Global Views","summary":"  This work presents ParGo, a novel Partial-Global projector designed to\nconnect the vision and language modalities for Multimodal Large Language Models\n(MLLMs). Unlike previous works that rely on global attention-based projectors,\nour ParGo bridges the representation gap between the separately pre-trained\nvision encoders and the LLMs by integrating global and partial views, which\nalleviates the overemphasis on prominent regions. To facilitate the effective\ntraining of ParGo, we collect a large-scale detail-captioned image-text dataset\nnamed ParGoCap-1M-PT, consisting of 1 million images paired with high-quality\ncaptions. Extensive experiments on several MLLM benchmarks demonstrate the\neffectiveness of our ParGo, highlighting its superiority in aligning vision and\nlanguage modalities. Compared to conventional Q-Former projector, our ParGo\nachieves an improvement of 259.96 in MME benchmark. Furthermore, our\nexperiments reveal that ParGo significantly outperforms other projectors,\nparticularly in tasks that emphasize detail perception ability.\n","authors":["An-Lan Wang","Bin Shan","Wei Shi","Kun-Yu Lin","Xiang Fei","Guozhi Tang","Lei Liao","Can Huang","Jingqun Tang","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.12928v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2503.11199v1","updated":"2025-03-14T08:46:56Z","published":"2025-03-14T08:46:56Z","title":"NF-SLAM: Effective, Normalizing Flow-supported Neural Field\n  representations for object-level visual SLAM in automotive applications","summary":"  We propose a novel, vision-only object-level SLAM framework for automotive\napplications representing 3D shapes by implicit signed distance functions. Our\nkey innovation consists of augmenting the standard neural representation by a\nnormalizing flow network. As a result, achieving strong representation power on\nthe specific class of road vehicles is made possible by compact networks with\nonly 16-dimensional latent codes. Furthermore, the newly proposed architecture\nexhibits a significant performance improvement in the presence of only sparse\nand noisy data, which is demonstrated through comparative experiments on\nsynthetic data. The module is embedded into the back-end of a stereo-vision\nbased framework for joint, incremental shape optimization. The loss function is\ngiven by a combination of a sparse 3D point-based SDF loss, a sparse rendering\nloss, and a semantic mask-based silhouette-consistency term. We furthermore\nleverage semantic information to determine keypoint extraction density in the\nfront-end. Finally, experimental results on real-world data reveal accurate and\nreliable performance comparable to alternative frameworks that make use of\ndirect depth readings. The proposed method performs well with only sparse 3D\npoints obtained from bundle adjustment, and eventually continues to deliver\nstable results even under exclusive use of the mask-consistency term.\n","authors":["Li Cui","Yang Ding","Richard Hartley","Zirui Xie","Laurent Kneip","Zhenghua Yu"],"pdf_url":"https://arxiv.org/pdf/2503.11199v1.pdf","comment":"9 pages, 5 figures, IROS 2024"},{"id":"http://arxiv.org/abs/2503.11195v1","updated":"2025-03-14T08:42:18Z","published":"2025-03-14T08:42:18Z","title":"Provenance Detection for AI-Generated Images: Combining Perceptual\n  Hashing, Homomorphic Encryption, and AI Detection Models","summary":"  As AI-generated sensitive images become more prevalent, identifying their\nsource is crucial for distinguishing them from real images. Conventional image\nwatermarking methods are vulnerable to common transformations like filters,\nlossy compression, and screenshots, often applied during social media sharing.\nWatermarks can also be faked or removed if models are open-sourced or leaked\nsince images can be rewatermarked. We have developed a three-part framework for\nsecure, transformation-resilient AI content provenance detection, to address\nthese limitations. We develop an adversarially robust state-of-the-art\nperceptual hashing model, DinoHash, derived from DINOV2, which is robust to\ncommon transformations like filters, compression, and crops. Additionally, we\nintegrate a Multi-Party Fully Homomorphic Encryption~(MP-FHE) scheme into our\nproposed framework to ensure the protection of both user queries and registry\nprivacy. Furthermore, we improve previous work on AI-generated media detection.\nThis approach is useful in cases where the content is absent from our registry.\nDinoHash significantly improves average bit accuracy by 12% over\nstate-of-the-art watermarking and perceptual hashing methods while maintaining\nsuperior true positive rate (TPR) and false positive rate (FPR) tradeoffs\nacross various transformations. Our AI-generated media detection results show a\n25% improvement in classification accuracy on commonly used real-world AI image\ngenerators over existing algorithms. By combining perceptual hashing, MP-FHE,\nand an AI content detection model, our proposed framework provides better\nrobustness and privacy compared to previous work.\n","authors":["Shree Singhi","Aayan Yadav","Aayush Gupta","Shariar Ebrahimi","Parisa Hassanizadeh"],"pdf_url":"https://arxiv.org/pdf/2503.11195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11194v1","updated":"2025-03-14T08:41:55Z","published":"2025-03-14T08:41:55Z","title":"Online Test-time Adaptation for 3D Human Pose Estimation: A Practical\n  Perspective with Estimated 2D Poses","summary":"  Online test-time adaptation for 3D human pose estimation is used for video\nstreams that differ from training data. Ground truth 2D poses are used for\nadaptation, but only estimated 2D poses are available in practice. This paper\naddresses adapting models to streaming videos with estimated 2D poses.\nComparing adaptations reveals the challenge of limiting estimation errors while\npreserving accurate pose information. To this end, we propose adaptive\naggregation, a two-stage optimization, and local augmentation for handling\nvarying levels of estimated pose error. First, we perform adaptive aggregation\nacross videos to initialize the model state with labeled representative\nsamples. Within each video, we use a two-stage optimization to benefit from 2D\nfitting while minimizing the impact of erroneous updates. Second, we employ\nlocal augmentation, using adjacent confident samples to update the model before\nadapting to the current non-confident sample. Our method surpasses\nstate-of-the-art by a large margin, advancing adaptation towards more practical\nsettings of using estimated 2D poses.\n","authors":["Qiuxia Lin","Kerui Gu","Linlin Yang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2503.11194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11187v1","updated":"2025-03-14T08:33:08Z","published":"2025-03-14T08:33:08Z","title":"FastVID: Dynamic Density Pruning for Fast Video Large Language Models","summary":"  Video Large Language Models have shown impressive capabilities in video\ncomprehension, yet their practical deployment is hindered by substantial\ninference costs caused by redundant video tokens. Existing pruning techniques\nfail to fully exploit the spatiotemporal redundancy inherent in video data. To\nbridge this gap, we perform a systematic analysis of video redundancy from two\nperspectives: temporal context and visual context. Leveraging this insight, we\npropose Dynamic Density Pruning for Fast Video LLMs termed FastVID.\nSpecifically, FastVID dynamically partitions videos into temporally ordered\nsegments to preserve temporal structure and applies a density-based token\npruning strategy to maintain essential visual information. Our method\nsignificantly reduces computational overhead while maintaining temporal and\nvisual integrity. Extensive evaluations show that FastVID achieves\nstate-of-the-art performance across various short- and long-video benchmarks on\nleading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID\neffectively prunes 90% of video tokens while retaining 98.0% of\nLLaVA-OneVision's original performance. The code is available at\nhttps://github.com/LunarShen/FastVID.\n","authors":["Leqi Shen","Guoqiang Gong","Tao He","Yifeng Zhang","Pengzhang Liu","Sicheng Zhao","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2503.11187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11183v1","updated":"2025-03-14T08:31:21Z","published":"2025-03-14T08:31:21Z","title":"Multimodal-Aware Fusion Network for Referring Remote Sensing Image\n  Segmentation","summary":"  Referring remote sensing image segmentation (RRSIS) is a novel visual task in\nremote sensing images segmentation, which aims to segment objects based on a\ngiven text description, with great significance in practical application.\nPrevious studies fuse visual and linguistic modalities by explicit feature\ninteraction, which fail to effectively excavate useful multimodal information\nfrom dual-branch encoder. In this letter, we design a multimodal-aware fusion\nnetwork (MAFN) to achieve fine-grained alignment and fusion between the two\nmodalities. We propose a correlation fusion module (CFM) to enhance multi-scale\nvisual features by introducing adaptively noise in transformer, and integrate\ncross-modal aware features. In addition, MAFN employs multi-scale refinement\nconvolution (MSRC) to adapt to the various orientations of objects at different\nscales to boost their representation ability to enhances segmentation accuracy.\nExtensive experiments have shown that MAFN is significantly more effective than\nthe state of the art on RRSIS-D datasets. The source code is available at\nhttps://github.com/Roaxy/MAFN.\n","authors":["Leideng Shi","Juan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11183v1.pdf","comment":"5 pages, 5 figures, accepted in IEEE Geoscience and Remote Sensing\n  Letters (GRSL)"},{"id":"http://arxiv.org/abs/2503.11181v1","updated":"2025-03-14T08:28:30Z","published":"2025-03-14T08:28:30Z","title":"Multi-Stage Generative Upscaler: Reconstructing Football Broadcast\n  Images via Diffusion Models","summary":"  The reconstruction of low-resolution football broadcast images presents a\nsignificant challenge in sports broadcasting, where detailed visuals are\nessential for analysis and audience engagement. This study introduces a\nmulti-stage generative upscaling framework leveraging Diffusion Models to\nenhance degraded images, transforming inputs as small as $64 \\times 64$ pixels\ninto high-fidelity $1024 \\times 1024$ outputs. By integrating an image-to-image\npipeline, ControlNet conditioning, and LoRA fine-tuning, our approach surpasses\ntraditional upscaling methods in restoring intricate textures and\ndomain-specific elements such as player details and jersey logos. The custom\nLoRA is trained on a custom football dataset, ensuring adaptability to sports\nbroadcast needs. Experimental results demonstrate substantial improvements over\nconventional models, with ControlNet refining fine details and LoRA enhancing\ntask-specific elements. These findings highlight the potential of\ndiffusion-based image reconstruction in sports media, paving the way for future\napplications in automated video enhancement and real-time sports analytics.\n","authors":["Luca Martini","Daniele Zolezzi","Saverio Iacono","Gianni Viardo Vercelli"],"pdf_url":"https://arxiv.org/pdf/2503.11181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11175v1","updated":"2025-03-14T08:22:26Z","published":"2025-03-14T08:22:26Z","title":"Zero-TIG: Temporal Consistency-Aware Zero-Shot Illumination-Guided\n  Low-light Video Enhancement","summary":"  Low-light and underwater videos suffer from poor visibility, low contrast,\nand high noise, necessitating enhancements in visual quality. However, existing\napproaches typically rely on paired ground truth, which limits their\npracticality and often fails to maintain temporal consistency. To overcome\nthese obstacles, this paper introduces a novel zero-shot learning approach\nnamed Zero-TIG, leveraging the Retinex theory and optical flow techniques. The\nproposed network consists of an enhancement module and a temporal feedback\nmodule. The enhancement module comprises three subnetworks: low-light image\ndenoising, illumination estimation, and reflection denoising. The temporal\nenhancement module ensures temporal consistency by incorporating histogram\nequalization, optical flow computation, and image warping to align the enhanced\nprevious frame with the current frame, thereby maintaining continuity.\nAdditionally, we address color distortion in underwater data by adaptively\nbalancing RGB channels. The experimental results demonstrate that our method\nachieves low-light video enhancement without the need for paired training data,\nmaking it a promising and applicable method for real-world scenario\nenhancement.\n","authors":["Yini Li","Nantheera Anantrasirichai"],"pdf_url":"https://arxiv.org/pdf/2503.11175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11172v1","updated":"2025-03-14T08:18:12Z","published":"2025-03-14T08:18:12Z","title":"Uncertainty-Aware Normal-Guided Gaussian Splatting for Surface\n  Reconstruction from Sparse Image Sequences","summary":"  3D Gaussian Splatting (3DGS) has achieved impressive rendering performance in\nnovel view synthesis. However, its efficacy diminishes considerably in sparse\nimage sequences, where inherent data sparsity amplifies geometric uncertainty\nduring optimization. This often leads to convergence at suboptimal local\nminima, resulting in noticeable structural artifacts in the reconstructed\nscenes.To mitigate these issues, we propose Uncertainty-aware Normal-Guided\nGaussian Splatting (UNG-GS), a novel framework featuring an explicit Spatial\nUncertainty Field (SUF) to quantify geometric uncertainty within the 3DGS\npipeline. UNG-GS enables high-fidelity rendering and achieves high-precision\nreconstruction without relying on priors. Specifically, we first integrate\nGaussian-based probabilistic modeling into the training of 3DGS to optimize the\nSUF, providing the model with adaptive error tolerance. An uncertainty-aware\ndepth rendering strategy is then employed to weight depth contributions based\non the SUF, effectively reducing noise while preserving fine details.\nFurthermore, an uncertainty-guided normal refinement method adjusts the\ninfluence of neighboring depth values in normal estimation, promoting robust\nresults. Extensive experiments demonstrate that UNG-GS significantly\noutperforms state-of-the-art methods in both sparse and dense sequences. The\ncode will be open-source.\n","authors":["Zhen Tan","Xieyuanli Chen","Jinpu Zhang","Lei Feng","Dewen Hu"],"pdf_url":"https://arxiv.org/pdf/2503.11172v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.11167v1","updated":"2025-03-14T08:12:28Z","published":"2025-03-14T08:12:28Z","title":"Neurons: Emulating the Human Visual Cortex Improves Fidelity and\n  Interpretability in fMRI-to-Video Reconstruction","summary":"  Decoding visual stimuli from neural activity is essential for understanding\nthe human brain. While fMRI methods have successfully reconstructed static\nimages, fMRI-to-video reconstruction faces challenges due to the need for\ncapturing spatiotemporal dynamics like motion and scene transitions. Recent\napproaches have improved semantic and perceptual alignment but struggle to\nintegrate coarse fMRI data with detailed visual features. Inspired by the\nhierarchical organization of the visual system, we propose NEURONS, a novel\nframework that decouples learning into four correlated sub-tasks: key object\nsegmentation, concept recognition, scene description, and blurry video\nreconstruction. This approach simulates the visual cortex's functional\nspecialization, allowing the model to capture diverse video content. In the\ninference stage, NEURONS generates robust conditioning signals for a\npre-trained text-to-video diffusion model to reconstruct the videos. Extensive\nexperiments demonstrate that NEURONS outperforms state-of-the-art baselines,\nachieving solid improvements in video consistency (26.6%) and semantic-level\naccuracy (19.1%). Notably, NEURONS shows a strong functional correlation with\nthe visual cortex, highlighting its potential for brain-computer interfaces and\nclinical applications. Code and model weights will be available at:\nhttps://github.com/xmed-lab/NEURONS.\n","authors":["Haonan Wang","Qixiang Zhang","Lehan Wang","Xuanqi Huang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2503.11167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06146v2","updated":"2025-03-14T08:10:18Z","published":"2024-12-09T01:59:40Z","title":"Homogeneous Dynamics Space for Heterogeneous Humans","summary":"  Analyses of human motion kinematics have achieved tremendous advances.\nHowever, the production mechanism, known as human dynamics, is still\nundercovered. In this paper, we aim to push data-driven human dynamics\nunderstanding forward. We identify a major obstacle to this as the\nheterogeneity of existing human motion understanding efforts. Specifically,\nheterogeneity exists in not only the diverse kinematics representations and\nhierarchical dynamics representations but also in the data from different\ndomains, namely biomechanics and reinforcement learning. With an in-depth\nanalysis of the existing heterogeneity, we propose to emphasize the beneath\nhomogeneity: all of them represent the homogeneous fact of human motion, though\nfrom different perspectives. Given this, we propose Homogeneous Dynamics Space\n(HDyS) as a fundamental space for human dynamics by aggregating heterogeneous\ndata and training a homogeneous latent space with inspiration from the\ninverse-forward dynamics procedure. Leveraging the heterogeneous\nrepresentations and datasets, HDyS achieves decent mapping between human\nkinematics and dynamics. We demonstrate the feasibility of HDyS with extensive\nexperiments and applications. The project page is\nhttps://foruck.github.io/HDyS.\n","authors":["Xinpeng Liu","Junxuan Liang","Chenshuo Zhang","Zixuan Cai","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2412.06146v2.pdf","comment":"Accepted by CVPR 2025. Cewu Lu and Yong-Lu Li are the corresponding\n  authors"},{"id":"http://arxiv.org/abs/2503.11163v1","updated":"2025-03-14T08:03:20Z","published":"2025-03-14T08:03:20Z","title":"A Benchmarking Study of Vision-based Robotic Grasping Algorithms","summary":"  We present a benchmarking study of vision-based robotic grasping algorithms\nwith distinct approaches, and provide a comparative analysis. In particular, we\ncompare two machine-learning-based and two analytical algorithms using an\nexisting benchmarking protocol from the literature and determine the\nalgorithm's strengths and weaknesses under different experimental conditions.\nThese conditions include variations in lighting, background textures, cameras\nwith different noise levels, and grippers. We also run analogous experiments in\nsimulations and with real robots and present the discrepancies. Some\nexperiments are also run in two different laboratories using same protocols to\nfurther analyze the repeatability of our results. We believe that this study,\ncomprising 5040 experiments, provides important insights into the role and\nchallenges of systematic experimentation in robotic manipulation, and guides\nthe development of new algorithms by considering the factors that could impact\nthe performance. The experiment recordings and our benchmarking software are\npublicly available.\n","authors":["Bharath K Rameshbabu","Sumukh S Balakrishna","Brian Flynn","Vinarak Kapoor","Adam Norton","Holly Yanco","Berk Calli"],"pdf_url":"https://arxiv.org/pdf/2503.11163v1.pdf","comment":"Submitted to The IEEE Robotics and Automation Magazine"},{"id":"http://arxiv.org/abs/2503.11159v1","updated":"2025-03-14T07:56:20Z","published":"2025-03-14T07:56:20Z","title":"Stabilizing Quantization-Aware Training by Implicit-Regularization on\n  Hessian Matrix","summary":"  Quantization-Aware Training (QAT) is one of the prevailing neural network\ncompression solutions. However, its stability has been challenged for yielding\ndeteriorating performances as the quantization error is inevitable. We find\nthat the sharp landscape of loss, which leads to a dramatic performance drop,\nis an essential factor that causes instability. Theoretically, we have\ndiscovered that the perturbations in the feature would bring a flat local\nminima. However, simply adding perturbations into either weight or feature\nempirically deteriorates the performance of the Full Precision (FP) model. In\nthis paper, we propose Feature-Perturbed Quantization (FPQ) to stochastically\nperturb the feature and employ the feature distillation method to the quantized\nmodel. Our method generalizes well to different network architectures and\nvarious QAT methods. Furthermore, we mathematically show that FPQ implicitly\nregularizes the Hessian norm, which calibrates the smoothness of a loss\nlandscape. Extensive experiments demonstrate that our approach significantly\noutperforms the current State-Of-The-Art (SOTA) QAT methods and even the FP\ncounterparts.\n","authors":["Junbiao Pang","Tianyang Cai"],"pdf_url":"https://arxiv.org/pdf/2503.11159v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.01035v2","updated":"2025-03-14T07:53:09Z","published":"2023-10-02T09:24:54Z","title":"Learnable Cross-modal Knowledge Distillation for Multi-modal Learning\n  with Missing Modality","summary":"  The problem of missing modalities is both critical and non-trivial to be\nhandled in multi-modal models. It is common for multi-modal tasks that certain\nmodalities contribute more compared to other modalities, and if those important\nmodalities are missing, the model performance drops significantly. Such fact\nremains unexplored by current multi-modal approaches that recover the\nrepresentation from missing modalities by feature reconstruction or blind\nfeature aggregation from other modalities, instead of extracting useful\ninformation from the best performing modalities. In this paper, we propose a\nLearnable Cross-modal Knowledge Distillation (LCKD) model to adaptively\nidentify important modalities and distil knowledge from them to help other\nmodalities from the cross-modal perspective for solving the missing modality\nissue. Our approach introduces a teacher election procedure to select the most\n``qualified'' teachers based on their single modality performance on certain\ntasks. Then, cross-modal knowledge distillation is performed between teacher\nand student modalities for each task to push the model parameters to a point\nthat is beneficial for all tasks. Hence, even if the teacher modalities for\ncertain tasks are missing during testing, the available student modalities can\naccomplish the task well enough based on the learned knowledge from their\nautomatically elected teacher modalities. Experiments on the Brain Tumour\nSegmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods\nby a considerable margin, improving the state-of-the-art performance by 3.61%\nfor enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in\nterms of segmentation Dice score.\n","authors":["Hu Wang","Congbo Ma","Jianpeng Zhang","Yuan Zhang","Jodie Avery","Louise Hull","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2310.01035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16870v2","updated":"2025-03-14T07:36:26Z","published":"2024-11-25T19:08:38Z","title":"RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks","summary":"  Incremental learning aims to adapt to new sets of categories over time with\nminimal computational overhead. Prior work often addresses this task by\ntraining efficient task-specific adaptors that modify frozen layer weights or\nfeatures to capture relevant information without affecting predictions on\npreviously learned categories. While these adaptors are generally more\nefficient than finetuning the entire network, they still require tens to\nhundreds of thousands of task-specific trainable parameters even for relatively\nsmall networks, making it challenging to operate on resource-constrained\nenvironments with high communication costs like edge devices or mobile phones.\nThus, we propose Reparameterized, Compact weight Adaptation for Sequential\nTasks (RECAST), a novel method that dramatically reduces task-specific\ntrainable parameters to fewer than 50 - several orders of magnitude less than\ncompeting methods like LoRA. RECAST accomplishes this efficiency by learning to\ndecompose layer weights into a soft parameter-sharing framework consisting of\nshared weight templates and very few module-specific scaling factors or\ncoefficients. This soft parameter-sharing framework allows for effective\ntask-wise reparameterization by tuning only these coefficients while keeping\ntemplates frozen.A key innovation of RECAST is the novel weight reconstruction\npipeline called Neural Mimicry, which eliminates the need for pretraining from\nscratch. This allows for high-fidelity emulation of existing pretrained weights\nwithin our framework and provides quick adaptability to any model scale and\narchitecture. Extensive experiments across six datasets demonstrate RECAST\noutperforms the state-of-the-art by up to 3% across various scales,\narchitectures, and parameter spaces Moreover, we show that RECAST's\narchitecture-agnostic nature allows for seamless integration with existing\nmethods, further boosting performance.\n","authors":["Nazia Tasnim","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2411.16870v2.pdf","comment":"Accepted as a conference paper in ICLR, 2025"},{"id":"http://arxiv.org/abs/2501.05067v2","updated":"2025-03-14T07:29:54Z","published":"2025-01-09T08:43:57Z","title":"LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion\n  for Video Understanding","summary":"  In this paper, we introduce LLaVA-Octopus, a novel video multimodal large\nlanguage model. LLaVA-Octopus adaptively weights features from different visual\nprojectors based on user instructions, enabling us to leverage the\ncomplementary strengths of each projector. We observe that different visual\nprojectors exhibit distinct characteristics when handling specific tasks. For\ninstance, some projectors excel at capturing static details, while others are\nmore effective at processing temporal information, and some are better suited\nfor tasks requiring temporal coherence. By dynamically adjusting feature\nweights according to user instructions, LLaVA-Octopus dynamically selects and\ncombines the most suitable features, significantly enhancing the model's\nperformance in multimodal tasks. Experimental results demonstrate that\nLLaVA-Octopus achieves excellent performance across multiple benchmarks,\nespecially in tasks such as video question answering, long video understanding,\nand comprehensive multi-choices benchmarks, highlighting its broad application\npotential.\n","authors":["Jiaxing Zhao","Boyuan Sun","Xiang Chen","Xihan Wei","Qibin Hou"],"pdf_url":"https://arxiv.org/pdf/2501.05067v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.11144v1","updated":"2025-03-14T07:22:07Z","published":"2025-03-14T07:22:07Z","title":"MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling","summary":"  Large-scale pre-training of deep models, followed by fine-tuning them, has\nbecome the cornerstone of natural language processing (NLP). The prevalence of\ndata coupled with computational resources has led to large models with a\nconsiderable number of parameters. While the massive size of these models has\nled to remarkable success in many NLP tasks, a detriment is the expense\nrequired to retrain all the base model's parameters for the adaptation to each\ntask or domain. Parameter Efficient Fine-Tuning (PEFT) provides an effective\nsolution for this challenge by minimizing the number of parameters required to\nbe fine-tuned while maintaining the quality of the model. While existing\nmethods have achieved impressive results, they mainly focus on adapting a\nsubset of parameters, weight reparameterization, and prompt engineering. In\nthis paper, we study layers as extractors of different types of linguistic\ninformation that are valuable when used in conjunction. We then propose the\nMixture of Layer Experts (MoLEx), a novel sparse mixture of experts (SMoE)\nwhose experts are layers in the pre-trained model. It performs a conditional\ncomputation of a mixture of layers during fine-tuning to provide the model with\nmore structural knowledge about the data. By providing an avenue for\ninformation exchange between layers, MoLEx enables the model to make a more\nwell-informed prediction for the downstream task, leading to better fine-tuning\nresults with the same number of effective parameters. As experts can be\nprocessed in parallel, MoLEx introduces minimal additional computational\noverhead. We empirically corroborate the advantages of MoLEx when combined with\npopular PEFT baseline methods on a variety of downstream fine-tuning tasks,\nincluding the popular GLUE benchmark as well as the End-to-End Challenge (E2E).\nThe code is publicly available at https://github.com/rachtsy/molex.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.11144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02731v3","updated":"2025-03-14T07:17:05Z","published":"2023-03-05T17:55:15Z","title":"Virtual Guidance as a Mid-level Representation for Navigation with\n  Augmented Reality","summary":"  In the context of autonomous navigation, effectively conveying abstract\nnavigational cues to agents in dynamic environments presents significant\nchallenges, particularly when navigation information is derived from diverse\nmodalities such as both vision and high-level language descriptions. To address\nthis issue, we introduce a novel technique termed `Virtual Guidance,' which is\ndesigned to visually represent non-visual instructional signals. These visual\ncues are overlaid onto the agent's camera view and served as comprehensible\nnavigational guidance signals. To validate the concept of virtual guidance, we\npropose a sim-to-real framework that enables the transfer of the trained policy\nfrom simulated environments to real world, ensuring the adaptability of virtual\nguidance in practical scenarios. We evaluate and compare the proposed method\nagainst a non-visual guidance baseline through detailed experiments in\nsimulation. The experimental results demonstrate that the proposed virtual\nguidance approach outperforms the baseline methods across multiple scenarios\nand offers clear evidence of its effectiveness in autonomous navigation tasks.\n","authors":["Hsuan-Kung Yang","Tsung-Chih Chiang","Jou-Min Liu","Ting-Ru Liu","Chun-Wei Huang","Tsu-Ching Hsiao","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2303.02731v3.pdf","comment":"Tsung-Chih Chiang, Jou-Min Liu, Ting-Ru Liu, and Chun-Wei Huang\n  contributed equally to this work; This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2503.11143v1","updated":"2025-03-14T07:16:43Z","published":"2025-03-14T07:16:43Z","title":"GaussianIP: Identity-Preserving Realistic 3D Human Generation via\n  Human-Centric Diffusion Prior","summary":"  Text-guided 3D human generation has advanced with the development of\nefficient 3D representations and 2D-lifting methods like Score Distillation\nSampling (SDS). However, current methods suffer from prolonged training times\nand often produce results that lack fine facial and garment details. In this\npaper, we propose GaussianIP, an effective two-stage framework for generating\nidentity-preserving realistic 3D humans from text and image prompts. Our core\ninsight is to leverage human-centric knowledge to facilitate the generation\nprocess. In stage 1, we propose a novel Adaptive Human Distillation Sampling\n(AHDS) method to rapidly generate a 3D human that maintains high identity\nconsistency with the image prompt and achieves a realistic appearance. Compared\nto traditional SDS methods, AHDS better aligns with the human-centric\ngeneration process, enhancing visual quality with notably fewer training steps.\nTo further improve the visual quality of the face and clothes regions, we\ndesign a View-Consistent Refinement (VCR) strategy in stage 2. Specifically, it\nproduces detail-enhanced results of the multi-view images from stage 1\niteratively, ensuring the 3D texture consistency across views via mutual\nattention and distance-guided attention fusion. Then a polished version of the\n3D human can be achieved by directly perform reconstruction with the refined\nimages. Extensive experiments demonstrate that GaussianIP outperforms existing\nmethods in both visual quality and training efficiency, particularly in\ngenerating identity-preserving results. Our code is available at:\nhttps://github.com/silence-tang/GaussianIP.\n","authors":["Zichen Tang","Yuan Yao","Miaomiao Cui","Liefeng Bo","Hongyu Yang"],"pdf_url":"https://arxiv.org/pdf/2503.11143v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11140v1","updated":"2025-03-14T07:08:22Z","published":"2025-03-14T07:08:22Z","title":"Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for\n  Stable Lesion Segmentation","summary":"  Deep learning has achieved significant advancements in medical image\nsegmentation, but existing models still face challenges in accurately\nsegmenting lesion regions. The main reason is that some lesion regions in\nmedical images have unclear boundaries, irregular shapes, and small tissue\ndensity differences, leading to label ambiguity. However, the existing model\ntreats all data equally without taking quality differences into account in the\ntraining process, resulting in noisy labels negatively impacting model training\nand unstable feature representations. In this paper, a data-driven alternating\nlearning (DALE) paradigm is proposed to optimize the model's training process,\nachieving stable and high-precision segmentation. The paradigm focuses on two\nkey points: (1) reducing the impact of noisy labels, and (2) calibrating\nunstable representations. To mitigate the negative impact of noisy labels, a\nloss consistency-based collaborative optimization method is proposed, and its\neffectiveness is theoretically demonstrated. Specifically, the label confidence\nparameters are introduced to dynamically adjust the influence of labels of\ndifferent confidence levels during model training, thus reducing the influence\nof noise labels. To calibrate the learning bias of unstable representations, a\ndistribution alignment method is proposed. This method restores the underlying\ndistribution of unstable representations, thereby enhancing the discriminative\ncapability of fuzzy region representations. Extensive experiments on various\nbenchmarks and model backbones demonstrate the superiority of the DALE\nparadigm, achieving an average performance improvement of up to 7.16%.\n","authors":["Lexin Fang","Yunyang Xu","Xiang Ma","Xuemei Li","Caiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11140v1.pdf","comment":"10 pages, 11 figures, accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.11133v1","updated":"2025-03-14T06:50:37Z","published":"2025-03-14T06:50:37Z","title":"SpaceSeg: A High-Precision Intelligent Perception Segmentation Method\n  for Multi-Spacecraft On-Orbit Targets","summary":"  With the continuous advancement of human exploration into deep space,\nintelligent perception and high-precision segmentation technology for on-orbit\nmulti-spacecraft targets have become critical factors for ensuring the success\nof modern space missions. However, the complex deep space environment, diverse\nimaging conditions, and high variability in spacecraft morphology pose\nsignificant challenges to traditional segmentation methods. This paper proposes\nSpaceSeg, an innovative vision foundation model-based segmentation framework\nwith four core technical innovations: First, the Multi-Scale Hierarchical\nAttention Refinement Decoder (MSHARD) achieves high-precision feature decoding\nthrough cross-resolution feature fusion via hierarchical attention. Second, the\nMulti-spacecraft Connected Component Analysis (MS-CCA) effectively resolves\ntopological structure confusion in dense targets. Third, the Spatial Domain\nAdaptation Transform framework (SDAT) eliminates cross-domain disparities and\nresist spatial sensor perturbations through composite enhancement strategies.\nFinally, a custom Multi-Spacecraft Segmentation Task Loss Function is created\nto significantly improve segmentation robustness in deep space scenarios. To\nsupport algorithm validation, we construct the first multi-scale on-orbit\nmulti-spacecraft semantic segmentation dataset SpaceES, which covers four types\nof spatial backgrounds and 17 typical spacecraft targets. In testing, SpaceSeg\nachieves state-of-the-art performance with 89.87$\\%$ mIoU and 99.98$\\%$ mAcc,\nsurpassing existing best methods by 5.71 percentage points. The dataset and\ncode are open-sourced at https://github.com/Akibaru/SpaceSeg to provide\ncritical technical support for next-generation space situational awareness\nsystems.\n","authors":["Hao Liu","Pengyu Guo","Siyuan Yang","Zeqing Jiang","Qinglei Hu","Dongyu Li"],"pdf_url":"https://arxiv.org/pdf/2503.11133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11129v1","updated":"2025-03-14T06:44:01Z","published":"2025-03-14T06:44:01Z","title":"Direction-Aware Diagonal Autoregressive Image Generation","summary":"  The raster-ordered image token sequence exhibits a significant Euclidean\ndistance between index-adjacent tokens at line breaks, making it unsuitable for\nautoregressive generation. To address this issue, this paper proposes\nDirection-Aware Diagonal Autoregressive Image Generation (DAR) method, which\ngenerates image tokens following a diagonal scanning order. The proposed\ndiagonal scanning order ensures that tokens with adjacent indices remain in\nclose proximity while enabling causal attention to gather information from a\nbroader range of directions. Additionally, two direction-aware modules: 4D-RoPE\nand direction embeddings are introduced, enhancing the model's capability to\nhandle frequent changes in generation direction. To leverage the\nrepresentational capacity of the image tokenizer, we use its codebook as the\nimage token embeddings. We propose models of varying scales, ranging from 485M\nto 2.0B. On the 256$\\times$256 ImageNet benchmark, our DAR-XL (2.0B)\noutperforms all previous autoregressive image generators, achieving a\nstate-of-the-art FID score of 1.37.\n","authors":["Yijia Xu","Jianzhong Ju","Jian Luan","Jinshi Cui"],"pdf_url":"https://arxiv.org/pdf/2503.11129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11122v1","updated":"2025-03-14T06:35:38Z","published":"2025-03-14T06:35:38Z","title":"DriveGEN: Generalized and Robust 3D Detection in Driving via\n  Controllable Text-to-Image Diffusion Generation","summary":"  In autonomous driving, vision-centric 3D detection aims to identify 3D\nobjects from images. However, high data collection costs and diverse real-world\nscenarios limit the scale of training data. Once distribution shifts occur\nbetween training and test data, existing methods often suffer from performance\ndegradation, known as Out-of-Distribution (OOD) problems. To address this,\ncontrollable Text-to-Image (T2I) diffusion offers a potential solution for\ntraining data enhancement, which is required to generate diverse OOD scenarios\nwith precise 3D object geometry. Nevertheless, existing controllable T2I\napproaches are restricted by the limited scale of training data or struggle to\npreserve all annotated 3D objects. In this paper, we present DriveGEN, a method\ndesigned to improve the robustness of 3D detectors in Driving via Training-Free\nControllable Text-to-Image Diffusion Generation. Without extra diffusion model\ntraining, DriveGEN consistently preserves objects with precise 3D geometry\nacross diverse OOD generations, consisting of 2 stages: 1) Self-Prototype\nExtraction: We empirically find that self-attention features are semantic-aware\nbut require accurate region selection for 3D objects. Thus, we extract precise\nobject features via layouts to capture 3D object geometry, termed\nself-prototypes. 2) Prototype-Guided Diffusion: To preserve objects across\nvarious OOD scenarios, we perform semantic-aware feature alignment and shallow\nfeature alignment during denoising. Extensive experiments demonstrate the\neffectiveness of DriveGEN in improving 3D detection. The code is available at\nhttps://github.com/Hongbin98/DriveGEN.\n","authors":["Hongbin Lin","Zilu Guo","Yifan Zhang","Shuaicheng Niu","Yafeng Li","Ruimao Zhang","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2503.11122v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.10636v2","updated":"2025-03-14T06:35:23Z","published":"2025-03-13T17:59:56Z","title":"The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation","summary":"  Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT\n","authors":["Ho Kei Cheng","Alexander Schwing"],"pdf_url":"https://arxiv.org/pdf/2503.10636v2.pdf","comment":"Project page: https://hkchengrex.github.io/C2OT"},{"id":"http://arxiv.org/abs/2503.11120v1","updated":"2025-03-14T06:32:42Z","published":"2025-03-14T06:32:42Z","title":"A Multi-Objective Evaluation Framework for Analyzing Utility-Fairness\n  Trade-Offs in Machine Learning Systems","summary":"  The evaluation of fairness models in Machine Learning involves complex\nchallenges, such as defining appropriate metrics, balancing trade-offs between\nutility and fairness, and there are still gaps in this stage. This work\npresents a novel multi-objective evaluation framework that enables the analysis\nof utility-fairness trade-offs in Machine Learning systems. The framework was\ndeveloped using criteria from Multi-Objective Optimization that collect\ncomprehensive information regarding this complex evaluation task. The\nassessment of multiple Machine Learning systems is summarized, both\nquantitatively and qualitatively, in a straightforward manner through a radar\nchart and a measurement table encompassing various aspects such as convergence,\nsystem capacity, and diversity. The framework's compact representation of\nperformance facilitates the comparative analysis of different Machine Learning\nstrategies for decision-makers, in real-world applications, with single or\nmultiple fairness requirements. The framework is model-agnostic and flexible to\nbe adapted to any kind of Machine Learning systems, that is, black- or\nwhite-box, any kind and quantity of evaluation metrics, including\nmultidimensional fairness criteria. The functionality and effectiveness of the\nproposed framework is shown with different simulations, and an empirical study\nconducted on a real-world dataset with various Machine Learning systems.\n","authors":["Gökhan Özbulak","Oscar Jimenez-del-Toro","Maíra Fatoretto","Lilian Berton","André Anjos"],"pdf_url":"https://arxiv.org/pdf/2503.11120v1.pdf","comment":"11 pages, 13 figures"},{"id":"http://arxiv.org/abs/2503.11117v1","updated":"2025-03-14T06:29:47Z","published":"2025-03-14T06:29:47Z","title":"Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied\n  Question Answering","summary":"  Embodied Question Answering (EQA) is a challenging task in embodied\nintelligence that requires agents to dynamically explore 3D environments,\nactively gather visual information, and perform multi-step reasoning to answer\nquestions. However, current EQA approaches suffer from critical limitations in\nexploration efficiency, dataset design, and evaluation metrics. Moreover,\nexisting datasets often introduce biases or prior knowledge, leading to\ndisembodied reasoning, while frontier-based exploration strategies struggle in\ncluttered environments and fail to ensure fine-grained exploration of\ntask-relevant areas. To address these challenges, we construct the\nEXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the\nlargest dataset designed specifically to evaluate both exploration and\nreasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories\nand 2,044 question-trajectory pairs. To improve exploration efficiency, we\npropose Fine-EQA, a hybrid exploration model that integrates frontier-based and\ngoal-oriented navigation to guide agents toward task-relevant regions more\neffectively. Additionally, we introduce a novel evaluation metric,\nExploration-Answer Consistency (EAC), which ensures faithful assessment by\nmeasuring the alignment between answer grounding and exploration reliability.\nExtensive experimental comparisons with state-of-the-art EQA models demonstrate\nthe effectiveness of our EXPRESS-Bench in advancing embodied exploration and\nquestion reasoning.\n","authors":["Kaixuan Jiang","Yang Liu","Weixing Chen","Jingzhou Luo","Ziliang Chen","Ling Pan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2503.11117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11115v1","updated":"2025-03-14T06:26:55Z","published":"2025-03-14T06:26:55Z","title":"Solution for 8th Competition on Affective & Behavior Analysis\n  in-the-wild","summary":"  In this report, we present our solution for the Action Unit (AU) Detection\nChallenge, in 8th Competition on Affective Behavior Analysis in-the-wild. In\norder to achieve robust and accurate classification of facial action unit in\nthe wild environment, we introduce an innovative method that leverages\naudio-visual multimodal data. Our method employs ConvNeXt as the image encoder\nand uses Whisper to extract Mel spectrogram features. For these features, we\nutilize a Transformer encoder-based feature fusion module to integrate the\naffective information embedded in audio and image features. This ensures the\nprovision of rich high-dimensional feature representations for the subsequent\nmultilayer perceptron (MLP) trained on the Aff-Wild2 dataset, enhancing the\naccuracy of AU detection.\n","authors":["Jun Yu","Yunxiang Zhang","Xilong Lu","Yang Zheng","Yongqi Wang","Lingsi Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.11115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11509v2","updated":"2025-03-14T06:12:38Z","published":"2024-12-16T07:33:23Z","title":"Skip Tuning: Pre-trained Vision-Language Models are Effective and\n  Efficient Adapters Themselves","summary":"  Prompt tuning (PT) has long been recognized as an effective and efficient\nparadigm for transferring large pre-trained vision-language models (VLMs) to\ndownstream tasks by learning a tiny set of context vectors. Nevertheless, in\nthis work, we reveal that freezing the parameters of VLMs during learning the\ncontext vectors neither facilitates the transferability of pre-trained\nknowledge nor improves the memory and time efficiency significantly. Upon\nfurther investigation, we find that reducing both the length and width of the\nfeature-gradient propagation flows of the full fine-tuning (FT) baseline is key\nto achieving effective and efficient knowledge transfer. Motivated by this, we\npropose Skip Tuning, a novel paradigm for adapting VLMs to downstream tasks.\nUnlike existing PT or adapter-based methods, Skip Tuning applies Layer-wise\nSkipping (LSkip) and Class-wise Skipping (CSkip) upon the FT baseline without\nintroducing extra context vectors or adapter modules. Extensive experiments\nacross a wide spectrum of benchmarks demonstrate the superior effectiveness and\nefficiency of our Skip Tuning over both PT and adapter-based methods. Code:\nhttps://github.com/Koorye/SkipTuning.\n","authors":["Shihan Wu","Ji Zhang","Pengpeng Zeng","Lianli Gao","Jingkuan Song","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.11509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15251v2","updated":"2025-03-14T05:54:56Z","published":"2025-02-21T07:02:05Z","title":"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training","summary":"  We present a framework for pre-training of 3D hand pose estimation from\nin-the-wild hand images sharing with similar hand characteristics, dubbed\nSimHand. Pre-training with large-scale images achieves promising results in\nvarious tasks, but prior methods for 3D hand pose pre-training have not fully\nutilized the potential of diverse hand images accessible from in-the-wild\nvideos. To facilitate scalable pre-training, we first prepare an extensive pool\nof hand images from in-the-wild videos and design our pre-training method with\ncontrastive learning. Specifically, we collect over 2.0M hand images from\nrecent human-centric videos, such as 100DOH and Ego4D. To extract\ndiscriminative information from these images, we focus on the similarity of\nhands: pairs of non-identical samples with similar hand poses. We then propose\na novel contrastive learning method that embeds similar hand pairs closer in\nthe feature space. Our method not only learns from similar samples but also\nadaptively weights the contrastive learning loss based on inter-sample\ndistance, leading to additional performance gains. Our experiments demonstrate\nthat our method outperforms conventional contrastive learning approaches that\nproduce positive pairs sorely from a single image with data augmentation. We\nachieve significant improvements over the state-of-the-art method (PeCLR) in\nvarious datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on\nAssemblyHands.\n  Our code is available at https://github.com/ut-vision/SiMHand.\n","authors":["Nie Lin","Takehiko Ohkawa","Yifei Huang","Mingfang Zhang","Minjie Cai","Ming Li","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2502.15251v2.pdf","comment":"ICLR 2025. arXiv admin note: text overlap with arXiv:2409.09714"},{"id":"http://arxiv.org/abs/2501.01709v2","updated":"2025-03-14T05:52:36Z","published":"2025-01-03T09:10:34Z","title":"MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders","summary":"  Visual encoders are fundamental components in vision-language models (VLMs),\neach showcasing unique strengths derived from various pre-trained visual\nfoundation models. To leverage the various capabilities of these encoders,\nrecent studies incorporate multiple encoders within a single VLM, leading to a\nconsiderable increase in computational cost. In this paper, we present\nMixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework\nthat distills the unique proficiencies of multiple vision encoders into a\nsingle, efficient encoder model. Specifically, to mitigate conflicts and retain\nthe unique characteristics of each teacher encoder, we employ low-rank\nadaptation (LoRA) and mixture-of-experts (MoEs) to selectively activate\nspecialized knowledge based on input features, enhancing both adaptability and\nefficiency. To regularize the KD process and enhance performance, we propose an\nattention-based distillation strategy that adaptively weighs the different\nencoders and emphasizes valuable visual tokens, reducing the burden of\nreplicating comprehensive but distinct features from multiple teachers.\nComprehensive experiments on popular VLMs, such as LLaVA and LLaVA-NeXT,\nvalidate the effectiveness of our method. Our code is available at:\nhttps://github.com/hey-cjj/MoVE-KD.\n","authors":["Jiajun Cao","Yuan Zhang","Tao Huang","Ming Lu","Qizhe Zhang","Ruichuan An","Ningning MA","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.01709v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.11103v1","updated":"2025-03-14T05:47:17Z","published":"2025-03-14T05:47:17Z","title":"Quantifying Interpretability in CLIP Models with Concept Consistency","summary":"  CLIP is one of the most popular foundational models and is heavily used for\nmany vision-language tasks. However, little is known about the inner workings\nof CLIP. While recent work has proposed decomposition-based interpretability\nmethods for identifying textual descriptions of attention heads in CLIP, the\nimplications of conceptual consistency in these text labels on interpretability\nand model performance has not been explored. To bridge this gap, we study the\nconceptual consistency of text descriptions for attention heads in CLIP-like\nmodels. We conduct extensive experiments on six different models from OpenAI\nand OpenCLIP which vary by size, type of pre-training data and patch size. We\npropose Concept Consistency Score (CCS), a novel interpretability metric that\nmeasures how consistently individual attention heads in CLIP models align with\nspecific concepts. To assign concept labels to heads, we use in-context\nlearning with ChatGPT, guided by a few manually-curated examples, and validate\nthese labels using an LLM-as-a-judge approach. Our soft-pruning experiments\nreveal that high CCS heads are critical for preserving model performance, as\npruning them leads to a significantly larger performance drop than pruning\nrandom or low CCS heads. Notably, we find that high CCS heads capture essential\nconcepts and play a key role in out-of-domain detection, concept-specific\nreasoning, and video-language understanding. These results position CCS as a\npowerful interpretability metric for analyzing CLIP-like models.\n","authors":["Avinash Madasu","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2503.11103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11101v1","updated":"2025-03-14T05:43:47Z","published":"2025-03-14T05:43:47Z","title":"A Survey on Self-supervised Contrastive Learning for Multimodal\n  Text-Image Analysis","summary":"  Self-supervised learning is a machine learning approach that generates\nimplicit labels by learning underlined patterns and extracting discriminative\nfeatures from unlabeled data without manual labelling. Contrastive learning\nintroduces the concept of \"positive\" and \"negative\" samples, where positive\npairs (e.g., variation of the same image/object) are brought together in the\nembedding space, and negative pairs (e.g., views from different images/objects)\nare pushed farther away. This methodology has shown significant improvements in\nimage understanding and image text analysis without much reliance on labeled\ndata. In this paper, we comprehensively discuss the terminologies, recent\ndevelopments and applications of contrastive learning with respect to\ntext-image models. Specifically, we provide an overview of the approaches of\ncontrastive learning in text-image models in recent years. Secondly, we\ncategorize the approaches based on different model structures. Thirdly, we\nfurther introduce and discuss the latest advances of the techniques used in the\nprocess such as pretext tasks for both images and text, architectural\nstructures, and key trends. Lastly, we discuss the recent state-of-art\napplications of self-supervised contrastive learning Text-Image based models.\n","authors":["Asifullah Khan","Laiba Asmatullah","Anza Malik","Shahzaib Khan","Hamna Asif"],"pdf_url":"https://arxiv.org/pdf/2503.11101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11097v1","updated":"2025-03-14T05:40:05Z","published":"2025-03-14T05:40:05Z","title":"A Novel Decomposed Feature-Oriented Framework for Open-Set Semantic\n  Segmentation on LiDAR Data","summary":"  Semantic segmentation is a key technique that enables mobile robots to\nunderstand and navigate surrounding environments autonomously. However, most\nexisting works focus on segmenting known objects, overlooking the\nidentification of unknown classes, which is common in real-world applications.\nIn this paper, we propose a feature-oriented framework for open-set semantic\nsegmentation on LiDAR data, capable of identifying unknown objects while\nretaining the ability to classify known ones. We design a decomposed\ndual-decoder network to simultaneously perform closed-set semantic segmentation\nand generate distinctive features for unknown objects. The network is trained\nwith multi-objective loss functions to capture the characteristics of known and\nunknown objects. Using the extracted features, we introduce an anomaly\ndetection mechanism to identify unknown objects. By integrating the results of\nclose-set semantic segmentation and anomaly detection, we achieve effective\nfeature-driven LiDAR open-set semantic segmentation. Evaluations on both\nSemanticKITTI and nuScenes datasets demonstrate that our proposed framework\nsignificantly outperforms state-of-the-art methods. The source code will be\nmade publicly available at https://github.com/nubot-nudt/DOSS.\n","authors":["Wenbang Deng","Xieyuanli Chen","Qinghua Yu","Yunze He","Junhao Xiao","Huimin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.11097v1.pdf","comment":"This paper has been accepted by 2025 ICRA"},{"id":"http://arxiv.org/abs/2503.11096v1","updated":"2025-03-14T05:38:53Z","published":"2025-03-14T05:38:53Z","title":"Augmenting Image Annotation: A Human-LMM Collaborative Framework for\n  Efficient Object Selection and Label Generation","summary":"  Traditional image annotation tasks rely heavily on human effort for object\nselection and label assignment, making the process time-consuming and prone to\ndecreased efficiency as annotators experience fatigue after extensive work.\nThis paper introduces a novel framework that leverages the visual understanding\ncapabilities of large multimodal models (LMMs), particularly GPT, to assist\nannotation workflows. In our proposed approach, human annotators focus on\nselecting objects via bounding boxes, while the LMM autonomously generates\nrelevant labels. This human-AI collaborative framework enhances annotation\nefficiency by reducing the cognitive and time burden on human annotators. By\nanalyzing the system's performance across various types of annotation tasks, we\ndemonstrate its ability to generalize to tasks such as object recognition,\nscene description, and fine-grained categorization. Our proposed framework\nhighlights the potential of this approach to redefine annotation workflows,\noffering a scalable and efficient solution for large-scale data labeling in\ncomputer vision. Finally, we discuss how integrating LMMs into the annotation\npipeline can advance bidirectional human-AI alignment, as well as the\nchallenges of alleviating the \"endless annotation\" burden in the face of\ninformation overload by shifting some of the work to AI.\n","authors":["He Zhang","Xinyi Fu","John M. Carroll"],"pdf_url":"https://arxiv.org/pdf/2503.11096v1.pdf","comment":"This paper will appear at ICLR 2025 Workshop on Bidirectional\n  Human-AI Alignment"},{"id":"http://arxiv.org/abs/2503.11094v1","updated":"2025-03-14T05:35:38Z","published":"2025-03-14T05:35:38Z","title":"Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with\n  Multimodal Large Language Model in Open Space","summary":"  Spatial reasoning is a fundamental capability of embodied agents and has\ngarnered widespread attention in the field of multimodal large language models\n(MLLMs). In this work, we propose a novel benchmark, Open3DVQA, to\ncomprehensively evaluate the spatial reasoning capacities of current\nstate-of-the-art (SOTA) foundation models in open 3D space. Open3DVQA consists\nof 9k VQA samples, collected using an efficient semi-automated tool in a\nhigh-fidelity urban simulator. We evaluate several SOTA MLLMs across various\naspects of spatial reasoning, such as relative and absolute spatial\nrelationships, situational reasoning, and object-centric spatial attributes.\nOur results reveal that: 1) MLLMs perform better at answering questions\nregarding relative spatial relationships than absolute spatial relationships,\n2) MLLMs demonstrate similar spatial reasoning abilities for both egocentric\nand allocentric perspectives, and 3) Fine-tuning large models significantly\nimproves their performance across different spatial reasoning tasks. We believe\nthat our open-source data collection tools and in-depth analyses will inspire\nfurther research on MLLM spatial reasoning capabilities. The benchmark is\navailable at https://github.com/WeichenZh/Open3DVQA.\n","authors":["Weichen Zhan","Zile Zhou","Zhiheng Zheng","Chen Gao","Jinqiang Cui","Yong Li","Xinlei Chen","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11093v1","updated":"2025-03-14T05:34:16Z","published":"2025-03-14T05:34:16Z","title":"OmniDiff: A Comprehensive Benchmark for Fine-grained Image Difference\n  Captioning","summary":"  Image Difference Captioning (IDC) aims to generate natural language\ndescriptions of subtle differences between image pairs, requiring both precise\nvisual change localization and coherent semantic expression. Despite recent\nadvancements, existing datasets often lack breadth and depth, limiting their\napplicability in complex and dynamic environments: (1) from a breadth\nperspective, current datasets are constrained to limited variations of objects\nin specific scenes, and (2) from a depth perspective, prior benchmarks often\nprovide overly simplistic descriptions. To address these challenges, we\nintroduce OmniDiff, a comprehensive dataset comprising 324 diverse\nscenarios-spanning real-world complex environments and 3D synthetic\nsettings-with fine-grained human annotations averaging 60 words in length and\ncovering 12 distinct change types. Building on this foundation, we propose\nM$^3$Diff, a MultiModal large language model enhanced by a plug-and-play\nMulti-scale Differential Perception (MDP) module. This module improves the\nmodel's ability to accurately identify and describe inter-image differences\nwhile maintaining the foundational model's generalization capabilities. With\nthe addition of the OmniDiff dataset, M$^3$Diff achieves state-of-the-art\nperformance across multiple benchmarks, including Spot-the-Diff, IEdit,\nCLEVR-Change, CLEVR-DC, and OmniDiff, demonstrating significant improvements in\ncross-scenario difference recognition accuracy compared to existing methods.\nThe dataset, code, and models will be made publicly available to support\nfurther research.\n","authors":["Yuan Liu","Saihui Hou","Saijie Hou","Jiabao Du","Shibei Meng","Yongzhen Huang"],"pdf_url":"https://arxiv.org/pdf/2503.11093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11091v1","updated":"2025-03-14T05:20:43Z","published":"2025-03-14T05:20:43Z","title":"Aerial Vision-and-Language Navigation with Grid-based View Selection and\n  Map Construction","summary":"  Aerial Vision-and-Language Navigation (Aerial VLN) aims to obtain an unmanned\naerial vehicle agent to navigate aerial 3D environments following human\ninstruction. Compared to ground-based VLN, aerial VLN requires the agent to\ndecide the next action in both horizontal and vertical directions based on the\nfirst-person view observations. Previous methods struggle to perform well due\nto the longer navigation path, more complicated 3D scenes, and the neglect of\nthe interplay between vertical and horizontal actions. In this paper, we\npropose a novel grid-based view selection framework that formulates aerial VLN\naction prediction as a grid-based view selection task, incorporating vertical\naction prediction in a manner that accounts for the coupling with horizontal\nactions, thereby enabling effective altitude adjustments. We further introduce\na grid-based bird's eye view map for aerial space to fuse the visual\ninformation in the navigation history, provide contextual scene information,\nand mitigate the impact of obstacles. Finally, a cross-modal transformer is\nadopted to explicitly align the long navigation history with the instruction.\nWe demonstrate the superiority of our method in extensive experiments.\n","authors":["Ganlong Zhao","Guanbin Li","Jia Pan","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2503.11091v1.pdf","comment":"Under Submission"},{"id":"http://arxiv.org/abs/2411.19895v4","updated":"2025-03-14T05:13:01Z","published":"2024-11-29T17:59:03Z","title":"GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for\nvarious applications. However, considering security, capacity, invisibility,\nand training efficiency, the copyright of 3DGS assets is not well protected as\nexisting watermarking methods are unsuited for its rendering pipeline. In this\npaper, we propose GuardSplat, an innovative and efficient framework for\nwatermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline\nfor optimizing the message decoder with minimal costs. The key objective is to\nachieve high-accuracy extraction by leveraging CLIP's aligning capability and\nrich representations, demonstrating exceptional capacity and efficiency. 2) We\ntailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS,\nseamlessly embedding messages into the SH features of each 3D Gaussian while\npreserving the original 3D structure. This enables watermarking 3DGS assets\nwith minimal fidelity trade-offs and prevents malicious users from removing the\nwatermarks from the model files, meeting the demands for invisibility and\nsecurity. 3) We present an Anti-distortion Message Extraction module to improve\nrobustness against various distortions. Experiments demonstrate that GuardSplat\noutperforms state-of-the-art and achieves fast optimization speed. Project page\nis at https://narcissusex.github.io/GuardSplat, and Code is at\nhttps://github.com/NarcissusEx/GuardSplat.\n","authors":["Zixuan Chen","Guangcong Wang","Jiahao Zhu","Jianhuang Lai","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2411.19895v4.pdf","comment":"This paper is accepted by the IEEE/CVF International Conference on\n  Computer Vision and Pattern Recognition (CVPR), 2025"},{"id":"http://arxiv.org/abs/2503.11089v1","updated":"2025-03-14T05:06:07Z","published":"2025-03-14T05:06:07Z","title":"EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for\n  Visual Spatial Tasks","summary":"  While multimodal large language models (MLLMs) have made groundbreaking\nprogress in embodied intelligence, they still face significant challenges in\nspatial reasoning for complex long-horizon tasks. To address this gap, we\npropose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that\nintegrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to\nenhance spatial understanding for embodied agents. By explicitly constructing\nstructured knowledge representations through dynamic scene graphs, our method\nenables zero-shot spatial reasoning without task-specific fine-tuning. This\napproach not only disentangles intricate spatial relationships but also aligns\nreasoning steps with actionable environmental dynamics. To rigorously evaluate\nperformance, we introduce the eSpatial-Benchmark, a comprehensive dataset\nincluding real-world embodied scenarios with fine-grained spatial annotations\nand adaptive task difficulty levels. Experiments demonstrate that our framework\nsignificantly outperforms existing MLLM-based methods in accuracy and reasoning\ncoherence, particularly in long-horizon tasks requiring iterative environment\ninteraction. The results reveal the untapped potential of MLLMs for embodied\nintelligence when equipped with structured, explainable reasoning mechanisms,\npaving the way for more reliable deployment in real-world spatial applications.\nThe codes and datasets will be released soon.\n","authors":["Yi Zhang","Qiang Zhang","Xiaozhu Ju","Zhaoyang Liu","Jilei Mao","Jingkai Sun","Jintao Wu","Shixiong Gao","Shihan Cai","Zhiyuan Qin","Linkai Liang","Jiaxu Wang","Yiqun Duan","Jiahang Cao","Renjing Xu","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2503.11089v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2501.05205v3","updated":"2025-03-14T05:05:12Z","published":"2025-01-09T12:55:55Z","title":"Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant\n  Learning","summary":"  Infants develop complex visual understanding rapidly, even preceding of the\nacquisition of linguistic inputs. As computer vision seeks to replicate the\nhuman vision system, understanding infant visual development may offer valuable\ninsights. In this paper, we present an interdisciplinary study exploring this\nquestion: can a computational model that imitates the infant learning process\ndevelop broader visual concepts that extend beyond the vocabulary it has heard,\nsimilar to how infants naturally learn? To investigate this, we analyze a\nrecently published model in Science by Vong et al.,which is trained on\nlongitudinal, egocentric images of a single child paired with transcribed\nparental speech. We introduce a training-free framework that can discover\nvisual concept neurons hidden in the model's internal representations. Our\nfindings show that these neurons can classify objects outside its original\nvocabulary. Furthermore, we compare the visual representations in infant-like\nmodels with those in moder computer vision models, such as CLIP or ImageNet\npre-trained model, highlighting key similarities and differences. Ultimately,\nour work bridges cognitive science and computer vision by analyzing the\ninternal representations of a computational model trained on an infant's visual\nand linguistic inputs.\n","authors":["Xueyi Ke","Satoshi Tsutsui","Yayun Zhang","Bihan Wen"],"pdf_url":"https://arxiv.org/pdf/2501.05205v3.pdf","comment":"12 pages, 11 figures. Accepted at CVPR 2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2008.13078v3","updated":"2025-03-14T15:01:29Z","published":"2020-08-30T02:57:38Z","title":"Probability-turbulence divergence: A tunable allotaxonometric instrument\n  for comparing heavy-tailed categorical distributions","summary":"  Real-world complex systems often comprise many distinct types of elements as\nwell as many more types of networked interactions between elements. When the\nrelative abundances of types can be measured well, we often observe\nheavy-tailed categorical distributions for type frequencies. For the comparison\nof type frequency distributions of two systems or a system with itself at\ndifferent time points in time -- a facet of allotaxonometry -- a great range of\nprobability divergences are available. Here, we introduce and explore\n`probability-turbulence divergence', a tunable, straightforward, and\ninterpretable instrument for comparing normalizable categorical frequency\ndistributions. We model probability-turbulence divergence (PTD) after\nrank-turbulence divergence (RTD). While probability-turbulence divergence is\nmore limited in application than rank-turbulence divergence, it is more\nsensitive to changes in type frequency. We build allotaxonographs to display\nprobability turbulence, incorporating a way to visually accommodate zero\nprobabilities for `exclusive types' which are types that appear in only one\nsystem. We explore comparisons of example distributions taken from literature,\nsocial media, and ecology. We show how probability-turbulence divergence either\nexplicitly or functionally generalizes many existing kinds of distances and\nmeasures, including, as special cases, $L^{(p)}$ norms, the S{\\o}rensen-Dice\ncoefficient (the $F_{1}$ statistic), and the Hellinger distance. We discuss\nsimilarities with the generalized entropies of R{\\'e}nyi and Tsallis, and the\ndiversity indices (or Hill numbers) from ecology. We close with thoughts on\nopen problems concerning the optimization of the tuning of rank- and\nprobability-turbulence divergence.\n","authors":["P. S. Dodds","J. R. Minot","M. V. Arnold","T. Alshaabi","J. L. Adams","A. J. Reagan","C. M. Danforth"],"pdf_url":"https://arxiv.org/pdf/2008.13078v3.pdf","comment":"19 pages, 8 figures (7 in manuscript, 1 in frontispiece), 2 tables"},{"id":"http://arxiv.org/abs/2503.08161v3","updated":"2025-03-14T10:09:13Z","published":"2025-03-11T08:26:37Z","title":"OASIS: Order-Augmented Strategy for Improved Code Search","summary":"  Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.\n","authors":["Zuchen Gao","Zizheng Zhan","Xianming Li","Erxin Yu","Haotian Zhang","Bin Chen","Yuqun Zhang","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2503.08161v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11233v1","updated":"2025-03-14T09:31:03Z","published":"2025-03-14T09:31:03Z","title":"Addressing Information Loss and Interaction Collapse: A Dual Enhanced\n  Attention Framework for Feature Interaction","summary":"  The Transformer has proven to be a significant approach in feature\ninteraction for CTR prediction, achieving considerable success in previous\nworks. However, it also presents potential challenges in handling feature\ninteractions. Firstly, Transformers may encounter information loss when\ncapturing feature interactions. By relying on inner products to represent\npairwise relationships, they compress raw interaction information, which can\nresult in a degradation of fidelity. Secondly, due to the long-tail features\ndistribution, feature fields with low information-abundance embeddings\nconstrain the information abundance of other fields, leading to collapsed\nembedding matrices. To tackle these issues, we propose a Dual Attention\nFramework for Enhanced Feature Interaction, known as Dual Enhanced Attention.\nThis framework integrates two attention mechanisms: the Combo-ID attention\nmechanism and the collapse-avoiding attention mechanism. The Combo-ID attention\nmechanism directly retains feature interaction pairs to mitigate information\nloss, while the collapse-avoiding attention mechanism adaptively filters out\nlow information-abundance interaction pairs to prevent interaction collapse.\nExtensive experiments conducted on industrial datasets have shown the\neffectiveness of Dual Enhanced Attention.\n","authors":["Yi Xu","Zhiyuan Lu","Xiaochen Li","Jinxin Hu","Hong Wen","Zulong Chen","Yu Zhang","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11067v1","updated":"2025-03-14T04:22:01Z","published":"2025-03-14T04:22:01Z","title":"Variational Bayesian Personalized Ranking","summary":"  Recommendation systems have found extensive applications across diverse\ndomains. However, the training data available typically comprises implicit\nfeedback, manifested as user clicks and purchase behaviors, rather than\nexplicit declarations of user preferences. This type of training data presents\nthree main challenges for accurate ranking prediction: First, the unobservable\nnature of user preferences makes likelihood function modeling inherently\ndifficult. Second, the resulting false positives (FP) and false negatives (FN)\nintroduce noise into the learning process, disrupting parameter learning.\nThird, data bias arises as observed interactions tend to concentrate on a few\npopular items, exacerbating the feedback loop of popularity bias. To address\nthese issues, we propose Variational BPR, a novel and easily implementable\nlearning objective that integrates key components for enhancing collaborative\nfiltering: likelihood optimization, noise reduction, and popularity debiasing.\nOur approach involves decomposing the pairwise loss under the ELBO-KL framework\nand deriving its variational lower bound to establish a manageable learning\nobjective for approximate inference. Within this bound, we introduce an\nattention-based latent interest prototype contrastive mechanism, replacing\ninstance-level contrastive learning, to effectively reduce noise from\nproblematic samples. The process of deriving interest prototypes implicitly\nincorporates a flexible hard sample mining strategy, capable of simultaneously\nidentifying hard positive and hard negative samples. Furthermore, we\ndemonstrate that this hard sample mining strategy promotes feature distribution\nuniformity, thereby alleviating popularity bias. Empirically, we demonstrate\nthe effectiveness of Variational BPR on popular backbone recommendation models.\nThe code and data are available at: https://github.com/liubin06/VariationalBPR\n","authors":["Bin Liu","Xiaohong Liu","Qin Luo","Ziqiao Shang","Jielei Chu","Lin Ma","Zhaoyu Li","Fei Teng","Guangtao Zhai","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2503.11067v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.18470v3","updated":"2025-03-14T02:48:55Z","published":"2025-02-04T01:30:06Z","title":"Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World\n  Spatial Reasoning Questions","summary":"  Spatial reasoning remains a challenge for Large Language Models (LLMs), which\nstruggle with spatial data retrieval and reasoning. We propose Spatial\nRetrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to\nspatial tasks by integrating sparse spatial retrieval (spatial databases) and\ndense semantic retrieval (LLM-based similarity). A multi-objective ranking\nstrategy balances spatial constraints and semantic relevance, while an\nLLM-guided generator ensures coherent responses. Experiments on a real-world\ntourism dataset show that Spatial-RAG significantly improves spatial question\nanswering, bridging the gap between LLMs and spatial intelligence.\n","authors":["Dazhou Yu","Riyang Bao","Gengchen Mai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18470v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11924v1","updated":"2025-03-14T23:47:46Z","published":"2025-03-14T23:47:46Z","title":"REGEN: A Dataset and Benchmarks with Natural Language Critiques and\n  Narratives","summary":"  This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative\nNarratives), designed to benchmark the conversational capabilities of\nrecommender Large Language Models (LLMs), addressing the limitations of\nexisting datasets that primarily focus on sequential item prediction. REGEN\nextends the Amazon Product Reviews dataset by inpainting two key natural\nlanguage features: (1) user critiques, representing user \"steering\" queries\nthat lead to the selection of a subsequent item, and (2) narratives, rich\ntextual outputs associated with each recommended item taking into account prior\ncontext. The narratives include product endorsements, purchase explanations,\nand summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of\nconversational recommendation, where models are trained to generate both\nrecommendations and corresponding narratives conditioned on user history (items\nand critiques). For this joint task, we introduce a modeling framework LUMEN\n(LLM-based Unified Multi-task Model with Critiques, Recommendations, and\nNarratives) which uses an LLM as a backbone for critiquing, retrieval and\ngeneration. We also evaluate the dataset's quality using standard auto-rating\ntechniques and benchmark it by training both traditional and LLM-based\nrecommender models. Our results demonstrate that incorporating critiques\nenhances recommendation quality by enabling the recommender to learn language\nunderstanding and integrate it with recommendation signals. Furthermore, LLMs\ntrained on our dataset effectively generate both recommendations and contextual\nnarratives, achieving performance comparable to state-of-the-art recommenders\nand language models.\n","authors":["Kun Su","Krishna Sayana","Hubert Pham","James Pine","Yuri Vasilevski","Raghavendra Vasudeva","Marialena Kyriakidi","Liam Hebert","Ambarish Jash","Anushya Subbiah","Sukhdeep Sodhi"],"pdf_url":"https://arxiv.org/pdf/2503.11924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13514v1","updated":"2025-03-14T11:50:16Z","published":"2025-03-14T11:50:16Z","title":"RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations\n  and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph\n  Learning Integration","summary":"  This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed\nto enhance the reasoning capabilities of Large Language Models (LLMs) by\nintegrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs)\nwith an Incremental Learning (IL) approach. Despite recent advancements, LLMs\nstill face significant challenges in reasoning with structured data, handling\ndynamic knowledge evolution, and mitigating hallucinations, particularly in\nmission-critical domains. Our proposed RAG-KG-IL framework addresses these\nlimitations by employing a multi-agent architecture that enables continuous\nknowledge updates, integrates structured knowledge, and incorporates autonomous\nagents for enhanced explainability and reasoning. The framework utilizes RAG to\nensure the generated responses are grounded in verifiable information, while\nKGs provide structured domain knowledge for improved consistency and depth of\nunderstanding. The Incremental Learning approach allows for dynamic updates to\nthe knowledge base without full retraining, significantly reducing\ncomputational overhead and improving the model's adaptability. We evaluate the\nframework using real-world case studies involving health-related queries,\ncomparing it to state-of-the-art models like GPT-4o and a RAG-only baseline.\nExperimental results demonstrate that our approach significantly reduces\nhallucination rates and improves answer completeness and reasoning accuracy.\nThe results underscore the potential of combining RAG, KGs, and multi-agent\nsystems to create intelligent, adaptable systems capable of real-time knowledge\nintegration and reasoning in complex domains.\n","authors":["Hong Qing Yu","Frank McQuade"],"pdf_url":"https://arxiv.org/pdf/2503.13514v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.11650v1","updated":"2025-03-14T17:59:41Z","published":"2025-03-14T17:59:41Z","title":"Centaur: Robust End-to-End Autonomous Driving with Test-Time Training","summary":"  How can we rely on an end-to-end autonomous vehicle's complex decision-making\nsystem during deployment? One common solution is to have a ``fallback layer''\nthat checks the planned trajectory for rule violations and replaces it with a\npre-defined safe action if necessary. Another approach involves adjusting the\nplanner's decisions to minimize a pre-defined ``cost function'' using\nadditional system predictions such as road layouts and detected obstacles.\nHowever, these pre-programmed rules or cost functions cannot learn and improve\nwith new training data, often resulting in overly conservative behaviors. In\nthis work, we propose Centaur (Cluster Entropy for Test-time trAining using\nUncertainty) which updates a planner's behavior via test-time training, without\nrelying on hand-engineered rules or cost functions. Instead, we measure and\nminimize the uncertainty in the planner's decisions. For this, we develop a\nnovel uncertainty measure, called Cluster Entropy, which is simple,\ninterpretable, and compatible with state-of-the-art planning algorithms. Using\ndata collected at prior test-time time-steps, we perform an update to the\nmodel's parameters using a gradient that minimizes the Cluster Entropy. With\nonly this sole gradient update prior to inference, Centaur exhibits significant\nimprovements, ranking first on the navtest leaderboard with notable gains in\nsafety-critical metrics such as time to collision. To provide detailed insights\non a per-scenario basis, we also introduce navsafe, a challenging new\nbenchmark, which highlights previously undiscovered failure modes of driving\nmodels.\n","authors":["Chonghao Sima","Kashyap Chitta","Zhiding Yu","Shiyi Lan","Ping Luo","Andreas Geiger","Hongyang Li","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2503.11650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09350v2","updated":"2025-03-14T17:59:04Z","published":"2024-04-14T20:17:14Z","title":"Machine learning-based identification of Gaia astrometric exoplanet\n  orbits","summary":"  The third Gaia data release (DR3) contains $\\sim$170\\,000 astrometric orbit\nsolutions of two-body systems located within $\\sim$500 pc of the Sun.\nDetermining component masses in these systems, in particular of stars hosting\nexoplanets, usually hinges on incorporating complementary observations in\naddition to the astrometry, e.g. spectroscopy and radial velocities. Several\nGaia DR3 two-body systems with exoplanet, brown-dwarf, stellar, and black-hole\ncomponents have been confirmed in this way. We developed an alternative machine\nlearning approach that uses only the Gaia DR3 orbital solutions with the aim of\nidentifying the best candidates for exoplanets and brown-dwarf companions.\nBased on confirmed substellar companions in the literature, we use\nsemi-supervised anomaly detection methods in combination with extreme gradient\nboosting and random forest classifiers to determine likely low-mass outliers in\nthe population of non-single sources. We employ and study feature importance to\ninvestigate the method's plausibility and produced a list of 20 best candidates\nof which two are exoplanet candidates and another five are either very-massive\nbrown dwarfs or very-low mass stars. Three candidates, including one initial\nexoplanet candidate, correspond to false-positive solutions where longer-period\nbinary star motion was fitted with a biased shorter-period orbit. We highlight\nnine candidates with brown-dwarf companions for preferential follow-up. The\ncompanion around the Sun-like star G\\,15-6 could be confirmed as a genuine\nbrown dwarf using external radial-velocity data. This new approach is a\npowerful complement to the traditional identification methods for substellar\ncompanions among Gaia astrometric orbits. It is particularly relevant in the\ncontext of Gaia DR4 and its expected exoplanet discovery yield.\n","authors":["Johannes Sahlmann","Pablo Gómez"],"pdf_url":"https://arxiv.org/pdf/2404.09350v2.pdf","comment":"16 pages, 15 figures. Published in MNRAS. The code and data needed to\n  reproduce the results are available at\n  https://github.com/esa/gaia-astrometric-exoplanet-orbit-ml"},{"id":"http://arxiv.org/abs/2411.01974v2","updated":"2025-03-14T17:58:33Z","published":"2024-11-04T10:50:37Z","title":"On the phase diagram of extensive-rank symmetric matrix denoising beyond\n  rotational invariance","summary":"  Matrix denoising is central to signal processing and machine learning. Its\nstatistical analysis when the matrix to infer has a factorised structure with a\nrank growing proportionally to its dimension remains a challenge, except when\nit is rotationally invariant. In this case the information theoretic limits and\nan efficient Bayes-optimal denoising algorithm, called rotational invariant\nestimator [1,2], are known. Beyond this setting few results can be found. The\nreason is that the model is not a usual spin system because of the growing rank\ndimension, nor a matrix model (as appearing in high-energy physics) due to the\nlack of rotation symmetry, but rather a hybrid between the two. Here we make\nprogress towards the understanding of Bayesian matrix denoising when the signal\nis a factored matrix $XX^\\intercal$ that is not rotationally invariant. Monte\nCarlo simulations suggest the existence of a \\emph{denoising-factorisation\ntransition} separating a phase where denoising using the rotational invariant\nestimator remains Bayes-optimal due to universality properties of the same\nnature as in random matrix theory, from one where universality breaks down and\nbetter denoising is possible, though algorithmically hard. We argue that it is\nonly beyond the transition that factorisation, i.e., estimating $X$ itself,\nbecomes possible up to irresolvable ambiguities. On the theory side, we combine\nmean-field techniques in an interpretable multiscale fashion in order to access\nthe minimum mean-square error and mutual information. Interestingly, our\nalternative method yields equations reproducible by the replica approach of\n[3]. Using numerical insights, we delimit the portion of phase diagram where we\nconjecture the mean-field theory to be exact, and correct it using universality\nwhen it is not. Our complete ansatz matches well the numerics in the whole\nphase diagram when considering finite size effects.\n","authors":["Jean Barbier","Francesco Camilli","Justin Ko","Koki Okajima"],"pdf_url":"https://arxiv.org/pdf/2411.01974v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11627v1","updated":"2025-03-14T17:46:34Z","published":"2025-03-14T17:46:34Z","title":"Are Deep Speech Denoising Models Robust to Adversarial Noise?","summary":"  Deep noise suppression (DNS) models enjoy widespread use throughout a variety\nof high-stakes speech applications. However, in this paper, we show that four\nrecent DNS models can each be reduced to outputting unintelligible gibberish\nthrough the addition of imperceptible adversarial noise. Furthermore, our\nresults show the near-term plausibility of targeted attacks, which could induce\nmodels to output arbitrary utterances, and over-the-air attacks. While the\nsuccess of these attacks varies by model and setting, and attacks appear to be\nstrongest when model-specific (i.e., white-box and non-transferable), our\nresults highlight a pressing need for practical countermeasures in DNS systems.\n","authors":["Will Schwarzer","Philip S. Thomas","Andrea Fanelli","Xiaoyu Liu"],"pdf_url":"https://arxiv.org/pdf/2503.11627v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.11615v1","updated":"2025-03-14T17:35:00Z","published":"2025-03-14T17:35:00Z","title":"From Denoising Score Matching to Langevin Sampling: A Fine-Grained Error\n  Analysis in the Gaussian Setting","summary":"  Sampling from an unknown distribution, accessible only through discrete\nsamples, is a fundamental problem at the core of generative AI. The current\nstate-of-the-art methods follow a two-step process: first estimating the score\nfunction (the gradient of a smoothed log-distribution) and then applying a\ngradient-based sampling algorithm. The resulting distribution's correctness can\nbe impacted by several factors: the generalization error due to a finite number\nof initial samples, the error in score matching, and the diffusion error\nintroduced by the sampling algorithm. In this paper, we analyze the sampling\nprocess in a simple yet representative setting-sampling from Gaussian\ndistributions using a Langevin diffusion sampler. We provide a sharp analysis\nof the Wasserstein sampling error that arises from the multiple sources of\nerror throughout the pipeline. This allows us to rigorously track how the\nanisotropy of the data distribution (encoded by its power spectrum) interacts\nwith key parameters of the end-to-end sampling method, including the noise\namplitude, the step sizes in both score matching and diffusion, and the number\nof initial samples. Notably, we show that the Wasserstein sampling error can be\nexpressed as a kernel-type norm of the data power spectrum, where the specific\nkernel depends on the method parameters. This result provides a foundation for\nfurther analysis of the tradeoffs involved in optimizing sampling accuracy,\nsuch as adapting the noise amplitude to the choice of step sizes.\n","authors":["Samuel Hurault","Matthieu Terris","Thomas Moreau","Gabriel Peyré"],"pdf_url":"https://arxiv.org/pdf/2503.11615v1.pdf","comment":"38 pages"},{"id":"http://arxiv.org/abs/2503.11612v1","updated":"2025-03-14T17:29:27Z","published":"2025-03-14T17:29:27Z","title":"Enhanced Soups for Graph Neural Networks","summary":"  Graph Neural Networks (GNN) have demonstrated state-of-the-art performance in\nnumerous scientific and high-performance computing (HPC) applications. Recent\nwork suggests that \"souping\" (combining) individually trained GNNs into a\nsingle model can improve performance without increasing compute and memory\ncosts during inference. However, existing souping algorithms are often slow and\nmemory-intensive, which limits their scalability.\n  We introduce Learned Souping for GNNs, a gradient-descent-based souping\nstrategy that substantially reduces time and memory overhead compared to\nexisting methods. Our approach is evaluated across multiple Open Graph\nBenchmark (OGB) datasets and GNN architectures, achieving up to 1.2% accuracy\nimprovement and 2.1X speedup. Additionally, we propose Partition Learned\nSouping, a novel partition-based variant of learned souping that significantly\nreduces memory usage. On the ogbn-products dataset with GraphSAGE, partition\nlearned souping achieves a 24.5X speedup and a 76% memory reduction without\ncompromising accuracy.\n","authors":["Joseph Zuber","Aishwarya Sarkar","Joseph Jennings","Ali Jannesari"],"pdf_url":"https://arxiv.org/pdf/2503.11612v1.pdf","comment":"10 pages, 4 figures, 3 tables, accepted to GrAPL 2025 (colocated with\n  IPDPS 2025)"},{"id":"http://arxiv.org/abs/2410.03461v2","updated":"2025-03-14T17:27:00Z","published":"2024-10-04T14:21:27Z","title":"Auto-GDA: Automatic Domain Adaptation for Efficient Grounding\n  Verification in Retrieval-Augmented Generation","summary":"  While retrieval-augmented generation (RAG) has been shown to enhance\nfactuality of large language model (LLM) outputs, LLMs still suffer from\nhallucination, generating incorrect or irrelevant information. A common\ndetection strategy involves prompting the LLM again to assess whether its\nresponse is grounded in the retrieved evidence, but this approach is costly.\nAlternatively, lightweight natural language inference (NLI) models for\nefficient grounding verification can be used at inference time. While existing\npre-trained NLI models offer potential solutions, their performance remains\nsubpar compared to larger models on realistic RAG inputs. RAG inputs are more\ncomplex than most datasets used for training NLI models and have\ncharacteristics specific to the underlying knowledge base, requiring adaptation\nof the NLI models to a specific target domain. Additionally, the lack of\nlabeled instances in the target domain makes supervised domain adaptation,\ne.g., through fine-tuning, infeasible. To address these challenges, we\nintroduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework\nenables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation\nstrategies, Auto-GDA employs an iterative process to continuously improve the\nquality of generated samples using weak labels from less efficient teacher\nmodels and discrete optimization to select the most promising augmented\nsamples. Experimental results demonstrate the effectiveness of our approach,\nwith models fine-tuned on synthetic data using Auto-GDA often surpassing the\nperformance of the teacher model and reaching the performance level of LLMs at\n10% of their computational cost.\n","authors":["Tobias Leemann","Periklis Petridis","Giuseppe Vietri","Dionysis Manousakas","Aaron Roth","Sergul Aydore"],"pdf_url":"https://arxiv.org/pdf/2410.03461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11609v1","updated":"2025-03-14T17:24:01Z","published":"2025-03-14T17:24:01Z","title":"Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages","summary":"  An old-school recipe for training a classifier is to (i) learn a good feature\nextractor and (ii) optimize a linear layer atop. When only a handful of samples\nare available per category, as in Few-Shot Adaptation (FSA), data are\ninsufficient to fit a large number of parameters, rendering the above\nimpractical. This is especially true with large pre-trained Vision-Language\nModels (VLMs), which motivated successful research at the intersection of\nParameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by\nanalyzing the learning dynamics of PEFT techniques when trained on few-shot\ndata from only a subset of categories, referred to as the ``base'' classes. We\nshow that such dynamics naturally splits into two distinct phases: (i)\ntask-level feature extraction and (ii) specialization to the available\nconcepts. To accommodate this dynamic, we then depart from prompt- or\nadapter-based methods and tackle FSA differently. Specifically, given a fixed\ncomputational budget, we split it to (i) learn a task-specific feature\nextractor via PEFT and (ii) train a linear classifier on top. We call this\nscheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established\nmethods, our scheme enables a novel form of selective inference at a category\nlevel, i.e., at test time, only novel categories are embedded by the adapted\ntext encoder, while embeddings of base categories are available within the\nclassifier. Results with fixed hyperparameters across two settings, three\nbackbones, and eleven datasets, show that 2SFS matches or surpasses the\nstate-of-the-art, while established methods degrade significantly across\nsettings.\n","authors":["Matteo Farina","Massimiliano Mancini","Giovanni Iacca","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.11609v1.pdf","comment":"Camera-ready version for CVPR 2025 (w/ SuppMat, 23 pages)"},{"id":"http://arxiv.org/abs/2503.09660v2","updated":"2025-03-14T17:09:50Z","published":"2025-03-12T13:36:25Z","title":"Power Spectrum Signatures of Graphs","summary":"  Point signatures based on the Laplacian operators on graphs, point clouds,\nand manifolds have become popular tools in machine learning for graphs,\nclustering, and shape analysis. In this work, we propose a novel point\nsignature, the power spectrum signature, a measure on $\\mathbb{R}$ defined as\nthe squared graph Fourier transform of a graph signal. Unlike eigenvectors of\nthe Laplacian from which it is derived, the power spectrum signature is\ninvariant under graph automorphisms. We show that the power spectrum signature\nis stable under perturbations of the input graph with respect to the\nWasserstein metric. We focus on the signature applied to classes of indicator\nfunctions, and its applications to generating descriptive features for vertices\nof graphs. To demonstrate the practical value of our signature, we showcase\nseveral applications in characterizing geometry and symmetries in point cloud\ndata, and graph regression problems.\n","authors":["Karamatou Yacoubou Djima","Ka Man Yim"],"pdf_url":"https://arxiv.org/pdf/2503.09660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02603v2","updated":"2025-03-14T17:09:03Z","published":"2024-10-03T15:44:42Z","title":"Agents' Room: Narrative Generation through Multi-step Collaboration","summary":"  Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.\n","authors":["Fantine Huot","Reinald Kim Amplayo","Jennimaria Palomaki","Alice Shoshana Jakobovits","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2410.02603v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2405.16236v3","updated":"2025-03-14T17:08:22Z","published":"2024-05-25T13:54:05Z","title":"A transfer learning framework for weak-to-strong generalization","summary":"  Modern large language model (LLM) alignment techniques rely on human\nfeedback, but it is unclear whether these techniques fundamentally limit the\ncapabilities of aligned LLMs. In particular, it is unknown if it is possible to\nalign (stronger) LLMs with superhuman capabilities with (weaker) human feedback\nwithout degrading their capabilities. This is an instance of the weak-to-strong\ngeneralization problem: using feedback from a weaker (less capable) model to\ntrain a stronger (more capable) model. We prove that weak-to-strong\ngeneralization is possible by eliciting latent knowledge from pre-trained LLMs.\nIn particular, we cast the weak-to-strong generalization problem as a transfer\nlearning problem in which we wish to transfer a latent concept prior from a\nweak model to a strong pre-trained model. We prove that a naive fine-tuning\napproach suffers from fundamental limitations, but an alternative\nrefinement-based approach suggested by the problem structure provably overcomes\nthe limitations of fine-tuning. Finally, we demonstrate the practical\napplicability of the refinement approach in multiple LLM alignment tasks.\n","authors":["Seamus Somerstep","Felipe Maia Polo","Moulinath Banerjee","Ya'acov Ritov","Mikhail Yurochkin","Yuekai Sun"],"pdf_url":"https://arxiv.org/pdf/2405.16236v3.pdf","comment":"v2: Major changes to set up, theory, and experiments v3: Camera ready"},{"id":"http://arxiv.org/abs/2503.10048v2","updated":"2025-03-14T17:02:11Z","published":"2025-03-13T05:00:23Z","title":"Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate\n  Rollout","summary":"  Modeling the evolution of physical systems is critical to many applications\nin science and engineering. As the evolution of these systems is governed by\npartial differential equations (PDEs), there are a number of computational\nsimulations which resolve these systems with high accuracy. However, as these\nsimulations incur high computational costs, they are infeasible to be employed\nfor large-scale analysis. A popular alternative to simulators are neural\nnetwork surrogates which are trained in a data-driven manner and are much more\ncomputationally efficient. However, these surrogate models suffer from high\nrollout error when used autoregressively, especially when confronted with\ntraining data paucity. Existing work proposes to improve surrogate rollout\nerror by either including physical loss terms directly in the optimization of\nthe model or incorporating computational simulators as `differentiable layers'\nin the neural network. Both of these approaches have their challenges, with\nphysical loss functions suffering from slow convergence for stiff PDEs and\nsimulator layers requiring gradients which are not always available, especially\nin legacy simulators. We propose the Hybrid PDE Predictor with Reinforcement\nLearning (HyPER) model: a model-agnostic, RL based, cost-aware model which\ncombines a neural surrogate, RL decision model, and a physics simulator (with\nor without gradients) to reduce surrogate rollout error significantly. In\naddition to reducing in-distribution rollout error by 47%-78%, HyPER learns an\nintelligent policy that is adaptable to changing physical conditions and\nresistant to noise corruption. Code available at\nhttps://github.com/scailab/HyPER.\n","authors":["Bharat Srikishan","Daniel O'Malley","Mohamed Mehana","Nicholas Lubbers","Nikhil Muralidhar"],"pdf_url":"https://arxiv.org/pdf/2503.10048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01066v2","updated":"2025-03-14T16:57:12Z","published":"2025-03-03T00:14:34Z","title":"Alchemist: Towards the Design of Efficient Online Continual Learning\n  System","summary":"  Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.\n","authors":["Yuyang Huang","Yuhan Liu","Haryadi S. Gunawi","Beibin Li","Changho Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.01066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00096v2","updated":"2025-03-14T16:52:55Z","published":"2023-09-29T19:09:27Z","title":"Towards Few-Call Model Stealing via Active Self-Paced Knowledge\n  Distillation and Diffusion-Based Image Generation","summary":"  Diffusion models showcase strong capabilities in image synthesis, being used\nin many computer vision tasks with great success. To this end, we propose to\nexplore a new use case, namely to copy black-box classification models without\nhaving access to the original training data, the architecture, and the weights\nof the model, i.e. the model is only exposed through an inference API. More\nspecifically, we can only observe the (soft or hard) labels for some image\nsamples passed as input to the model. Furthermore, we consider an additional\nconstraint limiting the number of model calls, mostly focusing our research on\nfew-call model stealing. In order to solve the model extraction task given the\napplied restrictions, we propose the following framework. As training data, we\ncreate a synthetic data set (called proxy data set) by leveraging the ability\nof diffusion models to generate realistic and diverse images. Given a maximum\nnumber of allowed API calls, we pass the respective number of samples through\nthe black-box model to collect labels. Finally, we distill the knowledge of the\nblack-box teacher (attacked model) into a student model (copy of the attacked\nmodel), harnessing both labeled and unlabeled data generated by the diffusion\nmodel. We employ a novel active self-paced learning framework to make the most\nof the proxy data during distillation. Our empirical results on three data sets\nconfirm the superiority of our framework over four state-of-the-art methods in\nthe few-call model extraction scenario. We release our code for free\nnon-commercial use at https://github.com/vladhondru25/model-stealing.\n","authors":["Vlad Hondru","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2310.00096v2.pdf","comment":"Accepted in Artificial Intelligence Review"},{"id":"http://arxiv.org/abs/2305.17063v2","updated":"2025-03-14T16:50:47Z","published":"2023-05-26T16:19:26Z","title":"Vecchia Gaussian Process Ensembles on Internal Representations of Deep\n  Neural Networks","summary":"  For regression tasks, standard Gaussian processes (GPs) provide natural\nuncertainty quantification (UQ), while deep neural networks (DNNs) excel at\nrepresentation learning. Deterministic UQ methods for neural networks have\nsuccessfully combined the two and require only a single pass through the neural\nnetwork. However, current methods necessitate changes to network training to\naddress feature collapse, where unique inputs map to identical feature vectors.\nWe propose an alternative solution, the deep Vecchia ensemble (DVE), which\nallows deterministic UQ to work in the presence of feature collapse, negating\nthe need for network retraining. DVE comprises an ensemble of GPs built on\nhidden-layer outputs of a DNN, achieving scalability via Vecchia approximations\nthat leverage nearest-neighbor conditional independence. DVE is compatible with\npretrained networks and incurs low computational overhead. We demonstrate DVE's\nutility on several datasets and carry out experiments to understand the inner\nworkings of the proposed method.\n","authors":["Felix Jimenez","Matthias Katzfuss"],"pdf_url":"https://arxiv.org/pdf/2305.17063v2.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.08121v3","updated":"2025-03-14T16:46:23Z","published":"2024-03-12T23:17:32Z","title":"Early Directional Convergence in Deep Homogeneous Neural Networks for\n  Small Initializations","summary":"  This paper studies the gradient flow dynamics that arise when training deep\nhomogeneous neural networks assumed to have locally Lipschitz gradients and an\norder of homogeneity strictly greater than two. It is shown here that for\nsufficiently small initializations, during the early stages of training, the\nweights of the neural network remain small in (Euclidean) norm and\napproximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of\nthe recently introduced neural correlation function. Additionally, this paper\nalso studies the KKT points of the neural correlation function for feed-forward\nnetworks with (Leaky) ReLU and polynomial (Leaky) ReLU activations, deriving\nnecessary and sufficient conditions for rank-one KKT points.\n","authors":["Akshay Kumar","Jarvis Haupt"],"pdf_url":"https://arxiv.org/pdf/2403.08121v3.pdf","comment":"tmlr-final-version"},{"id":"http://arxiv.org/abs/2410.02086v2","updated":"2025-03-14T16:36:53Z","published":"2024-10-02T23:19:23Z","title":"Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations","summary":"  A unified representation space in multi-modal learning is essential for\neffectively integrating diverse data sources, such as text, images, and audio,\nto enhance efficiency and performance across various downstream tasks. Recent\nbinding methods, such as ImageBind (Girdhar et al., 2023), typically rely on a\nsingle, fixed anchor modality for aligning multi-modal data. We mathematically\nanalyze these fixed anchor binding method and uncover significant limitations:\n(1) over-reliance on the choice of the anchor modality, (2) inadequate capture\nof intra-modal information, and (3) failure to account for cross-modal\ncorrelation among non-anchored modalities. To address these issues, we propose\nthe need for adaptive anchor binding methods, exemplified by our framework\nCentroBind. The proposed method uses adaptively adjustable centroid-based\nanchors generated from all available modalities, leading to a balanced and rich\nrepresentation space. We theoretically demonstrate that our approach captures\nthree critical properties of multi-modal learning -- intra-modal learning,\ninter-modal learning, and multi-modal alignment -- while constructing a unified\nrepresentation that spans all modalities. Experiments on both synthetic and\nreal-world datasets show that adaptive anchor methods such as CentroBind\nconsistently outperform fixed anchor binding methods, verifying our analysis.\n","authors":["Minoh Jeong","Min Namgung","Zae Myung Kim","Dongyeop Kang","Yao-Yi Chiang","Alfred Hero"],"pdf_url":"https://arxiv.org/pdf/2410.02086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.04517v2","updated":"2025-03-14T16:34:24Z","published":"2022-09-09T20:39:22Z","title":"Affinity-VAE: incorporating prior knowledge in representation learning\n  from scientific images","summary":"  Learning compact and interpretable representations of data is a critical\nchallenge in scientific image analysis. Here, we introduce Affinity-VAE, a\ngenerative model that enables us to impose our scientific intuition about the\nsimilarity of instances in the dataset on the learned representation during\ntraining. We demonstrate the utility of the approach in the scientific domain\nof cryo-electron tomography (cryo-ET) where a significant current challenge is\nto identify similar molecules within a noisy and low contrast tomographic image\nvolume. This task is distinct from classification in that, at inference time,\nit is unknown whether an instance is part of the training set or not. We\ntrained affinity-VAE using prior knowledge of protein structure to inform the\nlatent space. Our model is able to create rotationally-invariant,\nmorphologically homogeneous clusters in the latent representation, with\nimproved cluster separation compared to other approaches. It achieves\ncompetitive performance on protein classification with the added benefit of\ndisentangling object pose, structural similarity and an interpretable latent\nrepresentation. In the context of cryo-ET data, affinity-VAE captures the\norientation of identified proteins in 3D which can be used as a prior for\nsubsequent scientific experiments. Extracting physical principles from a\ntrained network is of significant importance in scientific imaging where a\nground truth training set is not always feasible.\n","authors":["Marjan Famili","Jola Mirecka","Camila Rangel Smith","Anna Kotańska","Nikolai Juraschko","Beatriz Costa-Gomes","Colin M. Palmer","Jeyan Thiyagalingam","Tom Burnley","Mark Basham","Alan R. Lowe"],"pdf_url":"https://arxiv.org/pdf/2209.04517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11562v1","updated":"2025-03-14T16:30:31Z","published":"2025-03-14T16:30:31Z","title":"Designing Neural Synthesizers for Low Latency Interaction","summary":"  Neural Audio Synthesis (NAS) models offer interactive musical control over\nhigh-quality, expressive audio generators. While these models can operate in\nreal-time, they often suffer from high latency, making them unsuitable for\nintimate musical interaction. The impact of architectural choices in deep\nlearning models on audio latency remains largely unexplored in the NAS\nliterature. In this work, we investigate the sources of latency and jitter\ntypically found in interactive NAS models. We then apply this analysis to the\ntask of timbre transfer using RAVE, a convolutional variational autoencoder for\naudio waveforms introduced by Caillon et al. in 2021. Finally, we present an\niterative design approach for optimizing latency. This culminates with a model\nwe call BRAVE (Bravely Realtime Audio Variational autoEncoder), which is\nlow-latency and exhibits better pitch and loudness replication while showing\ntimbre modification capabilities similar to RAVE. We implement it in a\nspecialized inference framework for low-latency, real-time inference and\npresent a proof-of-concept audio plugin compatible with audio signals from\nmusical instruments. We expect the challenges and guidelines described in this\ndocument to support NAS researchers in designing models for low-latency\ninference from the ground up, enriching the landscape of possibilities for\nmusicians.\n","authors":["Franco Caspe","Jordie Shier","Mark Sandler","Charalampos Saitis","Andrew McPherson"],"pdf_url":"https://arxiv.org/pdf/2503.11562v1.pdf","comment":"See website at fcaspe.github.io/brave - 13 pages, 5 figures, accepted\n  to the Journal of the Audio Engineering Society"},{"id":"http://arxiv.org/abs/2402.02857v2","updated":"2025-03-14T16:27:25Z","published":"2024-02-05T10:17:36Z","title":"Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation","summary":"  Stochastic Gradient Descent (SGD) with adaptive steps is widely used to train\ndeep neural networks and generative models. Most theoretical results assume\nthat it is possible to obtain unbiased gradient estimators, which is not the\ncase in several recent deep learning and reinforcement learning applications\nthat use Monte Carlo methods. This paper provides a comprehensive\nnon-asymptotic analysis of SGD with biased gradients and adaptive steps for\nnon-convex smooth functions. Our study incorporates time-dependent bias and\nemphasizes the importance of controlling the bias of the gradient estimator. In\nparticular, we establish that Adagrad, RMSProp, and AMSGRAD, an exponential\nmoving average variant of Adam, with biased gradients, converge to critical\npoints for smooth non-convex functions at a rate similar to existing results in\nthe literature for the unbiased case. Finally, we provide experimental results\nusing Variational Autoenconders (VAE) and applications to several learning\nframeworks that illustrate our convergence results and show how the effect of\nbias can be reduced by appropriate hyperparameter tuning.\n","authors":["Sobihan Surendran","Antoine Godichon-Baggioni","Adeline Fermanian","Sylvain Le Corff"],"pdf_url":"https://arxiv.org/pdf/2402.02857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11538v1","updated":"2025-03-14T16:04:10Z","published":"2025-03-14T16:04:10Z","title":"FLASHμ: Fast Localizing And Sizing of Holographic Microparticles","summary":"  Reconstructing the 3D location and size of microparticles from diffraction\nimages - holograms - is a computationally expensive inverse problem that has\ntraditionally been solved using physics-based reconstruction methods. More\nrecently, researchers have used machine learning methods to speed up the\nprocess. However, for small particles in large sample volumes the performance\nof these methods falls short of standard physics-based reconstruction methods.\nHere we designed a two-stage neural network architecture, FLASH$\\mu$, to detect\nsmall particles (6-100$\\mu$m) from holograms with large sample depths up to\n20cm. Trained only on synthetic data with added physical noise, our method\nreliably detects particles of at least 9$\\mu$m diameter in real holograms,\ncomparable to the standard reconstruction-based approaches while operating on\nsmaller crops, at quarter of the original resolution and providing roughly a\n600-fold speedup. In addition to introducing a novel approach to a non-local\nobject detection or signal demixing problem, our work could enable low-cost,\nreal-time holographic imaging setups.\n","authors":["Ayush Paliwal","Oliver Schlenczek","Birte Thiede","Manuel Santos Pereira","Katja Stieger","Eberhard Bodenschatz","Gholamhossein Bagheri","Alexander Ecker"],"pdf_url":"https://arxiv.org/pdf/2503.11538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11529v1","updated":"2025-03-14T15:57:31Z","published":"2025-03-14T15:57:31Z","title":"Bottom-up Iterative Anomalous Diffusion Detector (BI-ADD)","summary":"  In recent years, the segmentation of short molecular trajectories with\nvarying diffusive properties has drawn particular attention of researchers,\nsince it allows studying the dynamics of a particle. In the past decade,\nmachine learning methods have shown highly promising results, also in\nchangepoint detection and segmentation tasks. Here, we introduce a novel\niterative method to identify the changepoints in a molecular trajectory, i.e.,\nframes, where the diffusive behavior of a particle changes. A trajectory in our\ncase follows a fractional Brownian motion and we estimate the diffusive\nproperties of the trajectories. The proposed BI-ADD combines unsupervised and\nsupervised learning methods to detect the changepoints. Our approach can be\nused for the analysis of molecular trajectories at the individual level and\nalso be extended to multiple particle tracking, which is an important challenge\nin fundamental biology. We validated BI-ADD in various scenarios within the\nframework of the AnDi2 Challenge 2024 dedicated to single particle tracking.\nOur method is implemented in Python and is publicly available for research\npurposes.\n","authors":["Junwoo Park","Nataliya Sokolovska","Clément Cabriel","Ignacio Izeddin","Judith Miné-Hattab"],"pdf_url":"https://arxiv.org/pdf/2503.11529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07594v4","updated":"2025-03-14T15:37:14Z","published":"2024-02-12T11:48:54Z","title":"Zero-shot Imputation with Foundation Inference Models for Dynamical\n  Systems","summary":"  Dynamical systems governed by ordinary differential equations (ODEs) serve as\nmodels for a vast number of natural and social phenomena. In this work, we\noffer a fresh perspective on the classical problem of imputing missing time\nseries data, whose underlying dynamics are assumed to be determined by ODEs.\nSpecifically, we revisit ideas from amortized inference and neural operators,\nand propose a novel supervised learning framework for zero-shot time series\nimputation, through parametric functions satisfying some (hidden) ODEs. Our\nproposal consists of two components. First, a broad probability distribution\nover the space of ODE solutions, observation times and noise mechanisms, with\nwhich we generate a large, synthetic dataset of (hidden) ODE solutions, along\nwith their noisy and sparse observations. Second, a neural recognition model\nthat is trained offline, to map the generated time series onto the spaces of\ninitial conditions and time derivatives of the (hidden) ODE solutions, which we\nthen integrate to impute the missing data. We empirically demonstrate that one\nand the same (pretrained) recognition model can perform zero-shot imputation\nacross 63 distinct time series with missing values, each sampled from widely\ndifferent dynamical systems. Likewise, we demonstrate that it can perform\nzero-shot imputation of missing high-dimensional data in 10 vastly different\nsettings, spanning human motion, air quality, traffic and electricity studies,\nas well as Navier-Stokes simulations -- without requiring any fine-tuning. What\nis more, our proposal often outperforms state-of-the-art methods, which are\ntrained on the target datasets.\n  Our pretrained model, repository and tutorials are available online.\n","authors":["Patrick Seifner","Kostadin Cvejoski","Antonia Körner","Ramsés J. Sánchez"],"pdf_url":"https://arxiv.org/pdf/2402.07594v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02986v2","updated":"2025-03-14T15:36:05Z","published":"2023-11-06T09:46:16Z","title":"Hacking Cryptographic Protocols with Advanced Variational Quantum\n  Attacks","summary":"  Here we introduce an improved approach to Variational Quantum Attack\nAlgorithms (VQAA) on crytographic protocols. Our methods provide robust quantum\nattacks to well-known cryptographic algorithms, more efficiently and with\nremarkably fewer qubits than previous approaches. We implement simulations of\nour attacks for symmetric-key protocols such as S-DES, S-AES and Blowfish. For\ninstance, we show how our attack allows a classical simulation of a small\n8-qubit quantum computer to find the secret key of one 32-bit Blowfish instance\nwith 24 times fewer number of iterations than a brute-force attack. Our work\nalso shows improvements in attack success rates for lightweight ciphers such as\nS-DES and S-AES. Further applications beyond symmetric-key cryptography are\nalso discussed, including asymmetric-key protocols and hash functions. In\naddition, we also comment on potential future improvements of our methods. Our\nresults bring one step closer assessing the vulnerability of large-size\nclassical cryptographic protocols with Noisy Intermediate-Scale Quantum (NISQ)\ndevices, and set the stage for future research in quantum cybersecurity.\n","authors":["Borja Aizpurua","Pablo Bermejo","Josu Etxezarreta Martinez","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2311.02986v2.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.06639v2","updated":"2025-03-14T15:25:46Z","published":"2025-03-09T14:36:45Z","title":"Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss,\n  Dynamics, and Success Amplification","summary":"  Group Relative Policy Optimization (GRPO) was introduced and used\nsuccessfully to train DeepSeek R1 models for promoting reasoning capabilities\nof LLMs using verifiable or binary rewards. We show in this paper that GRPO\nwith verifiable rewards can be written as a Kullback Leibler ($\\mathsf{KL}$)\nregularized contrastive loss, where the contrastive samples are synthetic data\nsampled from the old policy. The optimal GRPO policy $\\pi_{n}$ can be expressed\nexplicitly in terms of the binary reward, as well as the first and second order\nstatistics of the old policy ($\\pi_{n-1}$) and the reference policy $\\pi_0$.\nIterating this scheme, we obtain a sequence of policies $\\pi_{n}$ for which we\ncan quantify the probability of success $p_n$. We show that the probability of\nsuccess of the policy satisfies a recurrence that converges to a fixed point of\na function that depends on the initial probability of success $p_0$ and the\nregularization parameter $\\beta$ of the $\\mathsf{KL}$ regularizer. We show that\nthe fixed point $p^*$ is guaranteed to be larger than $p_0$, thereby\ndemonstrating that GRPO effectively amplifies the probability of success of the\npolicy.\n","authors":["Youssef Mroueh"],"pdf_url":"https://arxiv.org/pdf/2503.06639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11488v1","updated":"2025-03-14T15:13:42Z","published":"2025-03-14T15:13:42Z","title":"Unicorn: A Universal and Collaborative Reinforcement Learning Approach\n  Towards Generalizable Network-Wide Traffic Signal Control","summary":"  Adaptive traffic signal control (ATSC) is crucial in reducing congestion,\nmaximizing throughput, and improving mobility in rapidly growing urban areas.\nRecent advancements in parameter-sharing multi-agent reinforcement learning\n(MARL) have greatly enhanced the scalable and adaptive optimization of complex,\ndynamic flows in large-scale homogeneous networks. However, the inherent\nheterogeneity of real-world traffic networks, with their varied intersection\ntopologies and interaction dynamics, poses substantial challenges to achieving\nscalable and effective ATSC across different traffic scenarios. To address\nthese challenges, we present Unicorn, a universal and collaborative MARL\nframework designed for efficient and adaptable network-wide ATSC. Specifically,\nwe first propose a unified approach to map the states and actions of\nintersections with varying topologies into a common structure based on traffic\nmovements. Next, we design a Universal Traffic Representation (UTR) module with\na decoder-only network for general feature extraction, enhancing the model's\nadaptability to diverse traffic scenarios. Additionally, we incorporate an\nIntersection Specifics Representation (ISR) module, designed to identify key\nlatent vectors that represent the unique intersection's topology and traffic\ndynamics through variational inference techniques. To further refine these\nlatent representations, we employ a contrastive learning approach in a\nself-supervised manner, which enables better differentiation of\nintersection-specific features. Moreover, we integrate the state-action\ndependencies of neighboring agents into policy optimization, which effectively\ncaptures dynamic agent interactions and facilitates efficient regional\ncollaboration. Our results show that Unicorn outperforms other methods across\nvarious evaluation metrics, highlighting its potential in complex, dynamic\ntraffic networks.\n","authors":["Yifeng Zhang","Yilin Liu","Ping Gong","Peizhuo Li","Mingfeng Fan","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2503.11488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11486v1","updated":"2025-03-14T15:11:29Z","published":"2025-03-14T15:11:29Z","title":"A Review of DeepSeek Models' Key Innovative Techniques","summary":"  DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models\n(LLMs) for general-purpose tasks and reasoning, achieving performance\ncomparable to state-of-the-art closed-source models from companies like OpenAI\nand Anthropic -- while requiring only a fraction of their training costs.\nUnderstanding the key innovative techniques behind DeepSeek's success is\ncrucial for advancing LLM research. In this paper, we review the core\ntechniques driving the remarkable effectiveness and efficiency of these models,\nincluding refinements to the transformer architecture, innovations such as\nMulti-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the\nco-design of algorithms, frameworks, and hardware, the Group Relative Policy\nOptimization algorithm, post-training with pure reinforcement learning and\niterative training alternating between supervised fine-tuning and reinforcement\nlearning. Additionally, we identify several open questions and highlight\npotential research opportunities in this rapidly advancing field.\n","authors":["Chengen Wang","Murat Kantarcioglu"],"pdf_url":"https://arxiv.org/pdf/2503.11486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11482v1","updated":"2025-03-14T15:07:04Z","published":"2025-03-14T15:07:04Z","title":"NeuMC -- a package for neural sampling for lattice field theories","summary":"  We present the \\texttt{NeuMC} software package, based on \\pytorch, aimed at\nfacilitating the research on neural samplers in lattice field theories. Neural\nsamplers based on normalizing flows are becoming increasingly popular in the\ncontext of Monte-Carlo simulations as they can effectively approximate target\nprobability distributions, possibly alleviating some shortcomings of the Markov\nchain Monte-Carlo methods. Our package provides tools to create such samplers\nfor two-dimensional field theories.\n","authors":["Piotr Bialas","Piotr Korcyl","Tomasz Stebel","Dawid Zapolski"],"pdf_url":"https://arxiv.org/pdf/2503.11482v1.pdf","comment":"42 pages, 15 figures, for associated code repository, see\n  https://github.com/nmcmc/neumc.git"},{"id":"http://arxiv.org/abs/2501.12962v2","updated":"2025-03-14T15:05:09Z","published":"2025-01-22T15:38:09Z","title":"It's complicated. The relationship of algorithmic fairness and\n  non-discrimination regulations in the EU AI Act","summary":"  What constitutes a fair decision? This question is not only difficult for\nhumans but becomes more challenging when Artificial Intelligence (AI) models\nare used. In light of discriminatory algorithmic behaviors, the EU has recently\npassed the AI Act, which mandates specific rules for AI models, incorporating\nboth traditional legal non-discrimination regulations and machine learning\nbased algorithmic fairness concepts. This paper aims to bridge these two\ndifferent concepts in the AI Act through: First a high-level introduction of\nboth concepts targeting legal and computer science-oriented scholars, and\nsecond an in-depth analysis of the AI Act's relationship between legal\nnon-discrimination regulations and algorithmic fairness. Our analysis reveals\nthree key findings: (1.), most non-discrimination regulations target only\nhigh-risk AI systems. (2.), the regulation of high-risk systems encompasses\nboth data input requirements and output monitoring, though these regulations\nare often inconsistent and raise questions of computational feasibility. (3.)\nRegulations for General Purpose AI Models, such as Large Language Models that\nare not simultaneously classified as high-risk systems, currently lack\nspecificity compared to other regulations. Based on these findings, we\nrecommend developing more specific auditing and testing methodologies for AI\nsystems. This paper aims to serve as a foundation for future interdisciplinary\ncollaboration between legal scholars and computer science-oriented machine\nlearning researchers studying discrimination in AI systems.\n","authors":["Kristof Meding"],"pdf_url":"https://arxiv.org/pdf/2501.12962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00115v4","updated":"2025-03-14T15:03:43Z","published":"2024-06-27T14:00:05Z","title":"Instance Temperature Knowledge Distillation","summary":"  Knowledge distillation (KD) enhances the performance of a student network by\nallowing it to learn the knowledge transferred from a teacher network\nincrementally. Existing methods dynamically adjust the temperature to enable\nthe student network to adapt to the varying learning difficulties at different\nlearning stages of KD. KD is a continuous process, but when adjusting the\ntemperature, these methods consider only the immediate benefits of the\noperation in the current learning phase and fail to take into account its\nfuture returns. To address this issue, we formulate the adjustment of\ntemperature as a sequential decision-making task and propose a method based on\nreinforcement learning, termed RLKD. Importantly, we design a novel state\nrepresentation to enable the agent to make more informed action (i.e. instance\ntemperature adjustment). To handle the problem of delayed rewards in our method\ndue to the KD setting, we explore an instance reward calibration approach. In\naddition,we devise an efficient exploration strategy that enables the agent to\nlearn valuable instance temperature adjustment policy more efficiently. Our\nframework can serve as a plug-and-play technique to be inserted into various KD\nmethods easily, and we validate its effectiveness on both image classification\nand object detection tasks. Our project is at\nhttps://www.zayx.me/ITKD.github.io/.\n","authors":["Zhengbo Zhang","Yuxi Zhou","Jia Gong","Jun Liu","Zhigang Tu"],"pdf_url":"https://arxiv.org/pdf/2407.00115v4.pdf","comment":"Serious updates are needed"},{"id":"http://arxiv.org/abs/2502.20292v2","updated":"2025-03-14T15:01:37Z","published":"2025-02-27T17:17:43Z","title":"Visual Adaptive Prompting for Compositional Zero-Shot Learning","summary":"  Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nlearning joint representations of visual and textual data, making them powerful\ntools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires\nmodels to generalize to novel combinations of visual primitives-such as\nattributes and objects-that were not explicitly encountered during training.\nRecent works in prompting for CZSL have focused on modifying inputs for the\ntext encoder, often using static prompts that do not change across varying\nvisual contexts. However, these approaches struggle to fully capture varying\nvisual contexts, as they focus on text adaptation rather than leveraging visual\nfeatures for compositional reasoning. To address this, we propose Visual\nAdaptive Prompting System (VAPS) that leverages a learnable visual prompt\nrepository and similarity-based retrieval mechanism within the framework of\nVLMs to bridge the gap between semantic and visual features. Our method\nintroduces a dynamic visual prompt repository mechanism that selects the most\nrelevant attribute and object prompts based on the visual features of the\nimage. Our proposed system includes a visual prompt adapter that encourages the\nmodel to learn a more generalizable embedding space. Experiments on three CZSL\nbenchmarks, across both closed and open-world scenarios, demonstrate\nstate-of-the-art results.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2502.20292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11469v1","updated":"2025-03-14T14:55:22Z","published":"2025-03-14T14:55:22Z","title":"A Real-World Energy Management Dataset from a Smart Company Building for\n  Optimization and Machine Learning","summary":"  We present a large real-world dataset obtained from monitoring a smart\ncompany facility over the course of six years, from 2018 to 2023. The dataset\nincludes energy consumption data from various facility areas and components,\nenergy production data from a photovoltaic system and a combined heat and power\nplant, operational data from heating and cooling systems, and weather data from\nan on-site weather station. The measurement sensors installed throughout the\nfacility are organized in a hierarchical metering structure with multiple\nsub-metering levels, which is reflected in the dataset. The dataset contains\nmeasurement data from 72 energy meters, 9 heat meters and a weather station.\nBoth raw and processed data at different processing levels, including labeled\nissues, is available. In this paper, we describe the data acquisition and\npost-processing employed to create the dataset. The dataset enables the\napplication of a wide range of methods in the domain of energy management,\nincluding optimization, modeling, and machine learning to optimize building\noperations and reduce costs and carbon emissions.\n","authors":["Jens Engel","Andrea Castellani","Patricia Wollstadt","Felix Lanfermann","Thomas Schmitt","Sebastian Schmitt","Lydia Fischer","Steffen Limmer","David Luttropp","Florian Jomrich","René Unger","Tobias Rodemann"],"pdf_url":"https://arxiv.org/pdf/2503.11469v1.pdf","comment":"22 pages, 9 figures. Preprint submitted to Scientific Data"},{"id":"http://arxiv.org/abs/2503.11467v1","updated":"2025-03-14T14:54:02Z","published":"2025-03-14T14:54:02Z","title":"Dynamic Obstacle Avoidance with Bounded Rationality Adversarial\n  Reinforcement Learning","summary":"  Reinforcement Learning (RL) has proven largely effective in obtaining stable\nlocomotion gaits for legged robots. However, designing control algorithms which\ncan robustly navigate unseen environments with obstacles remains an ongoing\nproblem within quadruped locomotion. To tackle this, it is convenient to solve\nnavigation tasks by means of a hierarchical approach with a low-level\nlocomotion policy and a high-level navigation policy. Crucially, the high-level\npolicy needs to be robust to dynamic obstacles along the path of the agent. In\nthis work, we propose a novel way to endow navigation policies with robustness\nby a training process that models obstacles as adversarial agents, following\nthe adversarial RL paradigm. Importantly, to improve the reliability of the\ntraining process, we bound the rationality of the adversarial agent resorting\nto quantal response equilibria, and place a curriculum over its rationality. We\ncalled this method Hierarchical policies via Quantal response Adversarial\nReinforcement Learning (Hi-QARL). We demonstrate the robustness of our method\nby benchmarking it in unseen randomized mazes with multiple obstacles. To prove\nits applicability in real scenarios, our method is applied on a Unitree GO1\nrobot in simulation.\n","authors":["Jose-Luis Holgado-Alvarez","Aryaman Reddi","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2503.11467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11466v1","updated":"2025-03-14T14:53:56Z","published":"2025-03-14T14:53:56Z","title":"In Shift and In Variance: Assessing the Robustness of HAR Deep Learning\n  Models against Variability","summary":"  Human Activity Recognition (HAR) using wearable inertial measurement unit\n(IMU) sensors can revolutionize healthcare by enabling continual health\nmonitoring, disease prediction, and routine recognition. Despite the high\naccuracy of Deep Learning (DL) HAR models, their robustness to real-world\nvariabilities remains untested, as they have primarily been trained and tested\non limited lab-confined data. In this study, we isolate subject, device,\nposition, and orientation variability to determine their effect on DL HAR\nmodels and assess the robustness of these models in real-world conditions. We\nevaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a\ncomprehensive discussion on the impact of variability on data distribution\nshifts and changes in model performance. Our experiments measured shifts in\ndata distribution using Maximum Mean Discrepancy (MMD) and observed DL model\nperformance drops due to variability. We concur that studied variabilities\naffect DL HAR models differently, and there is an inverse relationship between\ndata distribution shifts and model performance. The compounding effect of\nvariability was analyzed, and the implications of variabilities in real-world\nscenarios were highlighted. MMD proved an effective metric for calculating data\ndistribution shifts and explained the drop in performance due to variabilities\nin HARVAR and REALDISP datasets. Combining our understanding of variability\nwith evaluating its effects will facilitate the development of more robust DL\nHAR models and optimal training techniques. Allowing Future models to not only\nbe assessed based on their maximum F1 score but also on their ability to\ngeneralize effectively\n","authors":["Azhar Ali Khaked","Nobuyuki Oishi","Daniel Roggen","Paula Lago"],"pdf_url":"https://arxiv.org/pdf/2503.11466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11462v1","updated":"2025-03-14T14:48:12Z","published":"2025-03-14T14:48:12Z","title":"Make Optimization Once and for All with Fine-grained Guidance","summary":"  Learning to Optimize (L2O) enhances optimization efficiency with integrated\nneural networks. L2O paradigms achieve great outcomes, e.g., refitting\noptimizer, generating unseen solutions iteratively or directly. However,\nconventional L2O methods require intricate design and rely on specific\noptimization processes, limiting scalability and generalization. Our analyses\nexplore general framework for learning optimization, called Diff-L2O, focusing\non augmenting sampled solutions from a wider view rather than local updates in\nreal optimization process only. Meanwhile, we give the related generalization\nbound, showing that the sample diversity of Diff-L2O brings better performance.\nThis bound can be simply applied to other fields, discussing diversity,\nmean-variance, and different tasks. Diff-L2O's strong compatibility is\nempirically verified with only minute-level training, comparing with other\nhour-levels.\n","authors":["Mingjia Shi","Ruihan Lin","Xuxi Chen","Yuhao Zhou","Zezhen Ding","Pingzhi Li","Tong Wang","Kai Wang","Zhangyang Wang","Jiheng Zhang","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.11462v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.15100v2","updated":"2025-03-14T14:47:52Z","published":"2024-12-19T17:48:03Z","title":"Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks","summary":"  Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914.\n","authors":["Noemi Anau Montel","James Alvey","Christoph Weniger"],"pdf_url":"https://arxiv.org/pdf/2412.15100v2.pdf","comment":"11 pages, 5 figures. Code available on github (NoemiAM/mist) at\n  https://github.com/NoemiAM/mist - v2: version accepted by PRD"},{"id":"http://arxiv.org/abs/2503.11452v1","updated":"2025-03-14T14:41:08Z","published":"2025-03-14T14:41:08Z","title":"Deep Learning Agents Trained For Avoidance Behave Like Hawks And Doves","summary":"  We present heuristically optimal strategies expressed by deep learning agents\nplaying a simple avoidance game. We analyse the learning and behaviour of two\nagents within a symmetrical grid world that must cross paths to reach a target\ndestination without crashing into each other or straying off of the grid world\nin the wrong direction. The agent policy is determined by one neural network\nthat is employed in both agents. Our findings indicate that the fully trained\nnetwork exhibits behaviour similar to that of the game Hawks and Doves, in that\none agent employs an aggressive strategy to reach the target while the other\nlearns how to avoid the aggressive agent.\n","authors":["Aryaman Reddi","Glenn Vinnicombe"],"pdf_url":"https://arxiv.org/pdf/2503.11452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10386v2","updated":"2025-03-14T14:37:28Z","published":"2025-03-13T14:04:04Z","title":"Multi-objective Good Arm Identification with Bandit Feedback","summary":"  We consider a good arm identification problem in a stochastic bandit setting\nwith multi-objectives, where each arm $i\\in[K]$ is associated with a\ndistribution $\\mathcal{D}_i$ defined over $\\mathbb{R}^M$. For each round $t$,\nthe player/algorithm pulls one arm $i_t$ and receives a $M$ dimensional vector\nfeedback sampled according to $\\mathcal{D}_{i_t}$. The target is twofold, one\nis finding one arm whose means are larger than the predefined thresholds\n$\\xi_1,\\ldots,\\xi_M$ with a confidence bound $\\delta$ and an accuracy rate\n$\\epsilon$ with a bounded sample complexity, the other is output $\\bot$ to\nindicate no such arm exists. We propose an algorithm with a sample complexity\nbound. Our bound is the same as the one given in the previous work when $M=1$\nand $\\epsilon = 0$, and we give novel bounds for $M > 1$ and $\\epsilon > 0$.\nThe proposed algorithm attains better numerical performance than other\nbaselines in the experiments on synthetic and real datasets.\n","authors":["Xuanke Jiang","Kohei Hatano","Eiji Takimoto"],"pdf_url":"https://arxiv.org/pdf/2503.10386v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04662v4","updated":"2025-03-14T14:34:56Z","published":"2024-04-06T15:31:20Z","title":"Learning Minimal Neural Specifications","summary":"  Formal verification is only as good as the specification of a system, which\nis also true for neural network verification. Existing specifications follow\nthe paradigm of data as specification, where the local neighborhood around a\nreference data point is considered correct or robust. While these\nspecifications provide a fair testbed for assessing model robustness, they are\ntoo restrictive for verifying any unseen test data points, a challenging task\nwith significant real-world implications. Recent work shows great promise\nthrough a new paradigm, neural representation as specification, which uses\nneural activation patterns (NAPs) for this purpose. However, it computes the\nmost refined NAPs, which include many redundant neurons. In this paper, we\nstudy the following problem: Given a neural network, find a minimal (general)\nNAP specification that is sufficient for formal verification of its robustness\nproperties. Finding the minimal NAP specification not only expands verifiable\nbounds but also provides insights into which set of neurons contributes to the\nmodel's robustness. To address this problem, we propose three approaches:\nconservative, statistical, and optimistic. Each of these methods offers\ndistinct strengths and trade-offs in terms of minimality and computational\nspeed, making them suitable for scenarios with different priorities. Notably,\nthe optimistic approach can probe potential causal links between neurons and\nthe robustness of large vision neural networks without relying on verification\ntools, a task existing methods struggle to scale. Our experiments show that\nminimal NAP specifications use far fewer neurons than those from previous work\nwhile expanding verifiable boundaries by several orders of magnitude.\n","authors":["Chuqin Geng","Zhaoyue Wang","Haolin Ye","Xujie Si"],"pdf_url":"https://arxiv.org/pdf/2404.04662v4.pdf","comment":"30 pages,7 figures"},{"id":"http://arxiv.org/abs/2410.10604v2","updated":"2025-03-14T14:32:09Z","published":"2024-10-14T15:12:16Z","title":"Multi-modal Vision Pre-training for Medical Image Analysis","summary":"  Self-supervised learning has greatly facilitated medical image analysis by\nsuppressing the training data requirement for real-world applications. Current\nparadigms predominantly rely on self-supervision within uni-modal image data,\nthereby neglecting the inter-modal correlations essential for effective\nlearning of cross-modal image representations. This limitation is particularly\nsignificant for naturally grouped multi-modal data, e.g., multi-parametric MRI\nscans for a patient undergoing various functional imaging protocols in the same\nstudy. To bridge this gap, we conduct a novel multi-modal image pre-training\nwith three proxy tasks to facilitate the learning of cross-modality\nrepresentations and correlations using multi-modal brain MRI scans (over 2.4\nmillion images in 16,022 scans of 3,755 patients), i.e., cross-modal image\nreconstruction, modality-aware contrastive learning, and modality template\ndistillation. To demonstrate the generalizability of our pre-trained model, we\nconduct extensive experiments on various benchmarks with ten downstream tasks.\nThe superior performance of our method is reported in comparison to\nstate-of-the-art pre-training methods, with Dice Score improvement of\n0.28\\%-14.47\\% across six segmentation benchmarks and a consistent accuracy\nboost of 0.65\\%-18.07\\% in four individual image classification tasks.\n","authors":["Shaohao Rui","Lingzhi Chen","Zhenyu Tang","Lilong Wang","Mianxin Liu","Shaoting Zhang","Xiaosong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17308v2","updated":"2025-03-14T14:32:01Z","published":"2025-02-24T16:43:05Z","title":"Implicit Word Reordering with Knowledge Distillation for Cross-Lingual\n  Dependency Parsing","summary":"  Word order difference between source and target languages is a major obstacle\nto cross-lingual transfer, especially in the dependency parsing task. Current\nworks are mostly based on order-agnostic models or word reordering to mitigate\nthis problem. However, such methods either do not leverage grammatical\ninformation naturally contained in word order or are computationally expensive\nas the permutation space grows exponentially with the sentence length.\nMoreover, the reordered source sentence with an unnatural word order may be a\nform of noising that harms the model learning. To this end, we propose an\nImplicit Word Reordering framework with Knowledge Distillation (IWR-KD). This\nframework is inspired by that deep networks are good at learning feature\nlinearization corresponding to meaningful data transformation, e.g. word\nreordering. To realize this idea, we introduce a knowledge distillation\nframework composed of a word-reordering teacher model and a dependency parsing\nstudent model. We verify our proposed method on Universal Dependency Treebanks\nacross 31 different languages and show it outperforms a series of competitors,\ntogether with experimental analysis to illustrate how our method works towards\ntraining a robust parser.\n","authors":["Zhuoran Li","Chunming Hu","Junfan Chen","Zhijun Chen","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17308v2.pdf","comment":"9 pages, 5 figures, 3 tables. Accepted by The 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2503.11441v1","updated":"2025-03-14T14:28:19Z","published":"2025-03-14T14:28:19Z","title":"D3: Diversity, Difficulty, and Dependability-Aware Data Selection for\n  Sample-Efficient LLM Instruction Tuning","summary":"  Recent advancements in instruction tuning for large language models (LLMs)\nsuggest that a small, high-quality dataset can significantly equip LLMs with\ninstruction-following capabilities, outperforming large datasets often burdened\nby quality and redundancy issues. However, the challenge lies in automatically\nidentifying valuable subsets from large datasets to boost both the\neffectiveness and efficiency of instruction tuning. In this paper, we first\nestablish data selection criteria based on three distinct aspects of data\nvalue: diversity, difficulty, and dependability, and then propose the D3 method\ncomprising two key steps of scoring and selection. Specifically, in the scoring\nstep, we define the diversity function to measure sample distinctiveness and\nintroduce the uncertainty-based prediction difficulty to evaluate sample\ndifficulty by mitigating the interference of context-oriented generation\ndiversity. Additionally, we integrate an external LLM for dependability\nassessment. In the selection step, we formulate the D3 weighted coreset\nobjective, which jointly optimizes three aspects of data value to solve for the\nmost valuable subset. The two steps of D3 can iterate multiple rounds,\nincorporating feedback to refine the selection focus adaptively. Experiments on\nthree datasets demonstrate the effectiveness of D3 in endowing LLMs with\ncompetitive or even superior instruction-following capabilities using less than\n10% of the entire dataset.\n","authors":["Jia Zhang","Chen-Xi Zhang","Yao Liu","Yi-Xuan Jin","Xiao-Wen Yang","Bo Zheng","Yi Liu","Lan-Zhe Guo"],"pdf_url":"https://arxiv.org/pdf/2503.11441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07124v2","updated":"2025-03-14T14:26:57Z","published":"2023-06-12T13:59:48Z","title":"Diverse Projection Ensembles for Distributional Reinforcement Learning","summary":"  In contrast to classical reinforcement learning (RL), distributional RL\nalgorithms aim to learn the distribution of returns rather than their expected\nvalue. Since the nature of the return distribution is generally unknown a\npriori or arbitrarily complex, a common approach finds approximations within a\nset of representable, parametric distributions. Typically, this involves a\nprojection of the unconstrained distribution onto the set of simplified\ndistributions. We argue that this projection step entails a strong inductive\nbias when coupled with neural networks and gradient descent, thereby profoundly\nimpacting the generalization behavior of learned models. In order to facilitate\nreliable uncertainty estimation through diversity, we study the combination of\nseveral different projections and representations in a distributional ensemble.\nWe establish theoretical properties of such projection ensembles and derive an\nalgorithm that uses ensemble disagreement, measured by the average\n1-Wasserstein distance, as a bonus for deep exploration. We evaluate our\nalgorithm on the behavior suite benchmark and VizDoom and find that diverse\nprojection ensembles lead to significant performance improvements over existing\nmethods on a variety of tasks with the most pronounced gains in directed\nexploration problems.\n","authors":["Moritz A. Zanger","Wendelin Böhmer","Matthijs T. J. Spaan"],"pdf_url":"https://arxiv.org/pdf/2306.07124v2.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2503.11435v1","updated":"2025-03-14T14:24:27Z","published":"2025-03-14T14:24:27Z","title":"Preference Elicitation for Multi-objective Combinatorial Optimization\n  with Active Learning and Maximum Likelihood Estimation","summary":"  Real-life combinatorial optimization problems often involve several\nconflicting objectives, such as price, product quality and sustainability. A\ncomputationally-efficient way to tackle multiple objectives is to aggregate\nthem into a single-objective function, such as a linear combination. However,\ndefining the weights of the linear combination upfront is hard; alternatively,\nthe use of interactive learning methods that ask users to compare candidate\nsolutions is highly promising. The key challenges are to generate candidates\nquickly, to learn an objective function that leads to high-quality solutions\nand to do so with few user interactions. We build upon the Constructive\nPreference Elicitation framework and show how each of the three properties can\nbe improved: to increase the interaction speed we investigate using pools of\n(relaxed) solutions, to improve the learning we adopt Maximum Likelihood\nEstimation of a Bradley-Terry preference model; and to reduce the number of\nuser interactions, we select the pair of candidates to compare with an\nensemble-based acquisition function inspired from Active Learning. Our careful\nexperimentation demonstrates each of these improvements: on a PC configuration\ntask and a realistic multi-instance routing problem, our method selects queries\nfaster, needs fewer queries and synthesizes higher-quality combinatorial\nsolutions than previous CPE methods.\n","authors":["Marianne Defresne","Jayanta Mandi","Tias Guns"],"pdf_url":"https://arxiv.org/pdf/2503.11435v1.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.11433v1","updated":"2025-03-14T14:22:09Z","published":"2025-03-14T14:22:09Z","title":"Adaptive Torque Control of Exoskeletons under Spasticity Conditions via\n  Reinforcement Learning","summary":"  Spasticity is a common movement disorder symptom in individuals with cerebral\npalsy, hereditary spastic paraplegia, spinal cord injury and stroke, being one\nof the most disabling features in the progression of these diseases. Despite\nthe potential benefit of using wearable robots to treat spasticity, their use\nis not currently recommended to subjects with a level of spasticity above\n${1^+}$ on the Modified Ashworth Scale. The varying dynamics of this\nvelocity-dependent tonic stretch reflex make it difficult to deploy safe\npersonalized controllers. Here, we describe a novel adaptive torque controller\nvia deep reinforcement learning (RL) for a knee exoskeleton under joint\nspasticity conditions, which accounts for task performance and interaction\nforces reduction. To train the RL agent, we developed a digital twin, including\na musculoskeletal-exoskeleton system with joint misalignment and a\ndifferentiable spastic reflexes model for the muscles activation. Results for a\nsimulated knee extension movement showed that the agent learns to control the\nexoskeleton for individuals with different levels of spasticity. The proposed\ncontroller was able to reduce maximum torques applied to the human joint under\nspastic conditions by an average of 10.6\\% and decreases the root mean square\nuntil the settling time by 8.9\\% compared to a conventional compliant\ncontroller.\n","authors":["Andrés Chavarrías","David Rodriguez-Cianca","Pablo Lanillos"],"pdf_url":"https://arxiv.org/pdf/2503.11433v1.pdf","comment":"Accepted for publication in IEEE 19th International Conference on\n  Rehabilitation Robotics (ICORR2025)"},{"id":"http://arxiv.org/abs/2503.11429v1","updated":"2025-03-14T14:14:43Z","published":"2025-03-14T14:14:43Z","title":"Combining Causal Models for More Accurate Abstractions of Neural\n  Networks","summary":"  Mechanistic interpretability aims to reverse engineer neural networks by\nuncovering which high-level algorithms they implement. Causal abstraction\nprovides a precise notion of when a network implements an algorithm, i.e., a\ncausal model of the network contains low-level features that realize the\nhigh-level variables in a causal model of the algorithm. A typical problem in\npractical settings is that the algorithm is not an entirely faithful\nabstraction of the network, meaning it only partially captures the true\nreasoning process of a model. We propose a solution where we combine different\nsimple high-level models to produce a more faithful representation of the\nnetwork. Through learning this combination, we can model neural networks as\nbeing in different computational states depending on the input provided, which\nwe show is more accurate to GPT 2-small fine-tuned on two toy tasks. We observe\na trade-off between the strength of an interpretability hypothesis, which we\ndefine in terms of the number of inputs explained by the high-level models, and\nits faithfulness, which we define as the interchange intervention accuracy. Our\nmethod allows us to modulate between the two, providing the most accurate\ncombination of models that describe the behavior of a neural network given a\nfaithfulness level.\n","authors":["Theodora-Mara Pîslar","Sara Magliacane","Atticus Geiger"],"pdf_url":"https://arxiv.org/pdf/2503.11429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11427v1","updated":"2025-03-14T14:14:20Z","published":"2025-03-14T14:14:20Z","title":"FlowKac: An Efficient Neural Fokker-Planck solver using Temporal\n  Normalizing flows and the Feynman Kac-Formula","summary":"  Solving the Fokker-Planck equation for high-dimensional complex dynamical\nsystems remains a pivotal yet challenging task due to the intractability of\nanalytical solutions and the limitations of traditional numerical methods. In\nthis work, we present FlowKac, a novel approach that reformulates the\nFokker-Planck equation using the Feynman-Kac formula, allowing to query the\nsolution at a given point via the expected values of stochastic paths. A key\ninnovation of FlowKac lies in its adaptive stochastic sampling scheme which\nsignificantly reduces the computational complexity while maintaining high\naccuracy. This sampling technique, coupled with a time-indexed normalizing\nflow, designed for capturing time-evolving probability densities, enables\nrobust sampling of collocation points, resulting in a flexible and mesh-free\nsolver. This formulation mitigates the curse of dimensionality and enhances\ncomputational efficiency and accuracy, which is particularly crucial for\napplications that inherently require dimensions beyond the conventional three.\nWe validate the robustness and scalability of our method through various\nexperiments on a range of stochastic differential equations, demonstrating\nsignificant improvements over existing techniques.\n","authors":["Naoufal El Bekri","Lucas Drumetz","Franck Vermet"],"pdf_url":"https://arxiv.org/pdf/2503.11427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11198v2","updated":"2025-03-14T14:13:50Z","published":"2025-02-16T16:59:10Z","title":"ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity\n  Recognition","summary":"  ANCHOLIK-NER is a linguistically diverse dataset for Named Entity Recognition\n(NER) in Bangla regional dialects, capturing variations across Sylhet,\nChittagong, Barishal, Noakhali, and Mymensingh. The dataset has around 17,405\nsentences, 3,481 sentences per region. The data was collected from two publicly\navailable datasets and through web scraping from various online newspapers,\narticles. To ensure high-quality annotations, the BIO tagging scheme was\nemployed, and professional annotators with expertise in regional dialects\ncarried out the labeling process. The dataset is structured into separate\nsubsets for each region and is available in CSV format. Each entry contains\ntextual data along with identified named entities and their corresponding\nannotations. Named entities are categorized into ten distinct classes: Person,\nLocation, Organization, Food, Animal, Colour, Role, Relation, Object, and\nMiscellaneous. This dataset serves as a valuable resource for developing and\nevaluating NER models for Bangla dialectal variations, contributing to regional\nlanguage processing and low-resource NLP applications. It can be utilized to\nenhance NER systems in Bangla dialects, improve regional language\nunderstanding, and support applications in machine translation, information\nretrieval, and conversational AI.\n","authors":["Bidyarthi Paul","Faika Fairuj Preotee","Shuvashis Sarker","Shamim Rahim Refat","Shifat Islam","Tashreef Muhammad","Mohammad Ashraful Hoque","Shahriar Manzoor"],"pdf_url":"https://arxiv.org/pdf/2502.11198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11419v1","updated":"2025-03-14T14:03:28Z","published":"2025-03-14T14:03:28Z","title":"From Generative AI to Innovative AI: An Evolutionary Roadmap","summary":"  This paper explores the critical transition from Generative Artificial\nIntelligence (GenAI) to Innovative Artificial Intelligence (InAI). While recent\nadvancements in GenAI have enabled systems to produce high-quality content\nacross various domains, these models often lack the capacity for true\ninnovation. In this context, innovation is defined as the ability to generate\nnovel and useful outputs that go beyond mere replication of learned data. The\npaper examines this shift and proposes a roadmap for developing AI systems that\ncan generate content and engage in autonomous problem-solving and creative\nideation. The work provides both theoretical insights and practical strategies\nfor advancing AI to a stage where it can genuinely innovate, contributing\nmeaningfully to science, technology, and the arts.\n","authors":["Seyed Mahmoud Sajjadi Mohammadabadi"],"pdf_url":"https://arxiv.org/pdf/2503.11419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11414v1","updated":"2025-03-14T13:58:27Z","published":"2025-03-14T13:58:27Z","title":"Classifying Long-tailed and Label-noise Data via Disentangling and\n  Unlearning","summary":"  In real-world datasets, the challenges of long-tailed distributions and noisy\nlabels often coexist, posing obstacles to the model training and performance.\nExisting studies on long-tailed noisy label learning (LTNLL) typically assume\nthat the generation of noisy labels is independent of the long-tailed\ndistribution, which may not be true from a practical perspective. In real-world\nsituaiton, we observe that the tail class samples are more likely to be\nmislabeled as head, exacerbating the original degree of imbalance. We call this\nphenomenon as ``tail-to-head (T2H)'' noise. T2H noise severely degrades model\nperformance by polluting the head classes and forcing the model to learn the\ntail samples as head. To address this challenge, we investigate the dynamic\nmisleading process of the nosiy labels and propose a novel method called\nDisentangling and Unlearning for Long-tailed and Label-noisy data (DULL). It\nfirst employs the Inner-Feature Disentangling (IFD) to disentangle feature\ninternally. Based on this, the Inner-Feature Partial Unlearning (IFPU) is then\napplied to weaken and unlearn incorrect feature regions correlated to wrong\nclasses. This method prevents the model from being misled by noisy labels,\nenhancing the model's robustness against noise. To provide a controlled\nexperimental environment, we further propose a new noise addition algorithm to\nsimulate T2H noise. Extensive experiments on both simulated and real-world\ndatasets demonstrate the effectiveness of our proposed method.\n","authors":["Chen Shu","Mengke Li","Yiqun Zhang","Yang Lu","Bo Han","Yiu-ming Cheung","Hanzi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.11414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11411v1","updated":"2025-03-14T13:53:46Z","published":"2025-03-14T13:53:46Z","title":"Empowering Time Series Analysis with Synthetic Data: A Survey and\n  Outlook in the Era of Foundation Models","summary":"  Time series analysis is crucial for understanding dynamics of complex\nsystems. Recent advances in foundation models have led to task-agnostic Time\nSeries Foundation Models (TSFMs) and Large Language Model-based Time Series\nModels (TSLLMs), enabling generalized learning and integrating contextual\ninformation. However, their success depends on large, diverse, and high-quality\ndatasets, which are challenging to build due to regulatory, diversity, quality,\nand quantity constraints. Synthetic data emerge as a viable solution,\naddressing these challenges by offering scalable, unbiased, and high-quality\nalternatives. This survey provides a comprehensive review of synthetic data for\nTSFMs and TSLLMs, analyzing data generation strategies, their role in model\npretraining, fine-tuning, and evaluation, and identifying future research\ndirections.\n","authors":["Xu Liu","Taha Aksu","Juncheng Liu","Qingsong Wen","Yuxuan Liang","Caiming Xiong","Silvio Savarese","Doyen Sahoo","Junnan Li","Chenghao Liu"],"pdf_url":"https://arxiv.org/pdf/2503.11411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11408v1","updated":"2025-03-14T13:48:25Z","published":"2025-03-14T13:48:25Z","title":"A Neural Network Architecture Based on Attention Gate Mechanism for 3D\n  Magnetotelluric Forward Modeling","summary":"  Traditional three-dimensional magnetotelluric (MT) numerical forward modeling\nmethods, such as the finite element method (FEM) and finite volume method\n(FVM), suffer from high computational costs and low efficiency due to\nlimitations in mesh refinement and computational resources. We propose a novel\nneural network architecture named MTAGU-Net, which integrates an attention\ngating mechanism for 3D MT forward modeling. Specifically, a dual-path\nattention gating module is designed based on forward response data images and\nembedded in the skip connections between the encoder and decoder. This module\nenables the fusion of critical anomaly information from shallow feature maps\nduring the decoding of deep feature maps, significantly enhancing the network's\ncapability to extract features from anomalous regions. Furthermore, we\nintroduce a synthetic model generation method utilizing 3D Gaussian random\nfield (GRF), which accurately replicates the electrical structures of\nreal-world geological scenarios with high fidelity. Numerical experiments\ndemonstrate that MTAGU-Net outperforms conventional 3D U-Net in terms of\nconvergence stability and prediction accuracy, with the structural similarity\nindex (SSIM) of the forward response data consistently exceeding 0.98.\nMoreover, the network can accurately predict forward response data on\npreviously unseen datasets models, demonstrating its strong generalization\nability and validating the feasibility and effectiveness of this method in\npractical applications.\n","authors":["Xin Zhong","Weiwei Ling","Kejia Pan","Pinxia Wu","Jiajing Zhang","Zhiliang Zhan","Wenbo Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.11408v1.pdf","comment":"12 pages, 16 figures"},{"id":"http://arxiv.org/abs/2410.10537v3","updated":"2025-03-14T13:47:31Z","published":"2024-10-14T14:17:52Z","title":"Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature","summary":"  Purpose: We introduce a novel methodology for voice pathology detection using\nthe publicly available Saarbr\\\"ucken Voice Database (SVD) and a robust feature\nset combining commonly used acoustic handcrafted features with two novel ones:\npitch difference (relative variation in fundamental frequency) and NaN feature\n(failed fundamental frequency estimation).\n  Methods: We evaluate six machine learning (ML) algorithms -- support vector\nmachine, k-nearest neighbors, naive Bayes, decision tree, random forest, and\nAdaBoost -- using grid search for feasible hyperparameters and 20480 different\nfeature subsets. Top 1000 classification models -- feature subset combinations\nfor each ML algorithm are validated with repeated stratified cross-validation.\nTo address class imbalance, we apply K-Means SMOTE to augment the training\ndata.\n  Results: Our approach achieves 85.61%, 84.69% and 85.22% unweighted average\nrecall (UAR) for females, males and combined results respectively. We\nintentionally omit accuracy as it is a highly biased metric for imbalanced\ndata.\n  Conclusion: Our study demonstrates that by following the proposed methodology\nand feature engineering, there is a potential in detection of various voice\npathologies using ML models applied to the simplest vocal task, a sustained\nutterance of the vowel /a:/. To enable easier use of our methodology and to\nsupport our claims, we provide a publicly available GitHub repository with DOI\n10.5281/zenodo.13771573. Finally, we provide a REFORMS checklist to enhance\nreadability, reproducibility and justification of our approach\n","authors":["Jan Vrba","Jakub Steinbach","Tomáš Jirsa","Laura Verde","Roberta De Fazio","Yuwen Zeng","Kei Ichiji","Lukáš Hájek","Zuzana Sedláková","Zuzana Urbániová","Martin Chovanec","Jan Mareš","Noriyasu Homma"],"pdf_url":"https://arxiv.org/pdf/2410.10537v3.pdf","comment":"Code repository:\n  https://github.com/aailab-uct/Automated-Robust-and-Reproducible-Voice-Pathology-Detection,\n  Supplementary materials: https://doi.org/10.5281/zenodo.14793017"},{"id":"http://arxiv.org/abs/2312.04584v3","updated":"2025-03-14T13:36:51Z","published":"2023-12-03T09:12:14Z","title":"Towards Sample-specific Backdoor Attack with Clean Labels via Attribute\n  Trigger","summary":"  Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and\nmalicious methods since they can easily circumvent most of the current backdoor\ndefenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due\nto their poisoned-label nature, where users can discover anomalies if they\ncheck the image-label relationship. In particular, we demonstrate that it is\nineffective to directly generalize existing SSBAs to their clean-label variants\nby poisoning samples solely from the target class. We reveal that it is\nprimarily due to two reasons, including \\textbf{(1)} the `antagonistic effects'\nof ground-truth features and \\textbf{(2)} the learning difficulty of\nsample-specific features. Accordingly, trigger-related features of existing\nSSBAs cannot be effectively learned under the clean-label setting due to their\nmild trigger intensity required for ensuring stealthiness. We argue that the\nintensity constraint of existing SSBAs is mostly because their trigger patterns\nare `content-irrelevant' and therefore act as `noises' for both humans and\nDNNs. Motivated by this understanding, we propose to exploit content-relevant\nfeatures, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design\nclean-label SSBAs. This new attack paradigm is dubbed backdoor attack with\nattribute trigger (BAAT). Extensive experiments are conducted on benchmark\ndatasets, which verify the effectiveness of our BAAT and its resistance to\nexisting defenses.\n","authors":["Mingyan Zhu","Yiming Li","Junfeng Guo","Tao Wei","Shu-Tao Xia","Zhan Qin"],"pdf_url":"https://arxiv.org/pdf/2312.04584v3.pdf","comment":"This paper is accepted by IEEE Transactions on Dependable and Secure\n  Computing (TDSC), 2025. The first two authors contributed equally to this\n  work. 14 pages"},{"id":"http://arxiv.org/abs/2503.11389v1","updated":"2025-03-14T13:33:22Z","published":"2025-03-14T13:33:22Z","title":"Deepfake Detection of Face Images based on a Convolutional Neural\n  Network","summary":"  Fake News and especially deepfakes (generated, non-real image or video\ncontent) have become a serious topic over the last years. With the emergence of\nmachine learning algorithms it is now easier than ever before to generate such\nfake content, even for private persons. This issue of generated fake images is\nespecially critical in the context of politics and public figures. We want to\naddress this conflict by building a model based on a Convolutions Neural\nNetwork in order to detect such generated and fake images showing human\nportraits. As a basis, we use a pre-trained ResNet-50 model due to its\neffectiveness in terms of classifying images. We then adopted the base model to\nour task of classifying a single image as authentic/real or fake by adding an\nfully connected output layer containing a single neuron indicating the\nauthenticity of an image. We applied fine tuning and transfer learning to\ndevelop the model and improve its parameters. For the training process we\ncollected the image data set \"Diverse Face Fake Dataset\" containing a wide\nrange of different image manipulation methods and also diversity in terms of\nfaces visible on the images. With our final model we reached the following\noutstanding performance metrics: precision = 0.98, recall 0.96, F1-Score = 0.97\nand an area-under-curve = 0.99.\n","authors":["Lukas Kroiß","Johannes Reschke"],"pdf_url":"https://arxiv.org/pdf/2503.11389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11387v1","updated":"2025-03-14T13:30:38Z","published":"2025-03-14T13:30:38Z","title":"Hierarchical Information-Guided Spatio-Temporal Mamba for Stock Time\n  Series Forecasting","summary":"  Mamba has demonstrated excellent performance in various time series\nforecasting tasks due to its superior selection mechanism. Nevertheless,\nconventional Mamba-based models encounter significant challenges in accurately\npredicting stock time series, as they fail to adequately capture both the\noverarching market dynamics and the intricate interdependencies among\nindividual stocks. To overcome these constraints, we introduce the Hierarchical\nInformation-Guided Spatio-Temporal Mamba (HIGSTM) framework. HIGSTM introduces\nIndex-Guided Frequency Filtering Decomposition to extract commonality and\nspecificity from time series. The model architecture features a meticulously\ndesigned hierarchical framework that systematically captures both temporal\ndynamic patterns and global static relationships within the stock market.\nFurthermore, we propose an Information-Guided Mamba that integrates macro\ninformations into the sequence selection process, thereby facilitating more\nmarket-conscious decision-making. Comprehensive experimental evaluations\nconducted on the CSI500, CSI800 and CSI1000 datasets demonstrate that HIGSTM\nachieves state-of-the-art performance.\n","authors":["Wenbo Yan","Shurui Wang","Ying Tan"],"pdf_url":"https://arxiv.org/pdf/2503.11387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06466v2","updated":"2025-03-14T13:30:09Z","published":"2024-04-09T17:14:41Z","title":"Hyperparameter Selection in Continual Learning","summary":"  In continual learning (CL) -- where a learner trains on a stream of data --\nstandard hyperparameter optimisation (HPO) cannot be applied, as a learner does\nnot have access to all of the data at the same time. This has prompted the\ndevelopment of CL-specific HPO frameworks. The most popular way to tune\nhyperparameters in CL is to repeatedly train over the whole data stream with\ndifferent hyperparameter settings. However, this end-of-training HPO is\nunusable in practice since a learner can only see the stream once. Hence, there\nis an open question: what HPO framework should a practitioner use for a CL\nproblem in reality? This paper looks at this question by comparing several\nrealistic HPO frameworks. We find that none of the HPO frameworks considered,\nincluding end-of-training HPO, perform consistently better than the rest on\npopular CL benchmarks. We therefore arrive at a twofold conclusion: a) to be\nable to discriminate between HPO frameworks there is a need to move beyond the\ncurrent most commonly used CL benchmarks, and b) on the popular CL benchmarks\nexamined, a CL practitioner should use a realistic HPO framework and can select\nit based on factors separate from performance, for example compute efficiency.\n","authors":["Thomas L. Lee","Sigrid Passano Hellan","Linus Ericsson","Elliot J. Crowley","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2404.06466v2.pdf","comment":"Preprint, 16 pages"},{"id":"http://arxiv.org/abs/2503.05788v2","updated":"2025-03-14T13:28:04Z","published":"2025-02-28T01:20:01Z","title":"Emergent Abilities in Large Language Models: A Survey","summary":"  Large Language Models (LLMs) are leading a new technological revolution as\none of the most promising research streams toward artificial general\nintelligence. The scaling of these models, accomplished by increasing the\nnumber of parameters and the magnitude of the training datasets, has been\nlinked to various so-called emergent abilities that were previously unobserved.\nThese emergent abilities, ranging from advanced reasoning and in-context\nlearning to coding and problem-solving, have sparked an intense scientific\ndebate: Are they truly emergent, or do they simply depend on external factors,\nsuch as training dynamics, the type of problems, or the chosen metric? What\nunderlying mechanism causes them? Despite their transformative potential,\nemergent abilities remain poorly understood, leading to misconceptions about\ntheir definition, nature, predictability, and implications. In this work, we\nshed light on emergent abilities by conducting a comprehensive review of the\nphenomenon, addressing both its scientific underpinnings and real-world\nconsequences. We first critically analyze existing definitions, exposing\ninconsistencies in conceptualizing emergent abilities. We then explore the\nconditions under which these abilities appear, evaluating the role of scaling\nlaws, task complexity, pre-training loss, quantization, and prompting\nstrategies. Our review extends beyond traditional LLMs and includes Large\nReasoning Models (LRMs), which leverage reinforcement learning and\ninference-time search to amplify reasoning and self-reflection. However,\nemergence is not inherently positive. As AI systems gain autonomous reasoning\ncapabilities, they also develop harmful behaviors, including deception,\nmanipulation, and reward hacking. We highlight growing concerns about safety\nand governance, emphasizing the need for better evaluation frameworks and\nregulatory oversight.\n","authors":["Leonardo Berti","Flavio Giorgi","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2503.05788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11373v1","updated":"2025-03-14T13:18:02Z","published":"2025-03-14T13:18:02Z","title":"Exploring Performance-Complexity Trade-Offs in Sound Event Detection","summary":"  We target the problem of developing new low-complexity networks for the sound\nevent detection task. Our goal is to meticulously analyze the\nperformance-complexity trade-off, aiming to be competitive with the large\nstate-of-the-art models, at a fraction of the computational requirements. We\nfind that low-complexity convolutional models previously proposed for audio\ntagging can be effectively adapted for event detection (which requires\nframe-wise prediction) by adjusting convolutional strides, removing the global\npooling, and, importantly, adding a sequence model before the (now frame-wise)\nclassification heads. Systematic experiments reveal that the best choice for\nthe sequence model type depends on which complexity metric is most important\nfor the given application. We also investigate the impact of enhanced training\nstrategies such as knowledge distillation. In the end, we show that combined\nwith an optimized training strategy, we can reach event detection performance\ncomparable to state-of-the-art transformers while requiring only around 5% of\nthe parameters. We release all our pre-trained models and the code for\nreproducing this work to support future research in low-complexity sound event\ndetection at https://github.com/theMoro/EfficientSED.\n","authors":["Tobias Morocutti","Florian Schmid","Jonathan Greif","Francesco Foscarin","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2503.11373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12523v3","updated":"2025-03-14T13:11:28Z","published":"2024-11-19T14:13:25Z","title":"Data Pruning in Generative Diffusion Models","summary":"  Data pruning is the problem of identifying a core subset that is most\nbeneficial to training and discarding the remainder. While pruning strategies\nare well studied for discriminative models like those used in classification,\nlittle research has gone into their application to generative models.\nGenerative models aim to estimate the underlying distribution of the data, so\npresumably they should benefit from larger datasets. In this work we aim to\nshed light on the accuracy of this statement, specifically answer the question\nof whether data pruning for generative diffusion models could have a positive\nimpact. Contrary to intuition, we show that eliminating redundant or noisy data\nin large datasets is beneficial particularly when done strategically. We\nexperiment with several pruning methods including recent-state-of-art methods,\nand evaluate over CelebA-HQ and ImageNet datasets. We demonstrate that a simple\nclustering method outperforms other sophisticated and computationally demanding\nmethods. We further exhibit how we can leverage clustering to balance skewed\ndatasets in an unsupervised manner to allow fair sampling for underrepresented\npopulations in the data distribution, which is a crucial problem in generative\nmodels.\n","authors":["Rania Briq","Jiangtao Wang","Stefan Kesselheim"],"pdf_url":"https://arxiv.org/pdf/2411.12523v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11363v1","updated":"2025-03-14T12:57:12Z","published":"2025-03-14T12:57:12Z","title":"Creating a Good Teacher for Knowledge Distillation in Acoustic Scene\n  Classification","summary":"  Knowledge Distillation (KD) is a widespread technique for compressing the\nknowledge of large models into more compact and efficient models. KD has proved\nto be highly effective in building well-performing low-complexity Acoustic\nScene Classification (ASC) systems and was used in all the top-ranked\nsubmissions to this task of the annual DCASE challenge in the past three years.\nThere is extensive research available on establishing the KD process, designing\nefficient student models, and forming well-performing teacher ensembles.\nHowever, less research has been conducted on investigating which teacher model\nattributes are beneficial for low-complexity students. In this work, we try to\nclose this gap by studying the effects on the student's performance when using\ndifferent teacher network architectures, varying the teacher model size,\ntraining them with different device generalization methods, and applying\ndifferent ensembling strategies. The results show that teacher model sizes,\ndevice generalization methods, the ensembling strategy and the ensemble size\nare key factors for a well-performing student network.\n","authors":["Tobias Morocutti","Florian Schmid","Khaled Koutini","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2503.11363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11360v1","updated":"2025-03-14T12:53:37Z","published":"2025-03-14T12:53:37Z","title":"PARIC: Probabilistic Attention Regularization for Language Guided Image\n  Classification from Pre-trained Vison Language Models","summary":"  Language-guided attention frameworks have significantly enhanced both\ninterpretability and performance in image classification; however, the reliance\non deterministic embeddings from pre-trained vision-language foundation models\nto generate reference attention maps frequently overlooks the intrinsic\nmultivaluedness and ill-posed characteristics of cross-modal mappings. To\naddress these limitations, we introduce PARIC, a probabilistic framework for\nguiding visual attention via language specifications. Our approach enables\npre-trained vision-language models to generate probabilistic reference\nattention maps, which align textual and visual modalities more effectively\nwhile incorporating uncertainty estimates, as compared to their deterministic\ncounterparts. Experiments on benchmark test problems demonstrate that PARIC\nenhances prediction accuracy, mitigates bias, ensures consistent predictions,\nand improves robustness across various datasets.\n","authors":["Mayank Nautiyal","Stela Arranz Gheorghe","Kristiana Stefa","Li Ju","Ida-Maria Sintorn","Prashant Singh"],"pdf_url":"https://arxiv.org/pdf/2503.11360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04814v2","updated":"2025-03-14T12:41:59Z","published":"2024-02-07T13:04:35Z","title":"BOWL: A Deceptively Simple Open World Learner","summary":"  Traditional machine learning excels on static benchmarks, but the real world\nis dynamic and seldom as carefully curated as test sets. Practical applications\nmay generally encounter undesired inputs, are required to deal with novel\ninformation, and need to ensure operation through their full lifetime - aspects\nwhere standard deep models struggle. These three elements may have been\nresearched individually, but their practical conjunction, i.e., open world\nlearning, is much less consolidated. In this paper, we posit that neural\nnetworks already contain a powerful catalyst to turn them into open world\nlearners: the batch normalization layer. Leveraging its tracked statistics, we\nderive effective strategies to detect in- and out-of-distribution samples,\nselect informative data points, and update the model continuously. This, in\nturn, allows us to demonstrate that existing batch-normalized models can be\nmade more robust, less prone to forgetting over time, and be trained\nefficiently with less data.\n","authors":["Roshni . R. Kamath","Rupert Mitchell","Subarnaduti Paul","Kristian Kersting","Martin Mundt"],"pdf_url":"https://arxiv.org/pdf/2402.04814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11349v1","updated":"2025-03-14T12:36:15Z","published":"2025-03-14T12:36:15Z","title":"An experimental approach on Few Shot Class Incremental Learning","summary":"  Few-Shot Class-Incremental Learning (FSCIL) represents a cutting-edge\nparadigm within the broader scope of machine learning, designed to empower\nmodels with the ability to assimilate new classes of data with limited examples\nwhile safeguarding existing knowledge. The paper will present different\nsolutions which contain extensive experiments across large-scale datasets,\ndomain shifts, and network architectures to evaluate and compare the selected\nmethods. We highlight their advantages and then present an experimental\napproach with the purpose of improving the most promising one by replacing the\nvisual-language (V-L) model (CLIP) with another V-L model (CLOOB) that seem to\noutperform it on zero-shot learning tasks. The aim of this report is to present\nan experimental method for FSCIL that would improve its performance. We also\nplan to offer an overview followed by an analysis of the recent advancements in\nFSCIL domain, focusing on various strategies to mitigate catastrophic\nforgetting and improve the adaptability of models to evolving tasks and\ndatasets.\n","authors":["Marinela Adam"],"pdf_url":"https://arxiv.org/pdf/2503.11349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03551v4","updated":"2025-03-14T12:30:28Z","published":"2024-03-06T08:51:09Z","title":"Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers","summary":"  Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.\n","authors":["Tim Selig","Thomas März","Martin Storath","Andreas Weinmann"],"pdf_url":"https://arxiv.org/pdf/2403.03551v4.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.11347v1","updated":"2025-03-14T12:25:27Z","published":"2025-03-14T12:25:27Z","title":"Integrating Dynamical Systems Modeling with Spatiotemporal scRNA-seq\n  Data Analysis","summary":"  Understanding the dynamic nature of biological systems is fundamental to\ndeciphering cellular behavior, developmental processes, and disease\nprogression. Single-cell RNA sequencing (scRNA-seq) has provided static\nsnapshots of gene expression, offering valuable insights into cellular states\nat a single time point. Recent advancements in temporally resolved scRNA-seq,\nspatial transcriptomics (ST), and time-series spatial transcriptomics\n(temporal-ST) have further revolutionized our ability to study the\nspatiotemporal dynamics of individual cells. These technologies, when combined\nwith computational frameworks such as Markov chains, stochastic differential\nequations (SDEs), and generative models like optimal transport and\nSchr\\\"odinger bridges, enable the reconstruction of dynamic cellular\ntrajectories and cell fate decisions. This review discusses how these dynamical\nsystem approaches offer new opportunities to model and infer cellular dynamics\nfrom a systematic perspective.\n","authors":["Zhenyi Zhang","Yuhao Sun","Qiangwei Peng","Tiejun Li","Peijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.11347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16073v2","updated":"2025-03-14T12:21:37Z","published":"2025-01-27T14:21:34Z","title":"Challenging Assumptions in Learning Generic Text Style Embeddings","summary":"  Recent advancements in language representation learning primarily emphasize\nlanguage modeling for deriving meaningful representations, often neglecting\nstyle-specific considerations. This study addresses this gap by creating\ngeneric, sentence-level style embeddings crucial for style-centric tasks. Our\napproach is grounded on the premise that low-level text style changes can\ncompose any high-level style. We hypothesize that applying this concept to\nrepresentation learning enables the development of versatile text style\nembeddings. By fine-tuning a general-purpose text encoder using contrastive\nlearning and standard cross-entropy loss, we aim to capture these low-level\nstyle shifts, anticipating that they offer insights applicable to high-level\ntext styles. The outcomes prompt us to reconsider the underlying assumptions as\nthe results do not always show that the learned style representations capture\nhigh-level text styles.\n","authors":["Phil Ostheimer","Marius Kloft","Sophie Fellenz"],"pdf_url":"https://arxiv.org/pdf/2501.16073v2.pdf","comment":"Proceedings of the Sixth Workshop on Insights from Negative Results\n  in NLP at NAACL-HLT"},{"id":"http://arxiv.org/abs/2503.11339v1","updated":"2025-03-14T12:09:58Z","published":"2025-03-14T12:09:58Z","title":"Contextual Similarity Distillation: Ensemble Uncertainties with a Single\n  Model","summary":"  Uncertainty quantification is a critical aspect of reinforcement learning and\ndeep learning, with numerous applications ranging from efficient exploration\nand stable offline reinforcement learning to outlier detection in medical\ndiagnostics. The scale of modern neural networks, however, complicates the use\nof many theoretically well-motivated approaches such as full Bayesian\ninference. Approximate methods like deep ensembles can provide reliable\nuncertainty estimates but still remain computationally expensive. In this work,\nwe propose contextual similarity distillation, a novel approach that explicitly\nestimates the variance of an ensemble of deep neural networks with a single\nmodel, without ever learning or evaluating such an ensemble in the first place.\nOur method builds on the predictable learning dynamics of wide neural networks,\ngoverned by the neural tangent kernel, to derive an efficient approximation of\nthe predictive variance of an infinite ensemble. Specifically, we reinterpret\nthe computation of ensemble variance as a supervised regression problem with\nkernel similarities as regression targets. The resulting model can estimate\npredictive variance at inference time with a single forward pass, and can make\nuse of unlabeled target-domain data or data augmentations to refine its\nuncertainty estimates. We empirically validate our method across a variety of\nout-of-distribution detection benchmarks and sparse-reward reinforcement\nlearning environments. We find that our single-model method performs\ncompetitively and sometimes superior to ensemble-based baselines and serves as\na reliable signal for efficient exploration. These results, we believe,\nposition contextual similarity distillation as a principled and scalable\nalternative for uncertainty quantification in reinforcement learning and\ngeneral deep learning.\n","authors":["Moritz A. Zanger","Pascal R. Van der Vaart","Wendelin Böhmer","Matthijs T. J. Spaan"],"pdf_url":"https://arxiv.org/pdf/2503.11339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11331v1","updated":"2025-03-14T11:59:23Z","published":"2025-03-14T11:59:23Z","title":"Cardiomyopathy Diagnosis Model from Endomyocardial Biopsy Specimens:\n  Appropriate Feature Space and Class Boundary in Small Sample Size Data","summary":"  As the number of patients with heart failure increases, machine learning (ML)\nhas garnered attention in cardiomyopathy diagnosis, driven by the shortage of\npathologists. However, endomyocardial biopsy specimens are often small sample\nsize and require techniques such as feature extraction and dimensionality\nreduction. This study aims to determine whether texture features are effective\nfor feature extraction in the pathological diagnosis of cardiomyopathy.\nFurthermore, model designs that contribute toward improving generalization\nperformance are examined by applying feature selection (FS) and dimensional\ncompression (DC) to several ML models. The obtained results were verified by\nvisualizing the inter-class distribution differences and conducting statistical\nhypothesis testing based on texture features. Additionally, they were evaluated\nusing predictive performance across different model designs with varying\ncombinations of FS and DC (applied or not) and decision boundaries. The\nobtained results confirmed that texture features may be effective for the\npathological diagnosis of cardiomyopathy. Moreover, when the ratio of features\nto the sample size is high, a multi-step process involving FS and DC improved\nthe generalization performance, with the linear kernel support vector machine\nachieving the best results. This process was demonstrated to be potentially\neffective for models with reduced complexity, regardless of whether the\ndecision boundaries were linear, curved, perpendicular, or parallel to the\naxes. These findings are expected to facilitate the development of an effective\ncardiomyopathy diagnostic model for its rapid adoption in medical practice.\n","authors":["Masaya Mori","Yuto Omae","Yutaka Koyama","Kazuyuki Hara","Jun Toyotani","Yasuo Okumura","Hiroyuki Hao"],"pdf_url":"https://arxiv.org/pdf/2503.11331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11330v1","updated":"2025-03-14T11:57:51Z","published":"2025-03-14T11:57:51Z","title":"Learning to reset in target search problems","summary":"  Target search problems are central to a wide range of fields, from biological\nforaging to the optimization algorithms. Recently, the ability to reset the\nsearch has been shown to significantly improve the searcher's efficiency.\nHowever, the optimal resetting strategy depends on the specific properties of\nthe search problem and can often be challenging to determine. In this work, we\npropose a reinforcement learning (RL)-based framework to train agents capable\nof optimizing their search efficiency in environments by learning how to reset.\nFirst, we validate the approach in a well-established benchmark: the Brownian\nsearch with resetting. There, RL agents consistently recover strategies closely\nresembling the sharp resetting distribution, known to be optimal in this\nscenario. We then extend the framework by allowing agents to control not only\nwhen to reset, but also their spatial dynamics through turning actions. In this\nmore complex setting, the agents discover strategies that adapt both resetting\nand turning to the properties of the environment, outperforming the proposed\nbenchmarks. These results demonstrate how reinforcement learning can serve both\nas an optimization tool and a mechanism for uncovering new, interpretable\nstrategies in stochastic search processes with resetting.\n","authors":["Gorka Muñoz-Gil","Hans J. Briegel","Michele Caraglio"],"pdf_url":"https://arxiv.org/pdf/2503.11330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17064v2","updated":"2025-03-14T11:33:08Z","published":"2024-03-25T18:00:42Z","title":"Continuous, Subject-Specific Attribute Control in T2I Models by\n  Identifying Semantic Directions","summary":"  Recent advances in text-to-image (T2I) diffusion models have significantly\nimproved the quality of generated images. However, providing efficient control\nover individual subjects, particularly the attributes characterizing them,\nremains a key challenge. While existing methods have introduced mechanisms to\nmodulate attribute expression, they typically provide either detailed,\nobject-specific localization of such a modification or full-scale fine-grained,\nnuanced control of attributes. No current approach offers both simultaneously,\nresulting in a gap when trying to achieve precise continuous and\nsubject-specific attribute modulation in image generation. In this work, we\ndemonstrate that token-level directions exist within commonly used CLIP text\nembeddings that enable fine-grained, subject-specific control of high-level\nattributes in T2I models. We introduce two methods to identify these\ndirections: a simple, optimization-free technique and a learning-based approach\nthat utilizes the T2I model to characterize semantic concepts more\nspecifically. Our methods allow the augmentation of the prompt text input,\nenabling fine-grained control over multiple attributes of individual subjects\nsimultaneously, without requiring any modifications to the diffusion model\nitself. This approach offers a unified solution that fills the gap between\nglobal and localized control, providing competitive flexibility and precision\nin text-guided image generation. Project page:\nhttps://compvis.github.io/attribute-control. Code is available at\nhttps://github.com/CompVis/attribute-control.\n","authors":["Stefan Andreas Baumann","Felix Krause","Michael Neumayr","Nick Stracke","Melvin Sevi","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.17064v2.pdf","comment":"CVPR 2025. Project page: https://compvis.github.io/attribute-control"},{"id":"http://arxiv.org/abs/2503.11305v1","updated":"2025-03-14T11:18:47Z","published":"2025-03-14T11:18:47Z","title":"Lightweight Learning for Grant-Free Activity Detection in Cell-Free\n  Massive MIMO Networks","summary":"  Grant-free random access (GF-RA) is a promising access technique for massive\nmachine-type communications (mMTC) in future wireless networks, particularly in\nthe context of 5G and beyond (6G) systems. Within the context of GF-RA, this\nstudy investigates the efficiency of employing supervised machine learning\ntechniques to tackle the challenges on the device activity detection (AD).\nGF-RA addresses scalability by employing non-orthogonal pilot sequences, which\nprovides an efficient alternative comparing to conventional grant-based random\naccess (GB-RA) technique that are constrained by the scarcity of orthogonal\npreamble resources. In this paper, we propose a novel lightweight data-driven\nalgorithmic framework specifically designed for activity detection in GF-RA for\nmMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks.\nWe propose two distinct framework deployment strategies, centralized and\ndecentralized, both tailored to streamline the proposed approach implementation\nacross network infrastructures. Moreover, we introduce optimized post-detection\nmethodologies complemented by a clustering stage to enhance overall detection\nperformances. Our 3GPP-compliant simulations have validated that the proposed\nalgorithm achieves state-of-the-art model-based activity detection accuracy\nwhile significantly reducing complexity. Achieving 99% accuracy, it\ndemonstrates real-world viability and effectiveness.\n","authors":["Ali Elkeshawy","Haifa Fares","Amor Nafkha"],"pdf_url":"https://arxiv.org/pdf/2503.11305v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.07160"},{"id":"http://arxiv.org/abs/2503.11294v1","updated":"2025-03-14T11:04:46Z","published":"2025-03-14T11:04:46Z","title":"Latent Space Representation of Electricity Market Curves for Improved\n  Prediction Efficiency","summary":"  This work presents a three-phase ML prediction framework designed to handle a\nhigh dimensionality and multivariate time series character of the electricity\nmarket curves. In the preprocessing phase, we transform the original data to\nachieve a unified structure and mitigate the effect of possible outliers.\nFurther, to address the challenge of high dimensionality, we test three\ndimensionality reduction techniques (PCA, kPCA, UMAP). Finally, we predict\nsupply and demand curves, once represented in a latent space, with a variety of\nmachine learning methods (RF, LSTM, TSMixer). As our results on the MIBEL\ndataset show, a high dimensional structure of the market curves can be best\nhandled by the nonlinear reduction technique UMAP. Regardless of the ML\ntechnique used for prediction, we achieved the lowest values for all considered\nprecision metrics with a UMAP latent space representation in only two or three\ndimensions, even when compared to PCA and kPCA with five or six dimensions.\nFurther, we demonstrate that the most promising machine learning technique to\nhandle the complex structure of the electricity market curves is a novel\nTSMixer architecture. Finally, we fill the gap in the field of electricity\nmarket curves prediction literature: in addition to standard analysis on the\nsupply side, we applied the ML framework and predicted demand curves too. We\ndiscussed the differences in the achieved results for these two types of\ncurves.\n","authors":["Martin Výboh","Zuzana Chladná","Gabriela Grmanová","Mária Lucká"],"pdf_url":"https://arxiv.org/pdf/2503.11294v1.pdf","comment":"Submitted to Applied Soft Computing"},{"id":"http://arxiv.org/abs/2503.11283v1","updated":"2025-03-14T10:41:27Z","published":"2025-03-14T10:41:27Z","title":"Brain Effective Connectivity Estimation via Fourier Spatiotemporal\n  Attention","summary":"  Estimating brain effective connectivity (EC) from functional magnetic\nresonance imaging (fMRI) data can aid in comprehending the neural mechanisms\nunderlying human behavior and cognition, providing a foundation for disease\ndiagnosis. However, current spatiotemporal attention modules handle temporal\nand spatial attention separately, extracting temporal and spatial features\neither sequentially or in parallel. These approaches overlook the inherent\nspatiotemporal correlations present in real world fMRI data. Additionally, the\npresence of noise in fMRI data further limits the performance of existing\nmethods. In this paper, we propose a novel brain effective connectivity\nestimation method based on Fourier spatiotemporal attention (FSTA-EC), which\ncombines Fourier attention and spatiotemporal attention to simultaneously\ncapture inter-series (spatial) dynamics and intra-series (temporal)\ndependencies from high-noise fMRI data. Specifically, Fourier attention is\ndesigned to convert the high-noise fMRI data to frequency domain, and map the\ndenoised fMRI data back to physical domain, and spatiotemporal attention is\ncrafted to simultaneously learn spatiotemporal dynamics. Furthermore, through a\nseries of proofs, we demonstrate that incorporating learnable filter into fast\nFourier transform and inverse fast Fourier transform processes is\nmathematically equivalent to performing cyclic convolution. The experimental\nresults on simulated and real-resting-state fMRI datasets demonstrate that the\nproposed method exhibits superior performance when compared to state-of-the-art\nmethods.\n","authors":["Wen Xiong","Jinduo Liu","Junzhong Ji","Fenglong Ma"],"pdf_url":"https://arxiv.org/pdf/2503.11283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11282v1","updated":"2025-03-14T10:40:04Z","published":"2025-03-14T10:40:04Z","title":"OPTIMUS: Predicting Multivariate Outcomes in Alzheimer's Disease Using\n  Multi-modal Data amidst Missing Values","summary":"  Alzheimer's disease, a neurodegenerative disorder, is associated with neural,\ngenetic, and proteomic factors while affecting multiple cognitive and\nbehavioral faculties. Traditional AD prediction largely focuses on univariate\ndisease outcomes, such as disease stages and severity. Multimodal data encode\nbroader disease information than a single modality and may, therefore, improve\ndisease prediction; but they often contain missing values. Recent \"deeper\"\nmachine learning approaches show promise in improving prediction accuracy, yet\nthe biological relevance of these models needs to be further charted.\nIntegrating missing data analysis, predictive modeling, multimodal data\nanalysis, and explainable AI, we propose OPTIMUS, a predictive, modular, and\nexplainable machine learning framework, to unveil the many-to-many predictive\npathways between multimodal input data and multivariate disease outcomes amidst\nmissing values. OPTIMUS first applies modality-specific imputation to uncover\ndata from each modality while optimizing overall prediction accuracy. It then\nmaps multimodal biomarkers to multivariate outcomes using machine-learning and\nextracts biomarkers respectively predictive of each outcome. Finally, OPTIMUS\nincorporates XAI to explain the identified multimodal biomarkers. Using data\nfrom 346 cognitively normal subjects, 608 persons with mild cognitive\nimpairment, and 251 AD patients, OPTIMUS identifies neural and transcriptomic\nsignatures that jointly but differentially predict multivariate outcomes\nrelated to executive function, language, memory, and visuospatial function. Our\nwork demonstrates the potential of building a predictive and biologically\nexplainable machine-learning framework to uncover multimodal biomarkers that\ncapture disease profiles across varying cognitive landscapes. The results\nimprove our understanding of the complex many-to-many pathways in AD.\n","authors":["Christelle Schneuwly Diaz","Duy-Thanh Vu","Julien Bodelet","Duy-Cat Can","Guillaume Blanc","Haiting Jiang","Lin Yao","Guiseppe Pantaleo"," ADNI","Oliver Y. Chén"],"pdf_url":"https://arxiv.org/pdf/2503.11282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11276v1","updated":"2025-03-14T10:33:13Z","published":"2025-03-14T10:33:13Z","title":"Permutation Equivariant Neural Networks for Symmetric Tensors","summary":"  Incorporating permutation equivariance into neural networks has proven to be\nuseful in ensuring that models respect symmetries that exist in data. Symmetric\ntensors, which naturally appear in statistics, machine learning, and graph\ntheory, are essential for many applications in physics, chemistry, and\nmaterials science, amongst others. However, existing research on permutation\nequivariant models has not explored symmetric tensors as inputs, and most prior\nwork on learning from these tensors has focused on equivariance to Euclidean\ngroups. In this paper, we present two different characterisations of all linear\npermutation equivariant functions between symmetric power spaces of\n$\\mathbb{R}^n$. We show on two tasks that these functions are highly data\nefficient compared to standard MLPs and have potential to generalise well to\nsymmetric tensors of different sizes.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2503.11276v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2503.11273v1","updated":"2025-03-14T10:30:43Z","published":"2025-03-14T10:30:43Z","title":"Financial Fraud Detection with Entropy Computing","summary":"  We introduce CVQBoost, a novel classification algorithm that leverages early\nhardware implementing Quantum Computing Inc's Entropy Quantum Computing (EQC)\nparadigm, Dirac-3 [Nguyen et. al. arXiv:2407.04512]. We apply CVQBoost to a\nfraud detection test case and benchmark its performance against XGBoost, a\nwidely utilized ML method. Running on Dirac-3, CVQBoost demonstrates a\nsignificant runtime advantage over XGBoost, which we evaluate on\nhigh-performance hardware comprising up to 48 CPUs and four NVIDIA L4 GPUs\nusing the RAPIDS AI framework. Our results show that CVQBoost maintains\ncompetitive accuracy (measured by AUC) while significantly reducing training\ntime, particularly as dataset size and feature complexity increase. To assess\nscalability, we extend our study to large synthetic datasets ranging from 1M to\n70M samples, demonstrating that CVQBoost on Dirac-3 is well-suited for\nlarge-scale classification tasks. These findings position CVQBoost as a\npromising alternative to gradient boosting methods, offering superior\nscalability and efficiency for high-dimensional ML applications such as fraud\ndetection.\n","authors":["Babak Emami","Wesley Dyk","David Haycraft","Carrie Spear","Lac Nguyen","Nicholas Chancellor"],"pdf_url":"https://arxiv.org/pdf/2503.11273v1.pdf","comment":"15 pages including references and appendix, 6 figures"},{"id":"http://arxiv.org/abs/2503.11272v1","updated":"2025-03-14T10:30:42Z","published":"2025-03-14T10:30:42Z","title":"When Do Transformers Outperform Feedforward and Recurrent Networks? A\n  Statistical Perspective","summary":"  Theoretical efforts to prove advantages of Transformers in comparison with\nclassical architectures such as feedforward and recurrent neural networks have\nmostly focused on representational power. In this work, we take an alternative\nperspective and prove that even with infinite compute, feedforward and\nrecurrent networks may suffer from larger sample complexity compared to\nTransformers, as the latter can adapt to a form of dynamic sparsity.\nSpecifically, we consider a sequence-to-sequence data generating model on\nsequences of length $N$, in which the output at each position depends only on\n$q$ relevant tokens with $q \\ll N$, and the positions of these tokens are\ndescribed in the input prompt. We prove that a single-layer Transformer can\nlearn this model if and only if its number of attention heads is at least $q$,\nin which case it achieves a sample complexity almost independent of $N$, while\nrecurrent networks require $N^{\\Omega(1)}$ samples on the same problem. If we\nsimplify this model, recurrent networks may achieve a complexity almost\nindependent of $N$, while feedforward networks still require $N$ samples.\nConsequently, our proposed sparse retrieval model illustrates a natural\nhierarchy in sample complexity across these architectures.\n","authors":["Alireza Mousavi-Hosseini","Clayton Sanford","Denny Wu","Murat A. Erdogdu"],"pdf_url":"https://arxiv.org/pdf/2503.11272v1.pdf","comment":"43 pages, 2 figures"},{"id":"http://arxiv.org/abs/2402.10686v3","updated":"2025-03-14T10:13:46Z","published":"2024-02-16T13:41:18Z","title":"On the Impact of Uncertainty and Calibration on Likelihood-Ratio\n  Membership Inference Attacks","summary":"  In a membership inference attack (MIA), an attacker exploits the\noverconfidence exhibited by typical machine learning models to determine\nwhether a specific data point was used to train a target model. In this paper,\nwe analyze the performance of the likelihood ratio attack (LiRA) within an\ninformation-theoretical framework that allows the investigation of the impact\nof the aleatoric uncertainty in the true data generation process, of the\nepistemic uncertainty caused by a limited training data set, and of the\ncalibration level of the target model. We compare three different settings, in\nwhich the attacker receives decreasingly informative feedback from the target\nmodel: confidence vector (CV) disclosure, in which the output probability\nvector is released; true label confidence (TLC) disclosure, in which only the\nprobability assigned to the true label is made available by the model; and\ndecision set (DS) disclosure, in which an adaptive prediction set is produced\nas in conformal prediction. We derive bounds on the advantage of an MIA\nadversary with the aim of offering insights into the impact of uncertainty and\ncalibration on the effectiveness of MIAs. Simulation results demonstrate that\nthe derived analytical bounds predict well the effectiveness of MIAs.\n","authors":["Meiyi Zhu","Caili Guo","Chunyan Feng","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2402.10686v3.pdf","comment":"16 pages, 23 figures"},{"id":"http://arxiv.org/abs/2412.02097v3","updated":"2025-03-14T10:13:20Z","published":"2024-12-03T02:38:07Z","title":"Beyond Tree Models: A Hybrid Model of KAN and gMLP for Large-Scale\n  Financial Tabular Data","summary":"  Tabular data plays a critical role in real-world financial scenarios.\nTraditionally, tree models have dominated in handling tabular data. However,\nfinancial datasets in the industry often encounter some challenges, such as\ndata heterogeneity, the predominance of numerical features and the large scale\nof the data, which can range from tens of millions to hundreds of millions of\nrecords. These challenges can lead to significant memory and computational\nissues when using tree-based models. Consequently, there is a growing need for\nneural network-based solutions that can outperform these models. In this paper,\nwe introduce TKGMLP, an hybrid network for tabular data that combines shallow\nKolmogorov Arnold Networks with Gated Multilayer Perceptron. This model\nleverages the strengths of both architectures to improve performance and\nscalability. We validate TKGMLP on a real-world credit scoring dataset, where\nit achieves state-of-the-art results and outperforms current benchmarks.\nFurthermore, our findings demonstrate that the model continues to improve as\nthe dataset size increases, making it highly scalable. Additionally, we propose\na novel feature encoding method for numerical data, specifically designed to\naddress the predominance of numerical features in financial datasets. The\nintegration of this feature encoding method within TKGMLP significantly\nimproves prediction accuracy. This research not only advances table prediction\ntechnology but also offers a practical and effective solution for handling\nlarge-scale numerical tabular data in various industrial applications.\n","authors":["Mingming Zhang","Jiahao Hu","Pengfei Shi","Ningtao Wang","Ruizhe Gao","Guandong Sun","Feng Zhao","Yulin kang","Xing Fu","Weiqiang Wang","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.02097v3.pdf","comment":"the paper has mistakes in section3.1"},{"id":"http://arxiv.org/abs/2503.11255v1","updated":"2025-03-14T10:06:52Z","published":"2025-03-14T10:06:52Z","title":"Federated Koopman-Reservoir Learning for Large-Scale Multivariate\n  Time-Series Anomaly Detection","summary":"  The proliferation of edge devices has dramatically increased the generation\nof multivariate time-series (MVTS) data, essential for applications from\nhealthcare to smart cities. Such data streams, however, are vulnerable to\nanomalies that signal crucial problems like system failures or security\nincidents. Traditional MVTS anomaly detection methods, encompassing statistical\nand centralized machine learning approaches, struggle with the heterogeneity,\nvariability, and privacy concerns of large-scale, distributed environments. In\nresponse, we introduce FedKO, a novel unsupervised Federated Learning framework\nthat leverages the linear predictive capabilities of Koopman operator theory\nalong with the dynamic adaptability of Reservoir Computing. This enables\neffective spatiotemporal processing and privacy preservation for MVTS data.\nFedKO is formulated as a bi-level optimization problem, utilizing a specific\nfederated algorithm to explore a shared Reservoir-Koopman model across diverse\ndatasets. Such a model is then deployable on edge devices for efficient\ndetection of anomalies in local MVTS streams. Experimental results across\nvarious datasets showcase FedKO's superior performance against state-of-the-art\nmethods in MVTS anomaly detection. Moreover, FedKO reduces up to 8x\ncommunication size and 2x memory usage, making it highly suitable for\nlarge-scale systems.\n","authors":["Long Tan Le","Tung-Anh Nguyen","Han Shu","Suranga Seneviratne","Choong Seon Hong","Nguyen H. Tran"],"pdf_url":"https://arxiv.org/pdf/2503.11255v1.pdf","comment":"Accepted at SDM 2025"},{"id":"http://arxiv.org/abs/2410.16032v4","updated":"2025-03-14T10:04:53Z","published":"2024-10-21T14:06:53Z","title":"TimeMixer++: A General Time Series Pattern Machine for Universal\n  Predictive Analysis","summary":"  Time series analysis plays a critical role in numerous applications,\nsupporting tasks such as forecasting, classification, anomaly detection, and\nimputation. In this work, we present the time series pattern machine (TSPM), a\nmodel designed to excel in a broad range of time series tasks through powerful\nrepresentation and pattern extraction capabilities. Traditional time series\nmodels often struggle to capture universal patterns, limiting their\neffectiveness across diverse tasks. To address this, we define multiple scales\nin the time domain and various resolutions in the frequency domain, employing\nvarious mixing strategies to extract intricate, task-adaptive time series\npatterns. Specifically, we introduce a general-purpose TSPM that processes\nmulti-scale time series using (1) multi-resolution time imaging (MRTI), (2)\ntime image decomposition (TID), (3) multi-scale mixing (MCM), and (4)\nmulti-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI\ntransforms multi-scale time series into multi-resolution time images, capturing\npatterns across both temporal and frequency domains. TID leverages dual-axis\nattention to extract seasonal and trend patterns, while MCM hierarchically\naggregates these patterns across scales. MRM adaptively integrates all\nrepresentations across resolutions. This method achieves state-of-the-art\nperformance across 8 time series analytical tasks, consistently surpassing both\ngeneral-purpose and task-specific models. Our work marks a promising step\ntoward the next generation of TSPMs, paving the way for further advancements in\ntime series analysis.\n","authors":["Shiyu Wang","Jiawei Li","Xiaoming Shi","Zhou Ye","Baichuan Mo","Wenze Lin","Shengtong Ju","Zhixuan Chu","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2410.16032v4.pdf","comment":"Accepted by the 13th International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2503.11250v1","updated":"2025-03-14T10:00:24Z","published":"2025-03-14T10:00:24Z","title":"CRPS-Based Targeted Sequential Design with Application in Chemical Space","summary":"  Sequential design of real and computer experiments via Gaussian Process (GP)\nmodels has proven useful for parsimonious, goal-oriented data acquisition\npurposes. In this work, we focus on acquisition strategies for a GP model that\nneeds to be accurate within a predefined range of the response of interest.\nSuch an approach is useful in various fields including synthetic chemistry,\nwhere finding molecules with particular properties is essential for developing\nuseful materials and effective medications. GP modeling and sequential design\nof experiments have been successfully applied to a plethora of domains,\nincluding molecule research. Our main contribution here is to use the\nthreshold-weighted Continuous Ranked Probability Score (CRPS) as a basic\nbuilding block for acquisition functions employed within sequential design. We\nstudy pointwise and integral criteria relying on two different weighting\nmeasures and benchmark them against competitors, demonstrating improved\nperformance with respect to considered goals. The resulting acquisition\nstrategies are applicable to a wide range of fields and pave the way to further\ndeveloping sequential design relying on scoring rules.\n","authors":["Lea Friedli","Athénaïs Gautier","Anna Broccard","David Ginsbourger"],"pdf_url":"https://arxiv.org/pdf/2503.11250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11249v1","updated":"2025-03-14T10:00:13Z","published":"2025-03-14T10:00:13Z","title":"Spherical Tree-Sliced Wasserstein Distance","summary":"  Sliced Optimal Transport (OT) simplifies the OT problem in high-dimensional\nspaces by projecting supports of input measures onto one-dimensional lines and\nthen exploiting the closed-form expression of the univariate OT to reduce the\ncomputational burden of OT. Recently, the Tree-Sliced method has been\nintroduced to replace these lines with more intricate structures, known as tree\nsystems. This approach enhances the ability to capture topological information\nof integration domains in Sliced OT while maintaining low computational cost.\nInspired by this approach, in this paper, we present an adaptation of tree\nsystems on OT problems for measures supported on a sphere. As a counterpart to\nthe Radon transform variant on tree systems, we propose a novel spherical Radon\ntransform with a new integration domain called spherical trees. By leveraging\nthis transform and exploiting the spherical tree structures, we derive\nclosed-form expressions for OT problems on the sphere. Consequently, we obtain\nan efficient metric for measures on the sphere, named Spherical Tree-Sliced\nWasserstein (STSW) distance. We provide an extensive theoretical analysis to\ndemonstrate the topology of spherical trees and the well-definedness and\ninjectivity of our Radon transform variant, which leads to an orthogonally\ninvariant distance between spherical measures. Finally, we conduct a wide range\nof numerical experiments, including gradient flows and self-supervised\nlearning, to assess the performance of our proposed metric, comparing it to\nrecent benchmarks.\n","authors":["Hoang V. Tran","Thanh T. Chu","Khoi N. M. Nguyen","Trang Pham","Tam Le","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.11249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11248v1","updated":"2025-03-14T10:00:03Z","published":"2025-03-14T10:00:03Z","title":"Reasoning-Grounded Natural Language Explanations for Language Models","summary":"  We propose a large language model explainability technique for obtaining\nfaithful natural language explanations by grounding the explanations in a\nreasoning process. When converted to a sequence of tokens, the outputs of the\nreasoning process can become part of the model context and later be decoded to\nnatural language as the model produces either the final answer or the\nexplanation. To improve the faithfulness of the explanations, we propose to use\na joint predict-explain approach, in which the answers and explanations are\ninferred directly from the reasoning sequence, without the explanations being\ndependent on the answers and vice versa. We demonstrate the plausibility of the\nproposed technique by achieving a high alignment between answers and\nexplanations in several problem domains, observing that language models often\nsimply copy the partial decisions from the reasoning sequence into the final\nanswers or explanations. Furthermore, we show that the proposed use of\nreasoning can also improve the quality of the answers.\n","authors":["Vojtech Cahlik","Rodrigo Alves","Pavel Kordik"],"pdf_url":"https://arxiv.org/pdf/2503.11248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11246v1","updated":"2025-03-14T09:54:36Z","published":"2025-03-14T09:54:36Z","title":"Cost-effective Deep Learning Infrastructure with NVIDIA GPU","summary":"  The growing demand for computational power is driven by advancements in deep\nlearning, the increasing need for big data processing, and the requirements of\nscientific simulations for academic and research purposes. Developing countries\nlike Nepal often struggle with the resources needed to invest in new and better\nhardware for these purposes. However, optimizing and building on existing\ntechnology can still meet these computing demands effectively. To address these\nneeds, we built a cluster using four NVIDIA GeForce GTX 1650 GPUs. The cluster\nconsists of four nodes: one master node that controls and manages the entire\ncluster, and three compute nodes dedicated to processing tasks. The master node\nis equipped with all necessary software for package management, resource\nscheduling, and deployment, such as Anaconda and Slurm. In addition, a Network\nFile Storage (NFS) system was integrated to provide the additional storage\nrequired by the cluster. Given that the cluster is accessible via ssh by a\npublic domain address, which poses significant cybersecurity risks, we\nimplemented fail2ban to mitigate brute force attacks and enhance security.\nDespite the continuous challenges encountered during the design and\nimplementation process, this project demonstrates how powerful computational\nclusters can be built to handle resource-intensive tasks in various demanding\nfields.\n","authors":["Aatiz Ghimire","Shahnawaz Alam","Siman Giri","Madhav Prasad Ghimire"],"pdf_url":"https://arxiv.org/pdf/2503.11246v1.pdf","comment":"10 Pages,6 Figures, this paper was presented in National Data and\n  Computing Conference 2024 and will be published into KUSET Journal by\n  Kathmandu University"},{"id":"http://arxiv.org/abs/2503.11244v1","updated":"2025-03-14T09:52:30Z","published":"2025-03-14T09:52:30Z","title":"LLMPerf: GPU Performance Modeling meets Large Language Models","summary":"  Performance modeling, a pivotal domain in program cost analysis, currently\nrelies on manually crafted models constrained by various program and hardware\nlimitations, especially in the intricate landscape of GPGPU. Meanwhile, Large\nLanguage Models (LLMs) have demonstrated their effectiveness in addressing\ndiverse programming challenges. Our work establishes a connection between LLMs\nand performance modeling, employing the LLM as a performance estimator. Through\nexperimental exploration with carefully designed large-scale OpenCL datasets,\nwe highlight the potential capability as well as the main difficulties of using\nLLMs in handling performance modeling tasks for OpenCL device source programs.\nAs the first study for this line of work, our LLM-based performance model\nachieves a mean absolute percentage error of $24.25\\%$ for a large-scale\ngenerated validation set. On a set of publicly available OpenCL programs, our\nmodel achieves a mean absolute percentage error of $46.1\\%$.\n","authors":["Khoi N. M. Nguyen","Hoang Duy Nguyen Do","Huyen Thao Le","Thanh Tuan Dao"],"pdf_url":"https://arxiv.org/pdf/2503.11244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05609v2","updated":"2025-03-14T09:51:44Z","published":"2024-11-08T14:52:42Z","title":"A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis","summary":"  The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and an off-the-shelf\nLarge Language Model (LLM) to generate disease diagnoses based on the predicted\nconcepts. Furthermore, our approach supports test-time human intervention,\nenabling corrections to predicted concepts, which improves final diagnoses and\nenhances transparency in decision-making. We validate our approach on three\nskin lesion datasets, demonstrating that it outperforms traditional CBMs and\nstate-of-the-art explainable methods, all without requiring any training and\nutilizing only a few annotated examples. The code is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.\n","authors":["Cristiano Patrício","Luís F. Teixeira","João C. Neves"],"pdf_url":"https://arxiv.org/pdf/2411.05609v2.pdf","comment":"Published in the Computational and Structural Biotechnology Journal"},{"id":"http://arxiv.org/abs/2409.06316v2","updated":"2025-03-14T09:51:43Z","published":"2024-09-10T08:17:06Z","title":"PharmacoMatch: Efficient 3D Pharmacophore Screening via Neural Subgraph\n  Matching","summary":"  The increasing size of screening libraries poses a significant challenge for\nthe development of virtual screening methods for drug discovery, necessitating\na re-evaluation of traditional approaches in the era of big data. Although 3D\npharmacophore screening remains a prevalent technique, its application to very\nlarge datasets is limited by the computational cost associated with matching\nquery pharmacophores to database molecules. In this study, we introduce\nPharmacoMatch, a novel contrastive learning approach based on neural subgraph\nmatching. Our method reinterprets pharmacophore screening as an approximate\nsubgraph matching problem and enables efficient querying of conformational\ndatabases by encoding query-target relationships in the embedding space. We\nconduct comprehensive investigations of the learned representations and\nevaluate PharmacoMatch as pre-screening tool in a zero-shot setting. We\ndemonstrate significantly shorter runtimes and comparable performance metrics\nto existing solutions, providing a promising speed-up for screening very large\ndatasets.\n","authors":["Daniel Rose","Oliver Wieder","Thomas Seidel","Thierry Langer"],"pdf_url":"https://arxiv.org/pdf/2409.06316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11841v3","updated":"2025-03-14T09:45:53Z","published":"2023-11-20T15:17:20Z","title":"High Probability Guarantees for Random Reshuffling","summary":"  We consider the stochastic gradient method with random reshuffling\n($\\mathsf{RR}$) for tackling smooth nonconvex optimization problems.\n$\\mathsf{RR}$ finds broad applications in practice, notably in training neural\nnetworks. In this work, we provide high probability first-order and\nsecond-order complexity guarantees for this method. First, we establish a high\nprobability first-order sample complexity result for driving the Euclidean norm\nof the gradient (without taking expectation) below $\\varepsilon$. Our derived\ncomplexity matches the best existing in-expectation one up to a logarithmic\nterm while imposing no additional assumptions nor changing $\\mathsf{RR}$'s\nupdating rule. We then propose a simple and computable stopping criterion for\n$\\mathsf{RR}$ (denoted as $\\mathsf{RR}$-$\\mathsf{sc}$). This criterion is\nguaranteed to be triggered after a finite number of iterations, enabling us to\nprove a high probability first-order complexity guarantee for the last iterate.\nSecond, building on the proposed stopping criterion, we design a perturbed\nrandom reshuffling method ($\\mathsf{p}$-$\\mathsf{RR}$) that involves an\nadditional randomized perturbation procedure near stationary points. We derive\nthat $\\mathsf{p}$-$\\mathsf{RR}$ provably escapes strict saddle points and\nestablish a high probability second-order complexity result, without requiring\nany sub-Gaussian tail-type assumptions on the stochastic gradient errors. The\nfundamental ingredient in deriving the aforementioned results is the new\nconcentration property for sampling without replacement in $\\mathsf{RR}$, which\ncould be of independent interest. Finally, we conduct numerical experiments on\nneural network training to support our theoretical findings.\n","authors":["Hengxu Yu","Xiao Li"],"pdf_url":"https://arxiv.org/pdf/2311.11841v3.pdf","comment":"Made some organizational changes to make the underlying idea clearer"},{"id":"http://arxiv.org/abs/2503.11240v1","updated":"2025-03-14T09:45:19Z","published":"2025-03-14T09:45:19Z","title":"Towards Better Alignment: Training Diffusion Models with Reinforcement\n  Learning Against Sparse Rewards","summary":"  Diffusion models have achieved remarkable success in text-to-image\ngeneration. However, their practical applications are hindered by the\nmisalignment between generated images and corresponding text prompts. To tackle\nthis issue, reinforcement learning (RL) has been considered for diffusion model\nfine-tuning. Yet, RL's effectiveness is limited by the challenge of sparse\nreward, where feedback is only available at the end of the generation process.\nThis makes it difficult to identify which actions during the denoising process\ncontribute positively to the final generated image, potentially leading to\nineffective or unnecessary denoising policies. To this end, this paper presents\na novel RL-based framework that addresses the sparse reward problem when\ntraining diffusion models. Our framework, named $\\text{B}^2\\text{-DiffuRL}$,\nemploys two strategies: \\textbf{B}ackward progressive training and\n\\textbf{B}ranch-based sampling. For one thing, backward progressive training\nfocuses initially on the final timesteps of denoising process and gradually\nextends the training interval to earlier timesteps, easing the learning\ndifficulty from sparse rewards. For another, we perform branch-based sampling\nfor each training interval. By comparing the samples within the same branch, we\ncan identify how much the policies of the current training interval contribute\nto the final image, which helps to learn effective policies instead of\nunnecessary ones. $\\text{B}^2\\text{-DiffuRL}$ is compatible with existing\noptimization algorithms. Extensive experiments demonstrate the effectiveness of\n$\\text{B}^2\\text{-DiffuRL}$ in improving prompt-image alignment and maintaining\ndiversity in generated images. The code for this work is available.\n","authors":["Zijing Hu","Fengda Zhang","Long Chen","Kun Kuang","Jiahui Li","Kaifeng Gao","Jun Xiao","Xin Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.11240v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.03649v2","updated":"2025-03-14T09:36:47Z","published":"2025-03-05T16:25:58Z","title":"Limits of nonlinear and dispersive fiber propagation for photonic\n  extreme learning","summary":"  We report a generalized nonlinear Schr\\\"odinger equation simulation model of\nan extreme learning machine (ELM) based on optical fiber propagation. Using\nhandwritten digit classification as a benchmark, we study how accuracy depends\non propagation dynamics, as well as parameters governing spectral encoding,\nreadout, and noise. Test accuracies of over 91% and 93% are found for\npropagation in the anomalous and normal dispersion regimes respectively. Our\nsimulation results also suggest that quantum noise on the input pulses\nintroduces an intrinsic penalty to ELM performance.\n","authors":["Andrei V. Ermolaev","Mathilde Hary","Lev Leybov","Piotr Ryczkowski","Anas Skalli","Daniel Brunner","Goëry Genty","John M. Dudley"],"pdf_url":"https://arxiv.org/pdf/2503.03649v2.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.11233v1","updated":"2025-03-14T09:31:03Z","published":"2025-03-14T09:31:03Z","title":"Addressing Information Loss and Interaction Collapse: A Dual Enhanced\n  Attention Framework for Feature Interaction","summary":"  The Transformer has proven to be a significant approach in feature\ninteraction for CTR prediction, achieving considerable success in previous\nworks. However, it also presents potential challenges in handling feature\ninteractions. Firstly, Transformers may encounter information loss when\ncapturing feature interactions. By relying on inner products to represent\npairwise relationships, they compress raw interaction information, which can\nresult in a degradation of fidelity. Secondly, due to the long-tail features\ndistribution, feature fields with low information-abundance embeddings\nconstrain the information abundance of other fields, leading to collapsed\nembedding matrices. To tackle these issues, we propose a Dual Attention\nFramework for Enhanced Feature Interaction, known as Dual Enhanced Attention.\nThis framework integrates two attention mechanisms: the Combo-ID attention\nmechanism and the collapse-avoiding attention mechanism. The Combo-ID attention\nmechanism directly retains feature interaction pairs to mitigate information\nloss, while the collapse-avoiding attention mechanism adaptively filters out\nlow information-abundance interaction pairs to prevent interaction collapse.\nExtensive experiments conducted on industrial datasets have shown the\neffectiveness of Dual Enhanced Attention.\n","authors":["Yi Xu","Zhiyuan Lu","Xiaochen Li","Jinxin Hu","Hong Wen","Zulong Chen","Yu Zhang","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11232v1","updated":"2025-03-14T09:31:01Z","published":"2025-03-14T09:31:01Z","title":"PrivacyScalpel: Enhancing LLM Privacy via Interpretable Feature\n  Intervention with Sparse Autoencoders","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing but also pose significant privacy risks by\nmemorizing and leaking Personally Identifiable Information (PII). Existing\nmitigation strategies, such as differential privacy and neuron-level\ninterventions, often degrade model utility or fail to effectively prevent\nleakage. To address this challenge, we introduce PrivacyScalpel, a novel\nprivacy-preserving framework that leverages LLM interpretability techniques to\nidentify and mitigate PII leakage while maintaining performance. PrivacyScalpel\ncomprises three key steps: (1) Feature Probing, which identifies layers in the\nmodel that encode PII-rich representations, (2) Sparse Autoencoding, where a\nk-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive\nfeatures,\n  and (3) Feature-Level Interventions, which employ targeted ablation and\nvector steering to suppress PII leakage.\n  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron\ndataset, shows that PrivacyScalpel significantly reduces email leakage from\n5.15\\% to as low as 0.0\\%, while maintaining over 99.4\\% of the original\nmodel's utility. Notably, our method outperforms neuron-level interventions in\nprivacy-utility trade-offs, demonstrating that acting on sparse, monosemantic\nfeatures is more effective than manipulating polysemantic neurons. Beyond\nimproving LLM privacy, our approach offers insights into the mechanisms\nunderlying PII memorization, contributing to the broader field of model\ninterpretability and secure AI deployment.\n","authors":["Ahmed Frikha","Muhammad Reza Ar Razi","Krishna Kanth Nakka","Ricardo Mendes","Xue Jiang","Xuebing Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.11232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08215v2","updated":"2025-03-14T09:24:22Z","published":"2024-03-13T03:24:36Z","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual\n  Semantic Segmentation for Autonomous Driving","summary":"  Despite the impressive performance achieved by data-fusion networks with\nduplex encoders for visual semantic segmentation, they become ineffective when\nspatial geometric data are not available. Implicitly infusing the spatial\ngeometric prior knowledge acquired by a data-fusion teacher network into a\nsingle-modal student network is a practical, albeit less explored research\navenue. This article delves into this topic and resorts to knowledge\ndistillation approaches to address this problem. We introduce the Learning to\nInfuse ''X'' (LIX) framework, with novel contributions in both logit\ndistillation and feature distillation aspects. We present a mathematical proof\nthat underscores the limitation of using a single, fixed weight in decoupled\nknowledge distillation and introduce a logit-wise dynamic weight controller as\na solution to this issue. Furthermore, we develop an adaptively-recalibrated\nfeature distillation algorithm, including two novel techniques: feature\nrecalibration via kernel regression and in-depth feature consistency\nquantification via centered kernel alignment. Extensive experiments conducted\nwith intermediate-fusion and late-fusion networks across various public\ndatasets provide both quantitative and qualitative evaluations, demonstrating\nthe superior performance of our LIX framework when compared to other\nstate-of-the-art approaches.\n","authors":["Sicen Guo","Ziwei Long","Zhiyuan Wu","Qijun Chen","Ioannis Pitas","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2403.08215v2.pdf","comment":"13 pages, 7 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.17721v2","updated":"2025-03-14T09:21:43Z","published":"2024-07-25T02:48:22Z","title":"A Two-Stage Imaging Framework Combining CNN and Physics-Informed Neural\n  Networks for Full-Inverse Tomography: A Case Study in Electrical Impedance\n  Tomography (EIT)","summary":"  Electrical Impedance Tomography (EIT) is a highly ill-posed inverse problem,\nwith the challenge of reconstructing internal conductivities using only\nboundary voltage measurements. Although Physics-Informed Neural Networks\n(PINNs) have shown potential in solving inverse problems, existing approaches\nare limited in their applicability to EIT, as they often rely on impractical\nprior knowledge and assumptions that cannot be satisfied in real-world\nscenarios. To address these limitations, we propose a two-stage hybrid learning\nframework that combines Convolutional Neural Networks (CNNs) and PINNs. This\nframework integrates data-driven and model-driven paradigms, blending\nsupervised and unsupervised learning to reconstruct conductivity distributions\nwhile ensuring adherence to the underlying physical laws, thereby overcoming\nthe constraints of existing methods.\n","authors":["Xuanxuan Yang","Yangming Zhang","Haofeng Chen","Gang Ma","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2407.17721v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11224v1","updated":"2025-03-14T09:20:31Z","published":"2025-03-14T09:20:31Z","title":"Technologies on Effectiveness and Efficiency: A Survey of State Spaces\n  Models","summary":"  State Space Models (SSMs) have emerged as a promising alternative to the\npopular transformer-based models and have been increasingly gaining attention.\nCompared to transformers, SSMs excel at tasks with sequential data or longer\ncontexts, demonstrating comparable performances with significant efficiency\ngains. In this survey, we provide a coherent and systematic overview for SSMs,\nincluding their theoretical motivations, mathematical formulations, comparison\nwith existing model classes, and various applications. We divide the SSM series\ninto three main sections, providing a detailed introduction to the original\nSSM, the structured SSM represented by S4, and the selective SSM typified by\nMamba. We put an emphasis on technicality, and highlight the various key\ntechniques introduced to address the effectiveness and efficiency of SSMs. We\nhope this manuscript serves as an introduction for researchers to explore the\ntheoretical foundations of SSMs.\n","authors":["Xingtai Lv","Youbang Sun","Kaiyan Zhang","Shang Qu","Xuekai Zhu","Yuchen Fan","Yi Wu","Ermo Hua","Xinwei Long","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.11224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05334v2","updated":"2025-03-14T09:11:40Z","published":"2024-12-05T21:00:21Z","title":"Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models","summary":"  Traffic simulation aims to learn a policy for traffic agents that, when\nunrolled in closed-loop, faithfully recovers the joint distribution of\ntrajectories observed in the real world. Inspired by large language models,\ntokenized multi-agent policies have recently become the state-of-the-art in\ntraffic simulation. However, they are typically trained through open-loop\nbehavior cloning, and thus suffer from covariate shift when executed in\nclosed-loop during simulation. In this work, we present Closest Among Top-K\n(CAT-K) rollouts, a simple yet effective closed-loop fine-tuning strategy to\nmitigate covariate shift. CAT-K fine-tuning only requires existing trajectory\ndata, without reinforcement learning or generative adversarial imitation.\nConcretely, CAT-K fine-tuning enables a small 7M-parameter tokenized traffic\nsimulation policy to outperform a 102M-parameter model from the same model\nfamily, achieving the top spot on the Waymo Sim Agent Challenge leaderboard at\nthe time of submission. The code is available at\nhttps://github.com/NVlabs/catk.\n","authors":["Zhejun Zhang","Peter Karkus","Maximilian Igl","Wenhao Ding","Yuxiao Chen","Boris Ivanovic","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2412.05334v2.pdf","comment":"CVPR 2025. Project Page: https://zhejz.github.io/catk/"},{"id":"http://arxiv.org/abs/2503.11217v1","updated":"2025-03-14T09:09:21Z","published":"2025-03-14T09:09:21Z","title":"Optimal Transport and Adaptive Thresholding for Universal Domain\n  Adaptation on Time Series","summary":"  Universal Domain Adaptation (UniDA) aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain, even when their classes are not\nfully shared. Few dedicated UniDA methods exist for Time Series (TS), which\nremains a challenging case. In general, UniDA approaches align common class\nsamples and detect unknown target samples from emerging classes. Such detection\noften results from thresholding a discriminability metric. The threshold value\nis typically either a fine-tuned hyperparameter or a fixed value, which limits\nthe ability of the model to adapt to new data. Furthermore, discriminability\nmetrics exhibit overconfidence for unknown samples, leading to\nmisclassifications. This paper introduces UniJDOT, an optimal-transport-based\nmethod that accounts for the unknown target samples in the transport cost. Our\nmethod also proposes a joint decision space to improve the discriminability of\nthe detection module. In addition, we use an auto-thresholding algorithm to\nreduce the dependence on fixed or fine-tuned thresholds. Finally, we rely on a\nFourier transform-based layer inspired by the Fourier Neural Operator for\nbetter TS representation. Experiments on TS benchmarks demonstrate the\ndiscriminability, robustness, and state-of-the-art performance of UniJDOT.\n","authors":["Romain Mussard","Fannia Pacheco","Maxime Berar","Gilles Gasso","Paul Honeine"],"pdf_url":"https://arxiv.org/pdf/2503.11217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11215v1","updated":"2025-03-14T09:07:18Z","published":"2025-03-14T09:07:18Z","title":"Spatio-Temporal Graph Structure Learning for Earthquake Detection","summary":"  Earthquake detection is essential for earthquake early warning (EEW) systems.\nTraditional methods struggle with low signal-to-noise ratios and single-station\nreliance, limiting their effectiveness. We propose a Spatio-Temporal Graph\nConvolutional Network (GCN) using Spectral Structure Learning Convolution\n(Spectral SLC) to model static and dynamic relationships across seismic\nstations. Our approach processes multi-station waveform data and generates\nstation-specific detection probabilities. Experiments show superior performance\nover a conventional GCN baseline in terms of true positive rate (TPR) and false\npositive rate (FPR), highlighting its potential for robust multi-station\nearthquake detection. The code repository for this study is available at\nhttps://github.com/SuchanunP/eq_detector.\n","authors":["Suchanun Piriyasatit","Ercan Engin Kuruoglu","Mehmet Sinan Ozeren"],"pdf_url":"https://arxiv.org/pdf/2503.11215v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2503.11209v1","updated":"2025-03-14T08:56:30Z","published":"2025-03-14T08:56:30Z","title":"Clustering Items through Bandit Feedback: Finding the Right Feature out\n  of Many","summary":"  We study the problem of clustering a set of items based on bandit feedback.\nEach of the $n$ items is characterized by a feature vector, with a possibly\nlarge dimension $d$. The items are partitioned into two unknown groups such\nthat items within the same group share the same feature vector. We consider a\nsequential and adaptive setting in which, at each round, the learner selects\none item and one feature, then observes a noisy evaluation of the item's\nfeature. The learner's objective is to recover the correct partition of the\nitems, while keeping the number of observations as small as possible. We\nprovide an algorithm which relies on finding a relevant feature for the\nclustering task, leveraging the Sequential Halving algorithm. With probability\nat least $1-\\delta$, we obtain an accurate recovery of the partition and derive\nan upper bound on the budget required. Furthermore, we derive an\ninstance-dependent lower bound, which is tight in some relevant cases.\n","authors":["Maximilian Graf","Victor Thuot","Nicolas Verzelen"],"pdf_url":"https://arxiv.org/pdf/2503.11209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11207v1","updated":"2025-03-14T08:52:25Z","published":"2025-03-14T08:52:25Z","title":"Can Large Reasoning Models do Analogical Reasoning under Perceptual\n  Uncertainty?","summary":"  This work presents a first evaluation of two state-of-the-art Large Reasoning\nModels (LRMs), OpenAI's o3-mini and DeepSeek R1, on analogical reasoning,\nfocusing on well-established nonverbal human IQ tests based on Raven's\nprogressive matrices. We benchmark with the I-RAVEN dataset and its more\ndifficult extension, I-RAVEN-X, which tests the ability to generalize to longer\nreasoning rules and ranges of the attribute values. To assess the influence of\nvisual uncertainties on these nonverbal analogical reasoning tests, we extend\nthe I-RAVEN-X dataset, which otherwise assumes an oracle perception. We adopt a\ntwo-fold strategy to simulate this imperfect visual perception: 1) we introduce\nconfounding attributes which, being sampled at random, do not contribute to the\nprediction of the correct answer of the puzzles and 2) smoothen the\ndistributions of the input attributes' values. We observe a sharp decline in\nOpenAI's o3-mini task accuracy, dropping from 86.6% on the original I-RAVEN to\njust 17.0% -- approaching random chance -- on the more challenging I-RAVEN-X,\nwhich increases input length and range and emulates perceptual uncertainty.\nThis drop occurred despite spending 3.4x more reasoning tokens. A similar trend\nis also observed for DeepSeek R1: from 80.6% to 23.2%. On the other hand, a\nneuro-symbolic probabilistic abductive model, ARLC, that achieves\nstate-of-the-art performances on I-RAVEN, can robustly reason under all these\nout-of-distribution tests, maintaining strong accuracy with only a modest\nreduction from 98.6% to 88.0%. Our code is available at\nhttps://github.com/IBM/raven-large-language-models.\n","authors":["Giacomo Camposampiero","Michael Hersche","Roger Wattenhofer","Abu Sebastian","Abbas Rahimi"],"pdf_url":"https://arxiv.org/pdf/2503.11207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10220v2","updated":"2025-03-14T08:44:13Z","published":"2025-03-13T10:01:07Z","title":"Assessing the validity of new paradigmatic complexity measures as\n  criterial features for proficiency in L2 writings in English","summary":"  This article addresses Second Language (L2) writing development through an\ninvestigation of new grammatical and structural complexity metrics. We explore\nthe paradigmatic production in learner English by linking language functions to\nspecific grammatical paradigms. Using the EFCAMDAT as a gold standard and a\ncorpus of French learners as an external test set, we employ a supervised\nlearning framework to operationalise and evaluate seven microsystems. We show\nthat learner levels are associated with the seven microsystems (MS). Using\nordinal regression modelling for evaluation, the results show that all MS are\nsignificant but yield a low impact if taken individually. However, their\ninfluence is shown to be impactful if taken as a group. These microsystems and\ntheir measurement method suggest that it is possible to use them as part of\nbroader-purpose CALL systems focused on proficiency assessment.\n","authors":["Cyriel Mallart","Andrew Simpkin","Nicolas Ballier","Paula Lissón","Rémi Venant","Jen-Yu Li","Bernardo Stearns","Thomas Gaillat"],"pdf_url":"https://arxiv.org/pdf/2503.10220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11196v1","updated":"2025-03-14T08:43:36Z","published":"2025-03-14T08:43:36Z","title":"Physics-constrained DeepONet for Surrogate CFD models: a curved\n  backward-facing step case","summary":"  The Physics-Constrained DeepONet (PC-DeepONet), an architecture that\nincorporates fundamental physics knowledge into the data-driven DeepONet model,\nis presented in this study. This methodology is exemplified through surrogate\nmodeling of fluid dynamics over a curved backward-facing step, a benchmark\nproblem in computational fluid dynamics. The model was trained on computational\nfluid dynamics data generated for a range of parameterized geometries. The\nPC-DeepONet was able to learn the mapping from the parameters describing the\ngeometry to the velocity and pressure fields. While the DeepONet is solely\ndata-driven, the PC-DeepONet imposes the divergence constraint from the\ncontinuity equation onto the network. The PC-DeepONet demonstrates higher\naccuracy than the data-driven baseline, especially when trained on sparse data.\nBoth models attain convergence with a small dataset of 50 samples and require\nonly 50 iterations for convergence, highlighting the efficiency of neural\noperators in learning the dynamics governed by partial differential equations.\n","authors":["Anas Jnini","Harshinee Goordoyal","Sujal Dave","Flavio Vella","Katharine H. Fraser","Artem Korobenko"],"pdf_url":"https://arxiv.org/pdf/2503.11196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15767v2","updated":"2025-03-14T08:36:09Z","published":"2024-01-28T21:08:45Z","title":"LEACH-RLC: Enhancing IoT Data Transmission with Optimized Clustering and\n  Reinforcement Learning","summary":"  Wireless Sensor Networks (WSNs) play a pivotal role in enabling Internet of\nThings (IoT) devices with sensing and actuation capabilities. Operating in\nremote and resource-constrained environments, these IoT devices face challenges\nrelated to energy consumption, crucial for network longevity. Existing\nclustering protocols often suffer from high control overhead, inefficient\ncluster formation, and poor adaptability to dynamic network conditions, leading\nto suboptimal data transmission and reduced network lifetime. This paper\nintroduces Low-Energy Adaptive Clustering Hierarchy with Reinforcement\nLearning-based Controller (LEACH-RLC), a novel clustering protocol designed to\naddress these limitations by employing a Mixed Integer Linear Programming\n(MILP) approach for strategic selection of Cluster Heads (CHs) and\nnode-to-cluster assignments. Additionally, it integrates a Reinforcement\nLearning (RL) agent to minimize control overhead by learning optimal timings\nfor generating new clusters. LEACH-RLC aims to balance control overhead\nreduction without compromising overall network performance. Through extensive\nsimulations, this paper investigates the frequency and opportune moments for\ngenerating new clustering solutions. Results demonstrate the superior\nperformance of LEACH-RLC over state-of-the-art protocols, showcasing enhanced\nnetwork lifetime, reduced average energy consumption, and minimized control\noverhead. The proposed protocol contributes to advancing the efficiency and\nadaptability of WSNs, addressing critical challenges in IoT deployments.\n","authors":["F. Fernando Jurado-Lasso","J. F. Jurado","Xenofon Fafoutis"],"pdf_url":"https://arxiv.org/pdf/2401.15767v2.pdf","comment":"17 pages, 15 figures, 4 tables, journal"},{"id":"http://arxiv.org/abs/2412.06146v2","updated":"2025-03-14T08:10:18Z","published":"2024-12-09T01:59:40Z","title":"Homogeneous Dynamics Space for Heterogeneous Humans","summary":"  Analyses of human motion kinematics have achieved tremendous advances.\nHowever, the production mechanism, known as human dynamics, is still\nundercovered. In this paper, we aim to push data-driven human dynamics\nunderstanding forward. We identify a major obstacle to this as the\nheterogeneity of existing human motion understanding efforts. Specifically,\nheterogeneity exists in not only the diverse kinematics representations and\nhierarchical dynamics representations but also in the data from different\ndomains, namely biomechanics and reinforcement learning. With an in-depth\nanalysis of the existing heterogeneity, we propose to emphasize the beneath\nhomogeneity: all of them represent the homogeneous fact of human motion, though\nfrom different perspectives. Given this, we propose Homogeneous Dynamics Space\n(HDyS) as a fundamental space for human dynamics by aggregating heterogeneous\ndata and training a homogeneous latent space with inspiration from the\ninverse-forward dynamics procedure. Leveraging the heterogeneous\nrepresentations and datasets, HDyS achieves decent mapping between human\nkinematics and dynamics. We demonstrate the feasibility of HDyS with extensive\nexperiments and applications. The project page is\nhttps://foruck.github.io/HDyS.\n","authors":["Xinpeng Liu","Junxuan Liang","Chenshuo Zhang","Zixuan Cai","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2412.06146v2.pdf","comment":"Accepted by CVPR 2025. Cewu Lu and Yong-Lu Li are the corresponding\n  authors"},{"id":"http://arxiv.org/abs/2412.20796v2","updated":"2025-03-14T08:01:35Z","published":"2024-12-30T08:38:09Z","title":"FastCHGNet: Training one Universal Interatomic Potential to 1.5 Hours\n  with 32 GPUs","summary":"  Graph neural network universal interatomic potentials (GNN-UIPs) have\ndemonstrated remarkable generalization and transfer capabilities in material\ndiscovery and property prediction. These models can accelerate molecular\ndynamics (MD) simulation by several orders of magnitude while maintaining\n\\textit{ab initio} accuracy, making them a promising new paradigm in material\nsimulations. One notable example is Crystal Hamiltonian Graph Neural Network\n(CHGNet), pretrained on the energies, forces, stresses, and magnetic moments\nfrom the MPtrj dataset, representing a state-of-the-art GNN-UIP model for\ncharge-informed MD simulations. However, training the CHGNet model is\ntime-consuming(8.3 days on one A100 GPU) for three reasons: (i) requiring\nmulti-layer propagation to reach more distant atom information, (ii) requiring\nsecond-order derivatives calculation to finish weights updating and (iii) the\nimplementation of reference CHGNet does not fully leverage the computational\ncapabilities. This paper introduces FastCHGNet, an optimized CHGNet, with three\ncontributions: Firstly, we design innovative Force/Stress Readout modules to\ndecompose Force/Stress prediction. Secondly, we adopt massive optimizations\nsuch as kernel fusion, redundancy bypass, etc, to exploit GPU computation power\nsufficiently. Finally, we extend CHGNet to support multiple GPUs and propose a\nload-balancing technique to enhance GPU utilization. Numerical results show\nthat FastCHGNet reduces memory footprint by a factor of 3.59. The final\ntraining time of FastCHGNet can be decreased to \\textbf{1.53 hours} on 32 GPUs\nwithout sacrificing model accuracy.\n","authors":["Yuanchang Zhou","Siyu Hu","Chen Wang","Lin-Wang Wang","Guangming Tan","Weile Jia"],"pdf_url":"https://arxiv.org/pdf/2412.20796v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11160v1","updated":"2025-03-14T07:58:26Z","published":"2025-03-14T07:58:26Z","title":"Unifying Perplexing Behaviors in Modified BP Attributions through\n  Alignment Perspective","summary":"  Attributions aim to identify input pixels that are relevant to the\ndecision-making process. A popular approach involves using modified\nbackpropagation (BP) rules to reverse decisions, which improves\ninterpretability compared to the original gradients. However, these methods\nlack a solid theoretical foundation and exhibit perplexing behaviors, such as\nreduced sensitivity to parameter randomization, raising concerns about their\nreliability and highlighting the need for theoretical justification. In this\nwork, we present a unified theoretical framework for methods like GBP,\nRectGrad, LRP, and DTD, demonstrating that they achieve input alignment by\ncombining the weights of activated neurons. This alignment improves the\nvisualization quality and reduces sensitivity to weight randomization. Our\ncontributions include: (1) Providing a unified explanation for multiple\nbehaviors, rather than focusing on just one. (2) Accurately predicting novel\nbehaviors. (3) Offering insights into decision-making processes, including\nlayer-wise information changes and the relationship between attributions and\nmodel decisions.\n","authors":["Guanhua Zheng","Jitao Sang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2503.11160v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.01035v2","updated":"2025-03-14T07:53:09Z","published":"2023-10-02T09:24:54Z","title":"Learnable Cross-modal Knowledge Distillation for Multi-modal Learning\n  with Missing Modality","summary":"  The problem of missing modalities is both critical and non-trivial to be\nhandled in multi-modal models. It is common for multi-modal tasks that certain\nmodalities contribute more compared to other modalities, and if those important\nmodalities are missing, the model performance drops significantly. Such fact\nremains unexplored by current multi-modal approaches that recover the\nrepresentation from missing modalities by feature reconstruction or blind\nfeature aggregation from other modalities, instead of extracting useful\ninformation from the best performing modalities. In this paper, we propose a\nLearnable Cross-modal Knowledge Distillation (LCKD) model to adaptively\nidentify important modalities and distil knowledge from them to help other\nmodalities from the cross-modal perspective for solving the missing modality\nissue. Our approach introduces a teacher election procedure to select the most\n``qualified'' teachers based on their single modality performance on certain\ntasks. Then, cross-modal knowledge distillation is performed between teacher\nand student modalities for each task to push the model parameters to a point\nthat is beneficial for all tasks. Hence, even if the teacher modalities for\ncertain tasks are missing during testing, the available student modalities can\naccomplish the task well enough based on the learned knowledge from their\nautomatically elected teacher modalities. Experiments on the Brain Tumour\nSegmentation Dataset 2018 (BraTS2018) shows that LCKD outperforms other methods\nby a considerable margin, improving the state-of-the-art performance by 3.61%\nfor enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour in\nterms of segmentation Dice score.\n","authors":["Hu Wang","Congbo Ma","Jianpeng Zhang","Yuan Zhang","Jodie Avery","Louise Hull","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2310.01035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20348v2","updated":"2025-03-14T07:47:22Z","published":"2025-02-27T18:19:22Z","title":"Improving the Efficiency of a Deep Reinforcement Learning-Based Power\n  Management System for HPC Clusters Using Curriculum Learning","summary":"  High energy consumption remains a key challenge in high-performance computing\n(HPC) systems, which often feature hundreds or thousands of nodes drawing\nsubstantial power even in idle or standby modes. Although powering down unused\nnodes can improve energy efficiency, choosing the wrong time to do so can\ndegrade quality of service by delaying job execution. Machine learning, in\nparticular reinforcement learning (RL), has shown promise in determining\noptimal times to switch nodes on or off. In this study, we enhance the\nperformance of a deep reinforcement learning (DRL) agent for HPC power\nmanagement by integrating curriculum learning (CL), a training approach that\nintroduces tasks with gradually increasing difficulty. Using the Batsim-py\nsimulation framework, we compare the proposed CL-based agent to both a baseline\nDRL method (without CL) and the conventional fixed-time timeout strategy.\nExperimental results confirm that an easy-to-hard curriculum outperforms other\ntraining orders in terms of reducing wasted energy usage. The best agent\nachieves a 3.73% energy reduction over the baseline DRL method and a 4.66%\nimprovement compared to the best timeout configuration (shutdown every 15\nminutes of idle time). In addition, it reduces average job waiting time by\n9.24% and maintains a higher job-filling rate, indicating more effective\nresource utilization. Sensitivity tests across various switch-on durations,\npower levels, and cluster sizes further reveal the agent's adaptability to\nchanging system parameters without retraining. These findings demonstrate that\ncurriculum learning can significantly improve DRL-based power management in\nHPC, balancing energy savings, quality of service, and robustness to diverse\nconfigurations.\n","authors":["Thomas Budiarjo","Santana Yuda Pradata","Kadek Gemilang Santiyuda","Muhammad Alfian Amrizal","Reza Pulungan","Hiroyuki Takizawa"],"pdf_url":"https://arxiv.org/pdf/2502.20348v2.pdf","comment":"13 pages, 17 figures, accepted at Supercomputing Asia '25, published\n  by ACM"},{"id":"http://arxiv.org/abs/2503.11151v1","updated":"2025-03-14T07:40:37Z","published":"2025-03-14T07:40:37Z","title":"Enabling Weak Client Participation via On-device Knowledge Distillation\n  in Heterogenous Federated Learning","summary":"  Online Knowledge Distillation (KD) is recently highlighted to train large\nmodels in Federated Learning (FL) environments. Many existing studies adopt the\nlogit ensemble method to perform KD on the server side. However, they often\nassume that unlabeled data collected at the edge is centralized on the server.\nMoreover, the logit ensemble method personalizes local models, which can\ndegrade the quality of soft targets, especially when data is highly non-IID. To\naddress these critical limitations,we propose a novel on-device KD-based\nheterogeneous FL method. Our approach leverages a small auxiliary model to\nlearn from labeled local data. Subsequently, a subset of clients with strong\nsystem resources transfers knowledge to a large model through on-device KD\nusing their unlabeled data. Our extensive experiments demonstrate that our\non-device KD-based heterogeneous FL method effectively utilizes the system\nresources of all edge devices as well as the unlabeled data, resulting in\nhigher accuracy compared to SOTA KD-based FL methods.\n","authors":["Jihyun Lim","Junhyuk Jo","Tuo Zhang","Salman Avestimehr","Sunwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2503.11151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16870v2","updated":"2025-03-14T07:36:26Z","published":"2024-11-25T19:08:38Z","title":"RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks","summary":"  Incremental learning aims to adapt to new sets of categories over time with\nminimal computational overhead. Prior work often addresses this task by\ntraining efficient task-specific adaptors that modify frozen layer weights or\nfeatures to capture relevant information without affecting predictions on\npreviously learned categories. While these adaptors are generally more\nefficient than finetuning the entire network, they still require tens to\nhundreds of thousands of task-specific trainable parameters even for relatively\nsmall networks, making it challenging to operate on resource-constrained\nenvironments with high communication costs like edge devices or mobile phones.\nThus, we propose Reparameterized, Compact weight Adaptation for Sequential\nTasks (RECAST), a novel method that dramatically reduces task-specific\ntrainable parameters to fewer than 50 - several orders of magnitude less than\ncompeting methods like LoRA. RECAST accomplishes this efficiency by learning to\ndecompose layer weights into a soft parameter-sharing framework consisting of\nshared weight templates and very few module-specific scaling factors or\ncoefficients. This soft parameter-sharing framework allows for effective\ntask-wise reparameterization by tuning only these coefficients while keeping\ntemplates frozen.A key innovation of RECAST is the novel weight reconstruction\npipeline called Neural Mimicry, which eliminates the need for pretraining from\nscratch. This allows for high-fidelity emulation of existing pretrained weights\nwithin our framework and provides quick adaptability to any model scale and\narchitecture. Extensive experiments across six datasets demonstrate RECAST\noutperforms the state-of-the-art by up to 3% across various scales,\narchitectures, and parameter spaces Moreover, we show that RECAST's\narchitecture-agnostic nature allows for seamless integration with existing\nmethods, further boosting performance.\n","authors":["Nazia Tasnim","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2411.16870v2.pdf","comment":"Accepted as a conference paper in ICLR, 2025"},{"id":"http://arxiv.org/abs/2503.11147v1","updated":"2025-03-14T07:34:39Z","published":"2025-03-14T07:34:39Z","title":"Asynchronous Sharpness-Aware Minimization For Fast and Accurate Deep\n  Learning","summary":"  Sharpness-Aware Minimization (SAM) is an optimization method that improves\ngeneralization performance of machine learning models. Despite its superior\ngeneralization, SAM has not been actively used in real-world applications due\nto its expensive computational cost. In this work, we propose a novel\nasynchronous-parallel SAM which achieves nearly the same gradient norm\npenalizing effect like the original SAM while breaking the data dependency\nbetween the model perturbation and the model update. The proposed asynchronous\nSAM can even entirely hide the model perturbation time by adjusting the batch\nsize for the model perturbation in a system-aware manner. Thus, the proposed\nmethod enables to fully utilize heterogeneous system resources such as CPUs and\nGPUs. Our extensive experiments well demonstrate the practical benefits of the\nproposed asynchronous approach. E.g., the asynchronous SAM achieves comparable\nVision Transformer fine-tuning accuracy (CIFAR-100) as the original SAM while\nhaving almost the same training time as SGD.\n","authors":["Junhyuk Jo","Jihyun Lim","Sunwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2503.11147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11146v1","updated":"2025-03-14T07:33:15Z","published":"2025-03-14T07:33:15Z","title":"Layer-wise Update Aggregation with Recycling for Communication-Efficient\n  Federated Learning","summary":"  Expensive communication cost is a common performance bottleneck in Federated\nLearning (FL), which makes it less appealing in real-world applications. Many\ncommunication-efficient FL methods focus on discarding a part of model updates\nmostly based on gradient magnitude. In this study, we find that recycling\nprevious updates, rather than simply dropping them, more effectively reduces\nthe communication cost while maintaining FL performance. We propose FedLUAR, a\nLayer-wise Update Aggregation with Recycling scheme for communication-efficient\nFL. We first define a useful metric that quantifies the extent to which the\naggregated gradients influences the model parameter values in each layer.\nFedLUAR selects a few layers based on the metric and recycles their previous\nupdates on the server side. Our extensive empirical study demonstrates that the\nupdate recycling scheme significantly reduces the communication cost while\nmaintaining model accuracy. For example, our method achieves nearly the same AG\nNews accuracy as FedAvg, while reducing the communication cost to just 17%.\n","authors":["Jisoo Kim","Sungmin Kang","Sunwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2503.11146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11144v1","updated":"2025-03-14T07:22:07Z","published":"2025-03-14T07:22:07Z","title":"MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling","summary":"  Large-scale pre-training of deep models, followed by fine-tuning them, has\nbecome the cornerstone of natural language processing (NLP). The prevalence of\ndata coupled with computational resources has led to large models with a\nconsiderable number of parameters. While the massive size of these models has\nled to remarkable success in many NLP tasks, a detriment is the expense\nrequired to retrain all the base model's parameters for the adaptation to each\ntask or domain. Parameter Efficient Fine-Tuning (PEFT) provides an effective\nsolution for this challenge by minimizing the number of parameters required to\nbe fine-tuned while maintaining the quality of the model. While existing\nmethods have achieved impressive results, they mainly focus on adapting a\nsubset of parameters, weight reparameterization, and prompt engineering. In\nthis paper, we study layers as extractors of different types of linguistic\ninformation that are valuable when used in conjunction. We then propose the\nMixture of Layer Experts (MoLEx), a novel sparse mixture of experts (SMoE)\nwhose experts are layers in the pre-trained model. It performs a conditional\ncomputation of a mixture of layers during fine-tuning to provide the model with\nmore structural knowledge about the data. By providing an avenue for\ninformation exchange between layers, MoLEx enables the model to make a more\nwell-informed prediction for the downstream task, leading to better fine-tuning\nresults with the same number of effective parameters. As experts can be\nprocessed in parallel, MoLEx introduces minimal additional computational\noverhead. We empirically corroborate the advantages of MoLEx when combined with\npopular PEFT baseline methods on a variety of downstream fine-tuning tasks,\nincluding the popular GLUE benchmark as well as the End-to-End Challenge (E2E).\nThe code is publicly available at https://github.com/rachtsy/molex.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.11144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10021v2","updated":"2025-03-14T07:18:48Z","published":"2025-03-13T04:00:13Z","title":"DGNN: A Neural PDE Solver Induced by Discontinuous Galerkin Methods","summary":"  We propose a general framework for the Discontinuous Galerkin-induced Neural\nNetwork (DGNN), inspired by the Interior Penalty Discontinuous Galerkin Method\n(IPDGM). In this approach, the trial space consists of piecewise neural network\nspace defined over the computational domain, while the test function space is\ncomposed of piecewise polynomials. We demonstrate the advantages of DGNN in\nterms of accuracy and training efficiency across several numerical examples,\nincluding stationary and time-dependent problems. Specifically, DGNN easily\nhandles high perturbations, discontinuous solutions, and complex geometric\ndomains.\n","authors":["Guanyu Chen","Shengze Xu","Dong Ni","Tieyong Zeng"],"pdf_url":"https://arxiv.org/pdf/2503.10021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02731v3","updated":"2025-03-14T07:17:05Z","published":"2023-03-05T17:55:15Z","title":"Virtual Guidance as a Mid-level Representation for Navigation with\n  Augmented Reality","summary":"  In the context of autonomous navigation, effectively conveying abstract\nnavigational cues to agents in dynamic environments presents significant\nchallenges, particularly when navigation information is derived from diverse\nmodalities such as both vision and high-level language descriptions. To address\nthis issue, we introduce a novel technique termed `Virtual Guidance,' which is\ndesigned to visually represent non-visual instructional signals. These visual\ncues are overlaid onto the agent's camera view and served as comprehensible\nnavigational guidance signals. To validate the concept of virtual guidance, we\npropose a sim-to-real framework that enables the transfer of the trained policy\nfrom simulated environments to real world, ensuring the adaptability of virtual\nguidance in practical scenarios. We evaluate and compare the proposed method\nagainst a non-visual guidance baseline through detailed experiments in\nsimulation. The experimental results demonstrate that the proposed virtual\nguidance approach outperforms the baseline methods across multiple scenarios\nand offers clear evidence of its effectiveness in autonomous navigation tasks.\n","authors":["Hsuan-Kung Yang","Tsung-Chih Chiang","Jou-Min Liu","Ting-Ru Liu","Chun-Wei Huang","Tsu-Ching Hsiao","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2303.02731v3.pdf","comment":"Tsung-Chih Chiang, Jou-Min Liu, Ting-Ru Liu, and Chun-Wei Huang\n  contributed equally to this work; This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2503.11127v1","updated":"2025-03-14T06:43:19Z","published":"2025-03-14T06:43:19Z","title":"Don't Forget It! Conditional Sparse Autoencoder Clamping Works for\n  Unlearning","summary":"  Recent developments in Large Language Model (LLM) capabilities have brought\ngreat potential but also posed new risks. For example, LLMs with knowledge of\nbioweapons, advanced chemistry, or cyberattacks could cause violence if placed\nin the wrong hands or during malfunctions. Because of their nature as\nnear-black boxes, intuitive interpretation of LLM internals remains an open\nresearch question, preventing developers from easily controlling model behavior\nand capabilities. The use of Sparse Autoencoders (SAEs) has recently emerged as\na potential method of unraveling representations of concepts in LLMs internals,\nand has allowed developers to steer model outputs by directly modifying the\nhidden activations. In this paper, we use SAEs to identify unwanted concepts\nfrom the Weapons of Mass Destruction Proxy (WMDP) dataset within gemma-2-2b\ninternals and use feature steering to reduce the model's ability to answer\nharmful questions while retaining its performance on harmless queries. Our\nresults bring back optimism to the viability of SAE-based explicit knowledge\nunlearning techniques.\n","authors":["Matthew Khoriaty","Andrii Shportko","Gustavo Mercier","Zach Wood-Doughty"],"pdf_url":"https://arxiv.org/pdf/2503.11127v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.11126v1","updated":"2025-03-14T06:37:17Z","published":"2025-03-14T06:37:17Z","title":"MUSS: Multilevel Subset Selection for Relevance and Diversity","summary":"  The problem of relevant and diverse subset selection has a wide range of\napplications, including recommender systems and retrieval-augmented generation\n(RAG). For example, in recommender systems, one is interested in selecting\nrelevant items, while providing a diversified recommendation. Constrained\nsubset selection problem is NP-hard, and popular approaches such as Maximum\nMarginal Relevance (MMR) are based on greedy selection. Many real-world\napplications involve large data, but the original MMR work did not consider\ndistributed selection. This limitation was later addressed by a method called\nDGDS which allows for a distributed setting using random data partitioning.\nHere, we exploit structure in the data to further improve both scalability and\nperformance on the target application. We propose MUSS, a novel method that\nuses a multilevel approach to relevant and diverse selection. We provide a\nrigorous theoretical analysis and show that our method achieves a constant\nfactor approximation of the optimal objective. In a recommender system\napplication, our method can achieve the same level of performance as baselines,\nbut 4.5 to 20 times faster. Our method is also capable of outperforming\nbaselines by up to 6 percent points of RAG-based question answering accuracy.\n","authors":["Vu Nguyen","Andrey Kan"],"pdf_url":"https://arxiv.org/pdf/2503.11126v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2503.11125v1","updated":"2025-03-14T06:37:04Z","published":"2025-03-14T06:37:04Z","title":"Context-Aware Rule Mining Using a Dynamic Transformer-Based Framework","summary":"  This study proposes a dynamic rule data mining algorithm based on an improved\nTransformer architecture, aiming to improve the accuracy and efficiency of rule\nmining in a dynamic data environment. With the increase in data volume and\ncomplexity, traditional data mining methods are difficult to cope with dynamic\ndata with strong temporal and variable characteristics, so new algorithms are\nneeded to capture the temporal regularity in the data. By improving the\nTransformer architecture, and introducing a dynamic weight adjustment mechanism\nand a temporal dependency module, we enable the model to adapt to data changes\nand mine more accurate rules. Experimental results show that compared with\ntraditional rule mining algorithms, the improved Transformer model has achieved\nsignificant improvements in rule mining accuracy, coverage, and stability. The\ncontribution of each module in the algorithm performance is further verified by\nablation experiments, proving the importance of temporal dependency and dynamic\nweight adjustment mechanisms in improving the model effect. In addition,\nalthough the improved model has certain challenges in computational efficiency,\nits advantages in accuracy and coverage enable it to perform well in processing\ncomplex dynamic data. Future research will focus on optimizing computational\nefficiency and combining more deep learning technologies to expand the\napplication scope of the algorithm, especially in practical applications in the\nfields of finance, medical care, and intelligent recommendation.\n","authors":["Jie Liu","Yiwei Zhang","Yuan Sheng","Yujia Lou","Haige Wang","Bohuan Yang"],"pdf_url":"https://arxiv.org/pdf/2503.11125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10636v2","updated":"2025-03-14T06:35:23Z","published":"2025-03-13T17:59:56Z","title":"The Curse of Conditions: Analyzing and Improving Optimal Transport for\n  Conditional Flow-Based Generation","summary":"  Minibatch optimal transport coupling straightens paths in unconditional flow\nmatching. This leads to computationally less demanding inference as fewer\nintegration steps and less complex numerical solvers can be employed when\nnumerically solving an ordinary differential equation at test time. However, in\nthe conditional setting, minibatch optimal transport falls short. This is\nbecause the default optimal transport mapping disregards conditions, resulting\nin a conditionally skewed prior distribution during training. In contrast, at\ntest time, we have no access to the skewed prior, and instead sample from the\nfull, unbiased prior distribution. This gap between training and testing leads\nto a subpar performance. To bridge this gap, we propose conditional optimal\ntransport C^2OT that adds a conditional weighting term in the cost matrix when\ncomputing the optimal transport assignment. Experiments demonstrate that this\nsimple fix works with both discrete and continuous conditions in\n8gaussians-to-moons, CIFAR-10, ImageNet-32x32, and ImageNet-256x256. Our method\nperforms better overall compared to the existing baselines across different\nfunction evaluation budgets. Code is available at\nhttps://hkchengrex.github.io/C2OT\n","authors":["Ho Kei Cheng","Alexander Schwing"],"pdf_url":"https://arxiv.org/pdf/2503.10636v2.pdf","comment":"Project page: https://hkchengrex.github.io/C2OT"},{"id":"http://arxiv.org/abs/2503.11120v1","updated":"2025-03-14T06:32:42Z","published":"2025-03-14T06:32:42Z","title":"A Multi-Objective Evaluation Framework for Analyzing Utility-Fairness\n  Trade-Offs in Machine Learning Systems","summary":"  The evaluation of fairness models in Machine Learning involves complex\nchallenges, such as defining appropriate metrics, balancing trade-offs between\nutility and fairness, and there are still gaps in this stage. This work\npresents a novel multi-objective evaluation framework that enables the analysis\nof utility-fairness trade-offs in Machine Learning systems. The framework was\ndeveloped using criteria from Multi-Objective Optimization that collect\ncomprehensive information regarding this complex evaluation task. The\nassessment of multiple Machine Learning systems is summarized, both\nquantitatively and qualitatively, in a straightforward manner through a radar\nchart and a measurement table encompassing various aspects such as convergence,\nsystem capacity, and diversity. The framework's compact representation of\nperformance facilitates the comparative analysis of different Machine Learning\nstrategies for decision-makers, in real-world applications, with single or\nmultiple fairness requirements. The framework is model-agnostic and flexible to\nbe adapted to any kind of Machine Learning systems, that is, black- or\nwhite-box, any kind and quantity of evaluation metrics, including\nmultidimensional fairness criteria. The functionality and effectiveness of the\nproposed framework is shown with different simulations, and an empirical study\nconducted on a real-world dataset with various Machine Learning systems.\n","authors":["Gökhan Özbulak","Oscar Jimenez-del-Toro","Maíra Fatoretto","Lilian Berton","André Anjos"],"pdf_url":"https://arxiv.org/pdf/2503.11120v1.pdf","comment":"11 pages, 13 figures"},{"id":"http://arxiv.org/abs/2501.14469v2","updated":"2025-03-14T06:16:49Z","published":"2025-01-24T13:00:54Z","title":"Pesti-Gen: Unleashing a Generative Molecule Approach for Toxicity Aware\n  Pesticide Design","summary":"  Global climate change has reduced crop resilience and pesticide efficacy,\nmaking reliance on synthetic pesticides inevitable, even though their\nwidespread use poses significant health and environmental risks. While these\npesticides remain a key tool in pest management, previous machine-learning\napplications in pesticide and agriculture have focused on classification or\nregression, leaving the fundamental challenge of generating new molecular\nstructures or designing novel candidates unaddressed. In this paper, we propose\nPesti-Gen, a novel generative model based on variational auto-encoders,\ndesigned to create pesticide candidates with optimized properties for the first\ntime. Specifically, Pesti-Gen leverages a two-stage learning process: an\ninitial pre-training phase that captures a generalized chemical structure\nrepresentation, followed by a fine-tuning stage that incorporates\ntoxicity-specific information. The model simultaneously optimizes over multiple\ntoxicity metrics, such as (1) livestock toxicity and (2) aqua toxicity to\ngenerate environmentally friendly pesticide candidates. Notably, Pesti-Gen\nachieves approximately 68\\% structural validity in generating new molecular\nstructures, demonstrating the model's effectiveness in producing optimized and\nfeasible pesticide candidates, thereby providing a new way for safer and more\nsustainable pest management solutions.\n","authors":["Taehan Kim","Wonduk Seo"],"pdf_url":"https://arxiv.org/pdf/2501.14469v2.pdf","comment":"Accepted to the RECOMB 2025 Poster Track"},{"id":"http://arxiv.org/abs/2503.11108v1","updated":"2025-03-14T06:01:42Z","published":"2025-03-14T06:01:42Z","title":"Limits of KV Cache Compression for Tensor Attention based Autoregressive\n  Transformers","summary":"  The key-value (KV) cache in autoregressive transformers presents a\nsignificant bottleneck during inference, which restricts the context length\ncapabilities of large language models (LLMs). While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanism [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a novel\nreduction from communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. In the low\ndimensional regime where $d = o(\\log n)$, we analyze the theoretical bounds of\nthe space complexity as well. Overall, our work provides a theoretical\nfoundation for us to understand the compression-expressivity tradeoff in tensor\nattention mechanisms and offers more perspectives in developing more\nmemory-efficient transformer architectures.\n","authors":["Yifang Chen","Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2503.11108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11101v1","updated":"2025-03-14T05:43:47Z","published":"2025-03-14T05:43:47Z","title":"A Survey on Self-supervised Contrastive Learning for Multimodal\n  Text-Image Analysis","summary":"  Self-supervised learning is a machine learning approach that generates\nimplicit labels by learning underlined patterns and extracting discriminative\nfeatures from unlabeled data without manual labelling. Contrastive learning\nintroduces the concept of \"positive\" and \"negative\" samples, where positive\npairs (e.g., variation of the same image/object) are brought together in the\nembedding space, and negative pairs (e.g., views from different images/objects)\nare pushed farther away. This methodology has shown significant improvements in\nimage understanding and image text analysis without much reliance on labeled\ndata. In this paper, we comprehensively discuss the terminologies, recent\ndevelopments and applications of contrastive learning with respect to\ntext-image models. Specifically, we provide an overview of the approaches of\ncontrastive learning in text-image models in recent years. Secondly, we\ncategorize the approaches based on different model structures. Thirdly, we\nfurther introduce and discuss the latest advances of the techniques used in the\nprocess such as pretext tasks for both images and text, architectural\nstructures, and key trends. Lastly, we discuss the recent state-of-art\napplications of self-supervised contrastive learning Text-Image based models.\n","authors":["Asifullah Khan","Laiba Asmatullah","Anza Malik","Shahzaib Khan","Hamna Asif"],"pdf_url":"https://arxiv.org/pdf/2503.11101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11099v1","updated":"2025-03-14T05:42:10Z","published":"2025-03-14T05:42:10Z","title":"Approximating the Total Variation Distance between Gaussians","summary":"  The total variation distance is a metric of central importance in statistics\nand probability theory. However, somewhat surprisingly, questions about\ncomputing it algorithmically appear not to have been systematically studied\nuntil very recently. In this paper, we contribute to this line of work by\nstudying this question in the important special case of multivariate Gaussians.\nMore formally, we consider the problem of approximating the total variation\ndistance between two multivariate Gaussians to within an $\\epsilon$-relative\nerror. Previous works achieved a fixed constant relative error approximation\nvia closed-form formulas. In this work, we give algorithms that given any two\n$n$-dimensional Gaussians $D_1,D_2$, and any error bound $\\epsilon > 0$,\napproximate the total variation distance $D := d_{TV}(D_1,D_2)$ to\n$\\epsilon$-relative accuracy in $\\text{poly}(n,\\frac{1}{\\epsilon},\\log\n\\frac{1}{D})$ operations. The main technical tool in our work is a reduction\nthat helps us extend the recent progress on computing the TV-distance between\ndiscrete random variables to our continuous setting.\n","authors":["Arnab Bhattacharyya","Weiming Feng","Piyush Srivastava"],"pdf_url":"https://arxiv.org/pdf/2503.11099v1.pdf","comment":"Accepted by AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.09501v2","updated":"2025-03-14T05:33:47Z","published":"2025-03-12T16:05:31Z","title":"ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning","summary":"  Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.\n","authors":["Ziyu Wan","Yunxiang Li","Yan Song","Hanjing Wang","Linyi Yang","Mark Schmidt","Jun Wang","Weinan Zhang","Shuyue Hu","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2503.09501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18345v2","updated":"2025-03-14T05:17:27Z","published":"2024-06-26T13:42:11Z","title":"EmT: A Novel Transformer for Generalized Cross-subject EEG Emotion\n  Recognition","summary":"  Integrating prior knowledge of neurophysiology into neural network\narchitecture enhances the performance of emotion decoding. While numerous\ntechniques emphasize learning spatial and short-term temporal patterns, there\nhas been limited emphasis on capturing the vital long-term contextual\ninformation associated with emotional cognitive processes. In order to address\nthis discrepancy, we introduce a novel transformer model called emotion\ntransformer (EmT). EmT is designed to excel in both generalized cross-subject\nEEG emotion classification and regression tasks. In EmT, EEG signals are\ntransformed into a temporal graph format, creating a sequence of EEG feature\ngraphs using a temporal graph construction module (TGC). A novel residual\nmulti-view pyramid GCN module (RMPG) is then proposed to learn dynamic graph\nrepresentations for each EEG feature graph within the series, and the learned\nrepresentations of each graph are fused into one token. Furthermore, we design\na temporal contextual transformer module (TCT) with two types of token mixers\nto learn the temporal contextual information. Finally, the task-specific output\nmodule (TSO) generates the desired outputs. Experiments on four publicly\navailable datasets show that EmT achieves higher results than the baseline\nmethods for both EEG emotion classification and regression tasks. The code is\navailable at https://github.com/yi-ding-cs/EmT.\n","authors":["Yi Ding","Chengxuan Tong","Shuailei Zhang","Muyun Jiang","Yong Li","Kevin Lim Jun Liang","Cuntai Guan"],"pdf_url":"https://arxiv.org/pdf/2406.18345v2.pdf","comment":"This work has been accepted by IEEE TNNLS"},{"id":"http://arxiv.org/abs/2503.10412v2","updated":"2025-03-14T05:16:16Z","published":"2025-03-13T14:35:47Z","title":"dFLMoE: Decentralized Federated Learning via Mixture of Experts for\n  Medical Data Analysis","summary":"  Federated learning has wide applications in the medical field. It enables\nknowledge sharing among different healthcare institutes while protecting\npatients' privacy. However, existing federated learning systems are typically\ncentralized, requiring clients to upload client-specific knowledge to a central\nserver for aggregation. This centralized approach would integrate the knowledge\nfrom each client into a centralized server, and the knowledge would be already\nundermined during the centralized integration before it reaches back to each\nclient. Besides, the centralized approach also creates a dependency on the\ncentral server, which may affect training stability if the server malfunctions\nor connections are unstable. To address these issues, we propose a\ndecentralized federated learning framework named dFLMoE. In our framework,\nclients directly exchange lightweight head models with each other. After\nexchanging, each client treats both local and received head models as\nindividual experts, and utilizes a client-specific Mixture of Experts (MoE)\napproach to make collective decisions. This design not only reduces the\nknowledge damage with client-specific aggregations but also removes the\ndependency on the central server to enhance the robustness of the framework. We\nvalidate our framework on multiple medical tasks, demonstrating that our method\nevidently outperforms state-of-the-art approaches under both model homogeneity\nand heterogeneity settings.\n","authors":["Luyuan Xie","Tianyu Luan","Wenyuan Cai","Guochen Yan","Zhaoyu Chen","Nan Xi","Yuejian Fang","Qingni Shen","Zhonghai Wu","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.10412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11086v1","updated":"2025-03-14T04:53:27Z","published":"2025-03-14T04:53:27Z","title":"A Survey of Cross-domain Graph Learning: Progress and Future Directions","summary":"  Graph learning plays a vital role in mining and analyzing complex\nrelationships involved in graph data, which is widely used in many real-world\napplications like transaction networks and communication networks. Foundation\nmodels in CV and NLP have shown powerful cross-domain capabilities that are\nalso significant in graph domains. However, existing graph learning approaches\nstruggle with cross-domain tasks. Inspired by successes in CV and NLP,\ncross-domain graph learning has once again become a focal point of attention to\nrealizing true graph foundation models. In this survey, we present a\ncomprehensive review and analysis of existing works on cross-domain graph\nlearning. Concretely, we first propose a new taxonomy, categorizing existing\napproaches based on the learned cross-domain information: structure, feature,\nand structure-feature mixture. Next, we systematically survey representative\nmethods in these categories. Finally, we discuss the remaining limitations of\nexisting studies and highlight promising avenues for future research. Relevant\npapers are summarized and will be consistently updated at:\nhttps://github.com/cshhzhao/Awesome-Cross-Domain-Graph-Learning.\n","authors":["Haihong Zhao","Chenyi Zi","Aochuan Chen","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2503.11086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12761v2","updated":"2025-03-14T04:47:39Z","published":"2024-10-16T17:32:23Z","title":"SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And\n  Video Generation","summary":"  Recent advances in diffusion models have significantly enhanced their ability\nto generate high-quality images and videos, but they have also increased the\nrisk of producing unsafe content. Existing unlearning/editing-based methods for\nsafe generation remove harmful concepts from models but face several\nchallenges: (1) They cannot instantly remove harmful concepts without training.\n(2) Their safe generation capabilities depend on collected training data. (3)\nThey alter model weights, risking degradation in quality for content unrelated\nto toxic concepts. To address these, we propose SAFREE, a novel, training-free\napproach for safe T2I and T2V, that does not alter the model's weights.\nSpecifically, we detect a subspace corresponding to a set of toxic concepts in\nthe text embedding space and steer prompt embeddings away from this subspace,\nthereby filtering out harmful content while preserving intended semantics. To\nbalance the trade-off between filtering toxicity and preserving safe concepts,\nSAFREE incorporates a novel self-validating filtering mechanism that\ndynamically adjusts the denoising steps when applying the filtered embeddings.\nAdditionally, we incorporate adaptive re-attention mechanisms within the\ndiffusion latent space to selectively diminish the influence of features\nrelated to toxic concepts at the pixel level. In the end, SAFREE ensures\ncoherent safety checking, preserving the fidelity, quality, and safety of the\noutput. SAFREE achieves SOTA performance in suppressing unsafe content in T2I\ngeneration compared to training-free baselines and effectively filters targeted\nconcepts while maintaining high-quality images. It also shows competitive\nresults against training-based methods. We extend SAFREE to various T2I\nbackbones and T2V tasks, showcasing its flexibility and generalization. SAFREE\nprovides a robust and adaptable safeguard for ensuring safe visual generation.\n","authors":["Jaehong Yoon","Shoubin Yu","Vaidehi Patil","Huaxiu Yao","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.12761v2.pdf","comment":"ICLR 2025; The first two authors contributed equally; Project page:\n  https://safree-safe-t2i-t2v.github.io/"},{"id":"http://arxiv.org/abs/2503.05970v2","updated":"2025-03-14T04:46:50Z","published":"2025-03-07T22:48:35Z","title":"Generative Multi-Agent Q-Learning for Policy Optimization: Decentralized\n  Wireless Networks","summary":"  Q-learning is a widely used reinforcement learning (RL) algorithm for\noptimizing wireless networks, but faces challenges with large state-spaces.\nRecently proposed multi-environment mixed Q-learning (MEMQ) algorithm addresses\nthese challenges by employing multiple Q-learning algorithms across multiple\nsynthetically generated, distinct but structurally related environments,\nso-called digital cousins. In this paper, we propose a novel multi-agent MEMQ\n(M-MEMQ) for cooperative decentralized wireless networks with multiple\nnetworked transmitters (TXs) and base stations (BSs). TXs do not have access to\nglobal information (joint state and actions). The new concept of coordinated\nand uncoordinated states is introduced. In uncoordinated states, TXs act\nindependently to minimize their individual costs and update local Q-functions.\nIn coordinated states, TXs use a Bayesian approach to estimate the joint state\nand update the joint Q-functions. The cost of information-sharing scales\nlinearly with the number of TXs and is independent of the joint state-action\nspace size. Several theoretical guarantees, including deterministic and\nprobabilistic convergence, bounds on estimation error variance, and the\nprobability of misdetecting the joint states, are given. Numerical simulations\nshow that M-MEMQ outperforms several decentralized and centralized training\nwith decentralized execution (CTDE) multi-agent RL algorithms by achieving 55%\nlower average policy error (APE), 35% faster convergence, 50% reduced runtime\ncomplexity, and 45% less sample complexity. Furthermore, M-MEMQ achieves\ncomparable APE with significantly lower complexity than centralized methods.\nSimulations validate the theoretical analyses.\n","authors":["Talha Bozkus","Urbashi Mitra"],"pdf_url":"https://arxiv.org/pdf/2503.05970v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2503.11078v1","updated":"2025-03-14T04:38:53Z","published":"2025-03-14T04:38:53Z","title":"Understanding Flatness in Generative Models: Its Role and Benefits","summary":"  Flat minima, known to enhance generalization and robustness in supervised\nlearning, remain largely unexplored in generative models. In this work, we\nsystematically investigate the role of loss surface flatness in generative\nmodels, both theoretically and empirically, with a particular focus on\ndiffusion models. We establish a theoretical claim that flatter minima improve\nrobustness against perturbations in target prior distributions, leading to\nbenefits such as reduced exposure bias -- where errors in noise estimation\naccumulate over iterations -- and significantly improved resilience to model\nquantization, preserving generative performance even under strong quantization\nconstraints. We further observe that Sharpness-Aware Minimization (SAM), which\nexplicitly controls the degree of flatness, effectively enhances flatness in\ndiffusion models, whereas other well-known methods such as Stochastic Weight\nAveraging (SWA) and Exponential Moving Average (EMA), which promote flatness\nindirectly via ensembling, are less effective. Through extensive experiments on\nCIFAR-10, LSUN Tower, and FFHQ, we demonstrate that flat minima in diffusion\nmodels indeed improves not only generative performance but also robustness.\n","authors":["Taehwan Lee","Kyeongkook Seo","Jaejun Yoo","Sung Whan Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.11078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19225v4","updated":"2025-03-14T04:31:59Z","published":"2024-10-25T00:27:53Z","title":"Hierarchical Mixture of Experts: Generalizable Learning for High-Level\n  Synthesis","summary":"  High-level synthesis (HLS) is a widely used tool in designing Field\nProgrammable Gate Array (FPGA). HLS enables FPGA design with software\nprogramming languages by compiling the source code into an FPGA circuit. The\nsource code includes a program (called \"kernel\") and several pragmas that\ninstruct hardware synthesis, such as parallelization, pipeline, etc. While it\nis relatively easy for software developers to design the program, it heavily\nrelies on hardware knowledge to design the pragmas, posing a big challenge for\nsoftware developers. Recently, different machine learning algorithms, such as\nGNNs, have been proposed to automate the pragma design via performance\nprediction. However, when applying the trained model on new kernels, the\nsignificant domain shift often leads to unsatisfactory performance. We propose\na more domain-generalizable model structure: a two-level hierarchical Mixture\nof Experts (MoE), that can be flexibly adapted to any GNN model. Different\nexpert networks can learn to deal with different regions in the representation\nspace, and they can utilize similar patterns between the old kernels and new\nkernels. In the low-level MoE, we apply MoE on three natural granularities of a\nprogram: node, basic block, and graph. The high-level MoE learns to aggregate\nthe three granularities for the final decision. To train the hierarchical MoE\nstably, we further propose a two-stage training method to avoid expert\npolarization. Extensive experiments verify the effectiveness of the proposed\nhierarchical MoE. We publicized our codes at\nhttps://github.com/weikai-li/HierarchicalMoE.\n","authors":["Weikai Li","Ding Wang","Zijian Ding","Atefeh Sohrabizadeh","Zongyue Qin","Jason Cong","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2410.19225v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2503.11066v1","updated":"2025-03-14T04:19:23Z","published":"2025-03-14T04:19:23Z","title":"Further Exploration of Precise Binding Energies from Physics Informed\n  Machine Learning and the Development a Practical Ensemble Model","summary":"  Sixteen new physics informed machine learning models have been trained on\nbinding energy residuals from modern mass models that leverage shape parameters\nand other physical features. The models have been trained on a subset of AME\n2012 data and have been verified with a subset of the AME 2020 data. Among the\nmachine learning approaches tested in this work, the preferred approach is the\nleast squares boosted ensemble of trees which appears to have a superior\nability to both interpolate and extrapolate binding energy residuals. The\nmachine learning models for four mass models created from the ensemble of trees\napproach have been combined to create a composite model called the Four Model\nTree Ensemble (FMTE). The FMTE model predicts binding energy values from AME\n2020 with a standard deviation of 76 keV and a mean deviation of 34 keV for all\nnuclei with N > 7 and Z > 7. A comparison with new mass measurements for 33\nisotopes not included in AME 2012 or AME 2020 indicates that the FMTE performs\nbetter than all mass models that were tested.\n","authors":["I. Bentley","J. Tedder","M. Gebran","A. Paul"],"pdf_url":"https://arxiv.org/pdf/2503.11066v1.pdf","comment":"Submitted to PRC for review"},{"id":"http://arxiv.org/abs/2503.11065v1","updated":"2025-03-14T04:18:36Z","published":"2025-03-14T04:18:36Z","title":"Low-cost Real-world Implementation of the Swing-up Pendulum for Deep\n  Reinforcement Learning Experiments","summary":"  Deep reinforcement learning (DRL) has had success in virtual and simulated\ndomains, but due to key differences between simulated and real-world\nenvironments, DRL-trained policies have had limited success in real-world\napplications. To assist researchers to bridge the \\textit{sim-to-real gap}, in\nthis paper, we describe a low-cost physical inverted pendulum apparatus and\nsoftware environment for exploring sim-to-real DRL methods. In particular, the\ndesign of our apparatus enables detailed examination of the delays that arise\nin physical systems when sensing, communicating, learning, inferring and\nactuating. Moreover, we wish to improve access to educational systems, so our\napparatus uses readily available materials and parts to reduce cost and\nlogistical barriers. Our design shows how commercial, off-the-shelf electronics\nand electromechanical and sensor systems, combined with common metal\nextrusions, dowel and 3D printed couplings provide a pathway for affordable\nphysical DRL apparatus. The physical apparatus is complemented with a simulated\nenvironment implemented using a high-fidelity physics engine and OpenAI Gym\ninterface.\n","authors":["Peter Böhm","Pauline Pounds","Archie C. Chapman"],"pdf_url":"https://arxiv.org/pdf/2503.11065v1.pdf","comment":"Australasian Conference on Robotics and Automation (ACRA) 2022"},{"id":"http://arxiv.org/abs/2503.06254v2","updated":"2025-03-14T04:16:23Z","published":"2025-03-08T15:46:38Z","title":"Poisoned-MRAG: Knowledge Poisoning Attacks to Multimodal Retrieval\n  Augmented Generation","summary":"  Multimodal retrieval-augmented generation (RAG) enhances the visual reasoning\ncapability of vision-language models (VLMs) by dynamically accessing\ninformation from external knowledge bases. In this work, we introduce\n\\textit{Poisoned-MRAG}, the first knowledge poisoning attack on multimodal RAG\nsystems. Poisoned-MRAG injects a few carefully crafted image-text pairs into\nthe multimodal knowledge database, manipulating VLMs to generate the\nattacker-desired response to a target query. Specifically, we formalize the\nattack as an optimization problem and propose two cross-modal attack\nstrategies, dirty-label and clean-label, tailored to the attacker's knowledge\nand goals. Our extensive experiments across multiple knowledge databases and\nVLMs show that Poisoned-MRAG outperforms existing methods, achieving up to 98\\%\nattack success rate with just five malicious image-text pairs injected into the\nInfoSeek database (481,782 pairs). Additionally, We evaluate 4 different\ndefense strategies, including paraphrasing, duplicate removal, structure-driven\nmitigation, and purification, demonstrating their limited effectiveness and\ntrade-offs against Poisoned-MRAG. Our results highlight the effectiveness and\nscalability of Poisoned-MRAG, underscoring its potential as a significant\nthreat to multimodal RAG systems.\n","authors":["Yinuo Liu","Zenghui Yuan","Guiyao Tie","Jiawen Shi","Pan Zhou","Lichao Sun","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2503.06254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11064v1","updated":"2025-03-14T04:14:27Z","published":"2025-03-14T04:14:27Z","title":"MobiVital: Self-supervised Time-series Quality Estimation for\n  Contactless Respiration Monitoring Using UWB Radar","summary":"  Respiration waveforms are increasingly recognized as important biomarkers,\noffering insights beyond simple respiration rates, such as detecting breathing\nirregularities for disease diagnosis or monitoring breath patterns to guide\nrehabilitation training. Previous works in wireless respiration monitoring have\nprimarily focused on estimating respiration rate, where the breath waveforms\nare often generated as a by-product. As a result, issues such as waveform\ndeformation and inversion have largely been overlooked, reducing the signal's\nutility for applications requiring breathing waveforms. To address this\nproblem, we present a novel approach, MobiVital, that improves the quality of\nrespiration waveforms obtained from ultra-wideband (UWB) radar data. MobiVital\ncombines a self-supervised autoregressive model for breathing waveform\nextraction with a biology-informed algorithm to detect and correct waveform\ninversions. To encourage reproducible research efforts for developing wireless\nvital signal monitoring systems, we also release a 12-person, 24-hour UWB radar\nvital signal dataset, with time-synchronized ground truth obtained from\nwearable sensors. Our results show that the respiration waveforms produced by\nour system exhibit a 7-34% increase in fidelity to the ground truth compared to\nthe baselines and can benefit downstream tasks such as respiration rate\nestimation.\n","authors":["Ziqi Wang","Derek Hua","Wenjun Jiang","Tianwei Xing","Xun Chen","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2503.11064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10947v5","updated":"2025-03-14T04:05:05Z","published":"2023-05-18T13:09:45Z","title":"Standalone 16-bit Neural Network Training: Missing Study for\n  Hardware-Limited Deep Learning Practitioners","summary":"  With the increasing complexity of machine learning models, managing\ncomputational resources like memory and processing power has become a critical\nconcern. Mixed precision techniques, which leverage different numerical\nprecisions during model training and inference to optimize resource usage, have\nbeen widely adopted. However, access to hardware that supports lower precision\nformats (e.g., FP8 or FP4) remains limited, especially for practitioners with\nhardware constraints. For many with limited resources, the available options\nare restricted to using 32-bit, 16-bit, or a combination of the two. While it\nis commonly believed that 16-bit precision can achieve results comparable to\nfull (32-bit) precision, this study is the first to systematically validate\nthis assumption through both rigorous theoretical analysis and extensive\nempirical evaluation. Our theoretical formalization of floating-point errors\nand classification tolerance provides new insights into the conditions under\nwhich 16-bit precision can approximate 32-bit results. This study fills a\ncritical gap, proving for the first time that standalone 16-bit precision\nneural networks match 32-bit and mixed-precision in accuracy while boosting\ncomputational speed. Given the widespread availability of 16-bit across GPUs,\nthese findings are especially valuable for machine learning practitioners with\nlimited hardware resources to make informed decisions.\n","authors":["Juyoung Yun","Sol Choi","Francois Rameau","Byungkon Kang","Zhoulai Fu"],"pdf_url":"https://arxiv.org/pdf/2305.10947v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06190v2","updated":"2025-03-14T03:59:35Z","published":"2024-02-09T05:06:58Z","title":"Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain","summary":"  Standard modern machine-learning-based imaging methods have faced challenges\nin medical applications due to the high cost of dataset construction and,\nthereby, the limited labeled training data available. Additionally, upon\ndeployment, these methods are usually used to process a large volume of data on\na daily basis, imposing a high maintenance cost on medical facilities. In this\npaper, we introduce a new neural network architecture, termed LoGoNet, with a\ntailored self-supervised learning (SSL) method to mitigate such challenges.\nLoGoNet integrates a novel feature extractor within a U-shaped architecture,\nleveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture\nboth long-range and short-range feature dependencies adeptly. This is in\ncontrast to existing methods that rely on increasing network capacity to\nenhance feature extraction. This combination of novel techniques in our model\nis especially beneficial in medical image segmentation, given the difficulty of\nlearning intricate and often irregular body organ shapes, such as the spleen.\nComplementary, we propose a novel SSL method tailored for 3D images to\ncompensate for the lack of large labeled datasets. The method combines masking\nand contrastive learning techniques within a multi-task learning framework and\nis compatible with both Vision Transformer (ViT) and CNN-based models. We\ndemonstrate the efficacy of our methods in numerous tasks across two standard\ndatasets (i.e., BTCV and MSD). Benchmark comparisons with eight\nstate-of-the-art models highlight LoGoNet's superior performance in both\ninference time and accuracy.\n","authors":["Amin Karimi Monsefi","Payam Karisani","Mengxi Zhou","Stacey Choi","Nathan Doble","Heng Ji","Srinivasan Parthasarathy","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2402.06190v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11061v1","updated":"2025-03-14T03:54:43Z","published":"2025-03-14T03:54:43Z","title":"Generative Modelling for Mathematical Discovery","summary":"  We present a new implementation of the LLM-driven genetic algorithm {\\it\nfunsearch}, whose aim is to generate examples of interest to mathematicians and\nwhich has already had some success in problems in extremal combinatorics. Our\nimplementation is designed to be useful in practice for working mathematicians;\nit does not require expertise in machine learning or access to high-performance\ncomputing resources. Applying {\\it funsearch} to a new problem involves\nmodifying a small segment of Python code and selecting a large language model\n(LLM) from one of many third-party providers. We benchmarked our implementation\non three different problems, obtaining metrics that may inform applications of\n{\\it funsearch} to new problems. Our results demonstrate that {\\it funsearch}\nsuccessfully learns in a variety of combinatorial and number-theoretic\nsettings, and in some contexts learns principles that generalize beyond the\nproblem originally trained on.\n","authors":["Jordan S. Ellenberg","Cristofero S. Fraser-Taliente","Thomas R. Harvey","Karan Srivastava","Andrew V. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2503.11061v1.pdf","comment":"22 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.11054v1","updated":"2025-03-14T03:45:29Z","published":"2025-03-14T03:45:29Z","title":"LUSD: Localized Update Score Distillation for Text-Guided Image Editing","summary":"  While diffusion models show promising results in image editing given a target\nprompt, achieving both prompt fidelity and background preservation remains\ndifficult. Recent works have introduced score distillation techniques that\nleverage the rich generative prior of text-to-image diffusion models to solve\nthis task without additional fine-tuning. However, these methods often struggle\nwith tasks such as object insertion. Our investigation of these failures\nreveals significant variations in gradient magnitude and spatial distribution,\nmaking hyperparameter tuning highly input-specific or unsuccessful. To address\nthis, we propose two simple yet effective modifications: attention-based\nspatial regularization and gradient filtering-normalization, both aimed at\nreducing these variations during gradient updates. Experimental results show\nour method outperforms state-of-the-art score distillation techniques in prompt\nfidelity, improving successful edits while preserving the background. Users\nalso preferred our method over state-of-the-art techniques across three\nmetrics, and by 58-64% overall.\n","authors":["Worameth Chinchuthakun","Tossaporn Saengja","Nontawat Tritrong","Pitchaporn Rewatbowornwong","Pramook Khungurn","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2503.11054v1.pdf","comment":"Project page: https://github.com/sincostanx/LUSD"},{"id":"http://arxiv.org/abs/2407.11678v2","updated":"2025-03-14T03:37:35Z","published":"2024-07-16T12:53:53Z","title":"Theoretical Insights into CycleGAN: Analyzing Approximation and\n  Estimation Errors in Unpaired Data Generation","summary":"  In this paper, we focus on analyzing the excess risk of the unpaired data\ngeneration model, called CycleGAN. Unlike classical GANs, CycleGAN not only\ntransforms data between two unpaired distributions but also ensures the\nmappings are consistent, which is encouraged by the cycle-consistency term\nunique to CycleGAN. The increasing complexity of model structure and the\naddition of the cycle-consistency term in CycleGAN present new challenges for\nerror analysis. By considering the impact of both the model architecture and\ntraining procedure, the risk is decomposed into two terms: approximation error\nand estimation error. These two error terms are analyzed separately and\nultimately combined by considering the trade-off between them. Each component\nis rigorously analyzed; the approximation error through constructing\napproximations of the optimal transport maps, and the estimation error through\nestablishing an upper bound using Rademacher complexity. Our analysis not only\nisolates these errors but also explores the trade-offs between them, which\nprovides a theoretical insights of how CycleGAN's architecture and training\nprocedures influence its performance.\n","authors":["Luwei Sun","Dongrui Shen","Han Feng"],"pdf_url":"https://arxiv.org/pdf/2407.11678v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11050v1","updated":"2025-03-14T03:36:44Z","published":"2025-03-14T03:36:44Z","title":"Distance-Based Tree-Sliced Wasserstein Distance","summary":"  To overcome computational challenges of Optimal Transport (OT), several\nvariants of Sliced Wasserstein (SW) has been developed in the literature. These\napproaches exploit the closed-form expression of the univariate OT by\nprojecting measures onto (one-dimensional) lines. However, projecting measures\nonto low-dimensional spaces can lead to a loss of topological information.\nTree-Sliced Wasserstein distance on Systems of Lines (TSW-SL) has emerged as a\npromising alternative that replaces these lines with a more advanced structure\ncalled tree systems. The tree structures enhance the ability to capture\ntopological information of the metric while preserving computational\nefficiency. However, at the core of TSW-SL, the splitting maps, which serve as\nthe mechanism for pushing forward measures onto tree systems, focus solely on\nthe position of the measure supports while disregarding the projecting domains.\nMoreover, the specific splitting map used in TSW-SL leads to a metric that is\nnot invariant under Euclidean transformations, a typically expected property\nfor OT on Euclidean space. In this work, we propose a novel class of splitting\nmaps that generalizes the existing one studied in TSW-SL enabling the use of\nall positional information from input measures, resulting in a novel\nDistance-based Tree-Sliced Wasserstein (Db-TSW) distance. In addition, we\nintroduce a simple tree sampling process better suited for Db-TSW, leading to\nan efficient GPU-friendly implementation for tree systems, similar to the\noriginal SW. We also provide a comprehensive theoretical analysis of proposed\nclass of splitting maps to verify the injectivity of the corresponding Radon\nTransform, and demonstrate that Db-TSW is an Euclidean invariant metric. We\nempirically show that Db-TSW significantly improves accuracy compared to recent\nSW variants while maintaining low computational cost via a wide range of\nexperiments.\n","authors":["Hoang V. Tran","Khoi N. M. Nguyen","Trang Pham","Thanh T. Chu","Tam Le","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.11050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11046v1","updated":"2025-03-14T03:29:26Z","published":"2025-03-14T03:29:26Z","title":"Measuring Similarity in Causal Graphs: A Framework for Semantic and\n  Structural Analysis","summary":"  Causal graphs are commonly used to understand and model complex systems.\nResearchers often construct these graphs from different perspectives, leading\nto significant variations for the same problem. Comparing causal graphs is,\ntherefore, essential for evaluating assumptions, integrating insights, and\nresolving disagreements. The rise of AI tools has further amplified this need,\nas they are increasingly used to generate hypothesized causal graphs by\nsynthesizing information from various sources such as prior research and\ncommunity inputs, providing the potential for automating and scaling causal\nmodeling for complex systems. Similar to humans, these tools also produce\ninconsistent results across platforms, versions, and iterations. Despite its\nimportance, research on causal graph comparison remains scarce. Existing\nmethods often focus solely on structural similarities, assuming identical\nvariable names, and fail to capture nuanced semantic relationships, which is\nessential for causal graph comparison. We address these gaps by investigating\nmethods for comparing causal graphs from both semantic and structural\nperspectives. First, we reviewed over 40 existing metrics and, based on\npredefined criteria, selected nine for evaluation from two threads of machine\nlearning: four semantic similarity metrics and five learning graph kernels. We\ndiscuss the usability of these metrics in simple examples to illustrate their\nstrengths and limitations. We then generated a synthetic dataset of 2,000\ncausal graphs using generative AI based on a reference diagram. Our findings\nreveal that each metric captures a different aspect of similarity, highlighting\nthe need to use multiple metrics.\n","authors":["Ning-Yuan Georgia Liu","Flower Yang","Mohammad S. Jalali"],"pdf_url":"https://arxiv.org/pdf/2503.11046v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2411.12940v2","updated":"2025-03-14T03:22:16Z","published":"2024-11-20T00:18:46Z","title":"On the relationship between Koopman operator approximations and neural\n  ordinary differential equations for data-driven time-evolution predictions","summary":"  This work explores the relationship between state space methods and Koopman\noperator-based methods for predicting the time-evolution of nonlinear dynamical\nsystems. We demonstrate that extended dynamic mode decomposition with\ndictionary learning (EDMD-DL), when combined with a state space projection, is\nequivalent to a neural network representation of the nonlinear discrete-time\nflow map on the state space. We highlight how this projection step introduces\nnonlinearity into the evolution equations, enabling significantly improved\nEDMD-DL predictions. With this projection, EDMD-DL leads to a nonlinear\ndynamical system on the state space, which can be represented in either\ndiscrete or continuous time. This system has a natural structure for neural\nnetworks, where the state is first expanded into a high dimensional feature\nspace followed by a linear mapping which represents the discrete-time map or\nthe vector field as a linear combination of these features. Inspired by these\nobservations, we implement several variations of neural ordinary differential\nequations (ODEs) and EDMD-DL, developed by combining different aspects of their\nrespective model structures and training procedures. We evaluate these methods\nusing numerical experiments on chaotic dynamics in the Lorenz system and a\nnine-mode model of turbulent shear flow, showing comparable performance across\nmethods in terms of short-time trajectory prediction, reconstruction of\nlong-time statistics, and prediction of rare events. These results highlight\nthe equivalence of the EDMD-DL implementation with a state space projection to\na neural ODE representation of the dynamics. We also show that these methods\nprovide comparable performance to a non-Markovian approach in terms of\nprediction of extreme events.\n","authors":["Jake Buzhardt","C. Ricardo Constante-Amores","Michael D. Graham"],"pdf_url":"https://arxiv.org/pdf/2411.12940v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.11043v1","updated":"2025-03-14T03:13:55Z","published":"2025-03-14T03:13:55Z","title":"InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse\n  Problems in Physical Sciences","summary":"  Plug-and-play diffusion priors (PnPDP) have emerged as a promising research\ndirection for solving inverse problems.\n  However, current studies primarily focus on natural image restoration,\nleaving the performance of these algorithms in scientific inverse problems\nlargely unexplored. To address this gap, we introduce \\textsc{InverseBench}, a\nframework that evaluates diffusion models across five distinct scientific\ninverse problems. These problems present unique structural challenges that\ndiffer from existing benchmarks, arising from critical scientific applications\nsuch as optical tomography, medical imaging, black hole imaging, seismology,\nand fluid dynamics. With \\textsc{InverseBench}, we benchmark 14 inverse problem\nalgorithms that use plug-and-play diffusion priors against strong,\ndomain-specific baselines, offering valuable new insights into the strengths\nand weaknesses of existing algorithms. To facilitate further research and\ndevelopment, we open-source the codebase, along with datasets and pre-trained\nmodels, at https://devzhk.github.io/InverseBench/.\n","authors":["Hongkai Zheng","Wenda Chu","Bingliang Zhang","Zihui Wu","Austin Wang","Berthy T. Feng","Caifeng Zou","Yu Sun","Nikola Kovachki","Zachary E. Ross","Katherine L. Bouman","Yisong Yue"],"pdf_url":"https://arxiv.org/pdf/2503.11043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11029v1","updated":"2025-03-14T02:55:13Z","published":"2025-03-14T02:55:13Z","title":"Neural Tangent Kernel of Neural Networks with Loss Informed by\n  Differential Operators","summary":"  Spectral bias is a significant phenomenon in neural network training and can\nbe explained by neural tangent kernel (NTK) theory. In this work, we develop\nthe NTK theory for deep neural networks with physics-informed loss, providing\ninsights into the convergence of NTK during initialization and training, and\nrevealing its explicit structure. We find that, in most cases, the differential\noperators in the loss function do not induce a faster eigenvalue decay rate and\nstronger spectral bias. Some experimental results are also presented to verify\nthe theory.\n","authors":["Weiye Gan","Yicheng Li","Qian Lin","Zuoqiang Shi"],"pdf_url":"https://arxiv.org/pdf/2503.11029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16158v2","updated":"2025-03-14T02:54:43Z","published":"2023-03-25T03:06:43Z","title":"Behavioral Machine Learning? Computer Predictions of Corporate Earnings\n  also Overreact","summary":"  Machine learning algorithms are known to outperform human analysts in\npredicting corporate earnings, leading to their rapid adoption. However, we\nshow that leading methods (XGBoost, neural nets, ChatGPT) systematically\noverreact to news. The overreaction is primarily due to biases in the training\ndata and we show that it cannot be eliminated without compromising accuracy.\nAnalysts with machine learning training overreact much less than do traditional\nanalysts. We provide a model showing that there is a tradeoff between\npredictive power and rational behavior. Our findings suggest that AI tools\nreduce but do not eliminate behavioral biases in financial markets.\n","authors":["Murray Z. Frank","Jing Gao","Keer Yang"],"pdf_url":"https://arxiv.org/pdf/2303.16158v2.pdf","comment":"stock analysts, machine learning, behavioral, overreaction"},{"id":"http://arxiv.org/abs/2502.18470v3","updated":"2025-03-14T02:48:55Z","published":"2025-02-04T01:30:06Z","title":"Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World\n  Spatial Reasoning Questions","summary":"  Spatial reasoning remains a challenge for Large Language Models (LLMs), which\nstruggle with spatial data retrieval and reasoning. We propose Spatial\nRetrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to\nspatial tasks by integrating sparse spatial retrieval (spatial databases) and\ndense semantic retrieval (LLM-based similarity). A multi-objective ranking\nstrategy balances spatial constraints and semantic relevance, while an\nLLM-guided generator ensures coherent responses. Experiments on a real-world\ntourism dataset show that Spatial-RAG significantly improves spatial question\nanswering, bridging the gap between LLMs and spatial intelligence.\n","authors":["Dazhou Yu","Riyang Bao","Gengchen Mai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18470v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.11629v1","updated":"2025-03-14T17:48:06Z","published":"2025-03-14T17:48:06Z","title":"TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree\n  Sequencing","summary":"  We introduce TreeMeshGPT, an autoregressive Transformer designed to generate\nhigh-quality artistic meshes aligned with input point clouds. Instead of the\nconventional next-token prediction in autoregressive Transformer, we propose a\nnovel Autoregressive Tree Sequencing where the next input token is retrieved\nfrom a dynamically growing tree structure that is built upon the triangle\nadjacency of faces within the mesh. Our sequencing enables the mesh to extend\nlocally from the last generated triangular face at each step, and therefore\nreduces training difficulty and improves mesh quality. Our approach represents\neach triangular face with two tokens, achieving a compression rate of\napproximately 22% compared to the naive face tokenization. This efficient\ntokenization enables our model to generate highly detailed artistic meshes with\nstrong point cloud conditioning, surpassing previous methods in both capacity\nand fidelity. Furthermore, our method generates mesh with strong normal\norientation constraints, minimizing flipped normals commonly encountered in\nprevious methods. Our experiments show that TreeMeshGPT enhances the mesh\ngeneration quality with refined details and normal orientation consistency.\n","authors":["Stefan Lionar","Jiabin Liang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2503.11629v1.pdf","comment":"CVPR 2025. Code: https://github.com/sail-sg/TreeMeshGPT"},{"id":"http://arxiv.org/abs/2503.11609v1","updated":"2025-03-14T17:24:01Z","published":"2025-03-14T17:24:01Z","title":"Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages","summary":"  An old-school recipe for training a classifier is to (i) learn a good feature\nextractor and (ii) optimize a linear layer atop. When only a handful of samples\nare available per category, as in Few-Shot Adaptation (FSA), data are\ninsufficient to fit a large number of parameters, rendering the above\nimpractical. This is especially true with large pre-trained Vision-Language\nModels (VLMs), which motivated successful research at the intersection of\nParameter-Efficient Fine-tuning (PEFT) and FSA. In this work, we start by\nanalyzing the learning dynamics of PEFT techniques when trained on few-shot\ndata from only a subset of categories, referred to as the ``base'' classes. We\nshow that such dynamics naturally splits into two distinct phases: (i)\ntask-level feature extraction and (ii) specialization to the available\nconcepts. To accommodate this dynamic, we then depart from prompt- or\nadapter-based methods and tackle FSA differently. Specifically, given a fixed\ncomputational budget, we split it to (i) learn a task-specific feature\nextractor via PEFT and (ii) train a linear classifier on top. We call this\nscheme Two-Stage Few-Shot Adaptation (2SFS). Differently from established\nmethods, our scheme enables a novel form of selective inference at a category\nlevel, i.e., at test time, only novel categories are embedded by the adapted\ntext encoder, while embeddings of base categories are available within the\nclassifier. Results with fixed hyperparameters across two settings, three\nbackbones, and eleven datasets, show that 2SFS matches or surpasses the\nstate-of-the-art, while established methods degrade significantly across\nsettings.\n","authors":["Matteo Farina","Massimiliano Mancini","Giovanni Iacca","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2503.11609v1.pdf","comment":"Camera-ready version for CVPR 2025 (w/ SuppMat, 23 pages)"},{"id":"http://arxiv.org/abs/2503.11324v1","updated":"2025-03-14T11:45:10Z","published":"2025-03-14T11:45:10Z","title":"Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative\n  Watermarking","summary":"  With the success of autoregressive learning in large language models, it has\nbecome a dominant approach for text-to-image generation, offering high\nefficiency and visual quality. However, invisible watermarking for visual\nautoregressive (VAR) models remains underexplored, despite its importance in\nmisuse prevention. Existing watermarking methods, designed for diffusion\nmodels, often struggle to adapt to the sequential nature of VAR models. To\nbridge this gap, we propose Safe-VAR, the first watermarking framework\nspecifically designed for autoregressive text-to-image generation. Our study\nreveals that the timing of watermark injection significantly impacts generation\nquality, and watermarks of different complexities exhibit varying optimal\ninjection times. Motivated by this observation, we propose an Adaptive Scale\nInteraction Module, which dynamically determines the optimal watermark\nembedding strategy based on the watermark information and the visual\ncharacteristics of the generated image. This ensures watermark robustness while\nminimizing its impact on image quality. Furthermore, we introduce a Cross-Scale\nFusion mechanism, which integrates mixture of both heads and experts to\neffectively fuse multi-resolution features and handle complex interactions\nbetween image content and watermark patterns. Experimental results demonstrate\nthat Safe-VAR achieves state-of-the-art performance, significantly surpassing\nexisting counterparts regarding image quality, watermarking fidelity, and\nrobustness against perturbations. Moreover, our method exhibits strong\ngeneralization to an out-of-domain watermark dataset QR Codes.\n","authors":["Ziyi Wang","Songbai Tan","Gang Xu","Xuerui Qiu","Hongbin Xu","Xin Meng","Ming Li","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2503.11324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11315v1","updated":"2025-03-14T11:31:30Z","published":"2025-03-14T11:31:30Z","title":"MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with\n  Minimal Multimodal Speech Tokens","summary":"  Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in\nnoisy environments by combining auditory and visual information. However,\nrecent Large Language Model (LLM) based AVSR systems incur high computational\ncosts due to the high temporal resolution of audio-visual speech processed by\nLLMs. In this work, we introduce an efficient multimodal speech LLM framework\nthat minimizes token length while preserving essential linguistic content. Our\napproach employs an early av-fusion module for streamlined feature integration,\nan audio-visual speech Q-Former that dynamically allocates tokens based on\ninput duration, and a refined query allocation strategy with a speech rate\npredictor to adjust token allocation according to speaking speed of each audio\nsample. Extensive experiments on the LRS3 dataset show that our method achieves\nstate-of-the-art performance with a WER of 0.74% while using only 3.5 tokens\nper second. Moreover, our approach not only reduces token usage by 86% compared\nto the previous multimodal speech LLM framework, but also improves\ncomputational efficiency by reducing FLOPs by 35.7%.\n","authors":["Jeong Hun Yeo","Hyeongseop Rha","Se Jin Park","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2503.11315v1.pdf","comment":"The code and models are available\n  https://github.com/JeongHun0716/MMS-LLaMA"},{"id":"http://arxiv.org/abs/2407.07041v2","updated":"2025-03-14T11:31:15Z","published":"2024-07-09T17:03:57Z","title":"Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack","summary":"  The vast accessibility of Synthetic Aperture Radar (SAR) images through\nonline portals has propelled the research across various fields. This\nwidespread use and easy availability have unfortunately made SAR data\nsusceptible to malicious alterations, such as local editing applied to the\nimages for inserting or covering the presence of sensitive targets.\nVulnerability is further emphasized by the fact that most SAR products, despite\ntheir original complex nature, are often released as amplitude-only\ninformation, allowing even inexperienced attackers to edit and easily alter the\npixel content. To contrast malicious manipulations, in the last years the\nforensic community has begun to dig into the SAR manipulation issue, proposing\ndetectors that effectively localize the tampering traces in amplitude images.\nNonetheless, in this paper we demonstrate that an expert practitioner can\nexploit the complex nature of SAR data to obscure any signs of manipulation\nwithin a locally altered amplitude image. We refer to this approach as a\ncounter-forensic attack. To achieve the concealment of manipulation traces, the\nattacker can simulate a re-acquisition of the manipulated scene by the SAR\nsystem that initially generated the pristine image. In doing so, the attacker\ncan obscure any evidence of manipulation, making it appear as if the image was\nlegitimately produced by the system. This attack has unique features that make\nit both highly generalizable and relatively easy to apply. First, it is a\nblack-box attack, meaning it is not designed to deceive a specific forensic\ndetector. Furthermore, it does not require a training phase and is not based on\nadversarial operations. We assess the effectiveness of the proposed\ncounter-forensic approach across diverse scenarios, examining various\nmanipulation operations.\n","authors":["Sara Mandelli","Edoardo Daniele Cannas","Paolo Bestagini","Stefano Tebaldini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2407.07041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11190v1","updated":"2025-03-14T08:34:28Z","published":"2025-03-14T08:34:28Z","title":"Cross-Modal Learning for Music-to-Music-Video Description Generation","summary":"  Music-to-music-video generation is a challenging task due to the intrinsic\ndifferences between the music and video modalities. The advent of powerful\ntext-to-video diffusion models has opened a promising pathway for music-video\n(MV) generation by first addressing the music-to-MV description task and\nsubsequently leveraging these models for video generation. In this study, we\nfocus on the MV description generation task and propose a comprehensive\npipeline encompassing training data construction and multimodal model\nfine-tuning. We fine-tune existing pre-trained multimodal models on our newly\nconstructed music-to-MV description dataset based on the Music4All dataset,\nwhich integrates both musical and visual information. Our experimental results\ndemonstrate that music representations can be effectively mapped to textual\ndomains, enabling the generation of meaningful MV description directly from\nmusic inputs. We also identify key components in the dataset construction\npipeline that critically impact the quality of MV description and highlight\nspecific musical attributes that warrant greater focus for improved MV\ndescription generation.\n","authors":["Zhuoyuan Mao","Mengjie Zhao","Qiyu Wu","Zhi Zhong","Wei-Hsiang Liao","Hiromi Wakaki","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2503.11190v1.pdf","comment":"Accepted by RepL4NLP 2025 @ NAACL 2025"},{"id":"http://arxiv.org/abs/2503.11026v1","updated":"2025-03-14T02:48:43Z","published":"2025-03-14T02:48:43Z","title":"MAVFlow: Preserving Paralinguistic Elements with Conditional Flow\n  Matching for Zero-Shot AV2AV Multilingual Translation","summary":"  Despite recent advances in text-to-speech (TTS) models, audio-visual to\naudio-visual (AV2AV) translation still faces a critical challenge: maintaining\nspeaker consistency between the original and translated vocal and facial\nfeatures. To address this issue, we propose a conditional flow matching (CFM)\nzero-shot audio-visual renderer that utilizes strong dual guidance from both\naudio and visual modalities. By leveraging multi-modal guidance with CFM, our\nmodel robustly preserves speaker-specific characteristics and significantly\nenhances zero-shot AV2AV translation abilities. For the audio modality, we\nenhance the CFM process by integrating robust speaker embeddings with\nx-vectors, which serve to bolster speaker consistency. Additionally, we convey\nemotional nuances to the face rendering module. The guidance provided by both\naudio and visual cues remains independent of semantic or linguistic content,\nallowing our renderer to effectively handle zero-shot translation tasks for\nmonolingual speakers in different languages. We empirically demonstrate that\nthe inclusion of high-quality mel-spectrograms conditioned on facial\ninformation not only enhances the quality of the synthesized speech but also\npositively influences facial generation, leading to overall performance\nimprovements.\n","authors":["Sungwoo Cho","Jeongsoo Choi","Sungnyun Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2503.11026v1.pdf","comment":"Preliminary work"}]},"2025-03-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.13447v1","updated":"2025-03-17T17:59:54Z","published":"2025-03-17T17:59:54Z","title":"MetaScale: Test-Time Scaling with Evolving Meta-Thoughts","summary":"  One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.\n","authors":["Qin Liu","Wenxuan Zhou","Nan Xu","James Y. Huang","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2503.13447v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.13445v1","updated":"2025-03-17T17:59:39Z","published":"2025-03-17T17:59:39Z","title":"Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is\n  Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance","summary":"  As large language models (LLMs) become increasingly capable, ensuring that\ntheir self-generated explanations are faithful to their internal\ndecision-making process is critical for safety and oversight. In this work, we\nconduct a comprehensive counterfactual faithfulness analysis across 62 models\nfrom 8 families, encompassing both pretrained and instruction-tuned variants\nand significantly extending prior studies of counterfactual tests. We introduce\nphi-CCT, a simplified variant of the Correlational Counterfactual Test, which\navoids the need for token probabilities while explaining most of the variance\nof the original test. Our findings reveal clear scaling trends: larger models\nare consistently more faithful on our metrics. However, when comparing\ninstruction-tuned and human-imitated explanations, we find that observed\ndifferences in faithfulness can often be attributed to explanation verbosity,\nleading to shifts along the true-positive/false-positive Pareto frontier. While\ninstruction-tuning and prompting can influence this trade-off, we find limited\nevidence that they fundamentally expand the frontier of explanatory\nfaithfulness beyond what is achievable with pretrained models of comparable\nsize. Our analysis highlights the nuanced relationship between\ninstruction-tuning, verbosity, and the faithful representation of model\ndecision processes.\n","authors":["Noah Y. Siegel","Nicolas Heess","Maria Perez-Ortiz","Oana-Maria Camburu"],"pdf_url":"https://arxiv.org/pdf/2503.13445v1.pdf","comment":"38 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.18573v2","updated":"2025-03-17T17:58:13Z","published":"2024-12-24T17:56:08Z","title":"Top General Performance = Top Domain Performance? DomainCodeBench: A\n  Multi-domain Code Generation Benchmark","summary":"  With the rapid advancement of large language models (LLMs), extensive\nresearch has been conducted to investigate the code generation capabilities of\nLLMs. However, existing efforts primarily focus on general-domain tasks,\nleaving LLMs' code generation performance in real-world application domains\nunderexplored. This raises a critical question: can a model's general-domain\ncoding ability reliably represent its ability in specialized domains? In this\npaper, we introduce DomainCodeBench, a multi-domain code generation benchmark\ndesigned to systematically evaluate LLMs across 12 software application domains\nand 15 programming languages. DomainCodeBench contains 2,400 manually verified\ntasks with ground truth, human-annotated docstrings, and fine-grained\ndependency information to ensure more coverage of domain-specific challenges.\nSpecifically, we first identify the most popular application domains by topic\nmining. Then, we curate coding tasks based on commonly used frameworks and\nplatforms in each domain. We obtain several findings through extensive\nexperiments on DomainCodeBench with ten mainstream LLMs. (1) Performance\ndecoupling: experiments reveal that top general-domain models do not\nconsistently excel in specific application domains; (2) Domain-specific\nweaknesses: LLMs often fail due to domain knowledge gaps and third-party\nlibrary misusage; (3) Contextual enhancement: we show that augmenting prompts\nwith domain-specific knowledge improves performance by around 38.17%, providing\nactionable insights for performance optimization. Our replication package,\nincluding the benchmark, source code, and experimental results, is available at\nhttps://github.com/DeepSoftwareAnalytics/DomainCodeBench.\n","authors":["Dewu Zheng","Yanlin Wang","Ensheng Shi","Xilin Liu","Yuchi Ma","Hongyu Zhang","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2412.18573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13423v1","updated":"2025-03-17T17:53:23Z","published":"2025-03-17T17:53:23Z","title":"SuperBPE: Space Travel for Language Models","summary":"  The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.\n","authors":["Alisa Liu","Jonathan Hayase","Valentin Hofmann","Sewoong Oh","Noah A. Smith","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2503.13423v1.pdf","comment":"preprint, code and artifacts will become available at\n  https://superbpe.github.io/"},{"id":"http://arxiv.org/abs/2503.13413v1","updated":"2025-03-17T17:42:51Z","published":"2025-03-17T17:42:51Z","title":"DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective","summary":"  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.\n","authors":["Dengyun Peng","Yuhang Zhou","Qiguang Chen","Jinhao Liu","Jingjing Chen","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2503.13413v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.13401v1","updated":"2025-03-17T17:33:54Z","published":"2025-03-17T17:33:54Z","title":"Using the Tools of Cognitive Science to Understand Large Language Models\n  at Different Levels of Analysis","summary":"  Modern artificial intelligence systems, such as large language models, are\nincreasingly powerful but also increasingly hard to understand. Recognizing\nthis problem as analogous to the historical difficulties in understanding the\nhuman mind, we argue that methods developed in cognitive science can be useful\nfor understanding large language models. We propose a framework for applying\nthese methods based on Marr's three levels of analysis. By revisiting\nestablished cognitive science techniques relevant to each level and\nillustrating their potential to yield insights into the behavior and internal\norganization of large language models, we aim to provide a toolkit for making\nsense of these new kinds of minds.\n","authors":["Alexander Ku","Declan Campbell","Xuechunzi Bai","Jiayi Geng","Ryan Liu","Raja Marjieh","R. Thomas McCoy","Andrew Nam","Ilia Sucholutsky","Veniamin Veselovsky","Liyi Zhang","Jian-Qiao Zhu","Thomas L. Griffiths"],"pdf_url":"https://arxiv.org/pdf/2503.13401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13399v1","updated":"2025-03-17T17:33:10Z","published":"2025-03-17T17:33:10Z","title":"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research","summary":"  Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.\n","authors":["James Burgess","Jeffrey J Nirschl","Laura Bravo-Sánchez","Alejandro Lozano","Sanket Rajan Gupte","Jesus G. Galaz-Montoya","Yuhui Zhang","Yuchang Su","Disha Bhowmik","Zachary Coman","Sarina M. Hasan","Alexandra Johannesson","William D. Leineweber","Malvika G Nair","Ridhi Yarlagadda","Connor Zuraski","Wah Chiu","Sarah Cohen","Jan N. Hansen","Manuel D Leonetti","Chad Liu","Emma Lundberg","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2503.13399v1.pdf","comment":"CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa"},{"id":"http://arxiv.org/abs/2503.13390v1","updated":"2025-03-17T17:23:50Z","published":"2025-03-17T17:23:50Z","title":"Aligned Probing: Relating Toxic Behavior and Model Internals","summary":"  We introduce aligned probing, a novel interpretability framework that aligns\nthe behavior of language models (LMs), based on their outputs, and their\ninternal representations (internals). Using this framework, we examine over 20\nOLMo, Llama, and Mistral models, bridging behavioral and internal perspectives\nfor toxicity for the first time. Our results show that LMs strongly encode\ninformation about the toxicity level of inputs and subsequent outputs,\nparticularly in lower layers. Focusing on how unique LMs differ offers both\ncorrelative and causal evidence that they generate less toxic output when\nstrongly encoding information about the input toxicity. We also highlight the\nheterogeneity of toxicity, as model behavior and internals vary across unique\nattributes such as Threat. Finally, four case studies analyzing detoxification,\nmulti-prompt evaluations, model quantization, and pre-training dynamics\nunderline the practical impact of aligned probing with further concrete\ninsights. Our findings contribute to a more holistic understanding of LMs, both\nwithin and beyond the context of toxicity.\n","authors":["Andreas Waldis","Vagrant Gautam","Anne Lauscher","Dietrich Klakow","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2503.13390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01586v4","updated":"2025-03-17T17:17:16Z","published":"2024-09-03T03:59:22Z","title":"Booster: Tackling Harmful Fine-tuning for Large Language Models via\n  Attenuating Harmful Perturbation","summary":"  Harmful fine-tuning attack poses serious safety concerns for large language\nmodels' fine-tuning-as-a-service. While existing defenses have been proposed to\nmitigate the issue, their performances are still far away from satisfactory,\nand the root cause of the problem has not been fully recovered. To this end, we\nin this paper show that harmful perturbation over the model weights could be a\nprobable cause of alignment-broken. In order to attenuate the negative impact\nof harmful perturbation, we propose an alignment-stage solution, dubbed\nBooster. Technically, along with the original alignment loss, we append a loss\nregularizer in the alignment stage's optimization. The regularizer ensures that\nthe model's harmful loss reduction after the simulated harmful perturbation is\nattenuated, thereby mitigating the subsequent fine-tuning risk. Empirical\nresults show that Booster can effectively reduce the harmful score of the\nfine-tuned models while maintaining the performance of downstream tasks. Our\ncode is available at https://github.com/git-disl/Booster.\n","authors":["Tiansheng Huang","Sihao Hu","Fatih Ilhan","Selim Furkan Tekin","Ling Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01586v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13377v1","updated":"2025-03-17T17:04:20Z","published":"2025-03-17T17:04:20Z","title":"TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM","summary":"  We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero.\n","authors":["Ye Wang","Boshen Xu","Zihao Yue","Zihan Xiao","Ziheng Wang","Liang Zhang","Dingyi Yang","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2503.13377v1.pdf","comment":"Code: https://github.com/www-Ye/TimeZero"},{"id":"http://arxiv.org/abs/2412.04626v2","updated":"2025-03-17T16:32:24Z","published":"2024-12-05T21:41:20Z","title":"BigDocs: An Open Dataset for Training Multimodal Models on Document and\n  Code Tasks","summary":"  Multimodal AI has the potential to significantly enhance\ndocument-understanding tasks, such as processing receipts, understanding\nworkflows, extracting data from documents, and summarizing reports. Code\ngeneration tasks that require long-structured outputs can also be enhanced by\nmultimodality. Despite this, their use in commercial applications is often\nlimited due to limited access to training data and restrictive licensing, which\nhinders open access. To address these limitations, we introduce BigDocs-7.5M, a\nhigh-quality, open-access dataset comprising 7.5 million multimodal documents\nacross 30 tasks. We use an efficient data curation process to ensure our data\nis high-quality and license-permissive. Our process emphasizes accountability,\nresponsibility, and transparency through filtering rules, traceable metadata,\nand careful content analysis. Additionally, we introduce BigDocs-Bench, a\nbenchmark suite with 10 novel tasks where we create datasets that reflect\nreal-world use cases involving reasoning over Graphical User Interfaces (GUI)\nand code generation from images. Our experiments show that training with\nBigDocs-Bench improves average performance up to 25.8% over closed-source\nGPT-4o in document reasoning and structured output tasks such as\nScreenshot2HTML or Image2Latex generation. Finally, human evaluations showed a\npreference for outputs from models trained on BigDocs over GPT-4o. This\nsuggests that BigDocs can help both academics and the open-source community\nutilize and improve AI tools to enhance multimodal capabilities and document\nreasoning. The project is hosted at https://bigdocs.github.io .\n","authors":["Juan Rodriguez","Xiangru Jian","Siba Smarak Panigrahi","Tianyu Zhang","Aarash Feizi","Abhay Puri","Akshay Kalkunte","François Savard","Ahmed Masry","Shravan Nayak","Rabiul Awal","Mahsa Massoud","Amirhossein Abaskohi","Zichao Li","Suyuchen Wang","Pierre-André Noël","Mats Leon Richter","Saverio Vadacchino","Shubham Agarwal","Sanket Biswas","Sara Shanian","Ying Zhang","Noah Bolger","Kurt MacDonald","Simon Fauvel","Sathwik Tejaswi","Srinivas Sunkara","Joao Monteiro","Krishnamurthy DJ Dvijotham","Torsten Scholak","Nicolas Chapados","Sepideh Kharagani","Sean Hughes","M. Özsu","Siva Reddy","Marco Pedersoli","Yoshua Bengio","Christopher Pal","Issam Laradji","Spandana Gella","Perouz Taslakian","David Vazquez","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2412.04626v2.pdf","comment":"The project is hosted at https://bigdocs.github.io"},{"id":"http://arxiv.org/abs/2503.13342v1","updated":"2025-03-17T16:21:10Z","published":"2025-03-17T16:21:10Z","title":"Valid Text-to-SQL Generation with Unification-based DeepStochLog","summary":"  Large language models have been used to translate natural language questions\nto SQL queries. Without hard constraints on syntax and database schema, they\noccasionally produce invalid queries that are not executable. These failures\nlimit the usage of these systems in real-life scenarios. We propose a\nneurosymbolic framework that imposes SQL syntax and schema constraints with\nunification-based definite clause grammars and thus guarantees the generation\nof valid queries. Our framework also builds a bi-directional interface to\nlanguage models to leverage their natural language understanding abilities. The\nevaluation results on a subset of SQL grammars show that all our output queries\nare valid. This work is the first step towards extending language models with\nunification-based grammars. We demonstrate this extension enhances the\nvalidity, execution accuracy, and ground truth alignment of the underlying\nlanguage model by a large margin. Our code is available at\nhttps://github.com/ML-KULeuven/deepstochlog-lm.\n","authors":["Ying Jiao","Luc De Raedt","Giuseppe Marra"],"pdf_url":"https://arxiv.org/pdf/2503.13342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13335v1","updated":"2025-03-17T16:15:02Z","published":"2025-03-17T16:15:02Z","title":"Reliable and Efficient Amortized Model-based Evaluation","summary":"  Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.\n","authors":["Sang Truong","Yuheng Tu","Percy Liang","Bo Li","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2503.13335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07763v2","updated":"2025-03-17T16:10:45Z","published":"2024-11-12T12:52:17Z","title":"Spider 2.0: Evaluating Language Models on Real-World Enterprise\n  Text-to-SQL Workflows","summary":"  Real-world enterprise text-to-SQL workflows often involve complex cloud or\nlocal data across various database systems, multiple SQL queries in various\ndialects, and diverse operations from data transformation to analytics. We\nintroduce Spider 2.0, an evaluation framework comprising 632 real-world\ntext-to-SQL workflow problems derived from enterprise-level database use cases.\nThe databases in Spider 2.0 are sourced from real data applications, often\ncontaining over 1,000 columns and stored in local or cloud database systems\nsuch as BigQuery and Snowflake. We show that solving problems in Spider 2.0\nfrequently requires understanding and searching through database metadata,\ndialect documentation, and even project-level codebases. This challenge calls\nfor models to interact with complex SQL workflow environments, process\nextremely long contexts, perform intricate reasoning, and generate multiple SQL\nqueries with diverse operations, often exceeding 100 lines, which goes far\nbeyond traditional text-to-SQL challenges. Our evaluations indicate that based\non o1-preview, our code agent framework successfully solves only 21.3% of the\ntasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on\nSpider 2.0 show that while language models have demonstrated remarkable\nperformance in code generation -- especially in prior text-to-SQL benchmarks --\nthey require significant improvement in order to achieve adequate performance\nfor real-world enterprise usage. Progress on Spider 2.0 represents crucial\nsteps towards developing intelligent, autonomous, code agents for real-world\nenterprise settings. Our code, baseline models, and data are available at\nhttps://spider2-sql.github.io\n","authors":["Fangyu Lei","Jixuan Chen","Yuxiao Ye","Ruisheng Cao","Dongchan Shin","Hongjin Su","Zhaoqing Suo","Hongcheng Gao","Wenjing Hu","Pengcheng Yin","Victor Zhong","Caiming Xiong","Ruoxi Sun","Qian Liu","Sida Wang","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2411.07763v2.pdf","comment":"ICLR 2025 Oral"},{"id":"http://arxiv.org/abs/2503.13305v1","updated":"2025-03-17T15:47:37Z","published":"2025-03-17T15:47:37Z","title":"Computation Mechanism Behind LLM Position Generalization","summary":"  Most written natural languages are composed of sequences of words and\nsentences. Similar to humans, large language models (LLMs) exhibit flexibility\nin handling textual positions - a phenomenon we term position generalization.\nThey can understand texts with position perturbations and generalize to longer\ntexts than those encountered during training with the latest techniques. These\nphenomena suggest that LLMs handle positions tolerantly, but how LLMs\ncomputationally process positional relevance remains largely unexplored. This\nwork connects the linguistic phenomenon with LLMs' computational mechanisms. We\nshow how LLMs enforce certain computational mechanisms for the aforementioned\ntolerance in position perturbations. Despite the complex design of the\nself-attention mechanism, this work reveals that LLMs learn a counterintuitive\ndisentanglement of attention logits. Their values show a 0.959 linear\ncorrelation with an approximation of the arithmetic sum of positional relevance\nand semantic importance. Furthermore, we identify a prevalent pattern in\nintermediate features, which we prove theoretically enables this effect. The\npattern, which is different from how randomly initialized parameters would\nbehave, suggests that it is a learned behavior rather than a natural result of\nthe model architecture. Based on these findings, we provide computational\nexplanations and criteria for LLMs' position flexibilities. This work takes a\npioneering step in linking position generalization with modern LLMs' internal\nmechanisms.\n","authors":["Chi Han","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2503.13305v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2503.13299v1","updated":"2025-03-17T15:44:09Z","published":"2025-03-17T15:44:09Z","title":"A Survey on Transformer Context Extension: Approaches and Evaluation","summary":"  Large language models (LLMs) based on Transformer have been widely applied in\nthe filed of natural language processing (NLP), demonstrating strong\nperformance, particularly in handling short text tasks. However, when it comes\nto long context scenarios, the performance of LLMs degrades due to some\nchallenges. To alleviate this phenomenon, there is a number of work proposed\nrecently. In this survey, we first list the challenges of applying pre-trained\nLLMs to process long contexts. Then systematically review the approaches\nrelated to long context and propose our taxonomy categorizing them into four\nmain types: positional encoding, context compression, retrieval augmented, and\nattention pattern. In addition to the approaches, we focus on the evaluation of\nlong context, organizing relevant data, tasks, and metrics based on existing\nlong context benchmarks. Finally, we summarize unresolved issues in the long\ncontext domain and put forward our views on future developments.\n","authors":["Yijun Liu","Jinzheng Yu","Yang Xu","Zhongyang Li","Qingfu Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.13299v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2503.13288v1","updated":"2025-03-17T15:38:33Z","published":"2025-03-17T15:38:33Z","title":"$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation","summary":"  Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.\n","authors":["Fangzhi Xu","Hang Yan","Chang Ma","Haiteng Zhao","Jun Liu","Qika Lin","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2503.13288v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.01611v2","updated":"2025-03-17T15:32:53Z","published":"2025-03-03T14:47:23Z","title":"In-context Learning vs. Instruction Tuning: The Case of Small and\n  Multilingual Language Models","summary":"  Instruction following is a critical ability for Large Language Models to\nperform downstream tasks. The standard approach to instruction alignment has\nrelied on a specific phase of model tuning over curated instruction datasets,\noptionally complemented with an alignment step over human preferences. Recent\nwork has shown the potential of in-context learning (ICL) alternatives to guide\nbase models towards instruction following. This type of approach is\nparticularly relevant to extend instruction following across languages and\nmodels of varying sizes adapted to different types of usage. In this work we\ncompare ICL and instruction fine-tuning in English, French and Spanish, on\nSmall Language Models, and provide experimental results on applying Direct\nPreference Optimisation (DPO) over base models. Our results show that scenarios\ninvolving multilingual and smaller models result in downgraded ICL instruction\nfollowing performance, only partially mitigated by DPO alignment. This study\naims to further our understanding of current strengths and limitations of\nalternative methods for instruction following.\n","authors":["David Ponce","Thierry Etchegoyhen"],"pdf_url":"https://arxiv.org/pdf/2503.01611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13281v1","updated":"2025-03-17T15:31:55Z","published":"2025-03-17T15:31:55Z","title":"LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation","summary":"  Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models,\ncomparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models.\nLLM-Match outperformed all baselines.\n","authors":["Xiaodi Li","Shaika Chowdhury","Chung Il Wi","Maria Vassilaki","Ken Liu","Terence T Sio","Owen Garrick","Young J Juhn","James R Cerhan","Cui Tao","Nansu Zong"],"pdf_url":"https://arxiv.org/pdf/2503.13281v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2503.13262v1","updated":"2025-03-17T15:16:59Z","published":"2025-03-17T15:16:59Z","title":"TablePilot; Recommending Human-Preferred Tabular Data Analysis with\n  Large Language Models","summary":"  Tabular data analysis is crucial in many scenarios, yet efficiently\nidentifying the most relevant data analysis queries and results for a new table\nremains a significant challenge. The complexity of tabular data, diverse\nanalytical operations, and the demand for high-quality analysis make the\nprocess tedious. To address these challenges, we aim to recommend\nquery-code-result triplets tailored for new tables in tabular data analysis\nworkflows. In this paper, we present TablePilot, a pioneering tabular data\nanalysis framework leveraging large language models to autonomously generate\ncomprehensive and superior analytical results without relying on user profiles\nor prior interactions. The framework incorporates key designs in analysis\npreparation and analysis optimization to enhance accuracy. Additionally, we\npropose Rec-Align, a novel method to further improve recommendation quality and\nbetter align with human preferences. Experiments on DART, a dataset\nspecifically designed for comprehensive tabular data analysis recommendation,\ndemonstrate the effectiveness of our framework. Based on GPT-4o, the tuned\nTablePilot achieves 77.0% top-5 recommendation recall. Human evaluations\nfurther highlight its effectiveness in optimizing tabular data analysis\nworkflows.\n","authors":["Deyin Yi","Yihao Liu","Lang Cao","Mengyu Zhou","Haoyu Dong","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07107v2","updated":"2025-03-17T14:51:27Z","published":"2024-11-11T16:33:25Z","title":"Training Neural Networks as Recognizers of Formal Languages","summary":"  Characterizing the computational power of neural network architectures in\nterms of formal language theory remains a crucial line of research, as it\ndescribes lower and upper bounds on the reasoning capabilities of modern AI.\nHowever, when empirically testing these bounds, existing work often leaves a\ndiscrepancy between experiments and the formal claims they are meant to\nsupport. The problem is that formal language theory pertains specifically to\nrecognizers: machines that receive a string as input and classify whether it\nbelongs to a language. On the other hand, it is common instead to evaluate\nlanguage models on proxy tasks, e.g., language modeling or sequence-to-sequence\ntransduction, that are similar in only an informal sense to the underlying\ntheory. We correct this mismatch by training and evaluating neural networks\ndirectly as binary classifiers of strings, using a general method that can be\napplied to a wide variety of languages. As part of this, we extend an algorithm\nrecently proposed by Sn{\\ae}bjarnarson et al. (2025) for efficient\nlength-controlled sampling of strings from regular languages. We provide\nresults on a variety of languages across the Chomsky hierarchy for three neural\narchitectures: a simple RNN, an LSTM, and a causally-masked transformer. We\nfind that the RNN and LSTM often outperform the transformer, and that auxiliary\ntraining objectives such as language modeling can help, although no single\nobjective uniformly improves performance across languages and architectures.\nOur contributions will facilitate theoretically sound empirical testing of\nlanguage recognition claims in future work. We have released our datasets as a\nbenchmark called FLaRe (Formal Language Recognition), along with our code.\n","authors":["Alexandra Butoi","Ghazal Khalighinejad","Anej Svete","Josef Valvoda","Ryan Cotterell","Brian DuSell"],"pdf_url":"https://arxiv.org/pdf/2411.07107v2.pdf","comment":"44 pages, 3 figures. ICLR 2025"},{"id":"http://arxiv.org/abs/2501.14892v2","updated":"2025-03-17T14:32:08Z","published":"2025-01-24T19:31:06Z","title":"Causal Graphs Meet Thoughts: Enhancing Complex Reasoning in\n  Graph-Augmented LLMs","summary":"  In knowledge-intensive tasks, especially in high-stakes domains like medicine\nand law, it is critical not only to retrieve relevant information but also to\nprovide causal reasoning and explainability. Large language models (LLMs) have\nachieved remarkable performance in natural language understanding and\ngeneration tasks. However, they often suffer from limitations such as\ndifficulty in incorporating new knowledge, generating hallucinations, and\nexplaining their reasoning process. To address these challenges, integrating\nknowledge graphs with Graph Retrieval-Augmented Generation (Graph RAG) has\nemerged as an effective solution. Traditional Graph RAG methods often rely on\nsimple graph traversal or semantic similarity, which do not capture causal\nrelationships or align well with the model's internal reasoning steps. This\npaper proposes a novel pipeline that filters large knowledge graphs to\nemphasize cause-effect edges, aligns the retrieval process with the model's\nchain-of-thought (CoT), and enhances reasoning through multi-stage path\nimprovements. Experiments on medical question-answering tasks show consistent\ngains, with up to a 10\\% absolute improvement across multiple large language\nmodels (LLMs). This approach demonstrates the value of combining causal\nreasoning with stepwise retrieval, leading to more interpretable and logically\ngrounded solutions for complex queries.\n","authors":["Hang Luo","Jian Zhang","Chujun Li"],"pdf_url":"https://arxiv.org/pdf/2501.14892v2.pdf","comment":"18 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.13222v1","updated":"2025-03-17T14:31:37Z","published":"2025-03-17T14:31:37Z","title":"Can Language Models Follow Multiple Turns of Entangled Instructions?","summary":"  Despite significant achievements in improving the instruction-following\ncapabilities of large language models (LLMs), the ability to process multiple\npotentially entangled or conflicting instructions remains a considerable\nchallenge. Real-world scenarios often require consistency across multiple\ninstructions over time, such as secret privacy, personal preferences, and\nprioritization, which demand sophisticated abilities to integrate multiple\nturns and carefully balance competing objectives when instructions intersect or\nconflict. This work presents a systematic investigation of LLMs' capabilities\nin handling multiple turns of instructions, covering three levels of\ndifficulty: (1) retrieving information from instructions, (2) tracking and\nreasoning across turns, and (3) resolving conflicts among instructions. We\nconstruct MultiTurnInstruct with around 1.1K high-quality multi-turn\nconversations through the human-in-the-loop approach and result in nine\ncapability categories, including statics and dynamics, reasoning, and\nmultitasking. Our finding reveals an intriguing trade-off between different\ncapabilities. While GPT models demonstrate superior memorization, they show\nreduced effectiveness in privacy-protection tasks requiring selective\ninformation withholding. Larger models exhibit stronger reasoning capabilities\nbut still struggle with resolving conflicting instructions. Importantly, these\nperformance gaps cannot be attributed solely to information loss, as models\ndemonstrate strong BLEU scores on memorization tasks but their attention\nmechanisms fail to integrate multiple related instructions effectively. These\nfindings highlight critical areas for improvement in complex real-world tasks\ninvolving multi-turn instructions.\n","authors":["Chi Han"],"pdf_url":"https://arxiv.org/pdf/2503.13222v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2503.13208v1","updated":"2025-03-17T14:20:48Z","published":"2025-03-17T14:20:48Z","title":"Improving Complex Reasoning with Dynamic Prompt Corruption: A soft\n  prompt Optimization Approach","summary":"  Prompt-tuning (PT) for large language models (LLMs) can facilitate the\nperformance on various conventional NLP tasks with significantly fewer\ntrainable parameters. However, our investigation reveals that PT provides\nlimited improvement and may even degrade the primitive performance of LLMs on\ncomplex reasoning tasks. Such a phenomenon suggests that soft prompts can\npositively impact certain instances while negatively affecting others,\nparticularly during the later phases of reasoning. To address these challenges,\nWe first identify an information accumulation within the soft prompts. Through\ndetailed analysis, we demonstrate that this phenomenon is often accompanied by\nerroneous information flow patterns in the deeper layers of the model, which\nultimately lead to incorrect reasoning outcomes. we propose a novel method\ncalled \\textbf{D}ynamic \\textbf{P}rompt \\textbf{C}orruption (DPC) to take\nbetter advantage of soft prompts in complex reasoning tasks, which dynamically\nadjusts the influence of soft prompts based on their impact on the reasoning\nprocess. Specifically, DPC consists of two stages: Dynamic Trigger and Dynamic\nCorruption. First, Dynamic Trigger measures the impact of soft prompts,\nidentifying whether beneficial or detrimental. Then, Dynamic Corruption\nmitigates the negative effects of soft prompts by selectively masking key\ntokens that interfere with the reasoning process. We validate the proposed\napproach through extensive experiments on various LLMs and reasoning tasks,\nincluding GSM8K, MATH, and AQuA. Experimental results demonstrate that DPC can\nconsistently enhance the performance of PT, achieving 4\\%-8\\% accuracy gains\ncompared to vanilla prompt tuning, highlighting the effectiveness of our\napproach and its potential to enhance complex reasoning in LLMs.\n","authors":["Sinan Fan","Liang Xie","Chen Shen","Ge Teng","Xiaosong Yuan","Xiaofeng Zhang","Chenxi Huang","Wenxiao Wang","Xiaofei He","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2503.13208v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13205v1","updated":"2025-03-17T14:14:28Z","published":"2025-03-17T14:14:28Z","title":"MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for\n  Inpatient Pathways","summary":"  Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.\n","authors":["Zhen Chen","Zhihao Peng","Xusheng Liang","Cheng Wang","Peigan Liang","Linsheng Zeng","Minjie Ju","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.13205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04927v3","updated":"2025-03-17T13:34:07Z","published":"2024-06-07T13:33:22Z","title":"LLM-based speaker diarization correction: A generalizable approach","summary":"  Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth.\n","authors":["Georgios Efstathiadis","Vijay Yadav","Anzar Abbas"],"pdf_url":"https://arxiv.org/pdf/2406.04927v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13149v1","updated":"2025-03-17T13:20:09Z","published":"2025-03-17T13:20:09Z","title":"Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool\n  for Perceived Socio-Economic Bias in LLMs","summary":"  We introduce an Item Response Theory (IRT)-based framework to detect and\nquantify socioeconomic bias in large language models (LLMs) without relying on\nsubjective human judgments. Unlike traditional methods, IRT accounts for item\ndifficulty, improving ideological bias estimation. We fine-tune two LLM\nfamilies (Meta-LLaMa 3.2-1B-Instruct and Chat- GPT 3.5) to represent distinct\nideological positions and introduce a two-stage approach: (1) modeling response\navoidance and (2) estimating perceived bias in answered responses. Our results\nshow that off-the-shelf LLMs often avoid ideological engagement rather than\nexhibit bias, challenging prior claims of partisanship. This empirically\nvalidated framework enhances AI alignment research and promotes fairer AI\ngovernance.\n","authors":["Jasmin Wachter","Michael Radloff","Maja Smolej","Katharina Kinder-Kurlanda"],"pdf_url":"https://arxiv.org/pdf/2503.13149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13139v1","updated":"2025-03-17T13:07:34Z","published":"2025-03-17T13:07:34Z","title":"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical\n  Verification for Long Video Understanding","summary":"  Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to ``finding a needle in a haystack.''\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable.\n","authors":["Weiyu Guo","Ziyang Chen","Shaoguang Wang","Jianxiang He","Yijie Xu","Jinhui Ye","Ying Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.13139v1.pdf","comment":"18 pages, under review"},{"id":"http://arxiv.org/abs/2503.13111v1","updated":"2025-03-17T12:34:22Z","published":"2025-03-17T12:34:22Z","title":"MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.\n","authors":["Erik Daxberger","Nina Wenzel","David Griffiths","Haiming Gang","Justin Lazarow","Gefen Kohavi","Kai Kang","Marcin Eichner","Yinfei Yang","Afshin Dehghan","Peter Grasch"],"pdf_url":"https://arxiv.org/pdf/2503.13111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13109v1","updated":"2025-03-17T12:33:26Z","published":"2025-03-17T12:33:26Z","title":"Code-Driven Inductive Synthesis: Enhancing Reasoning Abilities of Large\n  Language Models with Sequences","summary":"  Large language models make remarkable progress in reasoning capabilities.\nExisting works focus mainly on deductive reasoning tasks (e.g., code and math),\nwhile another type of reasoning mode that better aligns with human learning,\ninductive reasoning, is not well studied. We attribute the reason to the fact\nthat obtaining high-quality process supervision data is challenging for\ninductive reasoning. Towards this end, we novelly employ number sequences as\nthe source of inductive reasoning data. We package sequences into algorithmic\nproblems to find the general term of each sequence through a code solution. In\nthis way, we can verify whether the code solution holds for any term in the\ncurrent sequence, and inject case-based supervision signals by using code unit\ntests. We build a sequence synthetic data pipeline and form a training dataset\nCodeSeq. Experimental results show that the models tuned with CodeSeq improve\non both code and comprehensive reasoning benchmarks.\n","authors":["Kedi Chen","Zhikai Lei","Fan Zhang","Yinqi Zhang","Qin Chen","Jie Zhou","Liang He","Qipeng Guo","Kai Chen","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10880v2","updated":"2025-03-17T12:29:05Z","published":"2024-10-09T15:36:42Z","title":"Fine-tuning can Help Detect Pretraining Data from Large Language Models","summary":"  In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.\n","authors":["Hengxiang Zhang","Songxin Zhang","Bingyi Jing","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2410.10880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13102v1","updated":"2025-03-17T12:15:16Z","published":"2025-03-17T12:15:16Z","title":"REPA: Russian Error Types Annotation for Evaluating Text Generation and\n  Judgment Capabilities","summary":"  Recent advances in large language models (LLMs) have introduced the novel\nparadigm of using LLMs as judges, where an LLM evaluates and scores the outputs\nof another LLM, which often correlates highly with human preferences. However,\nthe use of LLM-as-a-judge has been primarily studied in English. In this paper,\nwe evaluate this framework in Russian by introducing the Russian Error tyPes\nAnnotation dataset (REPA), a dataset of 1k user queries and 2k LLM-generated\nresponses. Human annotators labeled each response pair expressing their\npreferences across ten specific error types, as well as selecting an overall\npreference. We rank six generative LLMs across the error types using three\nrating systems based on human preferences. We also evaluate responses using\neight LLM judges in zero-shot and few-shot settings. We describe the results of\nanalyzing the judges and position and length biases. Our findings reveal a\nnotable gap between LLM judge performance in Russian and English. However,\nrankings based on human and LLM preferences show partial alignment, suggesting\nthat while current LLM judges struggle with fine-grained evaluation in Russian,\nthere is potential for improvement.\n","authors":["Alexander Pugachev","Alena Fenogenova","Vladislav Mikhailov","Ekaterina Artemova"],"pdf_url":"https://arxiv.org/pdf/2503.13102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13101v1","updated":"2025-03-17T12:13:37Z","published":"2025-03-17T12:13:37Z","title":"Who Wrote This? Identifying Machine vs Human-Generated Text in Hausa","summary":"  The advancement of large language models (LLMs) has allowed them to be\nproficient in various tasks, including content generation. However, their\nunregulated usage can lead to malicious activities such as plagiarism and\ngenerating and spreading fake news, especially for low-resource languages. Most\nexisting machine-generated text detectors are trained on high-resource\nlanguages like English, French, etc. In this study, we developed the first\nlarge-scale detector that can distinguish between human- and machine-generated\ncontent in Hausa. We scrapped seven Hausa-language media outlets for the\nhuman-generated text and the Gemini-2.0 flash model to automatically generate\nthe corresponding Hausa-language articles based on the human-generated article\nheadlines. We fine-tuned four pre-trained Afri-centric models (AfriTeVa,\nAfriBERTa, AfroXLMR, and AfroXLMR-76L) on the resulting dataset and assessed\ntheir performance using accuracy and F1-score metrics. AfroXLMR achieved the\nhighest performance with an accuracy of 99.23% and an F1 score of 99.21%,\ndemonstrating its effectiveness for Hausa text detection. Our dataset is made\npublicly available to enable further research.\n","authors":["Babangida Sani","Aakansha Soy","Sukairaj Hafiz Imam","Ahmad Mustapha","Lukman Jibril Aliyu","Idris Abdulmumin","Ibrahim Said Ahmad","Shamsuddeen Hassan Muhammad"],"pdf_url":"https://arxiv.org/pdf/2503.13101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04032v3","updated":"2025-03-17T12:05:36Z","published":"2024-11-06T16:31:28Z","title":"Beemo: Benchmark of Expert-edited Machine-generated Outputs","summary":"  The rapid proliferation of large language models (LLMs) has increased the\nvolume of machine-generated texts (MGTs) and blurred text authorship in various\ndomains. However, most existing MGT benchmarks include single-author texts\n(human-written and machine-generated). This conventional design fails to\ncapture more practical multi-author scenarios, where the user refines the LLM\nresponse for natural flow, coherence, and factual correctness. Our paper\nintroduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),\nwhich includes 6.5k texts written by humans, generated by ten\ninstruction-finetuned LLMs, and edited by experts for various use cases,\nranging from creative writing to summarization. Beemo additionally comprises\n13.1k machine-generated and LLM-edited texts, allowing for diverse MGT\ndetection evaluation across various edit types. We document Beemo's creation\nprotocol and present the results of benchmarking 33 configurations of MGT\ndetectors in different experimental setups. We find that expert-based editing\nevades MGT detection, while LLM-edited texts are unlikely to be recognized as\nhuman-written. Beemo and all materials are publicly available.\n","authors":["Ekaterina Artemova","Jason Lucas","Saranya Venkatraman","Jooyoung Lee","Sergei Tilga","Adaku Uchendu","Vladislav Mikhailov"],"pdf_url":"https://arxiv.org/pdf/2411.04032v3.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2503.13089v1","updated":"2025-03-17T11:52:16Z","published":"2025-03-17T11:52:16Z","title":"ClusComp: A Simple Paradigm for Model Compression and Efficient\n  Finetuning","summary":"  As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.\n","authors":["Baohao Liao","Christian Herold","Seyyed Hadi Hashemi","Stefan Vasilev","Shahram Khadivi","Christof Monz"],"pdf_url":"https://arxiv.org/pdf/2503.13089v1.pdf","comment":"26 pages, 11 figures, 18 tables"},{"id":"http://arxiv.org/abs/2503.13081v1","updated":"2025-03-17T11:39:44Z","published":"2025-03-17T11:39:44Z","title":"A Framework to Assess Multilingual Vulnerabilities of LLMs","summary":"  Large Language Models (LLMs) are acquiring a wider range of capabilities,\nincluding understanding and responding in multiple languages. While they\nundergo safety training to prevent them from answering illegal questions,\nimbalances in training data and human evaluation resources can make these\nmodels more susceptible to attacks in low-resource languages (LRL). This paper\nproposes a framework to automatically assess the multilingual vulnerabilities\nof commonly used LLMs. Using our framework, we evaluated six LLMs across eight\nlanguages representing varying levels of resource availability. We validated\nthe assessments generated by our automated framework through human evaluation\nin two languages, demonstrating that the framework's results align with human\njudgments in most cases. Our findings reveal vulnerabilities in LRL; however,\nthese may pose minimal risk as they often stem from the model's poor\nperformance, resulting in incoherent responses.\n","authors":["Likai Tang","Niruth Bogahawatta","Yasod Ginige","Jiarui Xu","Shixuan Sun","Surangika Ranathunga","Suranga Seneviratne"],"pdf_url":"https://arxiv.org/pdf/2503.13081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10677v2","updated":"2025-03-17T11:24:11Z","published":"2025-03-11T01:59:35Z","title":"A Survey on Knowledge-Oriented Retrieval-Augmented Generation","summary":"  Retrieval-Augmented Generation (RAG) has gained significant attention in\nrecent years for its potential to enhance natural language understanding and\ngeneration by combining large-scale retrieval systems with generative models.\nRAG leverages external knowledge sources, such as documents, databases, or\nstructured data, to improve model performance and generate more accurate and\ncontextually relevant outputs. This survey aims to provide a comprehensive\noverview of RAG by examining its fundamental components, including retrieval\nmechanisms, generation processes, and the integration between the two. We\ndiscuss the key characteristics of RAG, such as its ability to augment\ngenerative models with dynamic external knowledge, and the challenges\nassociated with aligning retrieved information with generative objectives. We\nalso present a taxonomy that categorizes RAG methods, ranging from basic\nretrieval-augmented approaches to more advanced models incorporating\nmulti-modal data and reasoning capabilities. Additionally, we review the\nevaluation benchmarks and datasets commonly used to assess RAG systems, along\nwith a detailed exploration of its applications in fields such as question\nanswering, summarization, and information retrieval. Finally, we highlight\nemerging research directions and opportunities for improving RAG systems, such\nas enhanced retrieval efficiency, model interpretability, and domain-specific\nadaptations. This paper concludes by outlining the prospects for RAG in\naddressing real-world challenges and its potential to drive further\nadvancements in natural language processing.\n","authors":["Mingyue Cheng","Yucong Luo","Jie Ouyang","Qi Liu","Huijie Liu","Li Li","Shuo Yu","Bohou Zhang","Jiawei Cao","Jie Ma","Daoyu Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.10677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12972v2","updated":"2025-03-17T10:45:15Z","published":"2024-10-16T19:07:37Z","title":"Evaluating the Instruction-following Abilities of Language Models using\n  Knowledge Tasks","summary":"  LLM evaluation benchmarks have traditionally separated the testing of\nknowledge/reasoning capabilities from instruction following. In this work, we\nstudy the interaction between knowledge and instruction following, and observe\nthat LLMs struggle to follow simple answer modifying instructions, and are also\ndistracted by instructions that should have no bearing on the original\nknowledge task answer. We leverage existing multiple-choice answer based\nknowledge benchmarks and apply a set of simple instructions which include\nmanipulating text (eg.: change case), numeric quantities (eg.: increase value,\nchange formatting), operate on lists (eg.: sort answer candidates) and\ndistractor instructions (eg.: change case of numeric answers).\n","authors":["Rudra Murthy","Praveen Venkateswaran","Prince Kumar","Danish Contractor"],"pdf_url":"https://arxiv.org/pdf/2410.12972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12782v2","updated":"2025-03-17T10:43:54Z","published":"2024-10-16T17:56:49Z","title":"In-Context Learning Enables Robot Action Prediction in LLMs","summary":"  Recently, Large Language Models (LLMs) have achieved remarkable success using\nin-context learning (ICL) in the language domain. However, leveraging the ICL\ncapabilities within LLMs to directly predict robot actions remains largely\nunexplored. In this paper, we introduce RoboPrompt, a framework that enables\noff-the-shelf text-only LLMs to directly predict robot actions through ICL\nwithout training. Our approach first heuristically identifies keyframes that\ncapture important moments from an episode. Next, we extract end-effector\nactions from these keyframes as well as the estimated initial object poses, and\nboth are converted into textual descriptions. Finally, we construct a\nstructured template to form ICL demonstrations from these textual descriptions\nand a task instruction. This enables an LLM to directly predict robot actions\nat test time. Through extensive experiments and analysis, RoboPrompt shows\nstronger performance over zero-shot and ICL baselines in simulated and\nreal-world settings. Our project page is available at\nhttps://davidyyd.github.io/roboprompt.\n","authors":["Yida Yin","Zekai Wang","Yuvan Sharma","Dantong Niu","Trevor Darrell","Roei Herzig"],"pdf_url":"https://arxiv.org/pdf/2410.12782v2.pdf","comment":"Published in ICRA 2025"},{"id":"http://arxiv.org/abs/2503.13038v1","updated":"2025-03-17T10:42:34Z","published":"2025-03-17T10:42:34Z","title":"Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task","summary":"  In this paper, we provide an overview of the NTCIR-18 Automatic Evaluation of\nLLMs (AEOLLM) task. As large language models (LLMs) grow popular in both\nacademia and industry, how to effectively evaluate the capacity of LLMs becomes\nan increasingly critical but still challenging issue. Existing methods can be\ndivided into two types: manual evaluation, which is expensive, and automatic\nevaluation, which faces many limitations including task format (the majority\nbelong to multiple-choice questions) and evaluation criteria (occupied by\nreference-based metrics). To advance the innovation of automatic evaluation, we\npropose the AEOLLM task which focuses on generative tasks and encourages\nreference-free methods. Besides, we set up diverse subtasks such as dialogue\ngeneration, text expansion, summary generation and non-factoid question\nanswering to comprehensively test different methods. This year, we received 48\nruns from 4 teams in total. This paper will describe the background of the\ntask, the data set, the evaluation measures and the evaluation results,\nrespectively.\n","authors":["Junjie Chen","Haitao Li","Zhumin Chu","Yiqun Liu","Qingyao Ai"],"pdf_url":"https://arxiv.org/pdf/2503.13038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04872v2","updated":"2025-03-17T10:36:30Z","published":"2025-03-06T16:25:53Z","title":"TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation","summary":"  The challenge of reducing the size of Large Language Models (LLMs) while\nmaintaining their performance has gained significant attention. However,\nexisting methods, such as model distillation and transfer learning, often fail\nto achieve high accuracy. To address this limitation, we introduce the\nBranch-Merge distillation approach, which enhances model compression through\ntwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\n\\textit{selectively distilled} into specialized student models via\ndomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, where\nthese student models are merged to enable cross-domain knowledge transfer and\nimprove generalization. We validate our distillation approach using DeepSeek-R1\nas the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resulting\nmerged model, TinyR1-32B-Preview, outperforms its counterpart\nDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics\n(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achieving\nnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Merge\ndistillation approach provides a scalable solution for creating smaller,\nhigh-performing LLMs with reduced computational cost and time.\n","authors":["Lin Sun","Guangxiang Zhao","Xiaoqi Jian","Yuhan Wu","Weihong Lin","Yongfu Zhu","Change Jia","Linglin Zhang","Jinzhu Wu","Junfeng Ran","Sai-er Hu","Zihan Jiang","Junting Zhou","Wenrui Liu","Bin Cui","Tong Yang","Xiangzheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04872v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2401.05787v2","updated":"2025-03-17T10:35:11Z","published":"2024-01-11T09:49:15Z","title":"Chain of Evidences and Evidence to Generate: Prompting for Context\n  Grounded and Retrieval Augmented Reasoning","summary":"  While chain-of-thoughts (CoT) prompting has revolutionized how LLMs perform\nreasoning tasks, its current methods and variations (e.g, Self-consistency,\nReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR) etc.,)\nsuffer from limitations like limited context grounding,\nhallucination/inconsistent output generation, and iterative sluggishness. To\novercome these challenges, we introduce a novel mono/dual-step zero-shot\nprompting framework built upon two unique strategies Chain of Evidences (CoE)}\nand Evidence to Generate (E2G). Instead of unverified reasoning claims, our\ninnovative approaches leverage the power of \"evidence for decision making\" by\nfirst focusing exclusively on the thought sequences explicitly mentioned in the\ncontext which then serve as extracted evidence, guiding the LLM's output\ngeneration process with greater precision and efficiency. This simple yet\npotent approach unlocks the full potential of chain-of-thoughts prompting,\nfacilitating faster, more reliable, and contextually aware reasoning in LLMs.\nOur framework consistently achieves remarkable results across various\nknowledge-intensive reasoning and generation tasks, surpassing baseline\napproaches with state-of-the-art LLMs. For instance, (i) on the LogiQA\nbenchmark using GPT-4, CoE achieves a new state-of-the-art accuracy of 53.8%,\nsurpassing CoT by 18%, ToT by 11%, and CR by 9%; (ii) CoE with PaLM-2\noutperforms the variable-shot performance of Gemini Ultra by 0.9 F1 points,\nachieving an F1 score of 83.3 on DROP. We release our prompts and outputs on\nthese benchmarks as a new instruction tuning dataset for future research at\nhttps://huggingface.co/datasets/kagnlp/Chain-of-Evidences/.\n","authors":["Md Rizwan Parvez"],"pdf_url":"https://arxiv.org/pdf/2401.05787v2.pdf","comment":"Accepted at NAACL KnowledgeNLP 2025"},{"id":"http://arxiv.org/abs/2503.13031v1","updated":"2025-03-17T10:33:39Z","published":"2025-03-17T10:33:39Z","title":"Halving transcription time: A fast, user-friendly and GDPR-compliant\n  workflow to create AI-assisted transcripts for content analysis","summary":"  In qualitative research, data transcription is often labor-intensive and\ntime-consuming. To expedite this process, a workflow utilizing artificial\nintelligence (AI) was developed. This workflow not only enhances transcription\nspeed but also addresses the issue of AI-generated transcripts often lacking\ncompatibility with standard content analysis software. Within this workflow,\nautomatic speech recognition is employed to create initial transcripts from\naudio recordings, which are then formatted to be compatible with content\nanalysis software such as ATLAS.ti or MAXQDA. Empirical data from a study of 12\ninterviews suggests that this workflow can reduce transcription time by up to\n46.2%. Furthermore, by using widely used standard software, this process is\nsuitable for both students and researchers while also being adaptable to a\nvariety of learning, teaching, and research environments. It is also\nparticularly beneficial for non-native speakers. In addition, the workflow is\nGDPR-compliant and facilitates local, offline transcript generation, which is\ncrucial when dealing with sensitive data.\n","authors":["Jakob Sponholz","Andreas Weilinghoff","Juliane Schopf"],"pdf_url":"https://arxiv.org/pdf/2503.13031v1.pdf","comment":"10 pages, 1 table"},{"id":"http://arxiv.org/abs/2503.13021v1","updated":"2025-03-17T10:24:27Z","published":"2025-03-17T10:24:27Z","title":"Dynamic Relation Inference via Verb Embeddings","summary":"  CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.\n","authors":["Omri Suissa","Muhiim Ali","Ariana Azarbal","Hui Shen","Shekhar Pradhan"],"pdf_url":"https://arxiv.org/pdf/2503.13021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19597v3","updated":"2025-03-17T10:09:29Z","published":"2024-04-30T14:43:57Z","title":"TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with\n  Instruction Tuning","summary":"  The implications of backdoor attacks on English-centric large language models\n(LLMs) have been widely examined - such attacks can be achieved by embedding\nmalicious behaviors during training and activated under specific conditions\nthat trigger malicious outputs. Despite the increasing support for multilingual\ncapabilities in open-source and proprietary LLMs, the impact of backdoor\nattacks on these systems remains largely under-explored. Our research focuses\non cross-lingual backdoor attacks against multilingual LLMs, particularly\ninvestigating how poisoning the instruction-tuning data for one or two\nlanguages can affect the outputs for languages whose instruction-tuning data\nwere not poisoned. Despite its simplicity, our empirical analysis reveals that\nour method exhibits remarkable efficacy in models like mT5 and GPT-4o, with\nhigh attack success rates, surpassing 90% in more than 7 out of 12 languages\nacross various scenarios. Our findings also indicate that more powerful models\nshow increased susceptibility to transferable cross-lingual backdoor attacks,\nwhich also applies to LLMs predominantly pre-trained on English data, such as\nLlama2, Llama3, and Gemma. Moreover, our experiments demonstrate 1) High\nTransferability: the backdoor mechanism operates successfully in cross-lingual\nresponse scenarios across 26 languages, achieving an average attack success\nrate of 99%, and 2) Robustness: the proposed attack remains effective even\nafter defenses are applied. These findings expose critical security\nvulnerabilities in multilingual LLMs and highlight the urgent need for more\nrobust, targeted defense strategies to address the unique challenges posed by\ncross-lingual backdoor transfer.\n","authors":["Xuanli He","Jun Wang","Qiongkai Xu","Pasquale Minervini","Pontus Stenetorp","Benjamin I. P. Rubinstein","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2404.19597v3.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2412.01262v2","updated":"2025-03-17T10:01:21Z","published":"2024-12-02T08:30:22Z","title":"Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and\n  Shortcomings","summary":"  Large language models (LLMs) gained immense popularity due to their\nimpressive capabilities in unstructured conversations. Empowering LLMs with\nadvanced prompting strategies such as reasoning and acting (ReAct) (Yao et al.,\n2022) has shown promise in solving complex tasks traditionally requiring\nreinforcement learning. In this work, we apply the ReAct strategy to guide LLMs\nperforming task-oriented dialogue (TOD). We evaluate ReAct-based LLMs\n(ReAct-LLMs) both in simulation and with real users. While ReAct-LLMs severely\nunderperform state-of-the-art approaches on success rate in simulation, this\ndifference becomes less pronounced in human evaluation. Moreover, compared to\nthe baseline, humans report higher subjective satisfaction with ReAct-LLM\ndespite its lower success rate, most likely thanks to its natural and\nconfidently phrased responses.\n","authors":["Michelle Elizabeth","Morgan Veyret","Miguel Couceiro","Ondrej Dusek","Lina M. Rojas-Barahona"],"pdf_url":"https://arxiv.org/pdf/2412.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12992v1","updated":"2025-03-17T09:47:11Z","published":"2025-03-17T09:47:11Z","title":"Intra-neuronal attention within language models Relationships between\n  activation and semantics","summary":"  This study investigates the ability of perceptron-type neurons in language\nmodels to perform intra-neuronal attention; that is, to identify different\nhomogeneous categorical segments within the synthetic thought category they\nencode, based on a segmentation of specific activation zones for the tokens to\nwhich they are particularly responsive. The objective of this work is therefore\nto determine to what extent formal neurons can establish a homomorphic\nrelationship between activation-based and categorical segmentations. The\nresults suggest the existence of such a relationship, albeit tenuous, only at\nthe level of tokens with very high activation levels. This intra-neuronal\nattention subsequently enables categorical restructuring processes at the level\nof neurons in the following layer, thereby contributing to the progressive\nformation of high-level categorical abstractions.\n","authors":["Michael Pichat","William Pogrund","Paloma Pichat","Armanouche Gasparian","Samuel Demarchi","Corbet Alois Georgeon","Michael Veillet-Guillem"],"pdf_url":"https://arxiv.org/pdf/2503.12992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12989v1","updated":"2025-03-17T09:44:50Z","published":"2025-03-17T09:44:50Z","title":"A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models","summary":"  Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations. To address these challenges, we propose a multi-stage framework\nconsisting of inference, retrieval, and reranking stages, which integrates\ntaxonomy-guided reasoning examples to enhance performance by aligning outputs\nwith taxonomic knowledge. Evaluations on a large-scale dataset show significant\nimprovements in classification accuracy. Furthermore, we demonstrate the\nframework's adaptability for multi-label skill classification. Our results\nindicate that the framework outperforms existing LLM-based methods, offering a\npractical and scalable solution for occupation classification and related tasks\nacross LLMs.\n","authors":["Palakorn Achananuparp","Ee-Peng Lim"],"pdf_url":"https://arxiv.org/pdf/2503.12989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10624v2","updated":"2025-03-17T09:28:43Z","published":"2024-10-14T15:30:41Z","title":"SensorLLM: Aligning Large Language Models with Motion Sensors for Human\n  Activity Recognition","summary":"  We introduce SensorLLM, a two-stage framework that enables Large Language\nModels (LLMs) to perform human activity recognition (HAR) from sensor data.\nDespite their strong reasoning and generalization capabilities, LLMs remain\nunderutilized for motion sensor data due to the lack of semantic context in\ntime-series, computational constraints, and challenges in processing numerical\ninputs. SensorLLM addresses these limitations through a Sensor-Language\nAlignment stage, where we introduce special tokens for each sensor channel and\nautomatically generate textual trend descriptions. This alignment enables LLMs\nto capture numerical variations, channel-specific features, and data of varying\nduration--without requiring human annotations. In the subsequent Task-Aware\nTuning stage, we refine the model for HAR classification, achieving performance\nthat matches or surpasses state-of-the-art methods. Our results demonstrate\nthat SensorLLM evolves into an effective sensor learner, reasoner, and\nclassifier through Sensor-Language Alignment, generalizing across diverse HAR\ndatasets. We believe this work establishes a foundation for future research on\ntime-series and text alignment, paving the way for foundation models in sensor\ndata analysis.\n","authors":["Zechen Li","Shohreh Deldari","Linyao Chen","Hao Xue","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2410.10624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09364v2","updated":"2025-03-17T09:19:05Z","published":"2024-07-12T15:44:56Z","title":"Is Contrasting All You Need? Contrastive Learning for the Detection and\n  Attribution of AI-generated Text","summary":"  The significant progress in the development of Large Language Models has\ncontributed to blurring the distinction between human and AI-generated text.\nThe increasing pervasiveness of AI-generated text and the difficulty in\ndetecting it poses new challenges for our society. In this paper, we tackle the\nproblem of detecting and attributing AI-generated text by proposing WhosAI, a\ntriplet-network contrastive learning framework designed to predict whether a\ngiven input text has been generated by humans or AI and to unveil the\nauthorship of the text. Unlike most existing approaches, our proposed framework\nis conceived to learn semantic similarity representations from multiple\ngenerators at once, thus equally handling both detection and attribution tasks.\nFurthermore, WhosAI is model-agnostic and scalable to the release of new AI\ntext-generation models by incorporating their generated instances into the\nembedding space learned by our framework. Experimental results on the\nTuringBench benchmark of 200K news articles show that our proposed framework\nachieves outstanding results in both the Turing Test and Authorship Attribution\ntasks, outperforming all the methods listed in the TuringBench benchmark\nleaderboards.\n","authors":["Lucio La Cava","Davide Costa","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2407.09364v2.pdf","comment":"Accepted for publication at the 27th European Conference on\n  Artificial Intelligence (ECAI-2024), Volume 392, Pages 3179 - 3186, October\n  2024"},{"id":"http://arxiv.org/abs/2503.12941v1","updated":"2025-03-17T08:56:03Z","published":"2025-03-17T08:56:03Z","title":"HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of\n  Multimodal Large Language Model","summary":"  Instruction tuning is widely used to improve a pre-trained Multimodal Large\nLanguage Model (MLLM) by training it on curated task-specific datasets,\nenabling better comprehension of human instructions. However, it is infeasible\nto collect all possible instruction datasets simultaneously in real-world\nscenarios. Thus, enabling MLLM with continual instruction tuning is essential\nfor maintaining their adaptability. However, existing methods often trade off\nmemory efficiency for performance gains, significantly compromising overall\nefficiency. In this paper, we propose a task-specific expansion and\ntask-general fusion framework based on the variations in Centered Kernel\nAlignment (CKA) similarity across different model layers when trained on\ndiverse datasets. Furthermore, we analyze the information leakage present in\nthe existing benchmark and propose a new and more challenging benchmark to\nrationally evaluate the performance of different methods. Comprehensive\nexperiments showcase a significant performance improvement of our method\ncompared to existing state-of-the-art methods. Our code will be public\navailable.\n","authors":["Haiyang Guo","Fanhu Zeng","Ziwei Xiang","Fei Zhu","Da-Han Wang","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.12941v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.12937v1","updated":"2025-03-17T08:51:44Z","published":"2025-03-17T08:51:44Z","title":"R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization","summary":"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.\n","authors":["Jingyi Zhang","Jiaxing Huang","Huanjin Yao","Shunyu Liu","Xikun Zhang","Shijian Lu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.12937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19951v4","updated":"2025-03-17T08:33:00Z","published":"2024-11-29T18:59:54Z","title":"Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation","summary":"  Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps://github.com/VITA-MLLM/Sparrow.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Yunhang Shen","Chunjiang Ge","Yan Yang","Zuwei Long","Yuhan Dai","Yongdong Luo","Haoyu Cao","Tong Xu","Xing Sun","Caifeng Shan","Ran He","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.19951v4.pdf","comment":"Project page: https://github.com/VITA-MLLM/Sparrow"},{"id":"http://arxiv.org/abs/2503.12918v1","updated":"2025-03-17T08:29:04Z","published":"2025-03-17T08:29:04Z","title":"ThinkPatterns-21k: A Systematic Study on the Impact of Thinking Patterns\n  in LLMs","summary":"  Large language models (LLMs) have demonstrated enhanced performance through\nthe \\textit{Thinking then Responding} paradigm, where models generate internal\nthoughts before final responses (aka, System 2 thinking). However, existing\nresearch lacks a systematic understanding of the mechanisms underlying how\nthinking patterns affect performance across model sizes. In this work, we\nconduct a comprehensive analysis of the impact of various thinking types on\nmodel performance and introduce ThinkPatterns-21k, a curated dataset comprising\n21k instruction-response pairs (QA) collected from existing\ninstruction-following datasets with five thinking types. For each pair, we\naugment it with five distinct internal thinking patterns: one unstructured\nthinking (monologue) and four structured variants (decomposition, self-ask,\nself-debate and self-critic), while maintaining the same instruction and\nresponse. Through extensive evaluation across different model sizes (3B-32B\nparameters), we have two key findings: (1) smaller models (<30B parameters) can\nbenefit from most of structured thinking patterns, while larger models (32B)\nwith structured thinking like decomposition would degrade performance and (2)\nunstructured monologue demonstrates broad effectiveness across different model\nsizes. Finally, we released all of our datasets, checkpoints, training logs of\ndiverse thinking patterns to reproducibility, aiming to facilitate further\nresearch in this direction.\n","authors":["Pengcheng Wen","Jiaming Ji","Chi-Min Chan","Juntao Dai","Donghai Hong","Yaodong Yang","Sirui Han","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2503.12918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12908v1","updated":"2025-03-17T08:17:28Z","published":"2025-03-17T08:17:28Z","title":"HICD: Hallucination-Inducing via Attention Dispersion for Contrastive\n  Decoding to Mitigate Hallucinations in Large Language Models","summary":"  Large Language Models (LLMs) often generate hallucinations, producing outputs\nthat are contextually inaccurate or factually incorrect. We introduce HICD, a\nnovel method designed to induce hallucinations for contrastive decoding to\nmitigate hallucinations. Unlike existing contrastive decoding methods, HICD\nselects attention heads crucial to the model's prediction as inducing heads,\nthen induces hallucinations by dispersing attention of these inducing heads and\ncompares the hallucinated outputs with the original outputs to obtain the final\nresult. Our approach significantly improves performance on tasks requiring\ncontextual faithfulness, such as context completion, reading comprehension, and\nquestion answering. It also improves factuality in tasks requiring accurate\nknowledge recall. We demonstrate that our inducing heads selection and\nattention dispersion method leads to more \"contrast-effective\" hallucinations\nfor contrastive decoding, outperforming other hallucination-inducing methods.\nOur findings provide a promising strategy for reducing hallucinations by\ninducing hallucinations in a controlled manner, enhancing the performance of\nLLMs in a wide range of tasks.\n","authors":["Xinyan Jiang","Hang Ye","Yongxin Zhu","Xiaoying Zheng","Zikang Chen","Jun Gong"],"pdf_url":"https://arxiv.org/pdf/2503.12908v1.pdf","comment":"Under review at ARR - February 2025"},{"id":"http://arxiv.org/abs/2503.12899v1","updated":"2025-03-17T07:59:42Z","published":"2025-03-17T07:59:42Z","title":"A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation","summary":"  Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12899v1.pdf","comment":"12 pages, 6 figure, 6 tables, under peer-review"},{"id":"http://arxiv.org/abs/2502.17848v2","updated":"2025-03-17T07:36:01Z","published":"2025-02-25T04:51:17Z","title":"LR$^2$Bench: Evaluating Long-chain Reflective Reasoning Capabilities of\n  Large Language Models via Constraint Satisfaction Problems","summary":"  Recent progress in o1-like models has significantly enhanced the reasoning\nabilities of Large Language Models (LLMs), empowering them to tackle\nincreasingly complex tasks through reflection capabilities, such as making\nassumptions, backtracking, and self-refinement. However, effectively evaluating\nsuch reflection capabilities remains challenging due to the lack of appropriate\nbenchmarks. To bridge this gap, we introduce LR$^2$Bench, a novel benchmark\ndesigned to evaluate the Long-chain Reflective Reasoning capabilities of LLMs.\nLR$^2$Bench comprises 850 samples across six Constraint Satisfaction Problems\n(CSPs) where reflective reasoning is crucial for deriving solutions that meet\nall given constraints. Each type of task focuses on distinct constraint\npatterns, such as knowledge-based, logical, and spatial constraints, providing\na comprehensive evaluation of diverse problem-solving scenarios. We conduct\nextensive evaluation on both conventional models and o1-like models. Our\nexperimental results reveal that even the most advanced reasoning-specific\nmodels, such as DeepSeek-R1 and OpenAI o1-preview, struggle with tasks in\nLR$^2$Bench, achieving an average Exact Match score of only 20.0% and 23.6%,\nrespectively. These findings underscore the significant room for improvement in\nthe reflective reasoning capabilities of current LLMs. The leaderboard of our\nbenchmark is available at https://huggingface.co/spaces/UltraRonin/LR2Bench\n","authors":["Jianghao Chen","Zhenlin Wei","Zhenjiang Ren","Ziyong Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17848v2.pdf","comment":"Submitted to ACL ARR 2025 February"},{"id":"http://arxiv.org/abs/2503.01090v2","updated":"2025-03-17T07:34:41Z","published":"2025-03-03T01:30:28Z","title":"Precise Localization of Memories: A Fine-grained Neuron-level Knowledge\n  Editing Technique for LLMs","summary":"  Knowledge editing aims to update outdated information in Large Language\nModels (LLMs). A representative line of study is locate-then-edit methods,\nwhich typically employ causal tracing to identify the modules responsible for\nrecalling factual knowledge about entities. However, we find these methods are\noften sensitive only to changes in the subject entity, leaving them less\neffective at adapting to changes in relations. This limitation results in poor\nediting locality, which can lead to the persistence of irrelevant or inaccurate\nfacts, ultimately compromising the reliability of LLMs. We believe this issue\narises from the insufficient precision of knowledge localization. To address\nthis, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method\nthat enhances editing locality without affecting overall success rates. By\nprecisely identifying and modifying specific neurons within feed-forward\nnetworks, FiNE significantly improves knowledge localization and editing.\nQuantitative experiments demonstrate that FiNE efficiently achieves better\noverall performance compared to existing techniques, providing new insights\ninto the localization and modification of knowledge within LLMs.\n","authors":["Haowen Pan","Xiaozhi Wang","Yixin Cao","Zenglin Shi","Xun Yang","Juanzi Li","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.01090v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12882v1","updated":"2025-03-17T07:25:32Z","published":"2025-03-17T07:25:32Z","title":"DAPI: Domain Adaptive Toxicity Probe Vector Intervention for\n  Fine-Grained Detoxification","summary":"  There have been attempts to utilize linear probe for detoxification, with\nexisting studies relying on a single toxicity probe vector to reduce toxicity.\nHowever, toxicity can be fine-grained into various subcategories, making it\ndifficult to remove certain types of toxicity by using a single toxicity probe\nvector. To address this limitation, we propose a category-specific toxicity\nprobe vector approach. First, we train multiple toxicity probe vectors for\ndifferent toxicity categories. During generation, we dynamically select the\nmost relevant toxicity probe vector based on the current context. Finally, the\nselected vector is dynamically scaled and subtracted from model. Our method\nsuccessfully mitigated toxicity from categories that the single probe vector\napproach failed to detoxify. Experiments demonstrate that our approach achieves\nup to a 78.52% reduction in toxicity on the evaluation dataset, while fluency\nremains nearly unchanged, with only a 0.052% drop compared to the unsteered\nmodel.\n","authors":["Cho Hyeonsu","Dooyoung Kim","Youngjoong Ko"],"pdf_url":"https://arxiv.org/pdf/2503.12882v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.12880v1","updated":"2025-03-17T07:20:11Z","published":"2025-03-17T07:20:11Z","title":"nvBench 2.0: A Benchmark for Natural Language to Visualization under\n  Ambiguity","summary":"  Natural Language to Visualization (NL2VIS) enables users to create\nvisualizations from natural language queries, making data insights more\naccessible. However, NL2VIS faces challenges in interpreting ambiguous queries,\nas users often express their visualization needs in imprecise language. To\naddress this challenge, we introduce nvBench 2.0, a new benchmark designed to\nevaluate NL2VIS systems in scenarios involving ambiguous queries. nvBench 2.0\nincludes 7,878 natural language queries and 24,076 corresponding\nvisualizations, derived from 780 tables across 153 domains. It is built using a\ncontrolled ambiguity-injection pipeline that generates ambiguous queries\nthrough a reverse-generation workflow. By starting with unambiguous seed\nvisualizations and selectively injecting ambiguities, the pipeline yields\nmultiple valid interpretations for each query, with each ambiguous query\ntraceable to its corresponding visualization through step-wise reasoning paths.\nWe evaluate various Large Language Models (LLMs) on their ability to perform\nambiguous NL2VIS tasks using nvBench 2.0. We also propose Step-NL2VIS, an\nLLM-based model trained on nvBench 2.0, which enhances performance in ambiguous\nscenarios through step-wise preference optimization. Our results show that\nStep-NL2VIS outperforms all baselines, setting a new state-of-the-art for\nambiguous NL2VIS tasks.\n","authors":["Tianqi Luo","Chuhan Huang","Leixian Shen","Boyan Li","Shuyu Shen","Wei Zeng","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2503.12880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07601v2","updated":"2025-03-17T07:11:04Z","published":"2025-02-11T14:50:43Z","title":"Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large\n  Language Models","summary":"  Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the\ntraditional unsupervised AD setting that requires a large number of normal\nsamples to train a model, ZSAD is more practical for handling data-restricted\nreal-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have\nshown revolutionary reasoning capabilities in various vision tasks. However,\nthe reasoning of image abnormalities remains underexplored due to the lack of\ncorresponding datasets and benchmarks. To facilitate research in AD &\nreasoning, we establish the first visual instruction tuning dataset,\nAnomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through\ninvestigation with our benchmark, we reveal that current MLLMs like GPT-4o\ncannot accurately detect and describe fine-grained anomalous details in images.\nTo address this, we propose Anomaly-OneVision (Anomaly-OV), the first\nspecialist visual assistant for ZSAD and reasoning. Inspired by human behavior\nin visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM)\nmechanism to adaptively select and emphasize abnormal visual tokens. Extensive\nexperiments demonstrate that Anomaly-OV achieves significant improvements over\nadvanced generalist models in both detection and reasoning. Extensions to\nmedical and 3D AD are provided for future study. The link to our project page:\nhttps://xujiacong.github.io/Anomaly-OV/\n","authors":["Jiacong Xu","Shao-Yuan Lo","Bardia Safaei","Vishal M. Patel","Isht Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2502.07601v2.pdf","comment":"19 pages, 10 figures, accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.12858v1","updated":"2025-03-17T06:40:06Z","published":"2025-03-17T06:40:06Z","title":"Harnessing Test-time Adaptation for NLU tasks Involving Dialects of\n  English","summary":"  Test-time adaptation (TTA) is an excellent method which helps generalize\nmodels across domains, tasks, and distributions without the use of labeled\ndatasets. Thus, TTA is very useful in natural language processing (NLP) in the\ndialectal setting, since oftentimes, models are trained on Standard American\nEnglish (SAE), evaluated on Indian English or Nigerian English, of which\ndistribution differs significantly from the former. This is especially useful\nsince dialectal datasets are scarce. In this paper, we explore one of the most\nfamous TTA techniques, SHOT, in dialectal NLP. We finetune and evaluate SHOT on\ndifferent combinations of dialectal GLUE. Our findings show that SHOT is a\nviable technique when labeled datasets are unavailable. We also theoretically\npropose the concept of dialectal gap and show that it has a positive\ncorrelation with the effectiveness of SHOT. We also find that in many cases,\nfinetuning on SAE yields higher performance than finetuning on dialectal data.\nOur code is available at https://github.com/dukenguyenxyz/dialect-adaptation\n","authors":["Duke Nguyen","Aditya Joshi","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2503.12858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.24160v3","updated":"2025-03-17T06:33:07Z","published":"2024-10-31T17:19:03Z","title":"Redefining <Creative> in Dictionary: Towards an Enhanced Semantic\n  Understanding of Creative Generation","summary":"  ``Creative'' remains an inherently abstract concept for both humans and\ndiffusion models. While text-to-image (T2I) diffusion models can easily\ngenerate out-of-distribution concepts like ``a blue banana'', they struggle\nwith generating combinatorial objects such as ``a creative mixture that\nresembles a lettuce and a mantis'', due to difficulties in understanding the\nsemantic depth of ``creative''. Current methods rely heavily on synthesizing\nreference prompts or images to achieve a creative effect, typically requiring\nretraining for each unique creative output-a process that is computationally\nintensive and limits practical applications. To address this, we introduce\nCreTok, which brings meta-creativity to diffusion models by redefining\n``creative'' as a new token, \\texttt{<CreTok>}, thus enhancing models' semantic\nunderstanding for combinatorial creativity. CreTok achieves such redefinition\nby iteratively sampling diverse text pairs from our proposed CangJie dataset to\nform adaptive prompts and restrictive prompts, and then optimizing the\nsimilarity between their respective text embeddings. Extensive experiments\ndemonstrate that <CreTok> enables the universal and direct generation of\ncombinatorial creativity across diverse concepts without additional training,\nachieving state-of-the-art performance with improved text-image alignment and\nhigher human preference ratings. Code will be made available at\nhttps://github.com/fu-feng/CreTok.\n","authors":["Fu Feng","Yucheng Xie","Xu Yang","Jing Wang","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2410.24160v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12855v1","updated":"2025-03-17T06:30:02Z","published":"2025-03-17T06:30:02Z","title":"VITED: Video Temporal Evidence Distillation","summary":"  We investigate complex video question answering via chain-of-evidence\nreasoning -- identifying sequences of temporal spans from multiple relevant\nparts of the video, together with visual evidence within them. Existing models\nstruggle with multi-step reasoning as they uniformly sample a fixed number of\nframes, which can miss critical evidence distributed nonuniformly throughout\nthe video. Moreover, they lack the ability to temporally localize such evidence\nin the broader context of the full video, which is required for answering\ncomplex questions. We propose a framework to enhance existing VideoQA datasets\nwith evidence reasoning chains, automatically constructed by searching for\noptimal intervals of interest in the video with supporting evidence, that\nmaximizes the likelihood of answering a given question. We train our model\n(VITED) to generate these evidence chains directly, enabling it to both\nlocalize evidence windows as well as perform multi-step reasoning across them\nin long-form video content. We show the value of our evidence-distilled models\non a suite of long video QA benchmarks where we outperform state-of-the-art\napproaches that lack evidence reasoning capabilities.\n","authors":["Yujie Lu","Yale Song","William Wang","Lorenzo Torresani","Tushar Nagarajan"],"pdf_url":"https://arxiv.org/pdf/2503.12855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12854v1","updated":"2025-03-17T06:28:25Z","published":"2025-03-17T06:28:25Z","title":"Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical\n  Investigation","summary":"  Recent advancements in post-training methodologies for large language models\n(LLMs) have highlighted reinforcement learning (RL) as a critical component for\nenhancing reasoning. However, the substantial computational costs associated\nwith RL-based approaches have led to growing interest in alternative paradigms,\nsuch as Direct Preference Optimization (DPO). In this study, we investigate the\neffectiveness of DPO in facilitating self-improvement for LLMs through\niterative preference-based learning. We demonstrate that a single round of DPO\nwith coarse filtering significantly enhances mathematical reasoning\nperformance, particularly for strong base model. Furthermore, we design an\niterative enhancement framework for both the generator and the reward model\n(RM), enabling their mutual improvement through online interaction across\nmultiple rounds of DPO. Finally, with simple verifiable rewards, our model\nDPO-VP achieves RL-level performance with significantly lower computational\noverhead. These findings highlight DPO as a scalable and cost-effective\nalternative to RL, offering a practical solution for enhancing LLM reasoning in\nresource-constrained situations.\n","authors":["Songjun Tu","Jiahao Lin","Xiangyu Tian","Qichao Zhang","Linjing Li","Yuqian Fu","Nan Xu","Wei He","Xiangyuan Lan","Dongmei Jiang","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.12854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16144v2","updated":"2025-03-17T06:04:48Z","published":"2024-06-23T15:50:22Z","title":"Chain-of-Probe: Examining the Necessity and Accuracy of CoT Step-by-Step","summary":"  Current research found the issue of Early Answering in large language models\n(LLMs), where the models already have an answer before generating the\nChain-of-Thought (CoT). This phenomenon suggests a potential lack of necessary\ndependency between the predicted answer and the reasoning process.\nConsequently, two important questions arise: (1) Is CoT still necessary if the\nmodel already has an answer? (2) Can the correctness of the answer serve as\nvalid evidence for the correctness of CoT? To address these questions, we\npropose a method, namely Chain-of-Probe (CoP), to probe changes in the mind\nduring the model's reasoning. The probing results show that in a significant\nnumber of question-answer cases, CoT appears to be unnecessary, and this\nnecessity correlates with the simplicity of the task, defined by reasoning\nsteps required. Furthermore, by analyzing patterns in mind change, we examine\nthe correctness of the model's reasoning. Our validation reveals that many\nresponses, although correct in their final answer, contain errors in their\nreasoning process. To this end, we propose a strategic approach based on CoP to\nprioritize answers with correct reasoning among multiple candidates, thereby\nbolstering the reliability of the model's reasoning.\n","authors":["Zezhong Wang","Xingshan Zeng","Weiwen Liu","Yufei Wang","Liangyou Li","Yasheng Wang","Lifeng Shang","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2406.16144v2.pdf","comment":"Accepted by Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2410.18447v2","updated":"2025-03-17T06:00:52Z","published":"2024-10-24T05:45:04Z","title":"ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent\n  Dialogue Synthesis","summary":"  Supervised fine-tuning (SFT) is a common method to enhance the tool calling\ncapabilities of Large Language Models (LLMs), with the training data often\nbeing synthesized. The current data synthesis process generally involves\nsampling a set of tools, formulating a requirement based on these tools, and\ngenerating the call statements. However, tools sampled randomly lack relevance,\nmaking them difficult to combine and thus reducing the diversity of the data.\nAdditionally, current work overlooks the coherence between turns of dialogues,\nleading to a gap between the synthesized data and real-world scenarios. To\naddress these issues, we propose a Graph-based Sampling strategy to sample more\nrelevant tool combinations, and a Planned-generation strategy to create plans\nthat guide the synthesis of coherent dialogues. We integrate these two\nstrategies and enable multiple agents to synthesize the dialogue data\ninteractively, resulting in our tool-calling data synthesis pipeline ToolFlow.\nData quality assessments demonstrate improvements in the naturalness and\ncoherence of our synthesized dialogues. Finally, we apply SFT on LLaMA-3.1-8B\nusing 8,000 synthetic dialogues generated with ToolFlow. Results show that the\nmodel achieves tool-calling performance comparable to or even surpassing GPT-4,\nwhile maintaining strong general capabilities.\n","authors":["Zezhong Wang","Xingshan Zeng","Weiwen Liu","Liangyou Li","Yasheng Wang","Lifeng Shang","Xin Jiang","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.18447v2.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2503.12832v1","updated":"2025-03-17T05:24:39Z","published":"2025-03-17T05:24:39Z","title":"Modelling Child Learning and Parsing of Long-range Syntactic\n  Dependencies","summary":"  This work develops a probabilistic child language acquisition model to learn\na range of linguistic phenonmena, most notably long-range syntactic\ndependencies of the sort found in object wh-questions, among other\nconstructions. The model is trained on a corpus of real child-directed speech,\nwhere each utterance is paired with a logical form as a meaning representation.\nIt then learns both word meanings and language-specific syntax simultaneously.\nAfter training, the model can deduce the correct parse tree and word meanings\nfor a given utterance-meaning pair, and can infer the meaning if given only the\nutterance. The successful modelling of long-range dependencies is theoretically\nimportant because it exploits aspects of the model that are, in general,\ntrans-context-free.\n","authors":["Louis Mahon","Mark Johnson","Mark Steedman"],"pdf_url":"https://arxiv.org/pdf/2503.12832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14675v2","updated":"2025-03-17T04:47:58Z","published":"2024-10-18T17:59:47Z","title":"To Trust or Not to Trust? Enhancing Large Language Models' Situated\n  Faithfulness to External Contexts","summary":"  Large Language Models (LLMs) are often augmented with external contexts, such\nas those used in retrieval-augmented generation (RAG). However, these contexts\ncan be inaccurate or intentionally misleading, leading to conflicts with the\nmodel's internal knowledge. We argue that robust LLMs should demonstrate\nsituated faithfulness, dynamically calibrating their trust in external\ninformation based on their confidence in the internal knowledge and the\nexternal context to resolve knowledge conflicts. To benchmark this capability,\nwe evaluate LLMs across several QA datasets, including a newly created dataset\nfeaturing in-the-wild incorrect contexts sourced from Reddit posts. We show\nthat when provided with both correct and incorrect contexts, both open-source\nand proprietary models tend to overly rely on external information, regardless\nof its factual accuracy. To enhance situated faithfulness, we propose two\napproaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence\nReasoning (RCR). SCR enables models to self-assess the confidence of external\ninformation relative to their own internal knowledge to produce the most\naccurate answer. RCR, in contrast, extracts explicit confidence signals from\nthe LLM and determines the final answer using predefined rules. Our results\nshow that for LLMs with strong reasoning capabilities, such as GPT-4o and\nGPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a\ndirect input augmentation baseline. Conversely, for a smaller model like\nLlama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence\nReasoning Direct Preference Optimization (CR-DPO) method improves performance\non both seen and unseen datasets, yielding an average improvement of 8.9% on\nLlama-3-8B. In addition to quantitative results, we offer insights into the\nrelative strengths of SCR and RCR.\n","authors":["Yukun Huang","Sanxing Chen","Hongyi Cai","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12811v1","updated":"2025-03-17T04:36:45Z","published":"2025-03-17T04:36:45Z","title":"A Multi-Power Law for Loss Curve Prediction Across Learning Rate\n  Schedules","summary":"  Training large models is both resource-intensive and time-consuming, making\nit crucial to understand the quantitative relationship between model\nperformance and hyperparameters. In this paper, we present an empirical law\nthat describes how the pretraining loss of large language models evolves under\ndifferent learning rate schedules, such as constant, cosine, and step decay\nschedules. Our proposed law takes a multi-power form, combining a power law\nbased on the sum of learning rates and additional power laws to account for a\nloss reduction effect induced by learning rate decay. We extensively validate\nthis law on various model sizes and architectures, and demonstrate that after\nfitting on a few learning rate schedules, the law accurately predicts the loss\ncurves for unseen schedules of different shapes and horizons. Moreover, by\nminimizing the predicted final pretraining loss across learning rate schedules,\nwe are able to find a schedule that outperforms the widely used cosine learning\nrate schedule. Interestingly, this automatically discovered schedule bears some\nresemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (Hu et\nal, 2024) but achieves a slightly lower final loss. We believe these results\ncould offer valuable insights for understanding the dynamics of pretraining and\ndesigning learning rate schedules to improve efficiency.\n","authors":["Kairong Luo","Haodong Wen","Shengding Hu","Zhenbo Sun","Zhiyuan Liu","Maosong Sun","Kaifeng Lyu","Wenguang Chen"],"pdf_url":"https://arxiv.org/pdf/2503.12811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11197v2","updated":"2025-03-17T04:20:29Z","published":"2025-03-14T08:43:53Z","title":"Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering","summary":"  Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.\n","authors":["Gang Li","Jizhong Liu","Heinrich Dinkel","Yadong Niu","Junbo Zhang","Jian Luan"],"pdf_url":"https://arxiv.org/pdf/2503.11197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12803v1","updated":"2025-03-17T04:19:20Z","published":"2025-03-17T04:19:20Z","title":"Leveraging Deep Neural Networks for Aspect-Based Sentiment\n  Classification","summary":"  Aspect-based sentiment analysis seeks to determine sentiment with a high\nlevel of detail. While graph convolutional networks (GCNs) are commonly used\nfor extracting sentiment features, their straightforward use in syntactic\nfeature extraction can lead to a loss of crucial information. This paper\npresents a novel edge-enhanced GCN, called EEGCN, which improves performance by\npreserving feature integrity as it processes syntactic graphs. We incorporate a\nbidirectional long short-term memory (Bi-LSTM) network alongside a\nself-attention-based transformer for effective text encoding, ensuring the\nretention of long-range dependencies. A bidirectional GCN (Bi-GCN) with message\npassing then captures the relationships between entities, while an\naspect-specific masking technique removes extraneous information. Extensive\nevaluations and ablation studies on four benchmark datasets show that EEGCN\nsignificantly enhances aspect-based sentiment analysis, overcoming issues with\nsyntactic feature extraction and advancing the field's methodologies.\n","authors":["Chen Li","Debo Cheng","Yasuhiko Morimoto"],"pdf_url":"https://arxiv.org/pdf/2503.12803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12797v1","updated":"2025-03-17T04:06:34Z","published":"2025-03-17T04:06:34Z","title":"DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding","summary":"  Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.\n","authors":["Xinyu Ma","Ziyang Ding","Zhicong Luo","Chi Chen","Zonghao Guo","Derek F. Wong","Xiaoyi Feng","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2503.12797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12796v1","updated":"2025-03-17T04:06:10Z","published":"2025-03-17T04:06:10Z","title":"A Reinforcement Learning-Driven Transformer GAN for Molecular Generation","summary":"  Generating molecules with desired chemical properties presents a critical\nchallenge in fields such as chemical synthesis and drug discovery. Recent\nadvancements in artificial intelligence (AI) and deep learning have\nsignificantly contributed to data-driven molecular generation. However,\nchallenges persist due to the inherent sensitivity of simplified molecular\ninput line entry system (SMILES) representations and the difficulties in\napplying generative adversarial networks (GANs) to discrete data. This study\nintroduces RL-MolGAN, a novel Transformer-based discrete GAN framework designed\nto address these challenges. Unlike traditional Transformer architectures,\nRL-MolGAN utilizes a first-decoder-then-encoder structure, facilitating the\ngeneration of drug-like molecules from both $de~novo$ and scaffold-based\ndesigns. In addition, RL-MolGAN integrates reinforcement learning (RL) and\nMonte Carlo tree search (MCTS) techniques to enhance the stability of GAN\ntraining and optimize the chemical properties of the generated molecules. To\nfurther improve the model's performance, RL-MolWGAN, an extension of RL-MolGAN,\nincorporates Wasserstein distance and mini-batch discrimination, which together\nenhance the stability of the GAN. Experimental results on two widely used\nmolecular datasets, QM9 and ZINC, validate the effectiveness of our models in\ngenerating high-quality molecular structures with diverse and desirable\nchemical properties.\n","authors":["Chen Li","Huidong Tang","Ye Zhu","Yoshihiro Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2503.12796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07996v3","updated":"2025-03-17T02:57:48Z","published":"2025-03-11T02:52:39Z","title":"SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic","summary":"  Recent advancements in Text-to-SQL systems have improved the conversion of\nnatural language queries into SQL, but challenges remain in ensuring accuracy\nand reliability. While self-correction techniques refine outputs, they often\nintroduce new errors. Existing methods focused on execution feedback mainly\naddress syntax issues, leaving semantic errors -- where the query's logic fails\nto align with the user's intent -- largely unaddressed. We propose a novel\napproach combining structured execution feedback with a trained critic agent\nthat provides detailed, interpretable critiques. This method effectively\nidentifies and corrects both syntactic and semantic errors, enhancing accuracy\nand interpretability. Experimental results show significant improvements on two\nmajor Text-to-SQL benchmarks, Spider and BIRD, demonstrating the effectiveness\nof our approach.\n","authors":["Jikai Chen"],"pdf_url":"https://arxiv.org/pdf/2503.07996v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07862v2","updated":"2025-03-17T02:56:18Z","published":"2025-03-10T21:21:13Z","title":"cantnlp@DravidianLangTech2025: A Bag-of-Sounds Approach to Multimodal\n  Hate Speech Detection","summary":"  This paper presents the systems and results for the Multimodal Social Media\nData Analysis in Dravidian Languages (MSMDA-DL) shared task at the Fifth\nWorkshop on Speech, Vision, and Language Technologies for Dravidian Languages\n(DravidianLangTech-2025). We took a `bag-of-sounds' approach by training our\nhate speech detection system on the speech (audio) data using transformed Mel\nspectrogram measures. While our candidate model performed poorly on the test\nset, our approach offered promising results during training and development for\nMalayalam and Tamil. With sufficient and well-balanced training data, our\nresults show that it is feasible to use both text and speech (audio) data in\nthe development of multimodal hate speech detection systems.\n","authors":["Sidney Wong","Andrew Li"],"pdf_url":"https://arxiv.org/pdf/2503.07862v2.pdf","comment":"Accepted Fifth Workshop on Speech and Language Technologies for\n  Dravidian Languages"},{"id":"http://arxiv.org/abs/2503.12759v1","updated":"2025-03-17T02:53:42Z","published":"2025-03-17T02:53:42Z","title":"RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum\n  Learning","summary":"  Recent research highlights the challenges retrieval models face in retrieving\nuseful contexts and the limitations of generation models in effectively\nutilizing those contexts in retrieval-augmented generation (RAG) settings. To\naddress these challenges, we introduce RAG-RL, the first reasoning language\nmodel (RLM) specifically trained for RAG. RAG-RL demonstrates that stronger\nanswer generation models can identify relevant contexts within larger sets of\nretrieved information -- thereby alleviating the burden on retrievers -- while\nalso being able to utilize those contexts more effectively. Moreover, we show\nthat curriculum design in the reinforcement learning (RL) post-training process\nis a powerful approach to enhancing model performance. We benchmark our method\non two open-domain question-answering datasets and achieve state-of-the-art\nresults, surpassing previous SOTA generative reader models. In addition, we\noffers empirical insights into various curriculum learning strategies,\nproviding a deeper understanding of their impact on model performance.\n","authors":["Jerry Huang","Siddarth Madala","Risham Sidhu","Cheng Niu","Julia Hockenmaier","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12759v1.pdf","comment":"11 Pages, 3 Figures, Preprint"},{"id":"http://arxiv.org/abs/2501.02189v5","updated":"2025-03-17T02:24:48Z","published":"2025-01-04T04:59:33Z","title":"A Survey of State of the Art Large Vision Language Models: Alignment,\n  Benchmark, Evaluations and Challenges","summary":"  Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntechnology at the intersection of computer vision and natural language\nprocessing, enabling machines to perceive and reason about the world through\nboth visual and textual modalities. For example, models such as CLIP, Claude,\nand GPT-4V demonstrate strong reasoning and understanding abilities on visual\nand textual data and beat classical single modality vision models on zero-shot\nclassification. Despite their rapid advancements in research and growing\npopularity in applications, a comprehensive survey of existing studies on VLMs\nis notably lacking, particularly for researchers aiming to leverage VLMs in\ntheir specific domains. To this end, we provide a systematic overview of VLMs\nin the following aspects: model information of the major VLMs developed over\nthe past five years (2019-2024); the main architectures and training methods of\nthese VLMs; summary and categorization of the popular benchmarks and evaluation\nmetrics of VLMs; the applications of VLMs including embodied agents, robotics,\nand video generation; the challenges and issues faced by current VLMs such as\nhallucination, fairness, and safety. Detailed collections including papers and\nmodel repository links are listed in\nhttps://github.com/zli12321/Vision-Language-Models-Overview.\n","authors":["Zongxia Li","Xiyang Wu","Hongyang Du","Huy Nghiem","Guangyao Shi"],"pdf_url":"https://arxiv.org/pdf/2501.02189v5.pdf","comment":"22 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.12739v1","updated":"2025-03-17T02:14:42Z","published":"2025-03-17T02:14:42Z","title":"TNCSE: Tensor's Norm Constraints for Unsupervised Contrastive Learning\n  of Sentence Embeddings","summary":"  Unsupervised sentence embedding representation has become a hot research\ntopic in natural language processing. As a tensor, sentence embedding has two\ncritical properties: direction and norm. Existing works have been limited to\nconstraining only the orientation of the samples' representations while\nignoring the features of their module lengths. To address this issue, we\npropose a new training objective that optimizes the training of unsupervised\ncontrastive learning by constraining the module length features between\npositive samples. We combine the training objective of Tensor's Norm\nConstraints with ensemble learning to propose a new Sentence Embedding\nrepresentation framework, TNCSE. We evaluate seven semantic text similarity\ntasks, and the results show that TNCSE and derived models are the current\nstate-of-the-art approach; in addition, we conduct extensive zero-shot\nevaluations, and the results show that TNCSE outperforms other baselines.\n","authors":["Tianyu Zong","Bingkang Shi","Hongzhu Yi","Jungang Xu"],"pdf_url":"https://arxiv.org/pdf/2503.12739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12722v1","updated":"2025-03-17T01:21:54Z","published":"2025-03-17T01:21:54Z","title":"Identifying Cooperative Personalities in Multi-agent Contexts through\n  Personality Steering with Representation Engineering","summary":"  As Large Language Models (LLMs) gain autonomous capabilities, their\ncoordination in multi-agent settings becomes increasingly important. However,\nthey often struggle with cooperation, leading to suboptimal outcomes. Inspired\nby Axelrod's Iterated Prisoner's Dilemma (IPD) tournaments, we explore how\npersonality traits influence LLM cooperation. Using representation engineering,\nwe steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and\nanalyze their impact on IPD decision-making. Our results show that higher\nAgreeableness and Conscientiousness improve cooperation but increase\nsusceptibility to exploitation, highlighting both the potential and limitations\nof personality-based steering for aligning AI agents.\n","authors":["Kenneth J. K. Ong","Lye Jia Jun","Hieu Minh \"Jord\" Nguyen","Seong Hah Cho","Natalia Pérez-Campanero Antolín"],"pdf_url":"https://arxiv.org/pdf/2503.12722v1.pdf","comment":"Poster, Technical AI Safety Conference 2025"},{"id":"http://arxiv.org/abs/2410.06981v3","updated":"2025-03-17T00:31:46Z","published":"2024-10-09T15:18:57Z","title":"Sparse Autoencoders Reveal Universal Feature Spaces Across Large\n  Language Models","summary":"  We investigate feature universality in large language models (LLMs), a\nresearch field that aims to understand how different models similarly represent\nconcepts in the latent spaces of their intermediate layers. Demonstrating\nfeature universality allows discoveries about latent representations to\ngeneralize across several models. However, comparing features across LLMs is\nchallenging due to polysemanticity, in which individual neurons often\ncorrespond to multiple features rather than distinct ones, making it difficult\nto disentangle and match features across different models. To address this\nissue, we employ a method known as dictionary learning by using sparse\nautoencoders (SAEs) to transform LLM activations into more interpretable spaces\nspanned by neurons corresponding to individual features. After matching feature\nneurons across models via activation correlation, we apply representational\nspace similarity metrics on SAE feature spaces across different LLMs. Our\nexperiments reveal significant similarities in SAE feature spaces across\nvarious LLMs, providing new evidence for feature universality.\n","authors":["Michael Lan","Philip Torr","Austin Meek","Ashkan Khakzar","David Krueger","Fazl Barez"],"pdf_url":"https://arxiv.org/pdf/2410.06981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21368v3","updated":"2025-03-17T00:27:45Z","published":"2024-07-31T06:34:38Z","title":"Prompting Medical Large Vision-Language Models to Diagnose Pathologies\n  by Visual Question Answering","summary":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.\n","authors":["Danfeng Guo","Demetri Terzopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.21368v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2025:004"},{"id":"http://arxiv.org/abs/2406.07155v3","updated":"2025-03-17T00:22:42Z","published":"2024-06-11T11:02:04Z","title":"Scaling Large Language Model-based Multi-Agent Collaboration","summary":"  Recent breakthroughs in large language model-driven autonomous agents have\nrevealed that multi-agent collaboration often surpasses each individual through\ncollective reasoning. Inspired by the neural scaling law--increasing neurons\nenhances performance, this study explores whether the continuous addition of\ncollaborative agents can yield similar benefits. Technically, we utilize\ndirected acyclic graphs to organize agents into a multi-agent collaboration\nnetwork (MacNet), upon which their interactive reasoning is topologically\norchestrated for autonomous task solving. Extensive evaluations reveal that it\neffectively supports collaboration among over a thousand agents, with irregular\ntopologies outperforming regular ones. We also identify a collaborative scaling\nlaw--the overall performance follows a logistic growth pattern as agents scale,\nwith collaborative emergence occurring earlier than traditional neural\nemergence. We speculate this may be because scaling agents catalyzes their\nmultidimensional considerations during interactive reflection and refinement,\nthereby producing more comprehensive artifacts. The code is available at\nhttps://github.com/OpenBMB/ChatDev/tree/macnet.\n","authors":["Chen Qian","Zihao Xie","YiFei Wang","Wei Liu","Kunlun Zhu","Hanchen Xia","Yufan Dang","Zhuoyun Du","Weize Chen","Cheng Yang","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.07155v3.pdf","comment":"Accepted to ICLR-2025"},{"id":"http://arxiv.org/abs/2503.13773v1","updated":"2025-03-17T23:38:29Z","published":"2025-03-17T23:38:29Z","title":"Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference","summary":"  In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.\n","authors":["Haiying Shen","Tanmoy Sen"],"pdf_url":"https://arxiv.org/pdf/2503.13773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08726v2","updated":"2025-03-17T21:49:49Z","published":"2024-11-13T16:08:40Z","title":"Analyst Reports and Stock Performance: Evidence from the Chinese Market","summary":"  This article applies natural language processing (NLP) to extract and\nquantify textual information to predict stock performance. Using an extensive\ndataset of Chinese analyst reports and employing a customized BERT deep\nlearning model for Chinese text, this study categorizes the sentiment of the\nreports as positive, neutral, or negative. The findings underscore the\npredictive capacity of this sentiment indicator for stock volatility, excess\nreturns, and trading volume. Specifically, analyst reports with strong positive\nsentiment will increase excess return and intraday volatility, and vice versa,\nreports with strong negative sentiment also increase volatility and trading\nvolume, but decrease future excess return. The magnitude of this effect is\ngreater for positive sentiment reports than for negative sentiment reports.\nThis article contributes to the empirical literature on sentiment analysis and\nthe response of the stock market to news in the Chinese stock market.\n","authors":["Rui Liu","Jiayou Liang","Haolong Chen","Yujia Hu"],"pdf_url":"https://arxiv.org/pdf/2411.08726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13737v1","updated":"2025-03-17T21:47:43Z","published":"2025-03-17T21:47:43Z","title":"AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications","summary":"  In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.\n","authors":["Haiying Shen","Tanmoy Sen"],"pdf_url":"https://arxiv.org/pdf/2503.13737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13733v1","updated":"2025-03-17T21:41:37Z","published":"2025-03-17T21:41:37Z","title":"CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual,\n  Multi-Generator and Multi-Domain Settings","summary":"  Large language models (LLMs) have revolutionized code generation, automating\nprogramming with remarkable efficiency. However, these advancements challenge\nprogramming skills, ethics, and assessment integrity, making the detection of\nLLM-generated code essential for maintaining accountability and standards.\nWhile, there has been some research on this problem, it generally lacks domain\ncoverage and robustness, and only covers a small number of programming\nlanguages. To this end, we propose a framework capable of distinguishing\nbetween human- and LLM-written code across multiple programming languages, code\ngenerators, and domains. We use a large-scale dataset from renowned platforms\nand LLM-based code generators, alongside applying rigorous data quality checks,\nfeature engineering, and comparative analysis using evaluation of traditional\nmachine learning models, pre-trained language models (PLMs), and LLMs for code\ndetection. We perform an evaluation on out-of-domain scenarios, such as\ndetecting the authorship and hybrid authorship of generated code and\ngeneralizing to unseen models, domains, and programming languages. Moreover,\nour extensive experiments show that our framework effectively distinguishes\nhuman- from LLM-written code and sets a new benchmark for this task.\n","authors":["Daniil Orel","Dilshod Azizov","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2503.13733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13730v1","updated":"2025-03-17T21:36:31Z","published":"2025-03-17T21:36:31Z","title":"TextInVision: Text and Prompt Complexity Driven Visual Text Generation\n  Benchmark","summary":"  Generating images with embedded text is crucial for the automatic production\nof visual and multimodal documents, such as educational materials and\nadvertisements. However, existing diffusion-based text-to-image models often\nstruggle to accurately embed text within images, facing challenges in spelling\naccuracy, contextual relevance, and visual coherence. Evaluating the ability of\nsuch models to embed text within a generated image is complicated due to the\nlack of comprehensive benchmarks. In this work, we introduce TextInVision, a\nlarge-scale, text and prompt complexity driven benchmark designed to evaluate\nthe ability of diffusion models to effectively integrate visual text into\nimages. We crafted a diverse set of prompts and texts that consider various\nattributes and text characteristics. Additionally, we prepared an image dataset\nto test Variational Autoencoder (VAE) models across different character\nrepresentations, highlighting that VAE architectures can also pose challenges\nin text generation within diffusion frameworks. Through extensive analysis of\nmultiple models, we identify common errors and highlight issues such as\nspelling inaccuracies and contextual mismatches. By pinpointing the failure\npoints across different prompts and texts, our research lays the foundation for\nfuture advancements in AI-generated multimodal content.\n","authors":["Forouzan Fallah","Maitreya Patel","Agneet Chatterjee","Vlad I. Morariu","Chitta Baral","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13730v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08437v2","updated":"2025-03-17T21:03:16Z","published":"2024-10-11T00:56:37Z","title":"AutoEval: Autonomous Evaluation of LLMs for Truth Maintenance and\n  Reasoning Tasks","summary":"  This paper presents AutoEval, a novel benchmark for scaling Large Language\nModel (LLM) assessment in formal tasks with clear notions of correctness, such\nas truth maintenance in translation and logical reasoning. AutoEval is the\nfirst benchmarking paradigm that offers several key advantages necessary for\nscaling objective evaluation of LLMs without human labeling: (a) ability to\nevaluate LLMs of increasing sophistication by auto-generating tasks at\ndifferent levels of difficulty; (b) auto-generation of ground truth that\neliminates dependence on expensive and time-consuming human annotation; (c) the\nuse of automatically generated, randomized datasets that mitigate the ability\nof successive LLMs to overfit to static datasets used in many contemporary\nbenchmarks. Empirical analysis shows that an LLM's performance on AutoEval is\nhighly indicative of its performance on a diverse array of other benchmarks\nfocusing on translation and reasoning tasks, making it a valuable autonomous\nevaluation paradigm in settings where hand-curated datasets can be hard to\nobtain and/or update.\n","authors":["Rushang Karia","Daniel Bramblett","Daksh Dobhal","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2410.08437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00847v2","updated":"2025-03-17T20:25:48Z","published":"2025-03-02T10:49:10Z","title":"Argument Summarization and its Evaluation in the Era of Large Language\n  Models","summary":"  Large Language Models (LLMs) have revolutionized various Natural Language\nGeneration (NLG) tasks, including Argument Summarization (ArgSum), a key\nsubfield of Argument Mining (AM). This paper investigates the integration of\nstate-of-the-art LLMs into ArgSum, including for its evaluation. In particular,\nwe propose a novel prompt-based evaluation scheme, and validate it through a\nnovel human benchmark dataset. Our work makes three main contributions: (i) the\nintegration of LLMs into existing ArgSum frameworks, (ii) the development of a\nnew LLM-based ArgSum system, benchmarked against prior methods, and (iii) the\nintroduction of an advanced LLM-based evaluation scheme. We demonstrate that\nthe use of LLMs substantially improves both the generation and evaluation of\nargument summaries, achieving state-of-the-art results and advancing the field\nof ArgSum.\n","authors":["Moritz Altemeyer","Steffen Eger","Johannes Daxenberger","Tim Altendorf","Philipp Cimiano","Benjamin Schiller"],"pdf_url":"https://arxiv.org/pdf/2503.00847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13690v1","updated":"2025-03-17T19:59:19Z","published":"2025-03-17T19:59:19Z","title":"Atyaephyra at SemEval-2025 Task 4: Low-Rank NPO","summary":"  We present a submission to the SemEval 2025 shared task on unlearning\nsensitive content from LLMs. Our approach employs negative preference\noptimization using low-rank adaptation. We show that we can utilize this\ncombination to cheaply compute additional regularization terms, which help with\nunlearning stabilization. The results of our approach significantly exceed the\nshared task baselines.\n","authors":["Jan Bronec","Jindřich Helcl"],"pdf_url":"https://arxiv.org/pdf/2503.13690v1.pdf","comment":"5 pages, 1 figure, 1 table, submitted to SemEval proceedings for ACL\n  Anthology"},{"id":"http://arxiv.org/abs/2503.13687v1","updated":"2025-03-17T19:52:43Z","published":"2025-03-17T19:52:43Z","title":"Feature Extraction and Analysis for GPT-Generated Text","summary":"  With the rise of advanced natural language models like GPT, distinguishing\nbetween human-written and GPT-generated text has become increasingly\nchallenging and crucial across various domains, including academia. The\nlong-standing issue of plagiarism has grown more pressing, now compounded by\nconcerns about the authenticity of information, as it is not always clear\nwhether the presented facts are genuine or fabricated. In this paper, we\npresent a comprehensive study of feature extraction and analysis for\ndifferentiating between human-written and GPT-generated text. By applying\nmachine learning classifiers to these extracted features, we evaluate the\nsignificance of each feature in detection. Our results demonstrate that human\nand GPT-generated texts exhibit distinct writing styles, which can be\neffectively captured by our features. Given sufficiently long text, the two can\nbe differentiated with high accuracy.\n","authors":["A. Selvioğlu","V. Adanova","M. Atagoziev"],"pdf_url":"https://arxiv.org/pdf/2503.13687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05022v2","updated":"2025-03-17T19:19:14Z","published":"2024-07-06T09:31:02Z","title":"A Principled Framework for Evaluating on Typologically Diverse Languages","summary":"  Beyond individual languages, multilingual natural language processing (NLP)\nresearch increasingly aims to develop models that perform well across languages\ngenerally. However, evaluating these systems on all the world's languages is\npractically infeasible. To attain generalizability, representative language\nsampling is essential. Previous work argues that generalizable multilingual\nevaluation sets should contain languages with diverse typological properties.\nHowever, 'typologically diverse' language samples have been found to vary\nconsiderably in this regard, and popular sampling methods are flawed and\ninconsistent. We present a language sampling framework for selecting highly\ntypologically diverse languages given a sampling frame, informed by language\ntypology. We compare sampling methods with a range of metrics and find that our\nsystematic methods consistently retrieve more typologically diverse language\nselections than previous methods in NLP. Moreover, we provide evidence that\nthis affects generalizability in multilingual model evaluation, emphasizing the\nimportance of diverse language sampling in NLP evaluation.\n","authors":["Esther Ploeger","Wessel Poelman","Andreas Holck Høeg-Petersen","Anders Schlichtkrull","Miryam de Lhoneux","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2407.05022v2.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2503.13661v1","updated":"2025-03-17T19:09:11Z","published":"2025-03-17T19:09:11Z","title":"Pensez: Less Data, Better Reasoning -- Rethinking French LLM","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, achieving strong\nperformance in specialized domains like mathematical reasoning and non-English\nlanguages often requires extensive training on massive datasets. This paper\ninvestigates a contrasting approach: strategic fine-tuning on a small,\nhigh-quality, bilingual (English-French) dataset to enhance both the reasoning\ncapabilities and French language proficiency of a large language model. Rather\nthan relying on scale, we explore the hypothesis that targeted data curation\nand optimized training can achieve competitive, or even superior, performance.\nWe demonstrate, through targeted supervised fine-tuning (SFT) on only 2,000\ncarefully selected samples, significant improvements in mathematical reasoning.\nSpecifically, Pensez 7B exhibits an increase in accuracy of the base model up\nto 20% on the AIME25 and a 12% increase on a French MATH level 5 benchmark.\nThese results challenge the prevailing assumption that massive datasets are\naprerequisite for strong reasoning performance in LLMs, highlighting the\npotential of strategic data curation and optimized fine-tuning for enhancing\nboth specialized skills and multilingual capabilities. Our findings have\nimplications for the efficient development of high-performing, multilingual\nLLMs, especially in resource-constrained scenarios.\n","authors":["Huy Hoang Ha"],"pdf_url":"https://arxiv.org/pdf/2503.13661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13625v1","updated":"2025-03-17T18:20:30Z","published":"2025-03-17T18:20:30Z","title":"Does the Appearance of Autonomous Conversational Robots Affect User\n  Spoken Behaviors in Real-World Conference Interactions?","summary":"  We investigate the impact of robot appearance on users' spoken behavior\nduring real-world interactions by comparing a human-like android, ERICA, with a\nless anthropomorphic humanoid, TELECO. Analyzing data from 42 participants at\nSIGDIAL 2024, we extracted linguistic features such as disfluencies and\nsyntactic complexity from conversation transcripts. The results showed moderate\neffect sizes, suggesting that participants produced fewer disfluencies and\nemployed more complex syntax when interacting with ERICA. Further analysis\ninvolving training classification models like Na\\\"ive Bayes, which achieved an\nF1-score of 71.60\\%, and conducting feature importance analysis, highlighted\nthe significant role of disfluencies and syntactic complexity in interactions\nwith robots of varying human-like appearances. Discussing these findings within\nthe frameworks of cognitive load and Communication Accommodation Theory, we\nconclude that designing robots to elicit more structured and fluent user speech\ncan enhance their communicative alignment with humans.\n","authors":["Zi Haur Pang","Yahui Fu","Divesh Lala","Mikey Elmers","Koji Inoue","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2503.13625v1.pdf","comment":"This paper has been accepted as Late-Breaking Work at CHI Conference\n  on Human Factors in Computing Systems (CHI EA '25)"},{"id":"http://arxiv.org/abs/2503.13427v1","updated":"2025-03-17T17:54:55Z","published":"2025-03-17T17:54:55Z","title":"xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference","summary":"  Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source.\n","authors":["Maximilian Beck","Korbinian Pöppel","Phillip Lippe","Richard Kurle","Patrick M. Blies","Günter Klambauer","Sebastian Böck","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2503.13427v1.pdf","comment":"Code available at: https://github.com/NX-AI/xlstm and\n  https://github.com/NX-AI/xlstm-jax"},{"id":"http://arxiv.org/abs/2503.13383v1","updated":"2025-03-17T17:11:22Z","published":"2025-03-17T17:11:22Z","title":"Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning","summary":"  The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.\n","authors":["Mengyao Lyu","Yan Li","Huasong Zhong","Wenhao Yang","Hui Chen","Jungong Han","Guiguang Ding","Zhenheng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13383v1.pdf","comment":"update comparison with sota and analysis"},{"id":"http://arxiv.org/abs/2311.04928v3","updated":"2025-03-17T15:50:13Z","published":"2023-11-03T18:27:21Z","title":"Leveraging Large Language Models for Collective Decision-Making","summary":"  In various work contexts, such as meeting scheduling, collaborating, and\nproject planning, collective decision-making is essential but often challenging\ndue to diverse individual preferences, varying work focuses, and power dynamics\namong members. To address this, we propose a system leveraging Large Language\nModels (LLMs) to facilitate group decision-making by managing conversations and\nbalancing preferences among individuals. Our system aims to extract individual\npreferences from each member's conversation with the system and suggest options\nthat satisfy the preferences of the members. We specifically apply this system\nto corporate meeting scheduling. We create synthetic employee profiles and\nsimulate conversations at scale, leveraging LLMs to evaluate the system\nperformance as a novel approach to conducting a user study. Our results\nindicate efficient coordination with reduced interactions between the members\nand the LLM-based system. The system refines and improves its proposed options\nover time, ensuring that many of the members' individual preferences are\nsatisfied in an equitable way. Finally, we conduct a survey study involving\nhuman participants to assess our system's ability to aggregate preferences and\nreasoning about them. Our findings show that the system exhibits strong\nperformance in both dimensions.\n","authors":["Marios Papachristou","Longqi Yang","Chin-Chia Hsu"],"pdf_url":"https://arxiv.org/pdf/2311.04928v3.pdf","comment":"To appear at ACM CSCW 2025"},{"id":"http://arxiv.org/abs/2306.07207v3","updated":"2025-03-17T13:51:51Z","published":"2023-06-12T16:11:10Z","title":"Valley: Video Assistant with Large Language model Enhanced abilitY","summary":"  Large Language Models (LLMs), with remarkable conversational capability, have\nemerged as AI assistants that can handle both visual and textual modalities.\nHowever, their effectiveness in joint video and language understanding has not\nbeen extensively explored. In the paper, we introduce Valley, a multi-modal\nfoundation model that is designed to enable enhanced video comprehension and\ninstruction-following capabilities. To this end, we construct two datasets,\nnamely Valley-702k and Valley-instruct-73k, to cover a diverse range of\nvideo-text alignment and video-based instruction tasks, such as multi-shot\ncaptions, long video descriptions, action recognition, causal inference, etc.\nThen, we adopt ViT-L/14 as the vision encoder and explore three different\ntemporal modeling modules to learn multifaceted features for enhanced video\nunderstanding. In addition, we implement a two-phase training approach for\nValley: the first phase focuses solely on training the projection module to\nfacilitate the LLM's capacity to understand visual input, and the second phase\njointly trains the projection module and the LLM to improve their instruction\nfollowing ability. Extensive experiments demonstrate that Valley has the\npotential to serve as an effective video assistant, simplifying complex\nvideo-understanding scenarios. Our code and data are published anonymously at\nhttps://github.com/valley-vl/Valley.\n","authors":["Ruipu Luo","Ziwang Zhao","Min Yang","Zheming Yang","Minghui Qiu","Tao Wang","Zhongyu Wei","Yanhao Wang","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2306.07207v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13575v1","updated":"2025-03-17T13:40:46Z","published":"2025-03-17T13:40:46Z","title":"Analytic Subspace Routing: How Recursive Least Squares Works in\n  Continual Learning of Large Language Model","summary":"  Large Language Models (LLMs) possess encompassing capabilities that can\nprocess diverse language-related tasks. However, finetuning on LLMs will\ndiminish this general skills and continual finetuning will further cause severe\ndegradation on accumulated knowledge. Recently, Continual Learning (CL) in\nLarge Language Models (LLMs) arises which aims to continually adapt the LLMs to\nnew tasks while maintaining previously learned knowledge and inheriting general\nskills. Existing techniques either leverage previous data to replay, leading to\nextra computational costs, or utilize a single parameter-efficient module to\nlearn the downstream task, constraining new knowledge absorption with\ninterference between different tasks. Toward these issues, this paper proposes\nAnalytic Subspace Routing(ASR) to address these challenges. For each task, we\nisolate the learning within a subspace of deep layers' features via low-rank\nadaptation, eliminating knowledge interference between different tasks.\nAdditionally, we propose an analytic routing mechanism to properly utilize\nknowledge learned in different subspaces. Our approach employs Recursive Least\nSquares to train a multi-task router model, allowing the router to dynamically\nadapt to incoming data without requiring access to historical data. Also, the\nrouter effectively assigns the current task to an appropriate subspace and has\na non-forgetting property of previously learned tasks with a solid theoretical\nguarantee. Experimental results demonstrate that our method achieves\nnear-perfect retention of prior knowledge while seamlessly integrating new\ninformation, effectively overcoming the core limitations of existing methods.\nOur code will be released after acceptance.\n","authors":["Kai Tong","Kang Pan","Xiao Zhang","Erli Meng","Run He","Yawen Cui","Nuoyan Guo","Huiping Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.13575v1.pdf","comment":"11 pages, 4 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.13446v1","updated":"2025-03-17T17:59:52Z","published":"2025-03-17T17:59:52Z","title":"MoManipVLA: Transferring Vision-language-action Models for General\n  Mobile Manipulation","summary":"  Mobile manipulation is the fundamental challenge for robotics to assist\nhumans with diverse tasks and environments in everyday life. However,\nconventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments because of the lack of large-scale training.\nIn contrast, recent advances in vision-language-action (VLA) models have shown\nimpressive generalization capabilities, but these foundation models are\ndeveloped for fixed-base manipulation tasks. Therefore, we propose an efficient\npolicy adaptation framework named MoManipVLA to transfer pre-trained VLA models\nof fix-base manipulation to mobile manipulation, so that high generalization\nability across tasks and environments can be achieved in mobile manipulation\npolicy. Specifically, we utilize pre-trained VLA models to generate waypoints\nof the end-effector with high generalization ability. We design motion planning\nobjectives for the mobile base and the robot arm, which aim at maximizing the\nphysical feasibility of the trajectory. Finally, we present an efficient\nbi-level objective optimization framework for trajectory generation, where the\nupper-level optimization predicts waypoints for base movement to enhance the\nmanipulator policy space, and the lower-level optimization selects the optimal\nend-effector trajectory to complete the manipulation task. In this way,\nMoManipVLA can adjust the position of the robot base in a zero-shot manner,\nthus making the waypoints predicted from the fixed-base VLA models feasible.\nExtensive experimental results on OVMM and the real world demonstrate that\nMoManipVLA achieves a 4.2% higher success rate than the state-of-the-art mobile\nmanipulation, and only requires 50 training cost for real world deployment due\nto the strong generalization ability in the pre-trained VLA models.\n","authors":["Zhenyu Wu","Yuheng Zhou","Xiuwei Xu","Ziwei Wang","Haibin Yan"],"pdf_url":"https://arxiv.org/pdf/2503.13446v1.pdf","comment":"Accepted to CVPR 2025. Project Page:\n  https://gary3410.github.io/momanipVLA/"},{"id":"http://arxiv.org/abs/2503.13444v1","updated":"2025-03-17T17:59:33Z","published":"2025-03-17T17:59:33Z","title":"VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning","summary":"  Videos, with their unique temporal dimension, demand precise grounded\nunderstanding, where answers are directly linked to visual, interpretable\nevidence. Despite significant breakthroughs in reasoning capabilities within\nLarge Language Models, multi-modal reasoning - especially for videos - remains\nunexplored. In this work, we introduce VideoMind, a novel video-language agent\ndesigned for temporal-grounded video understanding. VideoMind incorporates two\nkey innovations: (i) We identify essential capabilities for video temporal\nreasoning and develop a role-based agentic workflow, including a planner for\ncoordinating different roles, a grounder for temporal localization, a verifier\nto assess temporal interval accuracy, and an answerer for question-answering.\n(ii) To efficiently integrate these diverse roles, we propose a novel\nChain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA\nadaptors while avoiding the overhead of multiple models, thus balancing\nefficiency and flexibility. Extensive experiments on 14 public benchmarks\ndemonstrate that our agent achieves state-of-the-art performance on diverse\nvideo understanding tasks, including 3 on grounded video question-answering, 6\non video temporal grounding, and 5 on general video question-answering,\nunderscoring its effectiveness in advancing video agent and long-form temporal\nreasoning.\n","authors":["Ye Liu","Kevin Qinghong Lin","Chang Wen Chen","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2503.13444v1.pdf","comment":"Project Page: https://videomind.github.io/"},{"id":"http://arxiv.org/abs/2503.13443v1","updated":"2025-03-17T17:59:27Z","published":"2025-03-17T17:59:27Z","title":"DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models","summary":"  The Base-New Trade-off (BNT) problem universally exists during the\noptimization of CLIP-based prompt tuning, where continuous fine-tuning on base\n(target) classes leads to a simultaneous decrease of generalization ability on\nnew (unseen) classes. Existing approaches attempt to regulate the prompt tuning\nprocess to balance BNT by appending constraints. However, imposed on the same\ntarget prompt, these constraints fail to fully avert the mutual exclusivity\nbetween the optimization directions for base and new. As a novel solution to\nthis challenge, we propose the plug-and-play Dual-Prompt Collaboration (DPC)\nframework, the first that decoupling the optimization processes of base and new\ntasks at the prompt level. Specifically, we clone a learnable parallel prompt\nbased on the backbone prompt, and introduce a variable Weighting-Decoupling\nframework to independently control the optimization directions of dual prompts\nspecific to base or new tasks, thus avoiding the conflict in generalization.\nMeanwhile, we propose a Dynamic Hard Negative Optimizer, utilizing dual prompts\nto construct a more challenging optimization task on base classes for\nenhancement. For interpretability, we prove the feature channel invariance of\nthe prompt vector during the optimization process, providing theoretical\nsupport for the Weighting-Decoupling of DPC. Extensive experiments on multiple\nbackbones demonstrate that DPC can significantly improve base performance\nwithout introducing any external knowledge beyond the base classes, while\nmaintaining generalization to new classes. Code is available at:\nhttps://github.com/JREion/DPC.\n","authors":["Haoyang Li","Liang Wang","Chao Wang","Jing Jiang","Yan Peng","Guodong Long"],"pdf_url":"https://arxiv.org/pdf/2503.13443v1.pdf","comment":"Accepted by the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025 (CVPR 2025)"},{"id":"http://arxiv.org/abs/2503.13441v1","updated":"2025-03-17T17:59:09Z","published":"2025-03-17T17:59:09Z","title":"Humanoid Policy ~ Human Policy","summary":"  Training manipulation policies for humanoid robots with diverse data enhances\ntheir robustness and generalization across tasks and platforms. However,\nlearning solely from robot demonstrations is labor-intensive, requiring\nexpensive tele-operated data collection which is difficult to scale. This paper\ninvestigates a more scalable data source, egocentric human demonstrations, to\nserve as cross-embodiment training data for robot learning. We mitigate the\nembodiment gap between humanoids and humans from both the data and modeling\nperspectives. We collect an egocentric task-oriented dataset (PH2D) that is\ndirectly aligned with humanoid manipulation demonstrations. We then train a\nhuman-humanoid behavior policy, which we term Human Action Transformer (HAT).\nThe state-action space of HAT is unified for both humans and humanoid robots\nand can be differentiably retargeted to robot actions. Co-trained with\nsmaller-scale robot data, HAT directly models humanoid robots and humans as\ndifferent embodiments without additional supervision. We show that human data\nimproves both generalization and robustness of HAT with significantly better\ndata collection efficiency. Code and data: https://human-as-robot.github.io/\n","authors":["Ri-Zhao Qiu","Shiqi Yang","Xuxin Cheng","Chaitanya Chawla","Jialong Li","Tairan He","Ge Yan","Lars Paulsen","Ge Yang","Sha Yi","Guanya Shi","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13441v1.pdf","comment":"Code and data: https://human-as-robot.github.io/"},{"id":"http://arxiv.org/abs/2503.13439v1","updated":"2025-03-17T17:59:01Z","published":"2025-03-17T17:59:01Z","title":"Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images","summary":"  Most image-based 3D object reconstructors assume that objects are fully\nvisible, ignoring occlusions that commonly occur in real-world scenarios. In\nthis paper, we introduce Amodal3R, a conditional 3D generative model designed\nto reconstruct 3D objects from partial observations. We start from a\n\"foundation\" 3D generative model and extend it to recover plausible 3D geometry\nand appearance from occluded objects. We introduce a mask-weighted multi-head\ncross-attention mechanism followed by an occlusion-aware attention layer that\nexplicitly leverages occlusion priors to guide the reconstruction process. We\ndemonstrate that, by training solely on synthetic data, Amodal3R learns to\nrecover full 3D objects even in the presence of occlusions in real scenes. It\nsubstantially outperforms existing methods that independently perform 2D amodal\ncompletion followed by 3D reconstruction, thereby establishing a new benchmark\nfor occlusion-aware 3D reconstruction.\n","authors":["Tianhao Wu","Chuanxia Zheng","Frank Guan","Andrea Vedaldi","Tat-Jen Cham"],"pdf_url":"https://arxiv.org/pdf/2503.13439v1.pdf","comment":"Project Page: https://sm0kywu.github.io/Amodal3R/"},{"id":"http://arxiv.org/abs/2503.13440v1","updated":"2025-03-17T17:59:01Z","published":"2025-03-17T17:59:01Z","title":"MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling","summary":"  With the advancement of RNN models with linear complexity, the quadratic\ncomplexity challenge of transformers has the potential to be overcome. Notably,\nthe emerging Mamba-2 has demonstrated competitive performance, bridging the gap\nbetween RNN models and transformers. However, due to sequential processing and\nvanishing gradients, RNN models struggle to capture long-range dependencies,\nlimiting contextual understanding. This results in slow convergence, high\nresource demands, and poor performance on downstream understanding and complex\nreasoning tasks. In this work, we present a hybrid model MaTVLM by substituting\na portion of the transformer decoder layers in a pre-trained VLM with Mamba-2\nlayers. Leveraging the inherent relationship between attention and Mamba-2, we\ninitialize Mamba-2 with corresponding attention weights to accelerate\nconvergence. Subsequently, we employ a single-stage distillation process, using\nthe pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,\nfurther enhancing convergence speed and performance. Furthermore, we\ninvestigate the impact of differential distillation loss within our training\nframework. We evaluate the MaTVLM on multiple benchmarks, demonstrating\ncompetitive performance against the teacher model and existing VLMs while\nsurpassing both Mamba-based VLMs and models of comparable parameter scales.\nRemarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher\nmodel while reducing GPU memory consumption by 27.5%, all without compromising\nperformance. Code and models are released at http://github.com/hustvl/MaTVLM.\n","authors":["Yingyue Li","Bencheng Liao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13440v1.pdf","comment":"Code and model are available at http://github.com/hustvl/MaTVLM"},{"id":"http://arxiv.org/abs/2503.13436v1","updated":"2025-03-17T17:58:30Z","published":"2025-03-17T17:58:30Z","title":"Unified Autoregressive Visual Generation and Understanding with\n  Continuous Tokens","summary":"  We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding.\n","authors":["Lijie Fan","Luming Tang","Siyang Qin","Tianhong Li","Xuan Yang","Siyuan Qiao","Andreas Steiner","Chen Sun","Yuanzhen Li","Tao Zhu","Michael Rubinstein","Michalis Raptis","Deqing Sun","Radu Soricut"],"pdf_url":"https://arxiv.org/pdf/2503.13436v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2503.13435v1","updated":"2025-03-17T17:58:18Z","published":"2025-03-17T17:58:18Z","title":"WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range\n  Movements and Scenes","summary":"  With the rapid development of 3D reconstruction technology, research in 4D\nreconstruction is also advancing, existing 4D reconstruction methods can\ngenerate high-quality 4D scenes. However, due to the challenges in acquiring\nmulti-view video data, the current 4D reconstruction benchmarks mainly display\nactions performed in place, such as dancing, within limited scenarios. In\npractical scenarios, many scenes involve wide-range spatial movements,\nhighlighting the limitations of existing 4D reconstruction datasets.\nAdditionally, existing 4D reconstruction methods rely on deformation fields to\nestimate the dynamics of 3D objects, but deformation fields struggle with\nwide-range spatial movements, which limits the ability to achieve high-quality\n4D scene reconstruction with wide-range spatial movements. In this paper, we\nfocus on 4D scene reconstruction with significant object spatial movements and\npropose a novel 4D reconstruction benchmark, WideRange4D. This benchmark\nincludes rich 4D scene data with large spatial variations, allowing for a more\ncomprehensive evaluation of the generation capabilities of 4D generation\nmethods. Furthermore, we introduce a new 4D reconstruction method, Progress4D,\nwhich generates stable and high-quality 4D results across various complex 4D\nscene reconstruction tasks. We conduct both quantitative and qualitative\ncomparison experiments on WideRange4D, showing that our Progress4D outperforms\nexisting state-of-the-art 4D reconstruction methods. Project:\nhttps://github.com/Gen-Verse/WideRange4D\n","authors":["Ling Yang","Kaixin Zhu","Juanxi Tian","Bohan Zeng","Mingbao Lin","Hongjuan Pei","Wentao Zhang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2503.13435v1.pdf","comment":"Project: https://github.com/Gen-Verse/WideRange4D"},{"id":"http://arxiv.org/abs/2503.13434v1","updated":"2025-03-17T17:58:05Z","published":"2025-03-17T17:58:05Z","title":"BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing","summary":"  Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/\n","authors":["Yaowei Li","Lingen Li","Zhaoyang Zhang","Xiaoyu Li","Guangzhi Wang","Hongxiang Li","Xiaodong Cun","Ying Shan","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2503.13434v1.pdf","comment":"Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/"},{"id":"http://arxiv.org/abs/2503.13433v1","updated":"2025-03-17T17:57:29Z","published":"2025-03-17T17:57:29Z","title":"Less Biased Noise Scale Estimation for Threshold-Robust RANSAC","summary":"  The gold-standard for robustly estimating relative pose through image\nmatching is RANSAC. While RANSAC is powerful, it requires setting the inlier\nthreshold that determines whether the error of a correspondence under an\nestimated model is sufficiently small to be included in its consensus set.\nSetting this threshold is typically done by hand, and is difficult to tune\nwithout a access to ground truth data. Thus, a method capable of automatically\ndetermining the optimal threshold would be desirable. In this paper we revisit\ninlier noise scale estimation, which is an attractive approach as the inlier\nnoise scale is linear to the optimal threshold. We revisit the noise scale\nestimation method SIMFIT and find bias in the estimate of the noise scale. In\nparticular, we fix underestimates from using the same data for fitting the\nmodel as estimating the inlier noise, and from not taking the threshold itself\ninto account. Secondly, since the optimal threshold within a scene is\napproximately constant we propose a multi-pair extension of SIMFIT++, by\nfiltering of estimates, which improves results. Our approach yields robust\nperformance across a range of thresholds, shown in Figure 1.\n","authors":["Johan Edstedt"],"pdf_url":"https://arxiv.org/pdf/2503.13433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13430v1","updated":"2025-03-17T17:55:32Z","published":"2025-03-17T17:55:32Z","title":"AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation\n  for Enhanced Vectorized Online HD Map Construction","summary":"  Autonomous driving requires an understanding of the infrastructure elements,\nsuch as lanes and crosswalks. To navigate safely, this understanding must be\nderived from sensor data in real-time and needs to be represented in vectorized\nform. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set\nof camera images from multiple views into one joint latent BEV grid.\nTraditionally, from this latent space, an intermediate raster map is predicted,\nproviding dense spatial supervision but requiring post-processing into the\ndesired vectorized form. More recent models directly derive infrastructure\nelements as polylines using vectorized map decoders, providing instance-level\ninformation. Our approach, Augmentation Map Network (AugMapNet), proposes\nlatent BEV grid augmentation, a novel technique that significantly enhances the\nlatent BEV representation. AugMapNet combines vector decoding and dense spatial\nsupervision more effectively than existing architectures while remaining as\nstraightforward to integrate and as generic as auxiliary supervision.\nExperiments on nuScenes and Argoverse2 datasets demonstrate significant\nimprovements in vectorized map prediction performance up to 13.3% over the\nStreamMapNet baseline on 60m range and greater improvements on larger ranges.\nWe confirm transferability by applying our method to another baseline and find\nsimilar improvements. A detailed analysis of the latent BEV grid confirms a\nmore structured latent space of AugMapNet and shows the value of our novel\nconcept beyond pure performance improvement. The code will be released soon.\n","authors":["Thomas Monninger","Md Zafar Anwar","Stanislaw Antol","Steffen Staab","Sihao Ding"],"pdf_url":"https://arxiv.org/pdf/2503.13430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13429v1","updated":"2025-03-17T17:55:15Z","published":"2025-03-17T17:55:15Z","title":"Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable\n  3D Neural Object Volumes","summary":"  With the rise of neural networks, especially in high-stakes applications,\nthese networks need two properties (i) robustness and (ii) interpretability to\nensure their safety. Recent advances in classifiers with 3D volumetric object\nrepresentations have demonstrated a greatly enhanced robustness in\nout-of-distribution data. However, these 3D-aware classifiers have not been\nstudied from the perspective of interpretability. We introduce CAVE - Concept\nAware Volumes for Explanations - a new direction that unifies interpretability\nand robustness in image classification. We design an inherently-interpretable\nand robust classifier by extending existing 3D-aware classifiers with concepts\nextracted from their volumetric representations for classification. In an array\nof quantitative metrics for interpretability, we compare against different\nconcept-based approaches across the explainable AI literature and show that\nCAVE discovers well-grounded concepts that are used consistently across images,\nwhile achieving superior robustness.\n","authors":["Nhi Pham","Bernt Schiele","Adam Kortylewski","Jonas Fischer"],"pdf_url":"https://arxiv.org/pdf/2503.13429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08354v2","updated":"2025-03-17T17:54:40Z","published":"2025-03-11T12:09:11Z","title":"Robust Latent Matters: Boosting Image Generation with Sampling Error\n  Synthesis","summary":"  Recent image generation schemes typically capture image distribution in a\npre-constructed latent space relying on a frozen image tokenizer. Though the\nperformance of tokenizer plays an essential role to the successful generation,\nits current evaluation metrics (e.g. rFID) fail to precisely assess the\ntokenizer and correlate its performance to the generation quality (e.g. gFID).\nIn this paper, we comprehensively analyze the reason for the discrepancy of\nreconstruction and generation qualities in a discrete latent space, and, from\nwhich, we propose a novel plug-and-play tokenizer training scheme to facilitate\nlatent space construction. Specifically, a latent perturbation approach is\nproposed to simulate sampling noises, i.e., the unexpected tokens sampled, from\nthe generative process. With the latent perturbation, we further propose (1) a\nnovel tokenizer evaluation metric, i.e., pFID, which successfully correlates\nthe tokenizer performance to generation quality and (2) a plug-and-play\ntokenizer training scheme, which significantly enhances the robustness of\ntokenizer thus boosting the generation quality and convergence speed. Extensive\nbenchmarking are conducted with 11 advanced discrete image tokenizers with 2\nautoregressive generation models to validate our approach. The tokenizer\ntrained with our proposed latent perturbation achieve a notable 1.60 gFID with\nclassifier-free guidance (CFG) and 3.45 gFID without CFG with a $\\sim$400M\ngenerator. Code: https://github.com/lxa9867/ImageFolder.\n","authors":["Kai Qiu","Xiang Li","Jason Kuen","Hao Chen","Xiaohao Xu","Jiuxiang Gu","Yinyi Luo","Bhiksha Raj","Zhe Lin","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2503.08354v2.pdf","comment":"17 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.13424v1","updated":"2025-03-17T17:53:56Z","published":"2025-03-17T17:53:56Z","title":"Infinite Mobility: Scalable High-Fidelity Synthesis of Articulated\n  Objects via Procedural Generation","summary":"  Large-scale articulated objects with high quality are desperately needed for\nmultiple tasks related to embodied AI. Most existing methods for creating\narticulated objects are either data-driven or simulation based, which are\nlimited by the scale and quality of the training data or the fidelity and heavy\nlabour of the simulation. In this paper, we propose Infinite Mobility, a novel\nmethod for synthesizing high-fidelity articulated objects through procedural\ngeneration. User study and quantitative evaluation demonstrate that our method\ncan produce results that excel current state-of-the-art methods and are\ncomparable to human-annotated datasets in both physics property and mesh\nquality. Furthermore, we show that our synthetic data can be used as training\ndata for generative models, enabling next-step scaling up. Code is available at\nhttps://github.com/Intern-Nexus/Infinite-Mobility\n","authors":["Xinyu Lian","Zichao Yu","Ruiming Liang","Yitong Wang","Li Ray Luo","Kaixu Chen","Yuanzhen Zhou","Qihong Tang","Xudong Xu","Zhaoyang Lyu","Bo Dai","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2503.13424v1.pdf","comment":"Project page: https://infinite-mobility.github.io 10 pages,12 figures"},{"id":"http://arxiv.org/abs/2410.05270v3","updated":"2025-03-17T17:52:55Z","published":"2024-10-07T17:59:59Z","title":"CLIP's Visual Embedding Projector is a Few-shot Cornucopia","summary":"  We consider the problem of adapting a contrastively pretrained\nvision-language model like CLIP (Radford et al., 2021) for few-shot\nclassification. The literature addresses this problem by learning a linear\nclassifier of the frozen visual features, optimizing word embeddings, or\nlearning external feature adapters. We introduce an alternative way for\nfew-shot CLIP adaptation without adding ''external'' parameters to optimize. We\nfind that simply fine-tuning the embedding projection matrix of the vision\nencoder leads to better performance than all baselines. Furthermore, we show\nthat regularizing training with the distance between the fine-tuned and\npretrained matrices adds reliability for adapting CLIP, making the results\nstable across different learning rates in the ''validation-free'' setting. This\nsimple approach, coined ProLIP, yields state-of-the-art performance on 11\nfew-shot classification benchmarks, few-shot cross-dataset transfer, domain\ngeneralization, and base-to-new class generalization. We also show that ProLIP\nsignificantly outperforms prompt tuning when extended to another task of\ntest-time adaptation, while being one order of magnitude faster to train. Code\nwill be made available at: https://github.com/astra-vision/ProLIP .\n","authors":["Mohammad Fahes","Tuan-Hung Vu","Andrei Bursuc","Patrick Pérez","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2410.05270v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17698v4","updated":"2025-03-17T17:44:37Z","published":"2024-11-26T18:59:58Z","title":"Video-Guided Foley Sound Generation with Multimodal Controls","summary":"  Generating sound effects for videos often requires creating artistic sound\neffects that diverge significantly from real-life sources and flexible control\nin the sound design. To address this problem, we introduce MultiFoley, a model\ndesigned for video-guided sound generation that supports multimodal\nconditioning through text, audio, and video. Given a silent video and a text\nprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels\nspinning without wind noise) or more whimsical sounds (e.g., making a lion's\nroar sound like a cat's meow). MultiFoley also allows users to choose reference\naudio from sound effects (SFX) libraries or partial videos for conditioning. A\nkey novelty of our model lies in its joint training on both internet video\ndatasets with low-quality audio and professional SFX recordings, enabling\nhigh-quality, full-bandwidth (48kHz) audio generation. Through automated\nevaluations and human studies, we demonstrate that MultiFoley successfully\ngenerates synchronized high-quality sounds across varied conditional inputs and\noutperforms existing methods. Please see our project page for video results:\nhttps://ificl.github.io/MultiFoley/\n","authors":["Ziyang Chen","Prem Seetharaman","Bryan Russell","Oriol Nieto","David Bourgin","Andrew Owens","Justin Salamon"],"pdf_url":"https://arxiv.org/pdf/2411.17698v4.pdf","comment":"Accepted at CVPR 2025. Project site:\n  https://ificl.github.io/MultiFoley/"},{"id":"http://arxiv.org/abs/2412.04464v3","updated":"2025-03-17T17:42:59Z","published":"2024-12-05T18:59:48Z","title":"DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose\n  Reconstruction","summary":"  The choice of data representation is a key factor in the success of deep\nlearning in geometric tasks. For instance, DUSt3R has recently introduced the\nconcept of viewpoint-invariant point maps, generalizing depth prediction, and\nshowing that one can reduce all the key problems in the 3D reconstruction of\nstatic scenes to predicting such point maps. In this paper, we develop an\nanalogous concept for a very different problem, namely, the reconstruction of\nthe 3D shape and pose of deformable objects. To this end, we introduce the Dual\nPoint Maps (DualPM), where a pair of point maps is extracted from the same\nimage, one associating pixels to their 3D locations on the object, and the\nother to a canonical version of the object at rest pose. We also extend point\nmaps to amodal reconstruction, seeing through self-occlusions to obtain the\ncomplete shape of the object. We show that 3D reconstruction and 3D pose\nestimation reduce to the prediction of the DualPMs. We demonstrate empirically\nthat this representation is a good target for a deep network to predict;\nspecifically, we consider modeling horses, showing that DualPMs can be trained\npurely on 3D synthetic data, consisting of a single model of a horse, while\ngeneralizing very well to real images. With this, we improve by a large margin\nprevious methods for the 3D analysis and reconstruction of this type of\nobjects.\n","authors":["Ben Kaye","Tomas Jakab","Shangzhe Wu","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2412.04464v3.pdf","comment":"First two authors contributed equally. CVPR 2025. Project page:\n  https://dualpm.github.io"},{"id":"http://arxiv.org/abs/2503.13400v1","updated":"2025-03-17T17:33:32Z","published":"2025-03-17T17:33:32Z","title":"U2AD: Uncertainty-based Unsupervised Anomaly Detection Framework for\n  Detecting T2 Hyperintensity in MRI Spinal Cord","summary":"  T2 hyperintensities in spinal cord MR images are crucial biomarkers for\nconditions such as degenerative cervical myelopathy. However, current clinical\ndiagnoses primarily rely on manual evaluation. Deep learning methods have shown\npromise in lesion detection, but most supervised approaches are heavily\ndependent on large, annotated datasets. Unsupervised anomaly detection (UAD)\noffers a compelling alternative by eliminating the need for abnormal data\nannotations. However, existing UAD methods rely on curated normal datasets and\ntheir performance frequently deteriorates when applied to clinical datasets due\nto domain shifts. We propose an Uncertainty-based Unsupervised Anomaly\nDetection framework, termed U2AD, to address these limitations. Unlike\ntraditional methods, U2AD is designed to be trained and tested within the same\nclinical dataset, following a \"mask-and-reconstruction\" paradigm built on a\nVision Transformer-based architecture. We introduce an uncertainty-guided\nmasking strategy to resolve task conflicts between normal reconstruction and\nanomaly detection to achieve an optimal balance. Specifically, we employ a\nMonte-Carlo sampling technique to estimate reconstruction uncertainty mappings\nduring training. By iteratively optimizing reconstruction training under the\nguidance of both epistemic and aleatoric uncertainty, U2AD reduces overall\nreconstruction variance while emphasizing regions. Experimental results\ndemonstrate that U2AD outperforms existing supervised and unsupervised methods\nin patient-level identification and segment-level localization tasks. This\nframework establishes a new benchmark for incorporating uncertainty guidance\ninto UAD, highlighting its clinical utility in addressing domain shifts and\ntask conflicts in medical image anomaly detection. Our code is available:\nhttps://github.com/zhibaishouheilab/U2AD\n","authors":["Qi Zhang","Xiuyuan Chen","Ziyi He","Kun Wang","Lianming Wu","Hongxing Shen","Jianqi Sun"],"pdf_url":"https://arxiv.org/pdf/2503.13400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13399v1","updated":"2025-03-17T17:33:10Z","published":"2025-03-17T17:33:10Z","title":"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research","summary":"  Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.\n","authors":["James Burgess","Jeffrey J Nirschl","Laura Bravo-Sánchez","Alejandro Lozano","Sanket Rajan Gupte","Jesus G. Galaz-Montoya","Yuhui Zhang","Yuchang Su","Disha Bhowmik","Zachary Coman","Sarina M. Hasan","Alexandra Johannesson","William D. Leineweber","Malvika G Nair","Ridhi Yarlagadda","Connor Zuraski","Wah Chiu","Sarah Cohen","Jan N. Hansen","Manuel D Leonetti","Chad Liu","Emma Lundberg","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2503.13399v1.pdf","comment":"CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa"},{"id":"http://arxiv.org/abs/2503.13385v1","updated":"2025-03-17T17:13:43Z","published":"2025-03-17T17:13:43Z","title":"Scale Efficient Training for Large Datasets","summary":"  The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa.\n","authors":["Qing Zhou","Junyu Gao","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13385v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.13383v1","updated":"2025-03-17T17:11:22Z","published":"2025-03-17T17:11:22Z","title":"Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning","summary":"  The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.\n","authors":["Mengyao Lyu","Yan Li","Huasong Zhong","Wenhao Yang","Hui Chen","Jungong Han","Guiguang Ding","Zhenheng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13383v1.pdf","comment":"update comparison with sota and analysis"},{"id":"http://arxiv.org/abs/2503.13377v1","updated":"2025-03-17T17:04:20Z","published":"2025-03-17T17:04:20Z","title":"TimeZero: Temporal Video Grounding with Reasoning-Guided LVLM","summary":"  We introduce TimeZero, a reasoning-guided LVLM designed for the temporal\nvideo grounding (TVG) task. This task requires precisely localizing relevant\nvideo segments within long videos based on a given language query. TimeZero\ntackles this challenge by extending the inference process, enabling the model\nto reason about video-language relationships solely through reinforcement\nlearning. To evaluate the effectiveness of TimeZero, we conduct experiments on\ntwo benchmarks, where TimeZero achieves state-of-the-art performance on\nCharades-STA. Code is available at https://github.com/www-Ye/TimeZero.\n","authors":["Ye Wang","Boshen Xu","Zihao Yue","Zihan Xiao","Ziheng Wang","Liang Zhang","Dingyi Yang","Wenxuan Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2503.13377v1.pdf","comment":"Code: https://github.com/www-Ye/TimeZero"},{"id":"http://arxiv.org/abs/2503.13369v1","updated":"2025-03-17T16:52:46Z","published":"2025-03-17T16:52:46Z","title":"Sightation Counts: Leveraging Sighted User Feedback in Building a\n  BLV-aligned Dataset of Diagram Descriptions","summary":"  Often, the needs and visual abilities differ between the annotator group and\nthe end user group. Generating detailed diagram descriptions for blind and\nlow-vision (BLV) users is one such challenging domain. Sighted annotators could\ndescribe visuals with ease, but existing studies have shown that direct\ngenerations by them are costly, bias-prone, and somewhat lacking by BLV\nstandards. In this study, we ask sighted individuals to assess -- rather than\nproduce -- diagram descriptions generated by vision-language models (VLM) that\nhave been guided with latent supervision via a multi-pass inference. The\nsighted assessments prove effective and useful to professional educators who\nare themselves BLV and teach visually impaired learners. We release Sightation,\na collection of diagram description datasets spanning 5k diagrams and 137k\nsamples for completion, preference, retrieval, question answering, and\nreasoning training purposes and demonstrate their fine-tuning potential in\nvarious downstream tasks.\n","authors":["Wan Ju Kang","Eunki Kim","Na Min An","Sangryul Kim","Haemin Choi","Ki Hoon Kwak","James Thorne"],"pdf_url":"https://arxiv.org/pdf/2503.13369v1.pdf","comment":"37 pages, 10 figures, 21 tables"},{"id":"http://arxiv.org/abs/2406.12757v2","updated":"2025-03-17T16:51:43Z","published":"2024-06-18T16:24:48Z","title":"MAC: A Benchmark for Multiple Attributes Compositional Zero-Shot\n  Learning","summary":"  Compositional Zero-Shot Learning (CZSL) aims to learn semantic primitives\n(attributes and objects) from seen compositions and recognize unseen\nattribute-object compositions. Existing CZSL datasets focus on single\nattributes, neglecting the fact that objects naturally exhibit multiple\ninterrelated attributes. Their narrow attribute scope and single attribute\nlabeling introduce annotation biases, misleading the learning of attributes and\ncausing inaccurate evaluation. To address these issues, we introduce the\nMulti-Attribute Composition (MAC) dataset, encompassing 22,838 images and\n17,627 compositions with comprehensive and representative attribute\nannotations. MAC shows complex relationship between attributes and objects,\nwith each attribute type linked to an average of 82.2 object types, and each\nobject type associated with 31.4 attribute types. Based on MAC, we propose\nmulti-attribute compositional zero-shot learning that requires deeper semantic\nunderstanding and advanced attribute associations, establishing a more\nrealistic and challenging benchmark for CZSL. We also propose Multi-attribute\nVisual-Primitive Integrator (MVP-Integrator), a robust baseline for\nmulti-attribute CZSL, which disentangles semantic primitives and performs\neffective visual-primitive association. Experimental results demonstrate that\nMVP-Integrator significantly outperforms existing CZSL methods on MAC with\nimproved inference efficiency.\n","authors":["Shuo Xu","Sai Wang","Xinyue Hu","Yutian Lin","Bo Du","Yu Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12757v2.pdf","comment":"13pages,5figures"},{"id":"http://arxiv.org/abs/2503.08154v2","updated":"2025-03-17T16:50:29Z","published":"2025-03-11T08:10:03Z","title":"Structure-Activation Synergy: A Dual Efficiency Framework for\n  Parameter-Memory Optimized Transfer Learning","summary":"  While parameter-efficient transfer learning (PETL) successfully reduces\ntrainable parameters for adapting large pre-trained models, conventional\nmethods exhibit limited effectiveness in decreasing activation memory\nconsumption - a critical bottleneck for deployment on resource-constrained\ndevices. We present Structure-Activation Synergy (S2A), an innovative framework\nachieving dual optimization of parameters and memory through two synergistic\nmechanisms: (1) Structural activation modules (bias/prompt/side adaptations)\nthat strategically minimize both parametric complexity and intermediate feature\nstorage requirements, and (2) Derivative-aware 4-bit quantization for\nnon-parametric operators that maintains model fidelity through\ngradient-informed precision allocation. Extensive evaluations across multiple\narchitectures (ViT, Swin, ResNet) and datasets (ImageNet-1K, CIFAR, DomainNet)\ndemonstrate S2A's superior efficiency, reducing GPU memory consumption by 75\\%\n(4.2 average reduction) while maintaining 98.7\\% of full fine-tuning accuracy\nwith only 0.9\\% tunable parameters. This hardware-aware paradigm establishes\nnew state-of-the-art in efficient model adaptation, offering practical\ndeployment advantages through simultaneous parameter and memory optimization\nwithout compromising model capability\n","authors":["Tian Jin","Enjun Du","Changwei Wang","Wenhao Xu","Ding Luo"],"pdf_url":"https://arxiv.org/pdf/2503.08154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13360v1","updated":"2025-03-17T16:45:12Z","published":"2025-03-17T16:45:12Z","title":"Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning","summary":"  Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.\n","authors":["Hai-Long Sun","Zhun Sun","Houwen Peng","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2503.13360v1.pdf","comment":"The project page is available at\n  https://sun-hailong.github.io/projects/TVC"},{"id":"http://arxiv.org/abs/2503.13358v1","updated":"2025-03-17T16:44:08Z","published":"2025-03-17T16:44:08Z","title":"One-Step Residual Shifting Diffusion for Image Super-Resolution via\n  Distillation","summary":"  Diffusion models for super-resolution (SR) produce high-quality visual\nresults but require expensive computational costs. Despite the development of\nseveral methods to accelerate diffusion-based SR models, some (e.g., SinSR)\nfail to produce realistic perceptual details, while others (e.g., OSEDiff) may\nhallucinate non-existent structures. To overcome these issues, we present RSD,\na new distillation method for ResShift, one of the top diffusion-based SR\nmodels. Our method is based on training the student network to produce such\nimages that a new fake ResShift model trained on them will coincide with the\nteacher model. RSD achieves single-step restoration and outperforms the teacher\nby a large margin. We show that our distillation method can surpass the other\ndistillation-based method for ResShift - SinSR - making it on par with\nstate-of-the-art diffusion-based SR distillation methods. Compared to SR\nmethods based on pre-trained text-to-image models, RSD produces competitive\nperceptual quality, provides images with better alignment to degraded input\nimages, and requires fewer parameters and GPU memory. We provide experimental\nresults on various real-world and synthetic datasets, including RealSR,\nRealSet65, DRealSR, ImageNet, and DIV2K.\n","authors":["Daniil Selikhanovych","David Li","Aleksei Leonov","Nikita Gushchin","Sergei Kushneriuk","Alexander Filippov","Evgeny Burnaev","Iaroslav Koshelev","Alexander Korotin"],"pdf_url":"https://arxiv.org/pdf/2503.13358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13354v1","updated":"2025-03-17T16:35:04Z","published":"2025-03-17T16:35:04Z","title":"Parameter-free structure-texture image decomposition by unrolling","summary":"  In this work, we propose a parameter-free and efficient method to tackle the\nstructure-texture image decomposition problem. In particular, we present a\nneural network LPR-NET based on the unrolling of the Low Patch Rank model. On\nthe one hand, this allows us to automatically learn parameters from data, and\non the other hand to be computationally faster while obtaining qualitatively\nsimilar results compared to traditional iterative model-based methods.\nMoreover, despite being trained on synthetic images, numerical experiments show\nthe ability of our network to generalize well when applied to natural images.\n","authors":["Laura Girometti","Jean-François Aujol","Antoine Guennec","Yann Traonmilin"],"pdf_url":"https://arxiv.org/pdf/2503.13354v1.pdf","comment":"To be published in Conference Proceedings: Scale Space and\n  Variational Method in Computer Vision, 2025"},{"id":"http://arxiv.org/abs/2411.19895v5","updated":"2025-03-17T16:33:17Z","published":"2024-11-29T17:59:03Z","title":"GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for\nvarious applications. However, considering security, capacity, invisibility,\nand training efficiency, the copyright of 3DGS assets is not well protected as\nexisting watermarking methods are unsuited for its rendering pipeline. In this\npaper, we propose GuardSplat, an innovative and efficient framework for\nwatermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline\nfor optimizing the message decoder with minimal costs. The key objective is to\nachieve high-accuracy extraction by leveraging CLIP's aligning capability and\nrich representations, demonstrating exceptional capacity and efficiency. 2) We\ntailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS,\nseamlessly embedding messages into the SH features of each 3D Gaussian while\npreserving the original 3D structure. This enables watermarking 3DGS assets\nwith minimal fidelity trade-offs and prevents malicious users from removing the\nwatermarks from the model files, meeting the demands for invisibility and\nsecurity. 3) We present an Anti-distortion Message Extraction module to improve\nrobustness against various distortions. Experiments demonstrate that GuardSplat\noutperforms state-of-the-art and achieves fast optimization speed. Project page\nis at https://narcissusex.github.io/GuardSplat, and Code is at\nhttps://github.com/NarcissusEx/GuardSplat.\n","authors":["Zixuan Chen","Guangcong Wang","Jiahao Zhu","Jianhuang Lai","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2411.19895v5.pdf","comment":"This paper is accepted by the IEEE/CVF International Conference on\n  Computer Vision and Pattern Recognition (CVPR), 2025"},{"id":"http://arxiv.org/abs/2503.13347v1","updated":"2025-03-17T16:25:39Z","published":"2025-03-17T16:25:39Z","title":"TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing\n  Novel View Synthesis","summary":"  Remote sensing novel view synthesis (NVS) offers significant potential for 3D\ninterpretation of remote sensing scenes, with important applications in urban\nplanning and environmental monitoring. However, remote sensing scenes\nfrequently lack sufficient multi-view images due to acquisition constraints.\nWhile existing NVS methods tend to overfit when processing limited input views,\nadvanced few-shot NVS methods are computationally intensive and perform\nsub-optimally in remote sensing scenes. This paper presents TriDF, an efficient\nhybrid 3D representation for fast remote sensing NVS from as few as 3 input\nviews. Our approach decouples color and volume density information, modeling\nthem independently to reduce the computational burden on implicit radiance\nfields and accelerate reconstruction. We explore the potential of the triplane\nrepresentation in few-shot NVS tasks by mapping high-frequency color\ninformation onto this compact structure, and the direct optimization of feature\nplanes significantly speeds up convergence. Volume density is modeled as\ncontinuous density fields, incorporating reference features from neighboring\nviews through image-based rendering to compensate for limited input data.\nAdditionally, we introduce depth-guided optimization based on point clouds,\nwhich effectively mitigates the overfitting problem in few-shot NVS.\nComprehensive experiments across multiple remote sensing scenes demonstrate\nthat our hybrid representation achieves a 30x speed increase compared to\nNeRF-based methods, while simultaneously improving rendering quality metrics\nover advanced few-shot methods (7.4% increase in PSNR, 12.2% in SSIM, and 18.7%\nin LPIPS). The code is publicly available at https://github.com/kanehub/TriDF\n","authors":["Jiaming Kang","Keyan Chen","Zhengxia Zou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2503.13347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11506v2","updated":"2025-03-17T16:22:15Z","published":"2024-10-15T11:17:19Z","title":"Spatio-Temporal Distortion Aware Omnidirectional Video Super-Resolution","summary":"  Omnidirectional video (ODV) provides an immersive visual experience and is\nwidely utilized in virtual reality and augmented reality. However, restricted\ncapturing devices and transmission bandwidth lead to low-resolution ODVs. Video\nsuper-resolution (SR) is proposed to enhance resolution, but practical ODV\nspatial projection distortions and temporal flickering are not well addressed\ndirectly applying existing methods. To achieve better ODV-SR reconstruction, we\npropose a Spatio-Temporal Distortion Aware Network (STDAN) oriented to ODV\ncharacteristics. Specifically, a spatially continuous distortion modulation\nmodule is introduced to improve discrete projection distortions. Next, we\ndesign an interlaced multi-frame reconstruction mechanism to refine temporal\nconsistency across frames. Furthermore, we incorporate latitude-saliency\nadaptive weights during training to concentrate on regions with higher texture\ncomplexity and human-watching interest. In general, we explore inference-free\nand real-world viewing matched strategies to provide an application-friendly\nmethod on a novel ODV-SR dataset with practical scenarios. Extensive\nexperimental results demonstrate the superior performance of the proposed STDAN\nover state-of-the-art methods.\n","authors":["Hongyu An","Xinfeng Zhang","Shijie Zhao","Li Zhang","Ruiqin Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.11506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13344v1","updated":"2025-03-17T16:22:00Z","published":"2025-03-17T16:22:00Z","title":"STEP: Simultaneous Tracking and Estimation of Pose for Animals and\n  Humans","summary":"  We introduce STEP, a novel framework utilizing Transformer-based\ndiscriminative model prediction for simultaneous tracking and estimation of\npose across diverse animal species and humans. We are inspired by the fact that\nthe human brain exploits spatiotemporal continuity and performs concurrent\nlocalization and pose estimation despite the specialization of brain areas for\nform and motion processing. Traditional discriminative models typically require\npredefined target states for determining model weights, a challenge we address\nthrough Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter\n(OMRA) Modules. These modules remove the necessity of keypoint target states as\ninput, streamlining the process. Our method starts with a known target state\ninitialized through a pre-trained detector or manual initialization in the\ninitial frame of a given video sequence. It then seamlessly tracks the target\nand estimates keypoints of anatomical importance as output for subsequent\nframes. Unlike prevalent top-down pose estimation methods, our approach doesn't\nrely on per-frame target detections due to its tracking capability. This\nfacilitates a significant advancement in inference efficiency and potential\napplications. We train and validate our approach on datasets encompassing\ndiverse species. Our experiments demonstrate superior results compared to\nexisting methods, opening doors to various applications, including but not\nlimited to action recognition and behavioral analysis.\n","authors":["Shashikant Verma","Harish Katti","Soumyaratna Debnath","Yamuna Swamy","Shanmuganathan Raman"],"pdf_url":"https://arxiv.org/pdf/2503.13344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13330v1","updated":"2025-03-17T16:09:22Z","published":"2025-03-17T16:09:22Z","title":"LEAVS: An LLM-based Labeler for Abdominal CT Supervision","summary":"  Extracting structured labels from radiology reports has been employed to\ncreate vision models to simultaneously detect several types of abnormalities.\nHowever, existing works focus mainly on the chest region. Few works have been\ninvestigated on abdominal radiology reports due to more complex anatomy and a\nwider range of pathologies in the abdomen. We propose LEAVS (Large language\nmodel Extractor for Abdominal Vision Supervision). This labeler can annotate\nthe certainty of presence and the urgency of seven types of abnormalities for\nnine abdominal organs on CT radiology reports. To ensure broad coverage, we\nchose abnormalities that encompass most of the finding types from CT reports.\nOur approach employs a specialized chain-of-thought prompting strategy for a\nlocally-run LLM using sentence extraction and multiple-choice questions in a\ntree-based decision system. We demonstrate that the LLM can extract several\nabnormality types across abdominal organs with an average F1 score of 0.89,\nsignificantly outperforming competing labelers and humans. Additionally, we\nshow that extraction of urgency labels achieved performance comparable to human\nannotations. Finally, we demonstrate that the abnormality labels contain\nvaluable information for training a single vision model that classifies several\norgans as normal or abnormal. We release our code and structured annotations\nfor a public CT dataset containing over 1,000 CT volumes.\n","authors":["Ricardo Bigolin Lanfredi","Yan Zhuang","Mark Finkelstein","Praveen Thoppey Srinivasan Balamuralikrishna","Luke Krembs","Brandon Khoury","Arthi Reddy","Pritam Mukherjee","Neil M. Rofsky","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2503.13330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13817v3","updated":"2025-03-17T16:05:34Z","published":"2024-12-18T13:04:30Z","title":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models\n  via HalluSpace Projection","summary":"  Recent studies have shown that large vision-language models (LVLMs) often\nsuffer from the issue of object hallucinations (OH). To mitigate this issue, we\nintroduce an efficient method that edits the model weights based on an unsafe\nsubspace, which we call HalluSpace in this paper. With truthful and\nhallucinated text prompts accompanying the visual content as inputs, the\nHalluSpace can be identified by extracting the hallucinated embedding features\nand removing the truthful representations in LVLMs. By orthogonalizing the\nmodel weights, input features will be projected into the Null space of the\nHalluSpace to reduce OH, based on which we name our method Nullu. We reveal\nthat HalluSpaces generally contain prior information in the large language\nmodels (LLMs) applied to build LVLMs, which have been shown as essential causes\nof OH in previous studies. Therefore, null space projection suppresses the\nLLMs' priors to filter out the hallucinated features, resulting in contextually\naccurate outputs. Experiments show that our method can effectively mitigate OH\nacross different LVLM families without extra inference costs and also show\nstrong performance in general LVLM benchmarks. Code is released at\nhttps://github.com/Ziwei-Zheng/Nullu.\n","authors":["Le Yang","Ziwei Zheng","Boxu Chen","Zhengyu Zhao","Chenhao Lin","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2412.13817v3.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13327v1","updated":"2025-03-17T16:04:44Z","published":"2025-03-17T16:04:44Z","title":"Edit Transfer: Learning Image Editing via Vision In-Context Relations","summary":"  We introduce a new setting, Edit Transfer, where a model learns a\ntransformation from just a single source-target example and applies it to a new\nquery image. While text-based methods excel at semantic manipulations through\ntextual prompts, they often struggle with precise geometric details (e.g.,\nposes and viewpoint changes). Reference-based editing, on the other hand,\ntypically focuses on style or appearance and fails at non-rigid\ntransformations. By explicitly learning the editing transformation from a\nsource-target pair, Edit Transfer mitigates the limitations of both text-only\nand appearance-centric references. Drawing inspiration from in-context learning\nin large language models, we propose a visual relation in-context learning\nparadigm, building upon a DiT-based text-to-image model. We arrange the edited\nexample and the query image into a unified four-panel composite, then apply\nlightweight LoRA fine-tuning to capture complex spatial transformations from\nminimal examples. Despite using only 42 training samples, Edit Transfer\nsubstantially outperforms state-of-the-art TIE and RIE methods on diverse\nnon-rigid scenarios, demonstrating the effectiveness of few-shot visual\nrelation learning.\n","authors":["Lan Chen","Qi Mao","Yuchao Gu","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2503.13327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13319v1","updated":"2025-03-17T15:58:27Z","published":"2025-03-17T15:58:27Z","title":"MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale\n  Portrait Few-Step Synthesis","summary":"  Fine-tuning open-source large-scale VDMs for the portrait video synthesis\ntask can result in significant improvements across multiple dimensions, such as\nvisual quality and natural facial motion dynamics. Despite their advancements,\nhow to achieve step distillation and reduce the substantial computational\noverhead of large-scale VDMs remains unexplored. To fill this gap, this paper\nproposes Weak-to-Strong Video Distillation (W2SVD) to mitigate both the issue\nof insufficient training memory and the problem of training collapse observed\nin vanilla DMD during the training process. Specifically, we first leverage\nLoRA to fine-tune the fake diffusion transformer (DiT) to address the\nout-of-memory issue. Then, we employ the W2S distribution matching to adjust\nthe real DiT's parameter, subtly shifting it toward the fake DiT's parameter.\nThis adjustment is achieved by utilizing the weak weight of the low-rank\nbranch, effectively alleviate the conundrum where the video synthesized by the\nfew-step generator deviates from the real data distribution, leading to\ninaccuracies in the KL divergence approximation. Additionally, we minimize the\ndistance between the fake data distribution and the ground truth distribution\nto further enhance the visual quality of the synthesized videos. As\nexperimentally demonstrated on HunyuanVideo, W2SVD surpasses the standard\nEuler, LCM, DMD and even the 28-step standard sampling in FID/FVD and VBench in\n1/4-step video synthesis. The project page is in\nhttps://w2svd.github.io/W2SVD/.\n","authors":["Shitong Shao","Hongwei Yi","Hanzhong Guo","Tian Ye","Daquan Zhou","Michael Lingelbach","Zhiqiang Xu","Zeke Xie"],"pdf_url":"https://arxiv.org/pdf/2503.13319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16820v4","updated":"2025-03-17T15:53:14Z","published":"2024-04-25T17:58:43Z","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and\n  Human Ratings","summary":"  While text-to-image (T2I) generative models have become ubiquitous, they do\nnot necessarily generate images that align with a given prompt. While previous\nwork has evaluated T2I alignment by proposing metrics, benchmarks, and\ntemplates for collecting human judgements, the quality of these components is\nnot systematically measured. Human-rated prompt sets are generally small and\nthe reliability of the ratings -- and thereby the prompt set used to compare\nmodels -- is not evaluated. We address this gap by performing an extensive\nstudy evaluating auto-eval metrics and human templates. We provide three main\ncontributions: (1) We introduce a comprehensive skills-based benchmark that can\ndiscriminate models across different human templates. This skills-based\nbenchmark categorises prompts into sub-skills, allowing a practitioner to\npinpoint not only which skills are challenging, but at what level of complexity\na skill becomes challenging. (2) We gather human ratings across four templates\nand four T2I models for a total of >100K annotations. This allows us to\nunderstand where differences arise due to inherent ambiguity in the prompt and\nwhere they arise due to differences in metric and model quality. (3) Finally,\nwe introduce a new QA-based auto-eval metric that is better correlated with\nhuman ratings than existing metrics for our new dataset, across different human\ntemplates, and on TIFA160.\n","authors":["Olivia Wiles","Chuhan Zhang","Isabela Albuquerque","Ivana Kajić","Su Wang","Emanuele Bugliarello","Yasumasa Onoe","Pinelopi Papalampidi","Ira Ktena","Chris Knutsen","Cyrus Rashtchian","Anant Nawalgaria","Jordi Pont-Tuset","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2404.16820v4.pdf","comment":"Accepted to ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2411.16738v2","updated":"2025-03-17T15:50:58Z","published":"2024-11-23T15:36:03Z","title":"Classifier-Free Guidance inside the Attraction Basin May Cause\n  Memorization","summary":"  Diffusion models are prone to exactly reproduce images from the training\ndata. This exact reproduction of the training data is concerning as it can lead\nto copyright infringement and/or leakage of privacy-sensitive information. In\nthis paper, we present a novel perspective on the memorization phenomenon and\npropose a simple yet effective approach to mitigate it. We argue that\nmemorization occurs because of an attraction basin in the denoising process\nwhich steers the diffusion trajectory towards a memorized image. However, this\ncan be mitigated by guiding the diffusion trajectory away from the attraction\nbasin by not applying classifier-free guidance until an ideal transition point\noccurs from which classifier-free guidance is applied. This leads to the\ngeneration of non-memorized images that are high in image quality and\nwell-aligned with the conditioning mechanism. To further improve on this, we\npresent a new guidance technique, opposite guidance, that escapes the\nattraction basin sooner in the denoising process. We demonstrate the existence\nof attraction basins in various scenarios in which memorization occurs, and we\nshow that our proposed approach successfully mitigates memorization.\n","authors":["Anubhav Jain","Yuya Kobayashi","Takashi Shibuya","Yuhta Takida","Nasir Memon","Julian Togelius","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2411.16738v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13309v1","updated":"2025-03-17T15:48:56Z","published":"2025-03-17T15:48:56Z","title":"Integrating AI for Human-Centric Breast Cancer Diagnostics: A\n  Multi-Scale and Multi-View Swin Transformer Framework","summary":"  Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer\nremains one of the leading causes of cancer-related deaths among women\nworldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown\nsignificant promise in development of advanced Deep Learning (DL) architectures\nfor breast cancer diagnosis through mammography. In this context, the paper\nfocuses on the integration of AI within a Human-Centric workflow to enhance\nbreast cancer diagnostics. Key challenges are, however, largely overlooked such\nas reliance on detailed tumor annotations and susceptibility to missing views,\nparticularly during test time. To address these issues, we propose a hybrid,\nmulti-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that\nenhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework\nis designed to work as a decision-support tool, helping radiologists analyze\nmulti-view mammograms more effectively. More specifically, the MSMV-Swin\nframework leverages the Segment Anything Model (SAM) to isolate the breast\nlobe, reducing background noise and enabling comprehensive feature extraction.\nThe multi-scale nature of the proposed MSMV-Swin framework accounts for\ntumor-specific regions as well as the spatial characteristics of tissues\nsurrounding the tumor, capturing both localized and contextual information. The\nintegration of contextual and localized data ensures that MSMV-Swin's outputs\nalign with the way radiologists interpret mammograms, fostering better human-AI\ninteraction and trust. A hybrid fusion structure is then designed to ensure\nrobustness against missing views, a common occurrence in clinical practice when\nonly a single mammogram view is available.\n","authors":["Farnoush Bayatmakou","Reza Taleei","Milad Amir Toutounchian","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2503.13309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13303v1","updated":"2025-03-17T15:46:43Z","published":"2025-03-17T15:46:43Z","title":"UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose\n  Estimation","summary":"  Estimating the 3D pose of hand and potential hand-held object from monocular\nimages is a longstanding challenge. Yet, existing methods are specialized,\nfocusing on either bare-hand or hand interacting with object. No method can\nflexibly handle both scenarios and their performance degrades when applied to\nthe other scenario. In this paper, we propose UniHOPE, a unified approach for\ngeneral 3D hand-object pose estimation, flexibly adapting both scenarios.\nTechnically, we design a grasp-aware feature fusion module to integrate\nhand-object features with an object switcher to dynamically control the\nhand-object pose estimation according to grasping status. Further, to uplift\nthe robustness of hand pose estimation regardless of object presence, we\ngenerate realistic de-occluded image pairs to train the model to learn\nobject-induced hand occlusions, and formulate multi-level feature enhancement\ntechniques for learning occlusion-invariant features. Extensive experiments on\nthree commonly-used benchmarks demonstrate UniHOPE's SOTA performance in\naddressing hand-only and hand-object scenarios. Code will be released on\nhttps://github.com/JoyboyWang/UniHOPE_Pytorch.\n","authors":["Yinqiao Wang","Hao Xu","Pheng-Ann Heng","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2503.13303v1.pdf","comment":"8 pages, 6 figures, 7 tables"},{"id":"http://arxiv.org/abs/2503.13300v1","updated":"2025-03-17T15:45:06Z","published":"2025-03-17T15:45:06Z","title":"Progressive Human Motion Generation Based on Text and Few Motion Frames","summary":"  Although existing text-to-motion (T2M) methods can produce realistic human\nmotion from text description, it is still difficult to align the generated\nmotion with the desired postures since using text alone is insufficient for\nprecisely describing diverse postures. To achieve more controllable generation,\nan intuitive way is to allow the user to input a few motion frames describing\nprecise desired postures. Thus, we explore a new Text-Frame-to-Motion (TF2M)\ngeneration task that aims to generate motions from text and very few given\nframes. Intuitively, the closer a frame is to a given frame, the lower the\nuncertainty of this frame is when conditioned on this given frame. Hence, we\npropose a novel Progressive Motion Generation (PMG) method to progressively\ngenerate a motion from the frames with low uncertainty to those with high\nuncertainty in multiple stages. During each stage, new frames are generated by\na Text-Frame Guided Generator conditioned on frame-aware semantics of the text,\ngiven frames, and frames generated in previous stages. Additionally, to\nalleviate the train-test gap caused by multi-stage accumulation of incorrectly\ngenerated frames during testing, we propose a Pseudo-frame Replacement Strategy\nfor training. Experimental results show that our PMG outperforms existing T2M\ngeneration methods by a large margin with even one given frame, validating the\neffectiveness of our PMG. Code will be released.\n","authors":["Ling-An Zeng","Gaojie Wu","Ancong Wu","Jian-Fang Hu","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.13300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07658v2","updated":"2025-03-17T15:37:35Z","published":"2024-12-10T16:45:03Z","title":"TraSCE: Trajectory Steering for Concept Erasure","summary":"  Recent advancements in text-to-image diffusion models have brought them to\nthe public spotlight, becoming widely accessible and embraced by everyday\nusers. However, these models have been shown to generate harmful content such\nas not-safe-for-work (NSFW) images. While approaches have been proposed to\nerase such abstract concepts from the models, jail-breaking techniques have\nsucceeded in bypassing such safety measures. In this paper, we propose TraSCE,\nan approach to guide the diffusion trajectory away from generating harmful\ncontent. Our approach is based on negative prompting, but as we show in this\npaper, a widely used negative prompting strategy is not a complete solution and\ncan easily be bypassed in some corner cases. To address this issue, we first\npropose using a specific formulation of negative prompting instead of the\nwidely used one. Furthermore, we introduce a localized loss-based guidance that\nenhances the modified negative prompting technique by steering the diffusion\ntrajectory. We demonstrate that our proposed method achieves state-of-the-art\nresults on various benchmarks in removing harmful content, including ones\nproposed by red teams, and erasing artistic styles and objects. Our proposed\napproach does not require any training, weight modifications, or training data\n(either image or prompt), making it easier for model owners to erase new\nconcepts.\n","authors":["Anubhav Jain","Yuya Kobayashi","Takashi Shibuya","Yuhta Takida","Nasir Memon","Julian Togelius","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2412.07658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08992v2","updated":"2025-03-17T15:33:08Z","published":"2025-03-12T01:55:02Z","title":"Dual-Domain Homogeneous Fusion with Cross-Modal Mamba and Progressive\n  Decoder for 3D Object Detection","summary":"  Fusing LiDAR and image features in a homogeneous BEV domain has become\npopular for 3D object detection in autonomous driving. However, this paradigm\nis constrained by the excessive feature compression. While some works explore\ndense voxel fusion to enable better feature interaction, they face high\ncomputational costs and challenges in query generation. Additionally, feature\nmisalignment in both domains results in suboptimal detection accuracy. To\naddress these limitations, we propose a Dual-Domain Homogeneous Fusion network\n(DDHFusion), which leverages the complementarily of both BEV and voxel domains\nwhile mitigating their drawbacks. Specifically, we first transform image\nfeatures into BEV and sparse voxel representations using lift-splat-shot and\nour proposed Semantic-Aware Feature Sampling (SAFS) module. The latter\nsignificantly reduces computational overhead by discarding unimportant voxels.\nNext, we introduce Homogeneous Voxel and BEV Fusion (HVF and HBF) networks for\nmulti-modal fusion within respective domains. They are equipped with novel\ncross-modal Mamba blocks to resolve feature misalignment and enable\ncomprehensive scene perception. The output voxel features are injected into the\nBEV space to compensate for the information loss brought by direct height\ncompression. During query selection, the Progressive Query Generation (PQG)\nmechanism is implemented in the BEV domain to reduce false negatives caused by\nfeature compression. Furthermore, we propose a Progressive Decoder (QD) that\nsequentially aggregates not only context-rich BEV features but also\ngeometry-aware voxel features with deformable attention and the Multi-Modal\nVoxel Feature Mixing (MMVFM) block for precise classification and box\nregression.\n","authors":["Xuzhong Hu","Zaipeng Duan","Pei An","Jun zhang","Jie Ma"],"pdf_url":"https://arxiv.org/pdf/2503.08992v2.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.13277v1","updated":"2025-03-17T15:27:21Z","published":"2025-03-17T15:27:21Z","title":"Artificial Intelligence-Driven Prognostic Classification of COVID-19\n  Using Chest X-rays: A Deep Learning Approach","summary":"  Background: The COVID-19 pandemic has overwhelmed healthcare systems,\nemphasizing the need for AI-driven tools to assist in rapid and accurate\npatient prognosis. Chest X-ray imaging is a widely available diagnostic tool,\nbut existing methods for prognosis classification lack scalability and\nefficiency. Objective: This study presents a high-accuracy deep learning model\nfor classifying COVID-19 severity (Mild, Moderate, and Severe) using Chest\nX-ray images, developed on Microsoft Azure Custom Vision. Methods: Using a\ndataset of 1,103 confirmed COVID-19 X-ray images from AIforCOVID, we trained\nand validated a deep learning model leveraging Convolutional Neural Networks\n(CNNs). The model was evaluated on an unseen dataset to measure accuracy,\nprecision, and recall. Results: Our model achieved an average accuracy of 97%,\nwith specificity of 99%, sensitivity of 87%, and an F1-score of 93.11%. When\nclassifying COVID-19 severity, the model achieved accuracies of 89.03% (Mild),\n95.77% (Moderate), and 81.16% (Severe). These results demonstrate the model's\npotential for real-world clinical applications, aiding in faster\ndecision-making and improved resource allocation. Conclusion: AI-driven\nprognosis classification using deep learning can significantly enhance COVID-19\npatient management, enabling early intervention and efficient triaging. Our\nstudy provides a scalable, high-accuracy AI framework for integrating deep\nlearning into routine clinical workflows. Future work should focus on expanding\ndatasets, external validation, and regulatory compliance to facilitate clinical\nadoption.\n","authors":["Alfred Simbun","Suresh Kumar"],"pdf_url":"https://arxiv.org/pdf/2503.13277v1.pdf","comment":"27 pages, 6 figures, 10 tables"},{"id":"http://arxiv.org/abs/2503.13272v1","updated":"2025-03-17T15:24:04Z","published":"2025-03-17T15:24:04Z","title":"Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion\n  Priors","summary":"  Synthesizing consistent and photorealistic 3D scenes is an open problem in\ncomputer vision. Video diffusion models generate impressive videos but cannot\ndirectly synthesize 3D representations, i.e., lack 3D consistency in the\ngenerated sequences. In addition, directly training generative 3D models is\nchallenging due to a lack of 3D training data at scale. In this work, we\npresent Generative Gaussian Splatting (GGS) -- a novel approach that integrates\na 3D representation with a pre-trained latent video diffusion model.\nSpecifically, our model synthesizes a feature field parameterized via 3D\nGaussian primitives. The feature field is then either rendered to feature maps\nand decoded into multi-view images, or directly upsampled into a 3D radiance\nfield. We evaluate our approach on two common benchmark datasets for scene\nsynthesis, RealEstate10K and ScanNet+, and find that our proposed GGS model\nsignificantly improves both the 3D consistency of the generated multi-view\nimages, and the quality of the generated 3D scenes over all relevant baselines.\nCompared to a similar model without 3D representation, GGS improves FID on the\ngenerated 3D scenes by ~20% on both RealEstate10K and ScanNet+. Project page:\nhttps://katjaschwarz.github.io/ggs/\n","authors":["Katja Schwarz","Norman Mueller","Peter Kontschieder"],"pdf_url":"https://arxiv.org/pdf/2503.13272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09964v4","updated":"2025-03-17T15:19:09Z","published":"2024-03-15T02:05:20Z","title":"Boundary Constraint-free Biomechanical Model-Based Surface Matching for\n  Intraoperative Liver Deformation Correction","summary":"  In image-guided liver surgery, 3D-3D non-rigid registration methods play a\ncrucial role in estimating the mapping between the preoperative model and the\nintraoperative surface represented as point clouds, addressing the challenge of\ntissue deformation. Typically, these methods incorporate a biomechanical model,\nrepresented as a finite element model (FEM), into the strain energy term to\nregularize a surface matching term. We propose a 3D-3D non-rigid registration\nmethod that incorporates a modified FEM into the surface matching term. The\nmodified FEM alleviates the need to specify boundary conditions, which is\nachieved by modifying the stiffness matrix of a FEM and using diagonal loading\nfor stabilization. As a result, the modified surface matching term does not\nrequire the specification of boundary conditions or an additional strain energy\nterm to regularize the surface matching term. Optimization is achieved through\nan accelerated gradient algorithm, further enhanced by our proposed method for\ndetermining the optimal step size. We evaluated our method and compared it to\nseveral state-of-the-art methods across various datasets. Our straightforward\nand effective approach consistently outperformed or achieved comparable\nperformance to the state-of-the-art methods. Our code and datasets are\navailable at https://github.com/zixinyang9109/BCF-FEM.\n","authors":["Zixin Yang","Richard Simon","Kelly Merrell","Cristian. A. Linte"],"pdf_url":"https://arxiv.org/pdf/2403.09964v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13265v1","updated":"2025-03-17T15:18:38Z","published":"2025-03-17T15:18:38Z","title":"FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis","summary":"  Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming,\nfrom single images is challenging due to a lack of 3D data. To this end, we\nintroduce FlexWorld, a novel framework consisting of two key components: (1) a\nstrong video-to-video (V2V) diffusion model to generate high-quality novel view\nimages from incomplete input rendered from a coarse scene, and (2) a\nprogressive expansion process to construct a complete 3D scene. In particular,\nleveraging an advanced pre-trained video model and accurate depth-estimated\ntraining pairs, our V2V model can generate novel views under large camera pose\nvariations. Building upon it, FlexWorld progressively generates new 3D content\nand integrates it into the global scene through geometry-aware scene fusion.\nExtensive experiments demonstrate the effectiveness of FlexWorld in generating\nhigh-quality novel view videos and flexible-view 3D scenes from single images,\nachieving superior visual quality under multiple popular metrics and datasets\ncompared to existing state-of-the-art methods. Qualitatively, we highlight that\nFlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg}\nrotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.\n","authors":["Luxi Chen","Zihan Zhou","Min Zhao","Yikai Wang","Ge Zhang","Wenhao Huang","Hao Sun","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.13265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13260v1","updated":"2025-03-17T15:15:31Z","published":"2025-03-17T15:15:31Z","title":"Don't Judge Before You CLIP: A Unified Approach for Perceptual Tasks","summary":"  Visual perceptual tasks aim to predict human judgment of images (e.g.,\nemotions invoked by images, image quality assessment). Unlike objective tasks\nsuch as object/scene recognition, perceptual tasks rely on subjective human\nassessments, making its data-labeling difficult. The scarcity of such\nhuman-annotated data results in small datasets leading to poor generalization.\nTypically, specialized models were designed for each perceptual task, tailored\nto its unique characteristics and its own training dataset. We propose a\nunified architectural framework for solving multiple different perceptual tasks\nleveraging CLIP as a prior. Our approach is based on recent cognitive findings\nwhich indicate that CLIP correlates well with human judgment. While CLIP was\nexplicitly trained to align images and text, it implicitly also learned human\ninclinations. We attribute this to the inclusion of human-written image\ncaptions in CLIP's training data, which contain not only factual image\ndescriptions, but inevitably also human sentiments and emotions. This makes\nCLIP a particularly strong prior for perceptual tasks. Accordingly, we suggest\nthat minimal adaptation of CLIP suffices for solving a variety of perceptual\ntasks. Our simple unified framework employs a lightweight adaptation to\nfine-tune CLIP to each task, without requiring any task-specific architectural\nchanges. We evaluate our approach on three tasks: (i) Image Memorability\nPrediction, (ii) No-reference Image Quality Assessment, and (iii) Visual\nEmotion Analysis. Our model achieves state-of-the-art results on all three\ntasks, while demonstrating improved generalization across different datasets.\n","authors":["Amit Zalcher","Navve Wasserman","Roman Beliy","Oliver Heinimann","Michal Irani"],"pdf_url":"https://arxiv.org/pdf/2503.13260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13241v1","updated":"2025-03-17T14:54:13Z","published":"2025-03-17T14:54:13Z","title":"Sampling Innovation-Based Adaptive Compressive Sensing","summary":"  Scene-aware Adaptive Compressive Sensing (ACS) has attracted significant\ninterest due to its promising capability for efficient and high-fidelity\nacquisition of scene images. ACS typically prescribes adaptive sampling\nallocation (ASA) based on previous samples in the absence of ground truth.\nHowever, when confronting unknown scenes, existing ACS methods often lack\naccurate judgment and robust feedback mechanisms for ASA, thus limiting the\nhigh-fidelity sensing of the scene. In this paper, we introduce a Sampling\nInnovation-Based ACS (SIB-ACS) method that can effectively identify and\nallocate sampling to challenging image reconstruction areas, culminating in\nhigh-fidelity image reconstruction. An innovation criterion is proposed to\njudge ASA by predicting the decrease in image reconstruction error attributable\nto sampling increments, thereby directing more samples towards regions where\nthe reconstruction error diminishes significantly. A sampling innovation-guided\nmulti-stage adaptive sampling (AS) framework is proposed, which iteratively\nrefines the ASA through a multi-stage feedback process. For image\nreconstruction, we propose a Principal Component Compressed Domain Network\n(PCCD-Net), which efficiently and faithfully reconstructs images under AS\nscenarios. Extensive experiments demonstrate that the proposed SIB-ACS method\nsignificantly outperforms the state-of-the-art methods in terms of image\nreconstruction fidelity and visual effects. Codes are available at\nhttps://github.com/giant-pandada/SIB-ACS_CVPR2025.\n","authors":["Zhifu Tian","Tao Hu","Chaoyang Niu","Di Wu","Shu Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13241v1.pdf","comment":"CVPR2025 accepted"},{"id":"http://arxiv.org/abs/2503.13236v1","updated":"2025-03-17T14:48:57Z","published":"2025-03-17T14:48:57Z","title":"Gradient Extrapolation for Debiased Representation Learning","summary":"  Machine learning classification models trained with empirical risk\nminimization (ERM) often inadvertently rely on spurious correlations. When\nabsent in the test data, these unintended associations between non-target\nattributes and target labels lead to poor generalization. This paper addresses\nthis problem from a model optimization perspective and proposes a novel method,\nGradient Extrapolation for Debiased Representation Learning (GERNE), designed\nto learn debiased representations in both known and unknown attribute training\ncases. GERNE uses two distinct batches with different amounts of spurious\ncorrelations to define the target gradient as the linear extrapolation of two\ngradients computed from each batch's loss. It is demonstrated that the\nextrapolated gradient, if directed toward the gradient of the batch with fewer\namount of spurious correlation, can guide the training process toward learning\na debiased model. GERNE can serve as a general framework for debiasing with\nmethods, such as ERM, reweighting, and resampling, being shown as special\ncases. The theoretical upper and lower bounds of the extrapolation factor are\nderived to ensure convergence. By adjusting this factor, GERNE can be adapted\nto maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The\nproposed approach is validated on five vision and one NLP benchmarks,\ndemonstrating competitive and often superior performance compared to\nstate-of-the-art baseline methods.\n","authors":["Ihab Asaad","Maha Shadaydeh","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2503.13236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13229v1","updated":"2025-03-17T14:42:31Z","published":"2025-03-17T14:42:31Z","title":"HoloGest: Decoupled Diffusion and Motion Priors for Generating\n  Holisticly Expressive Co-speech Gestures","summary":"  Animating virtual characters with holistic co-speech gestures is a\nchallenging but critical task. Previous systems have primarily focused on the\nweak correlation between audio and gestures, leading to physically unnatural\noutcomes that degrade the user experience. To address this problem, we\nintroduce HoleGest, a novel neural network framework based on decoupled\ndiffusion and motion priors for the automatic generation of high-quality,\nexpressive co-speech gestures. Our system leverages large-scale human motion\ndatasets to learn a robust prior with low audio dependency and high motion\nreliance, enabling stable global motion and detailed finger movements. To\nimprove the generation efficiency of diffusion-based models, we integrate\nimplicit joint constraints with explicit geometric and conditional constraints,\ncapturing complex motion distributions between large strides. This integration\nsignificantly enhances generation speed while maintaining high-quality motion.\nFurthermore, we design a shared embedding space for gesture-transcription text\nalignment, enabling the generation of semantically correct gesture actions.\nExtensive experiments and user feedback demonstrate the effectiveness and\npotential applications of our model, with our method achieving a level of\nrealism close to the ground truth, providing an immersive user experience. Our\ncode, model, and demo are are available at\nhttps://cyk990422.github.io/HoloGest.github.io/.\n","authors":["Yongkang Cheng","Shaoli Huang"],"pdf_url":"https://arxiv.org/pdf/2503.13229v1.pdf","comment":"Accepted by 3DV 2025"},{"id":"http://arxiv.org/abs/2503.13227v1","updated":"2025-03-17T14:41:51Z","published":"2025-03-17T14:41:51Z","title":"Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised\n  Learning Across Pseudo-Mismatch","summary":"  Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data\nacross clients with limited labeled data to train a global model with strong\ngeneralization ability. Most FSSL methods rely on consistency regularization\nwith pseudo-labels, converting predictions from local or global models into\nhard pseudo-labels as supervisory signals. However, we discover that the\nquality of pseudo-label is largely deteriorated by data heterogeneity, an\nintrinsic facet of federated learning. In this paper, we study the problem of\nFSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label\nmismatches, further degrading model performance and convergence, and (2) local\nand global models' predictive tendencies diverge as heterogeneity increases.\nMotivated by these findings, we propose a simple and effective method called\nSemi-supervised Aggregation for Globally-Enhanced Ensemble (SAGE), that can\nflexibly correct pseudo-labels based on confidence discrepancies. This strategy\neffectively mitigates performance degradation caused by incorrect pseudo-labels\nand enhances consensus between local and global models. Experimental results\ndemonstrate that SAGE outperforms existing FSSL methods in both performance and\nconvergence. Our code is available at https://github.com/Jay-Codeman/SAGE\n","authors":["Yijie Liu","Xinyi Shang","Yiqun Zhang","Yang Lu","Chen Gong","Jing-Hao Xue","Hanzi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13227v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13217v1","updated":"2025-03-17T14:28:08Z","published":"2025-03-17T14:28:08Z","title":"Dense Policy: Bidirectional Autoregressive Learning of Actions","summary":"  Mainstream visuomotor policies predominantly rely on generative models for\nholistic action prediction, while current autoregressive policies, predicting\nthe next token or chunk, have shown suboptimal results. This motivates a search\nfor more effective learning methods to unleash the potential of autoregressive\npolicies for robotic manipulation. This paper introduces a bidirectionally\nexpanded learning approach, termed Dense Policy, to establish a new paradigm\nfor autoregressive policies in action prediction. It employs a lightweight\nencoder-only architecture to iteratively unfold the action sequence from an\ninitial single frame into the target sequence in a coarse-to-fine manner with\nlogarithmic-time inference. Extensive experiments validate that our dense\npolicy has superior autoregressive learning capabilities and can surpass\nexisting holistic generative policies. Our policy, example data, and training\ncode will be publicly available upon publication. Project page: https:\n//selen-suyue.github.io/DspNet/.\n","authors":["Yue Su","Xinyu Zhan","Hongjie Fang","Han Xue","Hao-Shu Fang","Yong-Lu Li","Cewu Lu","Lixin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13214v1","updated":"2025-03-17T14:24:00Z","published":"2025-03-17T14:24:00Z","title":"A General Adaptive Dual-level Weighting Mechanism for Remote Sensing\n  Pansharpening","summary":"  Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.\n","authors":["Jie Huang","Haorui Chen","Jiaxuan Ren","Siran Peng","Liangjian Deng"],"pdf_url":"https://arxiv.org/pdf/2503.13214v1.pdf","comment":"This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025"},{"id":"http://arxiv.org/abs/2503.13211v1","updated":"2025-03-17T14:22:49Z","published":"2025-03-17T14:22:49Z","title":"MedLoRD: A Medical Low-Resource Diffusion Model for High-Resolution 3D\n  CT Image Synthesis","summary":"  Advancements in AI for medical imaging offer significant potential. However,\ntheir applications are constrained by the limited availability of data and the\nreluctance of medical centers to share it due to patient privacy concerns.\nGenerative models present a promising solution by creating synthetic data as a\nsubstitute for real patient data. However, medical images are typically\nhigh-dimensional, and current state-of-the-art methods are often impractical\nfor computational resource-constrained healthcare environments. These models\nrely on data sub-sampling, raising doubts about their feasibility and\nreal-world applicability. Furthermore, many of these models are evaluated on\nquantitative metrics that alone can be misleading in assessing the image\nquality and clinical meaningfulness of the generated images. To address this,\nwe introduce MedLoRD, a generative diffusion model designed for computational\nresource-constrained environments. MedLoRD is capable of generating\nhigh-dimensional medical volumes with resolutions up to\n512$\\times$512$\\times$256, utilizing GPUs with only 24GB VRAM, which are\ncommonly found in standard desktop workstations. MedLoRD is evaluated across\nmultiple modalities, including Coronary Computed Tomography Angiography and\nLung Computed Tomography datasets. Extensive evaluations through radiological\nevaluation, relative regional volume analysis, adherence to conditional masks,\nand downstream tasks show that MedLoRD generates high-fidelity images closely\nadhering to segmentation mask conditions, surpassing the capabilities of\ncurrent state-of-the-art generative models for medical image synthesis in\ncomputational resource-constrained environments.\n","authors":["Marvin Seyfarth","Salman Ul Hassan Dar","Isabelle Ayx","Matthias Alexander Fink","Stefan O. Schoenberg","Hans-Ulrich Kauczor","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2503.13211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13205v1","updated":"2025-03-17T14:14:28Z","published":"2025-03-17T14:14:28Z","title":"MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for\n  Inpatient Pathways","summary":"  Inpatient pathways demand complex clinical decision-making based on\ncomprehensive patient information, posing critical challenges for clinicians.\nDespite advancements in large language models (LLMs) in medical applications,\nlimited research focused on artificial intelligence (AI) inpatient pathways\nsystems, due to the lack of large-scale inpatient datasets. Moreover, existing\nmedical benchmarks typically concentrated on medical question-answering and\nexaminations, ignoring the multifaceted nature of clinical decision-making in\ninpatient settings. To address these gaps, we first developed the Inpatient\nPathway Decision Support (IPDS) benchmark from the MIMIC-IV database,\nencompassing 51,274 cases across nine triage departments and 17 major disease\ncategories alongside 16 standardized treatment options. Then, we proposed the\nMulti-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways\nwith three clinical agents, including a triage agent managing the patient\nadmission, a diagnosis agent serving as the primary decision maker at the\ndepartment, and a treatment agent providing treatment plans. Additionally, our\nMAP framework includes a chief agent overseeing the inpatient pathways to guide\nand promote these three clinician agents. Extensive experiments showed our MAP\nimproved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM\nHuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant\nclinical compliance, outperforming three board-certified clinicians by 10%-12%,\nestablishing a foundation for inpatient pathways systems.\n","authors":["Zhen Chen","Zhihao Peng","Xusheng Liang","Cheng Wang","Peigan Liang","Linsheng Zeng","Minjie Ju","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.13205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13203v1","updated":"2025-03-17T14:12:08Z","published":"2025-03-17T14:12:08Z","title":"Clustering is back: Reaching state-of-the-art LiDAR instance\n  segmentation without training","summary":"  Panoptic segmentation of LiDAR point clouds is fundamental to outdoor scene\nunderstanding, with autonomous driving being a primary application. While\nstate-of-the-art approaches typically rely on end-to-end deep learning\narchitectures and extensive manual annotations of instances, the significant\ncost and time investment required for labeling large-scale point cloud datasets\nremains a major bottleneck in this field. In this work, we demonstrate that\ncompetitive panoptic segmentation can be achieved using only semantic labels,\nwith instances predicted without any training or annotations. Our method\nachieves performance comparable to current state-of-the-art supervised methods\non standard benchmarks including SemanticKITTI and nuScenes, and outperforms\nevery publicly available method on SemanticKITTI as a drop-in instance head\nreplacement, while running in real-time on a single-threaded CPU and requiring\nno instance labels. Our method is fully explainable, and requires no learning\nor parameter tuning. Code is available at https://github.com/valeoai/Alpine/\n","authors":["Corentin Sautier","Gilles Puy","Alexandre Boulch","Renaud Marlet","Vincent Lepetit"],"pdf_url":"https://arxiv.org/pdf/2503.13203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12944v2","updated":"2025-03-17T14:09:47Z","published":"2024-12-17T14:22:44Z","title":"Online optimisation for dynamic electrical impedance tomography","summary":"  Online optimisation studies the convergence of optimisation methods as the\ndata embedded in the problem changes. Based on this idea, we propose a primal\ndual online method for nonlinear time-discrete inverse problems. We analyse the\nmethod through regret theory and demonstrate its performance in real-time\nmonitoring of moving bodies in a fluid with Electrical Impedance Tomography\n(EIT). To do so, we also prove the second-order differentiability of the\nComplete Electrode Model (CEM) solution operator on $L^\\infty$.\n","authors":["Neil Dizon","Jyrki Jauhiainen","Tuomo Valkonen"],"pdf_url":"https://arxiv.org/pdf/2412.12944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15185v2","updated":"2025-03-17T14:05:49Z","published":"2024-08-27T16:40:14Z","title":"Human-Centric Video Anomaly Detection Through Spatio-Temporal Pose\n  Tokenization and Transformer","summary":"  Video Anomaly Detection (VAD) presents a significant challenge in computer\nvision, particularly due to the unpredictable and infrequent nature of\nanomalous events, coupled with the diverse and dynamic environments in which\nthey occur. Human-centric VAD, a specialized area within this domain, faces\nadditional complexities, including variations in human behavior, potential\nbiases in data, and substantial privacy concerns related to human subjects.\nThese issues complicate the development of models that are both robust and\ngeneralizable. To address these challenges, recent advancements have focused on\npose-based VAD, which leverages human pose as a high-level feature to mitigate\nprivacy concerns, reduce appearance biases, and minimize background\ninterference. In this paper, we introduce SPARTA, a novel transformer-based\narchitecture designed specifically for human-centric pose-based VAD. SPARTA\nintroduces an innovative Spatio-Temporal Pose and Relative Pose (ST-PRP)\ntokenization method that produces an enriched representation of human motion\nover time. This approach ensures that the transformer's attention mechanism\ncaptures both spatial and temporal patterns simultaneously, rather than\nfocusing on only one aspect. The addition of the relative pose further\nemphasizes subtle deviations from normal human movements. The architecture's\ncore, a novel Unified Encoder Twin Decoders (UETD) transformer, significantly\nimproves the detection of anomalous behaviors in video data. Extensive\nevaluations across multiple benchmark datasets demonstrate that SPARTA\nconsistently outperforms existing methods, establishing a new state-of-the-art\nin pose-based VAD.\n","authors":["Ghazal Alinezhad Noghre","Armin Danesh Pazho","Hamed Tabkhi"],"pdf_url":"https://arxiv.org/pdf/2408.15185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13188v1","updated":"2025-03-17T13:59:20Z","published":"2025-03-17T13:59:20Z","title":"3D Hierarchical Panoptic Segmentation in Real Orchard Environments\n  Across Different Sensors","summary":"  Crop yield estimation is a relevant problem in agriculture, because an\naccurate crop yield estimate can support farmers' decisions on harvesting or\nprecision intervention. Robots can help to automate this process. To do so,\nthey need to be able to perceive the surrounding environment to identify target\nobjects. In this paper, we introduce a novel approach to address the problem of\nhierarchical panoptic segmentation of apple orchards on 3D data from different\nsensors. Our approach is able to simultaneously provide semantic segmentation,\ninstance segmentation of trunks and fruits, and instance segmentation of plants\n(a single trunk with its fruits). This allows us to identify relevant\ninformation such as individual plants, fruits, and trunks, and capture the\nrelationship among them, such as precisely estimate the number of fruits\nassociated to each tree in an orchard. Additionally, to efficiently evaluate\nour approach for hierarchical panoptic segmentation, we provide a dataset\ndesigned specifically for this task. Our dataset is recorded in Bonn in a real\napple orchard with a variety of sensors, spanning from a terrestrial laser\nscanner to a RGB-D camera mounted on different robotic platforms. The\nexperiments show that our approach surpasses state-of-the-art approaches in 3D\npanoptic segmentation in the agricultural domain, while also providing full\nhierarchical panoptic segmentation. Our dataset has been made publicly\navailable at https://www.ipb.uni-bonn.de/data/hops/. We will provide the\nopen-source implementation of our approach and public competiton for\nhierarchical panoptic segmentation on the hidden test sets upon paper\nacceptance.\n","authors":["Matteo Sodano","Federico Magistri","Elias Marks","Fares Hosn","Aibek Zurbayev","Rodrigo Marcuzzi","Meher V. R. Malladi","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2503.13188v1.pdf","comment":"Submitted to IROS"},{"id":"http://arxiv.org/abs/2503.13185v1","updated":"2025-03-17T13:57:05Z","published":"2025-03-17T13:57:05Z","title":"3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o","summary":"  Multimodal Large Language Models (MLLMs) exhibit impressive capabilities\nacross a variety of tasks, especially when equipped with carefully designed\nvisual prompts. However, existing studies primarily focus on logical reasoning\nand visual understanding, while the capability of MLLMs to operate effectively\nin 3D vision remains an ongoing area of exploration. In this paper, we\nintroduce a novel visual prompting method, called 3DAxisPrompt, to elicit the\n3D understanding capabilities of MLLMs in real-world scenes. More specifically,\nour method leverages the 3D coordinate axis and masks generated from the\nSegment Anything Model (SAM) to provide explicit geometric priors to MLLMs and\nthen extend their impressive 2D grounding and reasoning ability to real-world\n3D scenarios. Besides, we first provide a thorough investigation of the\npotential visual prompting formats and conclude our findings to reveal the\npotential and limits of 3D understanding capabilities in GPT-4o, as a\nrepresentative of MLLMs. Finally, we build evaluation environments with four\ndatasets, i.e., ScanRefer, ScanNet, FMB, and nuScene datasets, covering various\n3D tasks. Based on this, we conduct extensive quantitative and qualitative\nexperiments, which demonstrate the effectiveness of the proposed method.\nOverall, our study reveals that MLLMs, with the help of 3DAxisPrompt, can\neffectively perceive an object's 3D position in real-world scenarios.\nNevertheless, a single prompt engineering approach does not consistently\nachieve the best outcomes for all 3D tasks. This study highlights the\nfeasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt\nengineering techniques.\n","authors":["Dingning Liu","Cheng Wang","Peng Gao","Renrui Zhang","Xinzhu Ma","Yuan Meng","Zhihui Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13184v1","updated":"2025-03-17T13:56:57Z","published":"2025-03-17T13:56:57Z","title":"Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided\n  Visual Tokenizer and Manufacturing Process","summary":"  Although recent methods have tried to introduce large multimodal models\n(LMMs) into industrial anomaly detection (IAD), their generalization in the IAD\nfield is far inferior to that for general purposes. We summarize the main\nreasons for this gap into two aspects. On one hand, general-purpose LMMs lack\ncognition of defects in the visual modality, thereby failing to sufficiently\nfocus on defect areas. Therefore, we propose to modify the AnyRes structure of\nthe LLaVA model, providing the potential anomalous areas identified by existing\nIAD models to the LMMs. On the other hand, existing methods mainly focus on\nidentifying defects by learning defect patterns or comparing with normal\nsamples, yet they fall short of understanding the causes of these defects.\nConsidering that the generation of defects is closely related to the\nmanufacturing process, we propose a manufacturing-driven IAD paradigm. An\ninstruction-tuning dataset for IAD (InstructIAD) and a data organization\napproach for Chain-of-Thought with manufacturing (CoT-M) are designed to\nleverage the manufacturing process for IAD. Based on the above two\nmodifications, we present Triad, a novel LMM-based method incorporating an\nexpert-guided region-of-interest tokenizer and manufacturing process for\nindustrial anomaly detection. Extensive experiments show that our Triad not\nonly demonstrates competitive performance against current LMMs but also\nachieves further improved accuracy when equipped with manufacturing processes.\nSource code, training data, and pre-trained models will be publicly available\nat https://github.com/tzjtatata/Triad.\n","authors":["Yuanze Li","Shihao Yuan","Haolin Wang","Qizhang Li","Ming Liu","Chen Xu","Guangming Shi","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2503.13184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13179v1","updated":"2025-03-17T13:54:26Z","published":"2025-03-17T13:54:26Z","title":"A super-resolution reconstruction method for lightweight building images\n  based on an expanding feature modulation network","summary":"  This study proposes a lightweight method for building image super-resolution\nusing a Dilated Contextual Feature Modulation Network (DCFMN). The process\nincludes obtaining high-resolution images, down-sampling them to\nlow-resolution, enhancing the low-resolution images, constructing and training\na lightweight network model, and generating super-resolution outputs. To\naddress challenges such as regular textures and long-range dependencies in\nbuilding images, the DCFMN integrates an expansion separable modulation unit\nand a local feature enhancement module. The former employs multiple expansion\nconvolutions equivalent to a large kernel to efficiently aggregate multi-scale\nfeatures while leveraging a simple attention mechanism for adaptivity. The\nlatter encodes local features, mixes channel information, and ensures no\nadditional computational burden during inference through reparameterization.\nThis approach effectively resolves the limitations of existing lightweight\nsuper-resolution networks in modeling long-range dependencies, achieving\naccurate and efficient global feature modeling without increasing computational\ncosts, and significantly improving both reconstruction quality and lightweight\nefficiency for building image super-resolution models.\n","authors":["Yi Zhang","Wenye Zhou","Ruonan Lin"],"pdf_url":"https://arxiv.org/pdf/2503.13179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13176v1","updated":"2025-03-17T13:53:04Z","published":"2025-03-17T13:53:04Z","title":"DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for\n  Distractor-free 3D Reconstruction","summary":"  Reconstructing clean, distractor-free 3D scenes from real-world captures\nremains a significant challenge, particularly in highly dynamic and cluttered\nsettings such as egocentric videos. To tackle this problem, we introduce\nDeGauss, a simple and robust self-supervised framework for dynamic scene\nreconstruction based on a decoupled dynamic-static Gaussian Splatting design.\nDeGauss models dynamic elements with foreground Gaussians and static content\nwith background Gaussians, using a probabilistic mask to coordinate their\ncomposition and enable independent yet complementary optimization. DeGauss\ngeneralizes robustly across a wide range of real-world scenarios, from casual\nimage collections to long, dynamic egocentric videos, without relying on\ncomplex heuristics or extensive supervision. Experiments on benchmarks\nincluding NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that\nDeGauss consistently outperforms existing methods, establishing a strong\nbaseline for generalizable, distractor-free 3D reconstructionin highly dynamic,\ninteraction-rich environments.\n","authors":["Rui Wang","Quentin Lohmeyer","Mirko Meboldt","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2503.13176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08722v2","updated":"2025-03-17T13:49:27Z","published":"2025-03-10T21:09:02Z","title":"A Recipe for Improving Remote Sensing VLM Zero Shot Generalization","summary":"  Foundation models have had a significant impact across various AI\napplications, enabling use cases that were previously impossible. Contrastive\nVisual Language Models (VLMs), in particular, have outperformed other\ntechniques in many tasks. However, their prevalence in remote sensing (RS) is\nstill limited, due to the scarcity of diverse remote-sensing visual-language\ndatasets. In this work we introduce two novel image-caption datasets for\ntraining of remote sensing foundation models. The first dataset pairs aerial\nand satellite imagery with captions generated by Gemini using landmarks\nextracted from Google Maps. The second dataset utilizes public web images and\ntheir corresponding alt-text, filtered for the remote sensing domain, resulting\nin a diverse dataset with greater breadth in image styles and subject matter.\nThese datasets are used to pre-train the\nMaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting\nin state-of-the-art generalization performance in zero-shot cross-modal\nretrieval on well-known public benchmarks. Finally, we present our ongoing\nresearch to distill image-level knowledge gained in the VLM contrastive\ntraining procedure to enhance the model's localization ability. Specifically,\nwe iteratively generate pseudo-labels for image regions based on the model's\nattention maps and use these labels for further training. To mitigate noisy\nattention maps and create robust segmentation masks, we introduce a novel\nattention-pooling mechanism called the Smooth-Attention-Operation.\n","authors":["Aviad Barzilai","Yotam Gigi","Amr Helmy","Vered Silverman","Yehonathan Refael","Bolous Jaber","Tomer Shekel","George Leifman","Genady Beryozkin"],"pdf_url":"https://arxiv.org/pdf/2503.08722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13165v1","updated":"2025-03-17T13:39:51Z","published":"2025-03-17T13:39:51Z","title":"From Zero to Detail: Deconstructing Ultra-High-Definition Image\n  Restoration from Progressive Spectral Perspective","summary":"  Ultra-high-definition (UHD) image restoration faces significant challenges\ndue to its high resolution, complex content, and intricate details. To cope\nwith these challenges, we analyze the restoration process in depth through a\nprogressive spectral perspective, and deconstruct the complex UHD restoration\nproblem into three progressive stages: zero-frequency enhancement,\nlow-frequency restoration, and high-frequency refinement. Building on this\ninsight, we propose a novel framework, ERR, which comprises three collaborative\nsub-networks: the zero-frequency enhancer (ZFE), the low-frequency restorer\n(LFR), and the high-frequency refiner (HFR). Specifically, the ZFE integrates\nglobal priors to learn global mapping, while the LFR restores low-frequency\ninformation, emphasizing reconstruction of coarse-grained content. Finally, the\nHFR employs our designed frequency-windowed kolmogorov-arnold networks (FW-KAN)\nto refine textures and details, producing high-quality image restoration. Our\napproach significantly outperforms previous UHD methods across various tasks,\nwith extensive ablation studies validating the effectiveness of each component.\nThe code is available at \\href{https://github.com/NJU-PCALab/ERR}{here}.\n","authors":["Chen Zhao","Zhizhou Chen","Yunzhe Xu","Enxuan Gu","Jian Li","Zili Yi","Qian Wang","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2503.13165v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13163v1","updated":"2025-03-17T13:36:49Z","published":"2025-03-17T13:36:49Z","title":"Beyond RGB: Adaptive Parallel Processing for RAW Object Detection","summary":"  Object detection models are typically applied to standard RGB images\nprocessed through Image Signal Processing (ISP) pipelines, which are designed\nto enhance sensor-captured RAW images for human vision. However, these ISP\nfunctions can lead to a loss of critical information that may be essential in\noptimizing for computer vision tasks, such as object detection. In this work,\nwe introduce Raw Adaptation Module (RAM), a module designed to replace the\ntraditional ISP, with parameters optimized specifically for RAW object\ndetection. Inspired by the parallel processing mechanisms of the human visual\nsystem, RAM departs from existing learned ISP methods by applying multiple ISP\nfunctions in parallel rather than sequentially, allowing for a more\ncomprehensive capture of image features. These processed representations are\nthen fused in a specialized module, which dynamically integrates and optimizes\nthe information for the target task. This novel approach not only leverages the\nfull potential of RAW sensor data but also enables task-specific\npre-processing, resulting in superior object detection performance. Our\napproach outperforms RGB-based methods and achieves state-of-the-art results\nacross diverse RAW image datasets under varying lighting conditions and dynamic\nranges.\n","authors":["Shani Gamrian","Hila Barel","Feiran Li","Masakazu Yoshimura","Daisuke Iso"],"pdf_url":"https://arxiv.org/pdf/2503.13163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13070v2","updated":"2025-03-17T13:32:12Z","published":"2024-12-17T16:34:32Z","title":"Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction","summary":"  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n","authors":["Stanislas Ducotterd","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2412.13070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13160v1","updated":"2025-03-17T13:31:19Z","published":"2025-03-17T13:31:19Z","title":"Language-guided Open-world Video Anomaly Detection","summary":"  Video anomaly detection models aim to detect anomalies that deviate from what\nis expected. In open-world scenarios, the expected events may change as\nrequirements change. For example, not wearing a mask is considered abnormal\nduring a flu outbreak but normal otherwise. However, existing methods assume\nthat the definition of anomalies is invariable, and thus are not applicable to\nthe open world. To address this, we propose a novel open-world VAD paradigm\nwith variable definitions, allowing guided detection through user-provided\nnatural language at inference time. This paradigm necessitates establishing a\nrobust mapping from video and textual definition to anomaly score. Therefore,\nwe propose LaGoVAD (Language-guided Open-world VAD), a model that dynamically\nadapts anomaly definitions through two regularization strategies: diversifying\nthe relative durations of anomalies via dynamic video synthesis, and enhancing\nfeature robustness through contrastive learning with negative mining. Training\nsuch adaptable models requires diverse anomaly definitions, but existing\ndatasets typically provide given labels without semantic descriptions. To\nbridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the\nlargest and most diverse video anomaly dataset to date, featuring 35,279\nannotated videos with multi-level category labels and descriptions that\nexplicitly define anomalies. Zero-shot experiments on seven datasets\ndemonstrate SOTA performance. Data and code will be released.\n","authors":["Zihao Liu","Xiaoyu Wu","Jianqin Wu","Xuxu Wang","Linlin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13156v1","updated":"2025-03-17T13:26:47Z","published":"2025-03-17T13:26:47Z","title":"DynSTG-Mamba: Dynamic Spatio-Temporal Graph Mamba with Cross-Graph\n  Knowledge Distillation for Gait Disorders Recognition","summary":"  Gait disorder recognition plays a crucial role in the early diagnosis and\nmonitoring of movement disorders. Existing approaches, including\nspatio-temporal graph convolutional networks (ST-GCNs), often face high memory\ndemands and struggle to capture complex spatio-temporal dependencies, limiting\ntheir efficiency in clinical applications. To address these challenges, we\nintroduce DynSTG-Mamba (Dynamic Spatio-Temporal Graph Mamba), a novel framework\nthat combines DF-STGNN and STG-Mamba to enhance motion sequence modeling. The\nDF-STGNN incorporates a dynamic spatio-temporal filter that adaptively adjusts\nspatial connections between skeletal joints and temporal interactions across\ndifferent movement phases. This approach ensures better feature propagation\nthrough dynamic graph structures by considering the hierarchical nature and\ndynamics of skeletal gait data. Meanwhile, STG-Mamba, an extension of Mamba\nadapted for skeletal motion data, ensures a continuous propagation of states,\nfacilitating the capture of long-term dependencies while reducing computational\ncomplexity. To reduce the number of model parameters and computational costs\nwhile maintaining consistency, we propose Cross-Graph Relational Knowledge\nDistillation, a novel knowledge transfer mechanism that aligns relational\ninformation between teacher (large architecture) and student models (small\narchitecture) while using shared memory. This ensures that the interactions and\nmovement patterns of the joints are accurately preserved in the motion\nsequences. We validate our DynSTG-Mamba on KOA-NM, PD-WALK, and ATAXIA\ndatasets, where it outperforms state-of-the-art approaches by achieving in\nterms of Accuracy, F1-score, and Recall. Our results highlight the efficiency\nand robustness of our approach, offering a lightweight yet highly accurate\nsolution for automated gait analysis and movement disorder assessment.\n","authors":["Zakariae Zrimek","Youssef Mourchid","Mohammed El Hassouni"],"pdf_url":"https://arxiv.org/pdf/2503.13156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13147v1","updated":"2025-03-17T13:18:35Z","published":"2025-03-17T13:18:35Z","title":"Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing","summary":"  We propose a novel Iterative Predictor-Critic Code Decoding framework for\nreal-world image dehazing, abbreviated as IPC-Dehaze, which leverages the\nhigh-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from\nprevious codebook-based methods that rely on one-shot decoding, our method\nutilizes high-quality codes obtained in the previous iteration to guide the\nprediction of the Code-Predictor in the subsequent iteration, improving code\nprediction accuracy and ensuring stable dehazing performance. Our idea stems\nfrom the observations that 1) the degradation of hazy images varies with haze\ndensity and scene depth, and 2) clear regions play crucial cues in restoring\ndense haze regions. However, it is non-trivial to progressively refine the\nobtained codes in subsequent iterations, owing to the difficulty in determining\nwhich codes should be retained or replaced at each iteration. Another key\ninsight of our study is to propose Code-Critic to capture interrelations among\ncodes. The Code-Critic is used to evaluate code correlations and then resample\na set of codes with the highest mask scores, i.e., a higher score indicates\nthat the code is more likely to be rejected, which helps retain more accurate\ncodes and predict difficult ones. Extensive experiments demonstrate the\nsuperiority of our method over state-of-the-art methods in real-world dehazing.\n","authors":["Jiayi Fu","Siyu Liu","Zikun Liu","Chun-Le Guo","Hyunhee Park","Ruiqi Wu","Guoqing Wang","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2503.13147v1.pdf","comment":"Acceptted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13139v1","updated":"2025-03-17T13:07:34Z","published":"2025-03-17T13:07:34Z","title":"Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical\n  Verification for Long Video Understanding","summary":"  Understanding long video content is a complex endeavor that often relies on\ndensely sampled frame captions or end-to-end feature selectors, yet these\ntechniques commonly overlook the logical relationships between textual queries\nand visual elements. In practice, computational constraints necessitate coarse\nframe subsampling, a challenge analogous to ``finding a needle in a haystack.''\nTo address this issue, we introduce a semantics-driven search framework that\nreformulates keyframe selection under the paradigm of Visual Semantic-Logical\nSearch. Specifically, we systematically define four fundamental logical\ndependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute\ndependency, and 4) causal order. These relations dynamically update frame\nsampling distributions through an iterative refinement process, enabling\ncontext-aware identification of semantically critical frames tailored to\nspecific query requirements. Our method establishes new SOTA performance on the\nmanually annotated benchmark in key-frame selection metrics. Furthermore, when\napplied to downstream video question-answering tasks, the proposed approach\ndemonstrates the best performance gains over existing methods on LongVideoBench\nand Video-MME, validating its effectiveness in bridging the logical gap between\ntextual queries and visual-temporal reasoning. The code will be publicly\navailable.\n","authors":["Weiyu Guo","Ziyang Chen","Shaoguang Wang","Jianxiang He","Yijie Xu","Jinhui Ye","Ying Sun","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.13139v1.pdf","comment":"18 pages, under review"},{"id":"http://arxiv.org/abs/2503.09151v2","updated":"2025-03-17T13:01:59Z","published":"2025-03-12T08:26:15Z","title":"Reangle-A-Video: 4D Video Generation as Video-to-Video Translation","summary":"  We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/\n","authors":["Hyeonho Jeong","Suhyeon Lee","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2503.09151v2.pdf","comment":"Project page: https://hyeonho99.github.io/reangle-a-video/"},{"id":"http://arxiv.org/abs/2503.13134v1","updated":"2025-03-17T12:59:34Z","published":"2025-03-17T12:59:34Z","title":"Enhancing zero-shot learning in medical imaging: integrating clip with\n  advanced techniques for improved chest x-ray analysis","summary":"  Due to the large volume of medical imaging data, advanced AI methodologies\nare needed to assist radiologists in diagnosing thoracic diseases from chest\nX-rays (CXRs). Existing deep learning models often require large, labeled\ndatasets, which are scarce in medical imaging due to the time-consuming and\nexpert-driven annotation process. In this paper, we extend the existing\napproach to enhance zero-shot learning in medical imaging by integrating\nContrastive Language-Image Pre-training (CLIP) with Momentum Contrast (MoCo),\nresulting in our proposed model, MoCoCLIP. Our method addresses challenges\nposed by class-imbalanced and unlabeled datasets, enabling improved detection\nof pulmonary pathologies. Experimental results on the NIH ChestXray14 dataset\ndemonstrate that MoCoCLIP outperforms the state-of-the-art CheXZero model,\nachieving relative improvement of approximately 6.5%. Furthermore, on the\nCheXpert dataset, MoCoCLIP demonstrates superior zero-shot performance,\nachieving an average AUC of 0.750 compared to CheXZero with 0.746 AUC,\nhighlighting its enhanced generalization capabilities on unseen data.\n","authors":["Prakhar Bhardwaj","Sheethal Bhat","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2503.13134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13131v1","updated":"2025-03-17T12:55:43Z","published":"2025-03-17T12:55:43Z","title":"Patient-specific radiomic feature selection with reconstructed healthy\n  persona of knee MR images","summary":"  Classical radiomic features have been designed to describe image appearance\nand intensity patterns. These features are directly interpretable and readily\nunderstood by radiologists. Compared with end-to-end deep learning (DL) models,\nlower dimensional parametric models that use such radiomic features offer\nenhanced interpretability but lower comparative performance in clinical tasks.\nIn this study, we propose an approach where a standard logistic regression\nmodel performance is substantially improved by learning to select radiomic\nfeatures for individual patients, from a pool of candidate features. This\napproach has potentials to maintain the interpretability of such approaches\nwhile offering comparable performance to DL. We also propose to expand the\nfeature pool by generating a patient-specific healthy persona via\nmask-inpainting using a denoising diffusion model trained on healthy subjects.\nSuch a pathology-free baseline feature set allows further opportunity in novel\nfeature discovery and improved condition classification. We demonstrate our\nmethod on multiple clinical tasks of classifying general abnormalities,\nanterior cruciate ligament tears, and meniscus tears. Experimental results\ndemonstrate that our approach achieved comparable or even superior performance\nthan state-of-the-art DL approaches while offering added interpretability by\nusing radiomic features extracted from images and supplemented by generating\nhealthy personas. Example clinical cases are discussed in-depth to demonstrate\nthe intepretability-enabled utilities such as human-explainable feature\ndiscovery and patient-specific location/view selection. These findings\nhighlight the potentials of the combination of subject-specific feature\nselection with generative models in augmenting radiomic analysis for more\ninterpretable decision-making. The codes are available at:\nhttps://github.com/YaxiiC/RadiomicsPersona.git\n","authors":["Yaxi Chen","Simin Ni","Aleksandra Ivanova","Shaheer U. Saeed","Rikin Hargunani","Jie Huang","Chaozong Liu","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2503.13131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13130v1","updated":"2025-03-17T12:55:34Z","published":"2025-03-17T12:55:34Z","title":"ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object\n  Interaction Generation","summary":"  We propose ChainHOI, a novel approach for text-driven human-object\ninteraction (HOI) generation that explicitly models interactions at both the\njoint and kinetic chain levels. Unlike existing methods that implicitly model\ninteractions using full-body poses as tokens, we argue that explicitly modeling\njoint-level interactions is more natural and effective for generating realistic\nHOIs, as it directly captures the geometric and semantic relationships between\njoints, rather than modeling interactions in the latent pose space. To this\nend, ChainHOI introduces a novel joint graph to capture potential interactions\nwith objects, and a Generative Spatiotemporal Graph Convolution Network to\nexplicitly model interactions at the joint level. Furthermore, we propose a\nKinematics-based Interaction Module that explicitly models interactions at the\nkinetic chain level, ensuring more realistic and biomechanically coherent\nmotions. Evaluations on two public datasets demonstrate that ChainHOI\nsignificantly outperforms previous methods, generating more realistic, and\nsemantically consistent HOIs. Code is available\n\\href{https://github.com/qinghuannn/ChainHOI}{here}.\n","authors":["Ling-An Zeng","Guohong Huang","Yi-Lin Wei","Shengbo Gu","Yu-Ming Tang","Jingke Meng","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.13130v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13125v1","updated":"2025-03-17T12:48:48Z","published":"2025-03-17T12:48:48Z","title":"Non-Destructive Detection of Sub-Micron Imperceptible Scratches On Laser\n  Chips Based On Consistent Texture Entropy Recursive Optimization\n  Semi-Supervised Network","summary":"  Laser chips, the core components of semiconductor lasers, are extensively\nutilized in various industries, showing great potential for future application.\nSmoothness emitting surfaces are crucial in chip production, as even\nimperceptible scratches can significantly degrade performance and lifespan,\nthus impeding production efficiency and yield. Therefore, non-destructively\ndetecting these imperceptible scratches on the emitting surfaces is essential\nfor enhancing yield and reducing costs. These sub-micron level scratches,\nbarely visible against the background, are extremely difficult to detect with\nconventional methods, compounded by a lack of labeled datasets. To address this\nchallenge, this paper introduces TexRecNet, a consistent texture entropy\nrecursive optimization semi-supervised network. The network, based on a\nrecursive optimization architecture, iteratively improves the detection\naccuracy of imperceptible scratch edges, using outputs from previous cycles to\ninform subsequent inputs and guide the network's positional encoding. It also\nintroduces image texture entropy, utilizing a substantial amount of unlabeled\ndata to expand the training set while maintaining training signal reliability.\nUltimately, by analyzing the inconsistency of the network output sequences\nobtained during the recursive process, a semi-supervised training strategy with\nrecursive consistency constraints is proposed, using outputs from the recursive\nprocess for non-destructive signal augmentation and consistently optimizes the\nloss function for efficient end-to-end training. Experimental results show that\nthis method, utilizing a substantial amount of unsupervised data, achieves\n75.6% accuracy and 74.8% recall in detecting imperceptible scratches, an 8.5%\nand 33.6% improvement over conventional Unet, enhancing quality control in\nlaser chips.\n","authors":["Pan Liu"],"pdf_url":"https://arxiv.org/pdf/2503.13125v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2503.13120v1","updated":"2025-03-17T12:47:33Z","published":"2025-03-17T12:47:33Z","title":"3D Human Interaction Generation: A Survey","summary":"  3D human interaction generation has emerged as a key research area, focusing\non producing dynamic and contextually relevant interactions between humans and\nvarious interactive entities. Recent rapid advancements in 3D model\nrepresentation methods, motion capture technologies, and generative models have\nlaid a solid foundation for the growing interest in this domain. Existing\nresearch in this field can be broadly categorized into three areas: human-scene\ninteraction, human-object interaction, and human-human interaction. Despite the\nrapid advancements in this area, challenges remain due to the need for\nnaturalness in human motion generation and the accurate interaction between\nhumans and interactive entities. In this survey, we present a comprehensive\nliterature review of human interaction generation, which, to the best of our\nknowledge, is the first of its kind. We begin by introducing the foundational\ntechnologies, including model representations, motion capture methods, and\ngenerative models. Subsequently, we introduce the approaches proposed for the\nthree sub-tasks, along with their corresponding datasets and evaluation\nmetrics. Finally, we discuss potential future research directions in this area\nand conclude the survey. Through this survey, we aim to offer a comprehensive\noverview of the current advancements in the field, highlight key challenges,\nand inspire future research works.\n","authors":["Siyuan Fan","Wenke Huang","Xiantao Cai","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2503.13120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19189v2","updated":"2025-03-17T12:43:52Z","published":"2024-11-28T14:50:14Z","title":"Video Depth without Video Models","summary":"  Video depth estimation lifts monocular video clips to 3D by inferring dense\ndepth at every frame. Recent advances in single-image depth estimation, brought\nabout by the rise of large foundation models and the use of synthetic training\ndata, have fueled a renewed interest in video depth. However, naively applying\na single-image depth estimator to every frame of a video disregards temporal\ncontinuity, which not only leads to flickering but may also break when camera\nmotion causes sudden changes in depth range. An obvious and principled solution\nwould be to build on top of video foundation models, but these come with their\nown limitations; including expensive training and inference, imperfect 3D\nconsistency, and stitching routines for the fixed-length (short) outputs. We\ntake a step back and demonstrate how to turn a single-image latent diffusion\nmodel (LDM) into a state-of-the-art video depth estimator. Our model, which we\ncall RollingDepth, has two main ingredients: (i) a multi-frame depth estimator\nthat is derived from a single-image LDM and maps very short video snippets\n(typically frame triplets) to depth snippets. (ii) a robust, optimization-based\nregistration algorithm that optimally assembles depth snippets sampled at\nvarious different frame rates back into a consistent video. RollingDepth is\nable to efficiently handle long videos with hundreds of frames and delivers\nmore accurate depth videos than both dedicated video depth estimators and\nhigh-performing single-frame models. Project page: rollingdepth.github.io.\n","authors":["Bingxin Ke","Dominik Narnhofer","Shengyu Huang","Lei Ke","Torben Peters","Katerina Fragkiadaki","Anton Obukhov","Konrad Schindler"],"pdf_url":"https://arxiv.org/pdf/2411.19189v2.pdf","comment":"Project page: rollingdepth.github.io"},{"id":"http://arxiv.org/abs/2411.16064v2","updated":"2025-03-17T12:35:16Z","published":"2024-11-25T03:28:09Z","title":"Multi-Granularity Class Prototype Topology Distillation for\n  Class-Incremental Source-Free Unsupervised Domain Adaptation","summary":"  This paper explores the Class-Incremental Source-Free Unsupervised Domain\nAdaptation (CI-SFUDA) problem, where the unlabeled target data come\nincrementally without access to labeled source instances. This problem poses\ntwo challenges, the interference of similar source-class knowledge in\ntarget-class representation learning and the shocks of new target knowledge to\nold ones.To address them, we propose the Multi-Granularity Class Prototype\nTopology Distillation (GROTO) algorithm, which effectively transfers the source\nknowledge to the class-incremental target domain.Concretely, we design the\nmulti-granularity class prototype self-organization module and the prototype\ntopology distillation module. First, we mine the positive classes by modeling\naccumulation distributions. Next, we introduce multi-granularity class\nprototypes to generate reliable pseudo-labels, and exploit them to promote the\npositive-class target feature self-organization. Second, the positive-class\nprototypes are leveraged to construct the topological structures of source and\ntarget feature spaces. Then, we perform the topology distillation to\ncontinually mitigate the shocks of new target knowledge to old ones. Extensive\nexperiments demonstrate that our proposed method achieves state-of-the-art\nperformance on three public datasets.\n","authors":["Peihua Deng","Jiehua Zhang","Xichun Sheng","Chenggang Yan","Yaoqi Sun","Ying Fu","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2411.16064v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13111v1","updated":"2025-03-17T12:34:22Z","published":"2025-03-17T12:34:22Z","title":"MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.\n","authors":["Erik Daxberger","Nina Wenzel","David Griffiths","Haiming Gang","Justin Lazarow","Gefen Kohavi","Kai Kang","Marcin Eichner","Yinfei Yang","Afshin Dehghan","Peter Grasch"],"pdf_url":"https://arxiv.org/pdf/2503.13111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13110v1","updated":"2025-03-17T12:34:14Z","published":"2025-03-17T12:34:14Z","title":"DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology\n  and Geometry","summary":"  Boundary representation (B-rep) of geometric models is a fundamental format\nin Computer-Aided Design (CAD). However, automatically generating valid and\nhigh-quality B-rep models remains challenging due to the complex\ninterdependence between the topology and geometry of the models. Existing\nmethods tend to prioritize geometric representation while giving insufficient\nattention to topological constraints, making it difficult to maintain\nstructural validity and geometric accuracy. In this paper, we propose\nDTGBrepGen, a novel topology-geometry decoupled framework for B-rep generation\nthat explicitly addresses both aspects. Our approach first generates valid\ntopological structures through a two-stage process that independently models\nedge-face and edge-vertex adjacency relationships. Subsequently, we employ\nTransformer-based diffusion models for sequential geometry generation,\nprogressively generating vertex coordinates, followed by edge geometries and\nface geometries which are represented as B-splines. Extensive experiments on\ndiverse CAD datasets show that DTGBrepGen significantly outperforms existing\nmethods in both topological validity and geometric accuracy, achieving higher\nvalidity rates and producing more diverse and realistic B-reps. Our code is\npublicly available at https://github.com/jinli99/DTGBrepGen.\n","authors":["Jing Li","Yihang Fu","Falai Chen"],"pdf_url":"https://arxiv.org/pdf/2503.13110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13108v1","updated":"2025-03-17T12:31:23Z","published":"2025-03-17T12:31:23Z","title":"Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways\n  to Faster Inference","summary":"  Multimodal large language models (MLLMs) improve performance on\nvision-language tasks by integrating visual features from pre-trained vision\nencoders into large language models (LLMs). However, how MLLMs process and\nutilize visual information remains unclear. In this paper, a shift in the\ndominant flow of visual information is uncovered: (1) in shallow layers, strong\ninteractions are observed between image tokens and instruction tokens, where\nmost visual information is injected into instruction tokens to form cross-modal\nsemantic representations; (2) in deeper layers, image tokens primarily interact\nwith each other, aggregating the remaining visual information to optimize\nsemantic representations within visual modality. Based on these insights, we\npropose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference\nacceleration method that dynamically prunes image tokens at specific layers,\nreducing computational costs by approximately 65% without sacrificing\nperformance. Our findings offer a new understanding of visual information\nprocessing in MLLMs and provide a state-of-the-art solution for efficient\ninference.\n","authors":["Hao Yin","Guangzong Si","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13107v1","updated":"2025-03-17T12:30:40Z","published":"2025-03-17T12:30:40Z","title":"ClearSight: Visual Signal Enhancement for Object Hallucination\n  Mitigation in Multimodal Large language Models","summary":"  Contrastive decoding strategies are widely used to mitigate object\nhallucinations in multimodal large language models (MLLMs). By reducing\nover-reliance on language priors, these strategies ensure that generated\ncontent remains closely grounded in visual inputs, producing contextually\naccurate outputs. Since contrastive decoding requires no additional training or\nexternal tools, it offers both computational efficiency and versatility, making\nit highly attractive. However, these methods present two main limitations: (1)\nbluntly suppressing language priors can compromise coherence and accuracy of\ngenerated content, and (2) processing contrastive inputs adds computational\nload, significantly slowing inference speed. To address these challenges, we\npropose Visual Amplification Fusion (VAF), a plug-and-play technique that\nenhances attention to visual signals within the model's middle layers, where\nmodality fusion predominantly occurs. This approach enables more effective\ncapture of visual features, reducing the model's bias toward language modality.\nExperimental results demonstrate that VAF significantly reduces hallucinations\nacross various MLLMs without affecting inference speed, while maintaining\ncoherence and accuracy in generated outputs.\n","authors":["Hao Yin","Guangzong Si","Zilei Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11741v3","updated":"2025-03-17T12:08:11Z","published":"2025-01-20T21:00:12Z","title":"FaceQSORT: a Multi-Face Tracking Method based on Biometric and\n  Appearance Features","summary":"  In this work, a novel multi-face tracking method named FaceQSORT is proposed.\nTo mitigate multi-face tracking challenges (e.g., partially occluded or lateral\nfaces), FaceQSORT combines biometric and visual appearance features (extracted\nfrom the same image (face) patch) for association. The Q in FaceQSORT refers to\nthe scenario for which FaceQSORT is desinged, i.e. tracking people's faces as\nthey move towards a gate in a Queue. This scenario is also reflected in the new\ndataset `Paris Lodron University Salzburg Faces in a Queue', which is made\npublicly available as part of this work. The dataset consists of a total of\nseven fully annotated and challenging sequences (12730 frames) and is utilized\ntogether with two other publicly available datasets for the experimental\nevaluation. It is shown that FaceQSORT outperforms state-of-the-art trackers in\nthe considered scenario. To provide a deeper insight into FaceQSORT,\ncomprehensive experiments are conducted evaluating the parameter selection, a\ndifferent similarity metric and the utilized face recognition model (used to\nextract biometric features).\n","authors":["Robert Jöchl","Andreas Uhl"],"pdf_url":"https://arxiv.org/pdf/2501.11741v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02187v2","updated":"2025-03-17T12:06:19Z","published":"2025-02-04T10:02:40Z","title":"ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel\n  Diffusion","summary":"  This paper proposes ShapeShifter, a new 3D generative model that learns to\nsynthesize shape variations based on a single reference model. While generative\nmethods for 3D objects have recently attracted much attention, current\ntechniques often lack geometric details and/or require long training times and\nlarge resources. Our approach remedies these issues by combining sparse voxel\ngrids and point, normal, and color sampling within a multiscale neural\narchitecture that can be trained efficiently and in parallel. We show that our\nresulting variations better capture the fine details of their original input\nand can handle more general types of surfaces than previous SDF-based methods.\nMoreover, we offer interactive generation of 3D shape variants, allowing more\nhuman control in the design loop if needed.\n","authors":["Nissim Maruani","Wang Yifan","Matthew Fisher","Pierre Alliez","Mathieu Desbrun"],"pdf_url":"https://arxiv.org/pdf/2502.02187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15786v3","updated":"2025-03-17T12:01:18Z","published":"2024-04-24T10:19:25Z","title":"Rethinking model prototyping through the MedMNIST+ dataset collection","summary":"  The integration of deep learning based systems in clinical practice is often\nimpeded by challenges rooted in limited and heterogeneous medical datasets. In\naddition, the field has increasingly prioritized marginal performance gains on\na few, narrowly scoped benchmarks over clinical applicability, slowing down\nmeaningful algorithmic progress. This trend often results in excessive\nfine-tuning of existing methods on selected datasets rather than fostering\nclinically relevant innovations. In response, this work introduces a\ncomprehensive benchmark for the MedMNIST+ dataset collection, designed to\ndiversify the evaluation landscape across several imaging modalities,\nanatomical regions, classification tasks and sample sizes. We systematically\nreassess commonly used Convolutional Neural Networks (CNNs) and Vision\nTransformer (ViT) architectures across distinct medical datasets, training\nmethodologies, and input resolutions to validate and refine existing\nassumptions about model effectiveness and development. Our findings suggest\nthat computationally efficient training schemes and modern foundation models\noffer viable alternatives to costly end-to-end training. Additionally, we\nobserve that higher image resolutions do not consistently improve performance\nbeyond a certain threshold. This highlights the potential benefits of using\nlower resolutions, particularly in prototyping stages, to reduce computational\ndemands without sacrificing accuracy. Notably, our analysis reaffirms the\ncompetitiveness of CNNs compared to ViTs, emphasizing the importance of\ncomprehending the intrinsic capabilities of different architectures. Finally,\nby establishing a standardized evaluation framework, we aim to enhance\ntransparency, reproducibility, and comparability within the MedMNIST+ dataset\ncollection. Code is available at\nhttps://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus .\n","authors":["Sebastian Doerrich","Francesco Di Salvo","Julius Brockmann","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2404.15786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13090v1","updated":"2025-03-17T11:57:41Z","published":"2025-03-17T11:57:41Z","title":"Multi-Platform Teach-and-Repeat Navigation by Visual Place Recognition\n  Based on Deep-Learned Local Features","summary":"  Uniform and variable environments still remain a challenge for stable visual\nlocalization and mapping in mobile robot navigation. One of the possible\napproaches suitable for such environments is appearance-based teach-and-repeat\nnavigation, relying on simplified localization and reactive robot motion\ncontrol - all without a need for standard mapping. This work brings an\ninnovative solution to such a system based on visual place recognition\ntechniques. Here, the major contributions stand in the employment of a new\nvisual place recognition technique, a novel horizontal shift computation\napproach, and a multi-platform system design for applications across various\ntypes of mobile robots. Secondly, a new public dataset for experimental testing\nof appearance-based navigation methods is introduced. Moreover, the work also\nprovides real-world experimental testing and performance comparison of the\nintroduced navigation system against other state-of-the-art methods. The\nresults confirm that the new system outperforms existing methods in several\ntesting scenarios, is capable of operation indoors and outdoors, and exhibits\nrobustness to day and night scene variations.\n","authors":["Václav Truhlařík","Tomáš Pivoňka","Michal Kasarda","Libor Přeučil"],"pdf_url":"https://arxiv.org/pdf/2503.13090v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.13086v1","updated":"2025-03-17T11:47:58Z","published":"2025-03-17T11:47:58Z","title":"Gaussian On-the-Fly Splatting: A Progressive Framework for Robust Near\n  Real-Time 3DGS Optimization","summary":"  3D Gaussian Splatting (3DGS) achieves high-fidelity rendering with fast\nreal-time performance, but existing methods rely on offline training after full\nStructure-from-Motion (SfM) processing. In contrast, this work introduces\nOn-the-Fly GS, a progressive framework enabling near real-time 3DGS\noptimization during image capture. As each image arrives, its pose and sparse\npoints are updated via on-the-fly SfM, and newly optimized Gaussians are\nimmediately integrated into the 3DGS field. We propose a progressive local\noptimization strategy to prioritize new images and their neighbors by their\ncorresponding overlapping relationship, allowing the new image and its\noverlapping images to get more training. To further stabilize training across\nold and new images, an adaptive learning rate schedule balances the iterations\nand the learning rate. Moreover, to maintain overall quality of the 3DGS field,\nan efficient global optimization scheme prevents overfitting to the newly added\nimages. Experiments on multiple benchmark datasets show that our On-the-Fly GS\nreduces training time significantly, optimizing each new image in seconds with\nminimal rendering loss, offering the first practical step toward rapid,\nprogressive 3DGS reconstruction.\n","authors":["Yiwei Xu","Yifei Yu","Wentian Gan","Tengfei Wang","Zongqian Zhan","Hao Cheng","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15959v5","updated":"2025-03-17T11:45:52Z","published":"2024-10-21T12:43:54Z","title":"Dita: Scaling Diffusion Transformer for Generalist\n  Vision-Language-Action Policy","summary":"  While recent vision-language-action models trained on diverse robot datasets\nexhibit promising generalization capabilities with limited in-domain data,\ntheir reliance on compact action heads to predict discretized or continuous\nactions constrains adaptability to heterogeneous action spaces. We present\nDita, a scalable framework that leverages Transformer architectures to directly\ndenoise continuous action sequences through a unified multimodal diffusion\nprocess. Departing from prior methods that condition denoising on fused\nembeddings via shallow networks, Dita employs in-context conditioning --\nenabling fine-grained alignment between denoised actions and raw visual tokens\nfrom historical observations. This design explicitly models action deltas and\nenvironmental nuances. By scaling the diffusion action denoiser alongside the\nTransformer's scalability, Dita effectively integrates cross-embodiment\ndatasets across diverse camera perspectives, observation scenes, tasks, and\naction spaces. Such synergy enhances robustness against various variances and\nfacilitates the successful execution of long-horizon tasks. Evaluations across\nextensive benchmarks demonstrate state-of-the-art or comparative performance in\nsimulation. Notably, Dita achieves robust real-world adaptation to\nenvironmental variances and complex long-horizon tasks through 10-shot\nfinetuning, using only third-person camera inputs. The architecture establishes\na versatile, lightweight and open-source baseline for generalist robot policy\nlearning. Project Page: https://robodita.github.io/\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Haonan Duan","Hengjun Pu","Ronglei Tong","Chengyang Zhao","Xizhou Zhu","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v5.pdf","comment":"I want to withdraw the recent replacement (v4), given that the author\n  is different, the title is also different and the content is totally\n  different"},{"id":"http://arxiv.org/abs/2503.13082v1","updated":"2025-03-17T11:41:16Z","published":"2025-03-17T11:41:16Z","title":"Free-form language-based robotic reasoning and grasping","summary":"  Performing robotic grasping from a cluttered bin based on human instructions\nis a challenging task, as it requires understanding both the nuances of\nfree-form language and the spatial relationships between objects.\nVision-Language Models (VLMs) trained on web-scale data, such as GPT-4o, have\ndemonstrated remarkable reasoning capabilities across both text and images. But\ncan they truly be used for this task in a zero-shot setting? And what are their\nlimitations? In this paper, we explore these research questions via the\nfree-form language-based robotic grasping task, and propose a novel method,\nFreeGrasp, leveraging the pre-trained VLMs' world knowledge to reason about\nhuman instructions and object spatial arrangements. Our method detects all\nobjects as keypoints and uses these keypoints to annotate marks on images,\naiming to facilitate GPT-4o's zero-shot spatial reasoning. This allows our\nmethod to determine whether a requested object is directly graspable or if\nother objects must be grasped and removed first. Since no existing dataset is\nspecifically designed for this task, we introduce a synthetic dataset\nFreeGraspData by extending the MetaGraspNetV2 dataset with human-annotated\ninstructions and ground-truth grasping sequences. We conduct extensive analyses\nwith both FreeGraspData and real-world validation with a gripper-equipped\nrobotic arm, demonstrating state-of-the-art performance in grasp reasoning and\nexecution. Project website: https://tev-fbk.github.io/FreeGrasp/.\n","authors":["Runyu Jiao","Alice Fasoli","Francesco Giuliari","Matteo Bortolon","Sergio Povoli","Guofeng Mei","Yiming Wang","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2503.13082v1.pdf","comment":"Project website: https://tev-fbk.github.io/FreeGrasp/"},{"id":"http://arxiv.org/abs/2503.13080v1","updated":"2025-03-17T11:36:58Z","published":"2025-03-17T11:36:58Z","title":"Vision-based automatic fruit counting with UAV","summary":"  The use of unmanned aerial vehicles (UAVs) for smart agriculture is becoming\nincreasingly popular. This is evidenced by recent scientific works, as well as\nthe various competitions organised on this topic. Therefore, in this work we\npresent a system for automatic fruit counting using UAVs. To detect them, our\nsolution uses a vision algorithm that processes streams from an RGB camera and\na depth sensor using classical image operations. Our system also allows the\nplanning and execution of flight trajectories, taking into account the\nminimisation of flight time and distance covered. We tested the proposed\nsolution in simulation and obtained an average score of 87.27/100 points from a\ntotal of 500 missions. We also submitted it to the UAV Competition organised as\npart of the ICUAS 2024 conference, where we achieved an average score of\n84.83/100 points, placing 6th in a field of 23 teams and advancing to the\nfinals.\n","authors":["Hubert Szolc","Mateusz Wasala","Remigiusz Mietla","Kacper Iwicki","Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2503.13080v1.pdf","comment":"Accepted for the 29th Conference on Automation - Innovations and\n  Future Perspectives Automation 2025, May 7 - 9, 2025, Warsaw, Poland"},{"id":"http://arxiv.org/abs/2503.13074v1","updated":"2025-03-17T11:25:48Z","published":"2025-03-17T11:25:48Z","title":"Rethinking Image Evaluation in Super-Resolution","summary":"  While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100\\%\ntrustworthy for model evaluations? How does GT quality affect this evaluation?\nAnd how to make fair evaluations if there exist imperfect GTs? To answer these\nquestions, this paper presents two main contributions. First, by systematically\nanalyzing seven state-of-the-art SR models across three real-world SR datasets,\nwe show that SR performances can be consistently affected across models by\nlow-quality GTs, and models can perform quite differently when GT quality is\ncontrolled. Second, we propose a novel perceptual quality metric, Relative\nQuality Index (RQI), that measures the relative quality discrepancy of image\npairs, thus issuing the biased evaluations caused by unreliable GTs. Our\nproposed model achieves significantly better consistency with human opinions.\nWe expect our work to provide insights for the SR community on how future\ndatasets, models, and metrics should be developed.\n","authors":["Shaolin Su","Josep M. Rocafort","Danna Xue","David Serrano-Lozano","Lei Sun","Javier Vazquez-Corral"],"pdf_url":"https://arxiv.org/pdf/2503.13074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13073v1","updated":"2025-03-17T11:25:05Z","published":"2025-03-17T11:25:05Z","title":"DehazeMamba: SAR-guided Optical Remote Sensing Image Dehazing with\n  Adaptive State Space Model","summary":"  Optical remote sensing image dehazing presents significant challenges due to\nits extensive spatial scale and highly non-uniform haze distribution, which\ntraditional single-image dehazing methods struggle to address effectively.\nWhile Synthetic Aperture Radar (SAR) imagery offers inherently haze-free\nreference information for large-scale scenes, existing SAR-guided dehazing\napproaches face two critical limitations: the integration of SAR information\noften diminishes the quality of haze-free regions, and the instability of\nfeature quality further exacerbates cross-modal domain shift. To overcome these\nchallenges, we introduce DehazeMamba, a novel SAR-guided dehazing network built\non a progressive haze decoupling fusion strategy. Our approach incorporates two\nkey innovations: a Haze Perception and Decoupling Module (HPDM) that\ndynamically identifies haze-affected regions through optical-SAR difference\nanalysis, and a Progressive Fusion Module (PFM) that mitigates domain shift\nthrough a two-stage fusion process based on feature quality assessment. To\nfacilitate research in this domain, we present MRSHaze, a large-scale benchmark\ndataset comprising 8,000 pairs of temporally synchronized, precisely\ngeo-registered SAR-optical images with high resolution and diverse haze\nconditions. Extensive experiments demonstrate that DehazeMamba significantly\noutperforms state-of-the-art methods, achieving a 0.73 dB improvement in PSNR\nand substantial enhancements in downstream tasks such as semantic segmentation.\nThe dataset is available at\nhttps://github.com/mmic-lcl/Datasets-and-benchmark-code.\n","authors":["Zhicheng Zhao","Jinquan Yan","Chenglong Li","Xiao Wang","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2503.13073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20237v2","updated":"2025-03-17T11:23:58Z","published":"2024-09-30T12:20:07Z","title":"Classroom-Inspired Multi-Mentor Distillation with Adaptive Learning\n  Strategies","summary":"  We propose ClassroomKD, a novel multi-mentor knowledge distillation framework\ninspired by classroom environments to enhance knowledge transfer between the\nstudent and multiple mentors with different knowledge levels. Unlike\ntraditional methods that rely on fixed mentor-student relationships, our\nframework dynamically selects and adapts the teaching strategies of diverse\nmentors based on their effectiveness for each data sample. ClassroomKD\ncomprises two main modules: the Knowledge Filtering (KF) module and the\nMentoring module. The KF Module dynamically ranks mentors based on their\nperformance for each input, activating only high-quality mentors to minimize\nerror accumulation and prevent information loss. The Mentoring Module adjusts\nthe distillation strategy by tuning each mentor's influence according to the\ndynamic performance gap between the student and mentors, effectively modulating\nthe learning pace. Extensive experiments on image classification (CIFAR-100 and\nImageNet) and 2D human pose estimation (COCO Keypoints and MPII Human Pose)\ndemonstrate that ClassroomKD outperforms existing knowledge distillation\nmethods for different network architectures. Our results highlight that a\ndynamic and adaptive approach to mentor selection and guidance leads to more\neffective knowledge transfer, paving the way for enhanced model performance\nthrough distillation.\n","authors":["Shalini Sarode","Muhammad Saif Ullah Khan","Tahira Shehzadi","Didier Stricker","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2409.20237v2.pdf","comment":"Accepted in IntelliSys 2025"},{"id":"http://arxiv.org/abs/2503.13070v1","updated":"2025-03-17T11:21:43Z","published":"2025-03-17T11:21:43Z","title":"Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation","summary":"  Aligning generated images to complicated text prompts and human preferences\nis a central challenge in Artificial Intelligence-Generated Content (AIGC).\nWith reward-enhanced diffusion distillation emerging as a promising approach\nthat boosts controllability and fidelity of text-to-image models, we identify a\nfundamental paradigm shift: as conditions become more specific and reward\nsignals stronger, the rewards themselves become the dominant force in\ngeneration. In contrast, the diffusion losses serve as an overly expensive form\nof regularization. To thoroughly validate our hypothesis, we introduce R0, a\nnovel conditional generation approach via regularized reward maximization.\nInstead of relying on tricky diffusion distillation losses, R0 proposes a new\nperspective that treats image generations as an optimization problem in data\nspace which aims to search for valid images that have high compositional\nrewards. By innovative designs of the generator parameterization and proper\nregularization techniques, we train state-of-the-art few-step text-to-image\ngenerative models with R0 at scales. Our results challenge the conventional\nwisdom of diffusion post-training and conditional generation by demonstrating\nthat rewards play a dominant role in scenarios with complex conditions. We hope\nour findings can contribute to further research into human-centric and\nreward-centric generation paradigms across the broader field of AIGC. Code is\navailable at https://github.com/Luo-Yihong/R0.\n","authors":["Yihong Luo","Tianyang Hu","Weijian Luo","Kenji Kawaguchi","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2503.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13068v1","updated":"2025-03-17T11:19:03Z","published":"2025-03-17T11:19:03Z","title":"Crab: A Unified Audio-Visual Scene Understanding Model with Explicit\n  Cooperation","summary":"  In recent years, numerous tasks have been proposed to encourage model to\ndevelop specified capability in understanding audio-visual scene, primarily\ncategorized into temporal localization, spatial localization, spatio-temporal\nreasoning, and pixel-level understanding. Instead, human possesses a unified\nunderstanding ability for diversified tasks. Therefore, designing an\naudio-visual model with general capability to unify these tasks is of great\nvalue. However, simply joint training for all tasks can lead to interference\ndue to the heterogeneity of audiovisual data and complex relationship among\ntasks. We argue that this problem can be solved through explicit cooperation\namong tasks. To achieve this goal, we propose a unified learning method which\nachieves explicit inter-task cooperation from both the perspectives of data and\nmodel thoroughly. Specifically, considering the labels of existing datasets are\nsimple words, we carefully refine these datasets and construct an Audio-Visual\nUnified Instruction-tuning dataset with Explicit reasoning process (AV-UIE),\nwhich clarifies the cooperative relationship among tasks. Subsequently, to\nfacilitate concrete cooperation in learning stage, an interaction-aware LoRA\nstructure with multiple LoRA heads is designed to learn different aspects of\naudiovisual data interaction. By unifying the explicit cooperation across the\ndata and model aspect, our method not only surpasses existing unified\naudio-visual model on multiple tasks, but also outperforms most specialized\nmodels for certain tasks. Furthermore, we also visualize the process of\nexplicit cooperation and surprisingly find that each LoRA head has certain\naudio-visual understanding ability. Code and dataset:\nhttps://github.com/GeWu-Lab/Crab\n","authors":["Henghui Du","Guangyao Li","Chang Zhou","Chunjie Zhang","Alan Zhao","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2503.13068v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12663v2","updated":"2025-03-17T11:12:15Z","published":"2024-05-21T10:24:06Z","title":"LAGA: Layered 3D Avatar Generation and Customization via Gaussian\n  Splatting","summary":"  Creating and customizing a 3D clothed avatar from textual descriptions is a\ncritical and challenging task. Traditional methods often treat the human body\nand clothing as inseparable, limiting users' ability to freely mix and match\ngarments. In response to this limitation, we present LAyered Gaussian Avatar\n(LAGA), a carefully designed framework enabling the creation of high-fidelity\ndecomposable avatars with diverse garments. By decoupling garments from avatar,\nour framework empowers users to conviniently edit avatars at the garment level.\nOur approach begins by modeling the avatar using a set of Gaussian points\norganized in a layered structure, where each layer corresponds to a specific\ngarment or the human body itself. To generate high-quality garments for each\nlayer, we introduce a coarse-to-fine strategy for diverse garment generation\nand a novel dual-SDS loss function to maintain coherence between the generated\ngarments and avatar components, including the human body and other garments.\nMoreover, we introduce three regularization losses to guide the movement of\nGaussians for garment transfer, allowing garments to be freely transferred to\nvarious avatars. Extensive experimentation demonstrates that our approach\nsurpasses existing methods in the generation of 3D clothed humans.\n","authors":["Jia Gong","Shenyu Ji","Lin Geng Foo","Kang Chen","Hossein Rahmani","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2405.12663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13063v1","updated":"2025-03-17T11:10:31Z","published":"2025-03-17T11:10:31Z","title":"Federated Learning with Domain Shift Eraser","summary":"  Federated learning (FL) is emerging as a promising technique for\ncollaborative learning without local data leaving their devices. However,\nclients' data originating from diverse domains may degrade model performance\ndue to domain shifts, preventing the model from learning consistent\nrepresentation space. In this paper, we propose a novel FL framework, Federated\nDomain Shift Eraser (FDSE), to improve model performance by differently erasing\neach client's domain skew and enhancing their consensus. First, we formulate\nthe model forward passing as an iterative deskewing process that extracts and\nthen deskews features alternatively. This is efficiently achieved by\ndecomposing each original layer in the neural network into a Domain-agnostic\nFeature Extractor (DFE) and a Domain-specific Skew Eraser (DSE). Then, a\nregularization term is applied to promise the effectiveness of feature\ndeskewing by pulling local statistics of DSE's outputs close to the globally\nconsistent ones. Finally, DFE modules are fairly aggregated and broadcast to\nall the clients to maximize their consensus, and DSE modules are personalized\nfor each client via similarity-aware aggregation to erase their domain skew\ndifferently. Comprehensive experiments were conducted on three datasets to\nconfirm the advantages of our method in terms of accuracy, efficiency, and\ngeneralizability.\n","authors":["Zheng Wang","Zihui Wang","Zheng Wang","Xiaoliang Fan","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13063v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.13060v1","updated":"2025-03-17T11:07:29Z","published":"2025-03-17T11:07:29Z","title":"Historic Scripts to Modern Vision: A Novel Dataset and A VLM Framework\n  for Transliteration of Modi Script to Devanagari","summary":"  In medieval India, the Marathi language was written using the Modi script.\nThe texts written in Modi script include extensive knowledge about medieval\nsciences, medicines, land records and authentic evidence about Indian history.\nAround 40 million documents are in poor condition and have not yet been\ntransliterated. Furthermore, only a few experts in this domain can\ntransliterate this script into English or Devanagari. Most of the past research\npredominantly focuses on individual character recognition. A system that can\ntransliterate Modi script documents to Devanagari script is needed. We propose\nthe MoDeTrans dataset, comprising 2,043 images of Modi script documents\naccompanied by their corresponding textual transliterations in Devanagari. We\nfurther introduce MoScNet (\\textbf{Mo}di \\textbf{Sc}ript \\textbf{Net}work), a\nnovel Vision-Language Model (VLM) framework for transliterating Modi script\nimages into Devanagari text. MoScNet leverages Knowledge Distillation, where a\nstudent model learns from a teacher model to enhance transliteration\nperformance. The final student model of MoScNet has better performance than the\nteacher model while having 163$\\times$ lower parameters. Our work is the first\nto perform direct transliteration from the handwritten Modi script to the\nDevanagari script. MoScNet also shows competitive results on the optical\ncharacter recognition (OCR) task.\n","authors":["Harshal Kausadikar","Tanvi Kale","Onkar Susladkar","Sparsh Mittal"],"pdf_url":"https://arxiv.org/pdf/2503.13060v1.pdf","comment":"Under submission at a conference"},{"id":"http://arxiv.org/abs/2503.07435v2","updated":"2025-03-17T11:06:08Z","published":"2025-03-10T15:18:10Z","title":"Open-Set Gait Recognition from Sparse mmWave Radar Point Clouds","summary":"  The adoption of Millimeter-Wave (mmWave) radar devices for human sensing,\nparticularly gait recognition, has recently gathered significant attention due\nto their efficiency, resilience to environmental conditions, and\nprivacy-preserving nature. In this work, we tackle the challenging problem of\nOpen-set Gait Recognition (OSGR) from sparse mmWave radar point clouds. Unlike\nmost existing research, which assumes a closed-set scenario, our work considers\nthe more realistic open-set case, where unknown subjects might be present at\ninference time, and should be correctly recognized by the system. Point clouds\nare well-suited for edge computing applications with resource constraints, but\nare more significantly affected by noise and random fluctuations than other\nrepresentations, like the more common micro-Doppler signature. This is the\nfirst work addressing open-set gait recognition with sparse point cloud data.\nTo do so, we propose a novel neural network architecture that combines\nsupervised classification with unsupervised reconstruction of the point clouds,\ncreating a robust, rich, and highly regularized latent space of gait features.\nTo detect unknown subjects at inference time, we introduce a probabilistic\nnovelty detection algorithm that leverages the structured latent space and\noffers a tunable trade-off between inference speed and prediction accuracy.\nAlong with this paper, we release mmGait10, an original human gait dataset\nfeaturing over five hours of measurements from ten subjects, under varied\nwalking modalities. Extensive experimental results show that our solution\nattains F1-Score improvements by 24% over state-of-the-art methods, on average,\nand across multiple openness levels.\n","authors":["Riccardo Mazzieri","Jacopo Pegoraro","Michele Rossi"],"pdf_url":"https://arxiv.org/pdf/2503.07435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13058v1","updated":"2025-03-17T11:02:53Z","published":"2025-03-17T11:02:53Z","title":"Do Vision Models Develop Human-Like Progressive Difficulty\n  Understanding?","summary":"  When a human undertakes a test, their responses likely follow a pattern: if\nthey answered an easy question $(2 \\times 3)$ incorrectly, they would likely\nanswer a more difficult one $(2 \\times 3 \\times 4)$ incorrectly; and if they\nanswered a difficult question correctly, they would likely answer the easy one\ncorrectly. Anything else hints at memorization. Do current visual recognition\nmodels exhibit a similarly structured learning capacity? In this work, we\nconsider the task of image classification and study if those models' responses\nfollow that pattern. Since real images aren't labeled with difficulty, we first\ncreate a dataset of 100 categories, 10 attributes, and 3 difficulty levels\nusing recent generative models: for each category (e.g., dog) and attribute\n(e.g., occlusion), we generate images of increasing difficulty (e.g., a dog\nwithout occlusion, a dog only partly visible). We find that most of the models\ndo in fact behave similarly to the aforementioned pattern around 80-90% of the\ntime. Using this property, we then explore a new way to evaluate those models.\nInstead of testing the model on every possible test image, we create an\nadaptive test akin to GRE, in which the model's performance on the current\nround of images determines the test images in the next round. This allows the\nmodel to skip over questions too easy/hard for itself, and helps us get its\noverall performance in fewer steps.\n","authors":["Zeyi Huang","Utkarsh Ojha","Yuyang Ji","Donghyun Lee","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2503.13058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13057v1","updated":"2025-03-17T11:02:28Z","published":"2025-03-17T11:02:28Z","title":"MaskSDM with Shapley values to improve flexibility, robustness, and\n  explainability in species distribution modeling","summary":"  Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.\n","authors":["Robin Zbinden","Nina van Tiel","Gencer Sumbul","Chiara Vanalli","Benjamin Kellenberger","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2503.13057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06646v2","updated":"2025-03-17T10:59:29Z","published":"2024-12-09T16:39:40Z","title":"The Narrow Gate: Localized Image-Text Communication in Vision-Language\n  Models","summary":"  Recent advances in multimodal training have significantly improved the\nintegration of image understanding and generation within a unified model. This\nstudy investigates how vision-language models (VLMs) handle image-understanding\ntasks, specifically focusing on how visual information is processed and\ntransferred to the textual domain. We compare VLMs that generate both images\nand text with those that output only text, highlighting key differences in\ninformation flow. We find that in models with multimodal outputs, image and\ntext embeddings are more separated within the residual stream. Additionally,\nmodels vary in how information is exchanged from visual to textual tokens. VLMs\nthat only output text exhibit a distributed communication pattern, where\ninformation is exchanged through multiple image tokens. In contrast, models\ntrained for image and text generation tend to rely on a single token that acts\nas a narrow gate for visual information. We demonstrate that ablating this\nsingle token significantly deteriorates performance on image understanding\ntasks. Furthermore, modifying this token enables effective steering of the\nimage semantics, showing that targeted, local interventions can reliably\ncontrol the model's global behavior.\n","authors":["Alessandro Serra","Francesco Ortu","Emanuele Panizon","Lucrezia Valeriani","Lorenzo Basile","Alessio Ansuini","Diego Doimo","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2412.06646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13053v1","updated":"2025-03-17T10:56:30Z","published":"2025-03-17T10:56:30Z","title":"Uncertainty-Aware Knowledge Distillation for Compact and Efficient 6DoF\n  Pose Estimation","summary":"  Compact and efficient 6DoF object pose estimation is crucial in applications\nsuch as robotics, augmented reality, and space autonomous navigation systems,\nwhere lightweight models are critical for real-time accurate performance. This\npaper introduces a novel uncertainty-aware end-to-end Knowledge Distillation\n(KD) framework focused on keypoint-based 6DoF pose estimation. Keypoints\npredicted by a large teacher model exhibit varying levels of uncertainty that\ncan be exploited within the distillation process to enhance the accuracy of the\nstudent model while ensuring its compactness. To this end, we propose a\ndistillation strategy that aligns the student and teacher predictions by\nadjusting the knowledge transfer based on the uncertainty associated with each\nteacher keypoint prediction. Additionally, the proposed KD leverages this\nuncertainty-aware alignment of keypoints to transfer the knowledge at key\nlocations of their respective feature maps. Experiments on the widely-used\nLINEMOD benchmark demonstrate the effectiveness of our method, achieving\nsuperior 6DoF object pose estimation with lightweight models compared to\nstate-of-the-art approaches. Further validation on the SPEED+ dataset for\nspacecraft pose estimation highlights the robustness of our approach under\ndiverse 6DoF pose estimation scenarios.\n","authors":["Nassim Ali Ousalah","Anis Kacem","Enjie Ghorbel","Emmanuel Koumandakis","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2503.13053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13051v1","updated":"2025-03-17T10:55:55Z","published":"2025-03-17T10:55:55Z","title":"Permutation Learning with Only N Parameters: From SoftSort to\n  Self-Organizing Gaussians","summary":"  Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical.\n","authors":["Kai Uwe Barthel","Florian Barthel","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2503.13051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13047v1","updated":"2025-03-17T10:52:32Z","published":"2025-03-17T10:52:32Z","title":"InsightDrive: Insight Scene Representation for End-to-End Autonomous\n  Driving","summary":"  Directly generating planning results from raw sensors has become increasingly\nprevalent due to its adaptability and robustness in complex scenarios. Scene\nrepresentation, as a key module in the pipeline, has traditionally relied on\nconventional perception, which focus on the global scene. However, in driving\nscenarios, human drivers typically focus only on regions that directly impact\ndriving, which often coincide with those required for end-to-end autonomous\ndriving. In this paper, a novel end-to-end autonomous driving method called\nInsightDrive is proposed, which organizes perception by language-guided scene\nrepresentation. We introduce an instance-centric scene tokenizer that\ntransforms the surrounding environment into map- and object-aware instance\ntokens. Scene attention language descriptions, which highlight key regions and\nobstacles affecting the ego vehicle's movement, are generated by a\nvision-language model that leverages the cognitive reasoning capabilities of\nfoundation models. We then align scene descriptions with visual features using\nthe vision-language model, guiding visual attention through these descriptions\nto give effectively scene representation. Furthermore, we employ self-attention\nand cross-attention mechanisms to model the ego-agents and ego-map\nrelationships to comprehensively build the topological relationships of the\nscene. Finally, based on scene understanding, we jointly perform motion\nprediction and planning. Extensive experiments on the widely used nuScenes\nbenchmark demonstrate that the proposed InsightDrive achieves state-of-the-art\nperformance in end-to-end autonomous driving. The code is available at\nhttps://github.com/songruiqi/InsightDrive\n","authors":["Ruiqi Song","Xianda Guo","Hangbin Wu","Qinggong Wei","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2503.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13045v1","updated":"2025-03-17T10:50:34Z","published":"2025-03-17T10:50:34Z","title":"All You Need to Know About Training Image Retrieval Models","summary":"  Image retrieval is the task of finding images in a database that are most\nsimilar to a given query image. The performance of an image retrieval pipeline\ndepends on many training-time factors, including the embedding model\narchitecture, loss function, data sampler, mining function, learning rate(s),\nand batch size. In this work, we run tens of thousands of training runs to\nunderstand the effect each of these factors has on retrieval accuracy. We also\ndiscover best practices that hold across multiple datasets. The code is\navailable at https://github.com/gmberton/image-retrieval\n","authors":["Gabriele Berton","Kevin Musgrave","Carlo Masone"],"pdf_url":"https://arxiv.org/pdf/2503.13045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09483v2","updated":"2025-03-17T10:38:39Z","published":"2025-03-12T15:38:11Z","title":"Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization","summary":"  We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters.\n","authors":["Andreas Kofler","Luca Calatroni","Christoph Kolbitsch","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2503.09483v2.pdf","comment":"To be submitted to the EUSIPCO 2025 conference"},{"id":"http://arxiv.org/abs/2502.06476v2","updated":"2025-03-17T10:32:40Z","published":"2025-02-10T13:54:55Z","title":"Image Intrinsic Scale Assessment: Bridging the Gap Between Quality and\n  Resolution","summary":"  Image Quality Assessment (IQA) measures and predicts perceived image quality\nby human observers. Although recent studies have highlighted the critical\ninfluence that variations in the scale of an image have on its perceived\nquality, this relationship has not been systematically quantified. To bridge\nthis gap, we introduce the Image Intrinsic Scale (IIS), defined as the largest\nscale where an image exhibits its highest perceived quality. We also present\nthe Image Intrinsic Scale Assessment (IISA) task, which involves subjectively\nmeasuring and predicting the IIS based on human judgments. We develop a\nsubjective annotation methodology and create the IISA-DB dataset, comprising\n785 image-IIS pairs annotated by experts in a rigorously controlled\ncrowdsourcing study. Furthermore, we propose WIISA (Weak-labeling for Image\nIntrinsic Scale Assessment), a strategy that leverages how the IIS of an image\nvaries with downscaling to generate weak labels. Experiments show that applying\nWIISA during the training of several IQA methods adapted for IISA consistently\nimproves the performance compared to using only ground-truth labels. We will\nrelease the code, dataset, and pre-trained models upon acceptance.\n","authors":["Vlad Hosu","Lorenzo Agnolucci","Daisuke Iso","Dietmar Saupe"],"pdf_url":"https://arxiv.org/pdf/2502.06476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13028v1","updated":"2025-03-17T10:30:26Z","published":"2025-03-17T10:30:26Z","title":"Beyond Role-Based Surgical Domain Modeling: Generalizable\n  Re-Identification in the Operating Room","summary":"  Surgical domain models improve workflow optimization through automated\npredictions of each staff member's surgical role. However, mounting evidence\nindicates that team familiarity and individuality impact surgical outcomes. We\npresent a novel staff-centric modeling approach that characterizes individual\nteam members through their distinctive movement patterns and physical\ncharacteristics, enabling long-term tracking and analysis of surgical personnel\nacross multiple procedures. To address the challenge of inter-clinic\nvariability, we develop a generalizable re-identification framework that\nencodes sequences of 3D point clouds to capture shape and articulated motion\npatterns unique to each individual. Our method achieves 86.19% accuracy on\nrealistic clinical data while maintaining 75.27% accuracy when transferring\nbetween different environments - a 12% improvement over existing methods. When\nused to augment markerless personnel tracking, our approach improves accuracy\nby over 50%. Through extensive validation across three datasets and the\nintroduction of a novel workflow visualization technique, we demonstrate how\nour framework can reveal novel insights into surgical team dynamics and space\nutilization patterns, advancing methods to analyze surgical workflows and team\ncoordination.\n","authors":["Tony Danjun Wang","Lennart Bastian","Tobias Czempiel","Christian Heiliger","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2503.13028v1.pdf","comment":"26 pages, 14 figures, Submitted to Medical Image Analysis"},{"id":"http://arxiv.org/abs/2503.13026v1","updated":"2025-03-17T10:29:08Z","published":"2025-03-17T10:29:08Z","title":"HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model","summary":"  The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.\n","authors":["Tao Wang","Changxu Cheng","Lingfeng Wang","Senda Chen","Wuyue Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.13026v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2503.13025v1","updated":"2025-03-17T10:28:35Z","published":"2025-03-17T10:28:35Z","title":"PoseSyn: Synthesizing Diverse 3D Pose Data from In-the-Wild 2D Data","summary":"  Despite considerable efforts to enhance the generalization of 3D pose\nestimators without costly 3D annotations, existing data augmentation methods\nstruggle in real world scenarios with diverse human appearances and complex\nposes. We propose PoseSyn, a novel data synthesis framework that transforms\nabundant in the wild 2D pose dataset into diverse 3D pose image pairs. PoseSyn\ncomprises two key components: Error Extraction Module (EEM), which identifies\nchallenging poses from the 2D pose datasets, and Motion Synthesis Module (MSM),\nwhich synthesizes motion sequences around the challenging poses. Then, by\ngenerating realistic 3D training data via a human animation model aligned with\nchallenging poses and appearances PoseSyn boosts the accuracy of various 3D\npose estimators by up to 14% across real world benchmarks including various\nbackgrounds and occlusions, challenging poses, and multi view scenarios.\nExtensive experiments further confirm that PoseSyn is a scalable and effective\napproach for improving generalization without relying on expensive 3D\nannotations, regardless of the pose estimator's model size or design.\n","authors":["ChangHee Yang","Hyeonseop Song","Seokhun Choi","Seungwoo Lee","Jaechul Kim","Hoseok Do"],"pdf_url":"https://arxiv.org/pdf/2503.13025v1.pdf","comment":"The first three authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2503.13023v1","updated":"2025-03-17T10:25:33Z","published":"2025-03-17T10:25:33Z","title":"Real-Time Multi-Object Tracking using YOLOv8 and SORT on a SoC FPGA","summary":"  Multi-object tracking (MOT) is one of the most important problems in computer\nvision and a key component of any vision-based perception system used in\nadvanced autonomous mobile robotics. Therefore, its implementation on low-power\nand real-time embedded platforms is highly desirable. Modern MOT algorithms\nshould be able to track objects of a given class (e.g. people or vehicles). In\naddition, the number of objects to be tracked is not known in advance, and they\nmay appear and disappear at any time, as well as be obscured. For these\nreasons, the most popular and successful approaches have recently been based on\nthe tracking paradigm. Therefore, the presence of a high quality object\ndetector is essential, which in practice accounts for the vast majority of the\ncomputational and memory complexity of the whole MOT system. In this paper, we\npropose an FPGA (Field-Programmable Gate Array) implementation of an embedded\nMOT system based on a quantized YOLOv8 detector and the SORT (Simple Online\nRealtime Tracker) tracker. We use a modified version of the FINN framework to\nutilize external memory for model parameters and to support operations\nnecessary required by YOLOv8. We discuss the evaluation of detection and\ntracking performance using the COCO and MOT15 datasets, where we achieve 0.21\nmAP and 38.9 MOTA respectively. As the computational platform, we use an MPSoC\nsystem (Zynq UltraScale+ device from AMD/Xilinx) where the detector is deployed\nin reprogrammable logic and the tracking algorithm is implemented in the\nprocessor system.\n","authors":["Michal Danilowicz","Tomasz Kryjak"],"pdf_url":"https://arxiv.org/pdf/2503.13023v1.pdf","comment":"Accepted for the 21st International Symposium on Applied\n  Reconfigurable Computing ARC 2025, Sevilla, Spain, April 9-11, 2025"},{"id":"http://arxiv.org/abs/2503.13021v1","updated":"2025-03-17T10:24:27Z","published":"2025-03-17T10:24:27Z","title":"Dynamic Relation Inference via Verb Embeddings","summary":"  CLIP has demonstrated exceptional image-text matching capabilities due to its\ntraining on contrastive learning tasks. Past research has suggested that\nwhereas CLIP effectively matches text to images when the matching can be\nachieved just by matching the text with the objects in the image, CLIP\nstruggles when the matching depends on representing the relationship among the\nobjects in the images (i.e., inferring relations). Previous attempts to address\nthis limitation by training CLIP on relation detection datasets with only\nlinguistic supervision have met with limited success. In this paper, we offer\ninsights and practical methods to advance the field of relation inference from\nimages. This paper approaches the task of creating a model that effectively\ndetects relations among the objects in images by producing text and image\nembeddings that capture relationships through linguistic supervision. To this\nend, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which\naugments the COCO dataset, fine-tunes CLIP with hard negatives\nsubject-relation-object triples and corresponding images, and introduces a\nnovel loss function to improve relation detection. Evaluated on multiple\nCLIP-based models, our method significantly improves zero-shot relation\ninference accuracy in both frozen and fine-tuned settings, significantly\noutperforming CLIP and state-of-the-art models while generalizing well on\nunseen data.\n","authors":["Omri Suissa","Muhiim Ali","Ariana Azarbal","Hui Shen","Shekhar Pradhan"],"pdf_url":"https://arxiv.org/pdf/2503.13021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13016v1","updated":"2025-03-17T10:20:05Z","published":"2025-03-17T10:20:05Z","title":"Efficient Motion-Aware Video MLLM","summary":"  Most current video MLLMs rely on uniform frame sampling and image-level\nencoders, resulting in inefficient data processing and limited motion\nawareness. To address these challenges, we introduce EMA, an Efficient\nMotion-Aware video MLLM that utilizes compressed video structures as inputs. We\npropose a motion-aware GOP (Group of Pictures) encoder that fuses spatial and\nmotion information within a GOP unit in the compressed video stream, generating\ncompact, informative visual tokens. By integrating fewer but denser RGB frames\nwith more but sparser motion vectors in this native slow-fast input\narchitecture, our approach reduces redundancy and enhances motion\nrepresentation. Additionally, we introduce MotionBench, a benchmark for\nevaluating motion understanding across four motion types: linear, curved,\nrotational, and contact-based. Experimental results show that EMA achieves\nstate-of-the-art performance on both MotionBench and popular video question\nanswering benchmarks, while reducing inference costs. Moreover, EMA\ndemonstrates strong scalability, as evidenced by its competitive performance on\nlong video understanding benchmarks.\n","authors":["Zijia Zhao","Yuqi Huo","Tongtian Yue","Longteng Guo","Haoyu Lu","Bingning Wang","Weipeng Chen","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2503.13016v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13012v1","updated":"2025-03-17T10:11:11Z","published":"2025-03-17T10:11:11Z","title":"Test-Time Domain Generalization via Universe Learning: A Multi-Graph\n  Matching Approach for Medical Image Segmentation","summary":"  Despite domain generalization (DG) has significantly addressed the\nperformance degradation of pre-trained models caused by domain shifts, it often\nfalls short in real-world deployment. Test-time adaptation (TTA), which adjusts\na learned model using unlabeled test data, presents a promising solution.\nHowever, most existing TTA methods struggle to deliver strong performance in\nmedical image segmentation, primarily because they overlook the crucial prior\nknowledge inherent to medical images. To address this challenge, we incorporate\nmorphological information and propose a framework based on multi-graph\nmatching. Specifically, we introduce learnable universe embeddings that\nintegrate morphological priors during multi-source training, along with novel\nunsupervised test-time paradigms for domain adaptation. This approach\nguarantees cycle-consistency in multi-matching while enabling the model to more\neffectively capture the invariant priors of unseen data, significantly\nmitigating the effects of domain shifts. Extensive experiments demonstrate that\nour method outperforms other state-of-the-art approaches on two medical image\nsegmentation benchmarks for both multi-source and single-source domain\ngeneralization tasks. The source code is available at\nhttps://github.com/Yore0/TTDG-MGM.\n","authors":["Xingguo Lv","Xingbo Dong","Liwen Wang","Jiewen Yang","Lei Zhao","Bin Pu","Zhe Jin","Xuejun Li"],"pdf_url":"https://arxiv.org/pdf/2503.13012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03714v2","updated":"2025-03-17T10:09:50Z","published":"2025-01-07T11:43:13Z","title":"MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval\n  Adjustment for Compact Dynamic 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has made significant strides in scene\nrepresentation and neural rendering, with intense efforts focused on adapting\nit for dynamic scenes. Despite delivering remarkable rendering quality and\nspeed, existing methods struggle with storage demands and representing complex\nreal-world motions. To tackle these issues, we propose MoDecGS, a\nmemory-efficient Gaussian splatting framework designed for reconstructing novel\nviews in challenging scenarios with complex motions. We introduce GlobaltoLocal\nMotion Decomposition (GLMD) to effectively capture dynamic motions in a\ncoarsetofine manner. This approach leverages Global Canonical Scaffolds (Global\nCS) and Local Canonical Scaffolds (Local CS), extending static Scaffold\nrepresentation to dynamic video reconstruction. For Global CS, we propose\nGlobal Anchor Deformation (GAD) to efficiently represent global dynamics along\ncomplex motions, by directly deforming the implicit Scaffold attributes which\nare anchor position, offset, and local context features. Next, we finely adjust\nlocal motions via the Local Gaussian Deformation (LGD) of Local CS explicitly.\nAdditionally, we introduce Temporal Interval Adjustment (TIA) to automatically\ncontrol the temporal coverage of each Local CS during training, allowing\nMoDecGS to find optimal interval assignments based on the specified number of\ntemporal segments. Extensive evaluations demonstrate that MoDecGS achieves an\naverage 70% reduction in model size over stateoftheart methods for dynamic 3D\nGaussians from realworld dynamic videos while maintaining or even improving\nrendering quality.\n","authors":["Sangwoon Kwak","Joonsoo Kim","Jun Young Jeong","Won-Sik Cheong","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2501.03714v2.pdf","comment":"The last two authors are co-corresponding authors. Please visit our\n  project page at https://kaist-viclab.github.io/MoDecGS-site/"},{"id":"http://arxiv.org/abs/2503.13008v1","updated":"2025-03-17T10:07:50Z","published":"2025-03-17T10:07:50Z","title":"Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients","summary":"  Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.\n","authors":["David E. Hernandez","Jose Ramon Chang","Torbjörn E. M. Nordling"],"pdf_url":"https://arxiv.org/pdf/2503.13008v1.pdf","comment":"15 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2503.10156v2","updated":"2025-03-17T10:05:34Z","published":"2025-03-13T08:34:40Z","title":"Automatic quality control in multi-centric fetal brain MRI\n  super-resolution reconstruction","summary":"  Quality control (QC) has long been considered essential to guarantee the\nreliability of neuroimaging studies. It is particularly important for fetal\nbrain MRI, where acquisitions and image processing techniques are less\nstandardized than in adult imaging. In this work, we focus on automated quality\ncontrol of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an\nimportant processing step where multiple stacks of thick 2D slices are\nregistered together and combined to build a single, isotropic and artifact-free\nT2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that\nextracts more than 100 image quality metrics to predict image quality scores\nusing a random forest model. This approach is well suited to a problem that is\nhigh dimensional, with highly heterogeneous data and small datasets. We\nvalidate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high\nperformance (ROC AUC = 0.89), even when faced with data from an unknown site or\nSRR method. We also investigate failure cases and show that they occur in\n$45\\%$ of the images due to ambiguous configurations for which the rating from\nthe expert is arguable. These results are encouraging and illustrate how a non\ndeep learning-based method like FetMRQC$_{SR}$ is well suited to this\nmultifaceted problem. Our tool, along with all the code used to generate, train\nand evaluate the model will be released upon acceptance of the paper.\n","authors":["Thomas Sanchez","Vladyslav Zalevskyi","Angeline Mihailov","Gerard Martí-Juan","Elisenda Eixarch","Andras Jakab","Vincent Dunet","Mériam Koob","Guillaume Auzias","Meritxell Bach Cuadra"],"pdf_url":"https://arxiv.org/pdf/2503.10156v2.pdf","comment":"11 pages, 3 figures; Submitted to MICCAI 2025"},{"id":"http://arxiv.org/abs/2503.13004v1","updated":"2025-03-17T10:00:14Z","published":"2025-03-17T10:00:14Z","title":"TFDM: Time-Variant Frequency-Based Point Cloud Diffusion with Mamba","summary":"  Diffusion models currently demonstrate impressive performance over various\ngenerative tasks. Recent work on image diffusion highlights the strong\ncapabilities of Mamba (state space models) due to its efficient handling of\nlong-range dependencies and sequential data modeling. Unfortunately, joint\nconsideration of state space models with 3D point cloud generation remains\nlimited. To harness the powerful capabilities of the Mamba model for 3D point\ncloud generation, we propose a novel diffusion framework containing dual latent\nMamba block (DM-Block) and a time-variant frequency encoder (TF-Encoder). The\nDM-Block apply a space-filling curve to reorder points into sequences suitable\nfor Mamba state-space modeling, while operating in a latent space to mitigate\nthe computational overhead that arises from direct 3D data processing.\nMeanwhile, the TF-Encoder takes advantage of the ability of the diffusion model\nto refine fine details in later recovery stages by prioritizing key points\nwithin the U-Net architecture. This frequency-based mechanism ensures enhanced\ndetail quality in the final stages of generation. Experimental results on the\nShapeNet-v2 dataset demonstrate that our method achieves state-of-the-art\nperformance (ShapeNet-v2: 0.14\\% on 1-NNA-Abs50 EMD and 57.90\\% on COV EMD) on\ncertain metrics for specific categories while reducing computational parameters\nand inference time by up to 10$\\times$ and 9$\\times$, respectively. Source code\nis available in Supplementary Materials and will be released upon accpetance.\n","authors":["Jiaxu Liu","Li Li","Hubert P. H. Shum","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2503.13004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05869v3","updated":"2025-03-17T09:56:24Z","published":"2024-10-08T09:57:14Z","title":"Believing is Seeing: Unobserved Object Detection using Generative Models","summary":"  Can objects that are not visible in an image -- but are in the vicinity of\nthe camera -- be detected? This study introduces the novel tasks of 2D, 2.5D\nand 3D unobserved object detection for predicting the location of nearby\nobjects that are occluded or lie outside the image frame. We adapt several\nstate-of-the-art pre-trained generative models to address this task, including\n2D and 3D diffusion models and vision-language models, and show that they can\nbe used to infer the presence of objects that are not directly observed. To\nbenchmark this task, we propose a suite of metrics that capture different\naspects of performance. Our empirical evaluation on indoor scenes from the\nRealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the\nuse of generative models for the unobserved object detection task.\n","authors":["Subhransu S. Bhattacharjee","Dylan Campbell","Rahul Shome"],"pdf_url":"https://arxiv.org/pdf/2410.05869v3.pdf","comment":"IEEE/CVF Computer Vision and Pattern Recognition 2025; 22 pages"},{"id":"http://arxiv.org/abs/2503.12999v1","updated":"2025-03-17T09:55:01Z","published":"2025-03-17T09:55:01Z","title":"Concept-as-Tree: Synthetic Data is All You Need for VLM Personalization","summary":"  Vision-Language Models (VLMs) have demonstrated exceptional performance in\nvarious multi-modal tasks. Recently, there has been an increasing interest in\nimproving the personalization capabilities of VLMs. To better integrate\nuser-provided concepts into VLMs, many methods use positive and negative\nsamples to fine-tune these models. However, the scarcity of user-provided\npositive samples and the low quality of retrieved negative samples pose\nchallenges for fine-tuning. To reveal the relationship between sample and model\nperformance, we systematically investigate the impact of positive and negative\nsamples (easy and hard) and their diversity on VLM personalization tasks. Based\non the detailed analysis, we introduce Concept-as-Tree (CaT), which represents\na concept as a tree structure, thereby enabling the data generation of positive\nand negative samples with varying difficulty and diversity for VLM\npersonalization. With a well-designed data filtering strategy, our CaT\nframework can ensure the quality of generated data, constituting a powerful\npipeline. We perform thorough experiments with various VLM personalization\nbaselines to assess the effectiveness of the pipeline, alleviating the lack of\npositive samples and the low quality of negative samples. Our results\ndemonstrate that CaT equipped with the proposed data filter significantly\nenhances the personalization capabilities of VLMs across the MyVLM, Yo'LLaVA,\nand MC-LLaVA datasets. To our knowledge, this work is the first controllable\nsynthetic data pipeline for VLM personalization. The code is released at\n\\href{https://github.com/zengkaiya/CaT}{https://github.com/zengkaiya/CaT}.\n","authors":["Ruichuan An","Kai Zeng","Ming Lu","Sihan Yang","Renrui Zhang","Huitong Ji","Qizhe Zhang","Yulin Luo","Hao Liang","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02454v2","updated":"2025-03-17T09:53:55Z","published":"2025-02-04T16:20:41Z","title":"IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View\n  Automated Prompt Learning","summary":"  Using extensive training data from SA-1B, the Segment Anything Model (SAM)\nhas demonstrated exceptional generalization and zero-shot capabilities,\nattracting widespread attention in areas such as medical image segmentation and\nremote sensing image segmentation. However, its performance in the field of\nimage manipulation detection remains largely unexplored and unconfirmed. There\nare two main challenges in applying SAM to image manipulation detection: a)\nreliance on manual prompts, and b) the difficulty of single-view information in\nsupporting cross-dataset generalization. To address these challenges, we\ndevelops a cross-view prompt learning paradigm called IMDPrompter based on SAM.\nBenefiting from the design of automated prompts, IMDPrompter no longer relies\non manual guidance, enabling automated detection and localization.\nAdditionally, we propose components such as Cross-view Feature Perception,\nOptimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate\ncross-view perceptual learning and guide SAM to generate accurate masks.\nExtensive experimental results from five datasets (CASIA, Columbia, Coverage,\nIMD2020, and NIST16) validate the effectiveness of our proposed method.\n","authors":["Quan Zhang","Yuxin Qi","Xi Tang","Jinwei Fang","Xi Lin","Ke Zhang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.02454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12990v1","updated":"2025-03-17T09:45:22Z","published":"2025-03-17T09:45:22Z","title":"How Good is my Histopathology Vision-Language Foundation Model? A\n  Holistic Benchmark","summary":"  Recently, histopathology vision-language foundation models (VLMs) have gained\npopularity due to their enhanced performance and generalizability across\ndifferent downstream tasks. However, most existing histopathology benchmarks\nare either unimodal or limited in terms of diversity of clinical tasks, organs,\nand acquisition instruments, as well as their partial availability to the\npublic due to patient data privacy. As a consequence, there is a lack of\ncomprehensive evaluation of existing histopathology VLMs on a unified benchmark\nsetting that better reflects a wide range of clinical scenarios. To address\nthis gap, we introduce HistoVL, a fully open-source comprehensive benchmark\ncomprising images acquired using up to 11 various acquisition tools that are\npaired with specifically crafted captions by incorporating class names and\ndiverse pathology descriptions. Our Histo-VL includes 26 organs, 31 cancer\ntypes, and a wide variety of tissue obtained from 14 heterogeneous patient\ncohorts, totaling more than 5 million patches obtained from over 41K WSIs\nviewed under various magnification levels. We systematically evaluate existing\nhistopathology VLMs on Histo-VL to simulate diverse tasks performed by experts\nin real-world clinical scenarios. Our analysis reveals interesting findings,\nincluding large sensitivity of most existing histopathology VLMs to textual\nchanges with a drop in balanced accuracy of up to 25% in tasks such as\nMetastasis detection, low robustness to adversarial attacks, as well as\nimproper calibration of models evident through high ECE values and low model\nprediction confidence, all of which can affect their clinical implementation.\n","authors":["Roba Al Majzoub","Hashmat Malik","Muzammal Naseer","Zaigham Zaheer","Tariq Mahmood","Salman Khan","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2503.12990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12982v1","updated":"2025-03-17T09:38:53Z","published":"2025-03-17T09:38:53Z","title":"SparseAlign: A Fully Sparse Framework for Cooperative Object Detection","summary":"  Cooperative perception can increase the view field and decrease the occlusion\nof an ego vehicle, hence improving the perception performance and safety of\nautonomous driving. Despite the success of previous works on cooperative object\ndetection, they mostly operate on dense Bird's Eye View (BEV) feature maps,\nwhich are computationally demanding and can hardly be extended to long-range\ndetection problems. More efficient fully sparse frameworks are rarely explored.\nIn this work, we design a fully sparse framework, SparseAlign, with three key\nfeatures: an enhanced sparse 3D backbone, a query-based temporal context\nlearning module, and a robust detection head specially tailored for sparse\nfeatures. Extensive experimental results on both OPV2V and DairV2X datasets\nshow that our framework, despite its sparsity, outperforms the state of the art\nwith less communication bandwidth requirements. In addition, experiments on the\nOPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also\nshow a significant performance gain compared to the baseline works.\n","authors":["Yunshuang Yuan","Yan Xia","Daniel Cremers","Monika Sester"],"pdf_url":"https://arxiv.org/pdf/2503.12982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12981v1","updated":"2025-03-17T09:38:44Z","published":"2025-03-17T09:38:44Z","title":"Analyzing Swimming Performance Using Drone Captured Aerial Videos","summary":"  Monitoring swimmer performance is crucial for improving training and\nenhancing athletic techniques. Traditional methods for tracking swimmers, such\nas above-water and underwater cameras, face limitations due to the need for\nmultiple cameras and obstructions from water splashes. This paper presents a\nnovel approach for tracking swimmers using a moving UAV. The proposed system\nemploys a UAV equipped with a high-resolution camera to capture aerial footage\nof the swimmers. The footage is then processed using computer vision algorithms\nto extract the swimmers' positions and movements. This approach offers several\nadvantages, including single camera use and comprehensive coverage. The\nsystem's accuracy is evaluated with both training and in competition videos.\nThe results demonstrate the system's ability to accurately track swimmers'\nmovements, limb angles, stroke duration and velocity with the maximum error of\n0.3 seconds and 0.35~m/s for stroke duration and velocity, respectively.\n","authors":["Thu Tran","Kenny Tsu Wei Choo","Shaohui Foong","Hitesh Bhardwaj","Shane Kyi Hla Win","Wei Jun Ang","Kenneth Goh","Rajesh Krishna Balan"],"pdf_url":"https://arxiv.org/pdf/2503.12981v1.pdf","comment":"6 pages, published to ACM Dronet'24"},{"id":"http://arxiv.org/abs/2503.12974v1","updated":"2025-03-17T09:33:58Z","published":"2025-03-17T09:33:58Z","title":"Exploring 3D Activity Reasoning and Planning: From Implicit Human\n  Intentions to Route-Aware Planning","summary":"  3D activity reasoning and planning has attracted increasing attention in\nhuman-robot interaction and embodied AI thanks to the recent advance in\nmultimodal learning. However, most existing works share two constraints: 1)\nheavy reliance on explicit instructions with little reasoning on implicit user\nintention; 2) negligence of inter-step route planning on robot moves. To bridge\nthe gaps, we propose 3D activity reasoning and planning, a novel 3D task that\nreasons the intended activities from implicit instructions and decomposes them\ninto steps with inter-step routes and planning under the guidance of\nfine-grained 3D object shapes and locations from scene segmentation. We tackle\nthe new 3D task from two perspectives. First, we construct ReasonPlan3D, a\nlarge-scale benchmark that covers diverse 3D scenes with rich implicit\ninstructions and detailed annotations for multi-step task planning, inter-step\nroute planning, and fine-grained segmentation. Second, we design a novel\nframework that introduces progressive plan generation with contextual\nconsistency across multiple steps, as well as a scene graph that is updated\ndynamically for capturing critical objects and their spatial relations.\nExtensive experiments demonstrate the effectiveness of our benchmark and\nframework in reasoning activities from implicit human instructions, producing\naccurate stepwise task plans, and seamlessly integrating route planning for\nmulti-step moves. The dataset and code will be released.\n","authors":["Xueying Jiang","Wenhao Li","Xiaoqin Zhang","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2503.12974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12973v1","updated":"2025-03-17T09:32:47Z","published":"2025-03-17T09:32:47Z","title":"Prospects for Mitigating Spectral Variability in Tropical Species\n  Classification Using Self-Supervised Learning","summary":"  Airborne hyperspectral imaging is a promising method for identifying tropical\nspecies, but spectral variability between acquisitions hinders consistent\nresults. This paper proposes using Self-Supervised Learning (SSL) to encode\nspectral features that are robust to abiotic variability and relevant for\nspecies identification. By employing the state-of-the-art Barlow-Twins approach\non repeated spectral acquisitions, we demonstrate the ability to develop stable\nfeatures. For the classification of 40 tropical species, experiments show that\nthese features can outperform typical reflectance products in terms of\nrobustness to spectral variability by 10 points of accuracy across dates.\n","authors":["Colin Prieur","Nassim Ait Ali Braham","Paul Tresson","Grégoire Vincent","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2503.12973v1.pdf","comment":"5 pages, 3 figures, published as proceeding of the \"2024 14th\n  Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote\n  Sensing (WHISPERS)\""},{"id":"http://arxiv.org/abs/2503.12972v1","updated":"2025-03-17T09:31:14Z","published":"2025-03-17T09:31:14Z","title":"Aligning Vision to Language: Text-Free Multimodal Knowledge Graph\n  Construction for Enhanced LLMs Reasoning","summary":"  Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.\n","authors":["Junming Liu","Siyuan Meng","Yanting Gao","Song Mao","Pinlong Cai","Guohang Yan","Yirong Chen","Zilin Bian","Botian Shi","Ding Wang"],"pdf_url":"https://arxiv.org/pdf/2503.12972v1.pdf","comment":"14 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.12969v1","updated":"2025-03-17T09:26:06Z","published":"2025-03-17T09:26:06Z","title":"Action tube generation by person query matching for spatio-temporal\n  action detection","summary":"  This paper proposes a method for spatio-temporal action detection (STAD) that\ndirectly generates action tubes from the original video without relying on\npost-processing steps such as IoU-based linking and clip splitting. Our\napproach applies query-based detection (DETR) to each frame and matches DETR\nqueries to link the same person across frames. We introduce the Query Matching\nModule (QMM), which uses metric learning to bring queries for the same person\ncloser together across frames compared to queries for different people. Action\nclasses are predicted using the sequence of queries obtained from QMM matching,\nallowing for variable-length inputs from videos longer than a single clip.\nExperimental results on JHMDB, UCF101-24, and AVA datasets demonstrate that our\nmethod performs well for large position changes of people while offering\nsuperior computational efficiency and lower resource requirements.\n","authors":["Kazuki Omi","Jion Oshima","Toru Tamaki"],"pdf_url":"https://arxiv.org/pdf/2503.12969v1.pdf","comment":"extended version of VISAPP2025"},{"id":"http://arxiv.org/abs/2411.02136v2","updated":"2025-03-17T09:25:50Z","published":"2024-11-04T14:49:01Z","title":"Advanced computer vision for extracting georeferenced vehicle\n  trajectories from drone imagery","summary":"  This paper presents a framework for extracting georeferenced vehicle\ntrajectories from high-altitude drone imagery, addressing key challenges in\nurban traffic monitoring and the limitations of traditional ground-based\nsystems. Our approach integrates several novel contributions, including a\ntailored object detector optimized for high-altitude bird's-eye view\nperspectives, a unique track stabilization method that uses detected vehicle\nbounding boxes as exclusion masks during image registration, and an orthophoto\nand master frame-based georeferencing strategy that enhances consistent\nalignment across multiple drone viewpoints. Additionally, our framework\nfeatures robust vehicle dimension estimation and detailed road segmentation,\nenabling comprehensive traffic analysis. Conducted in the Songdo International\nBusiness District, South Korea, the study utilized a multi-drone experiment\ncovering 20 intersections, capturing approximately 12TB of 4K video data over\nfour days. The framework produced two high-quality datasets: the Songdo Traffic\ndataset, comprising approximately 700,000 unique vehicle trajectories, and the\nSongdo Vision dataset, containing over 5,000 human-annotated images with about\n300,000 vehicle instances in four classes. Comparisons with high-precision\nsensor data from an instrumented probe vehicle highlight the accuracy and\nconsistency of our extraction pipeline in dense urban environments. The public\nrelease of Songdo Traffic and Songdo Vision, and the complete source code for\nthe extraction pipeline, establishes new benchmarks in data quality,\nreproducibility, and scalability in traffic research. Results demonstrate the\npotential of integrating drone technology with advanced computer vision for\nprecise and cost-effective urban traffic monitoring, providing valuable\nresources for developing intelligent transportation systems and enhancing\ntraffic management strategies.\n","authors":["Robert Fonod","Haechan Cho","Hwasoo Yeo","Nikolas Geroliminis"],"pdf_url":"https://arxiv.org/pdf/2411.02136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12968v1","updated":"2025-03-17T09:24:26Z","published":"2025-03-17T09:24:26Z","title":"OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson\n  Multi-Bernoulli Filtering","summary":"  Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as\nit enables robust perception, navigation, and planning in complex environments.\nWhile deep learning-based solutions have demonstrated impressive 3D MOT\nperformance, model-based approaches remain appealing for their simplicity,\ninterpretability, and data efficiency. Conventional model-based trackers\ntypically rely on random vector-based Bayesian filters within the\ntracking-by-detection (TBD) framework but face limitations due to heuristic\ndata association and track management schemes. In contrast, random finite set\n(RFS)-based Bayesian filtering handles object birth, survival, and death in a\ntheoretically sound manner, facilitating interpretability and parameter tuning.\nIn this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs\nan optimized Poisson multi-Bernoulli (PMB) filter while incorporating several\nkey innovative designs within the TBD framework. Specifically, we propose a\nmeasurement-driven hybrid adaptive birth model for improved track\ninitialization, employ adaptive detection probability parameters to effectively\nmaintain tracks for occluded objects, and optimize density pruning and track\nextraction modules to further enhance overall tracking performance. Extensive\nevaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior\ntracking accuracy compared with state-of-the-art methods, thereby establishing\na new benchmark for model-based 3D MOT and offering valuable insights for\nfuture research on RFS-based trackers in autonomous driving.\n","authors":["Guanhua Ding","Yuxuan Xia","Runwei Guan","Qinchen Wu","Tao Huang","Weiping Ding","Jinping Sun","Guoqiang Mao"],"pdf_url":"https://arxiv.org/pdf/2503.12968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16446v2","updated":"2025-03-17T09:19:45Z","published":"2024-11-25T14:51:22Z","title":"VQ-SGen: A Vector Quantized Stroke Representation for Creative Sketch\n  Generation","summary":"  This paper presents VQ-SGen, a novel algorithm for high-quality creative\nsketch generation. Recent approaches have framed the task as pixel-based\ngeneration either as a whole or part-by-part, neglecting the intrinsic and\ncontextual relationships among individual strokes, such as the shape and\nspatial positioning of both proximal and distant strokes. To overcome these\nlimitations, we propose treating each stroke within a sketch as an entity and\nintroducing a vector-quantized (VQ) stroke representation for fine-grained\nsketch generation. Our method follows a two-stage framework - in stage one, we\ndecouple each stroke's shape and location information to ensure the VQ\nrepresentation prioritizes stroke shape learning. In stage two, we feed the\nprecise and compact representation into an auto-decoding Transformer to\nincorporate stroke semantics, positions, and shapes into the generation\nprocess. By utilizing tokenized stroke representation, our approach generates\nstrokes with high fidelity and facilitates novel applications, such as text or\nclass label conditioned generation and sketch completion. Comprehensive\nexperiments demonstrate our method surpasses existing state-of-the-art\ntechniques on the CreativeSketch dataset, underscoring its effectiveness.\n","authors":["Jiawei Wang","Zhiming Cui","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2411.16446v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12964v1","updated":"2025-03-17T09:19:12Z","published":"2025-03-17T09:19:12Z","title":"Training Video Foundation Models with NVIDIA NeMo","summary":"  Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.\n","authors":["Zeeshan Patel","Ethan He","Parth Mannan","Xiaowei Ren","Ryan Wolf","Niket Agarwal","Jacob Huffman","Zhuoyao Wang","Carl Wang","Jack Chang","Yan Bai","Tommy Huang","Linnan Wang","Sahil Jain","Shanmugam Ramasamy","Joseph Jennings","Ekaterina Sirazitdinova","Oleg Sudakov","Mingyuan Ma","Bobby Chen","Forrest Lin","Hao Wang","Vasanth Rao Naik Sabavat","Sriharsha Niverty","Rong Ou","Pallab Bhattacharya","David Page","Nima Tajbakhsh","Ashwath Aithal"],"pdf_url":"https://arxiv.org/pdf/2503.12964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12963v1","updated":"2025-03-17T09:18:31Z","published":"2025-03-17T09:18:31Z","title":"Unlock Pose Diversity: Accurate and Efficient Implicit Keypoint-based\n  Spatiotemporal Diffusion for Audio-driven Talking Portrait","summary":"  Audio-driven single-image talking portrait generation plays a crucial role in\nvirtual reality, digital human creation, and filmmaking. Existing approaches\nare generally categorized into keypoint-based and image-based methods.\nKeypoint-based methods effectively preserve character identity but struggle to\ncapture fine facial details due to the fixed points limitation of the 3D\nMorphable Model. Moreover, traditional generative networks face challenges in\nestablishing causality between audio and keypoints on limited datasets,\nresulting in low pose diversity. In contrast, image-based approaches produce\nhigh-quality portraits with diverse details using the diffusion network but\nincur identity distortion and expensive computational costs. In this work, we\npropose KDTalker, the first framework to combine unsupervised implicit 3D\nkeypoint with a spatiotemporal diffusion model. Leveraging unsupervised\nimplicit 3D keypoints, KDTalker adapts facial information densities, allowing\nthe diffusion process to model diverse head poses and capture fine facial\ndetails flexibly. The custom-designed spatiotemporal attention mechanism\nensures accurate lip synchronization, producing temporally consistent,\nhigh-quality animations while enhancing computational efficiency. Experimental\nresults demonstrate that KDTalker achieves state-of-the-art performance\nregarding lip synchronization accuracy, head pose diversity, and execution\nefficiency.Our codes are available at https://github.com/chaolongy/KDTalker.\n","authors":["Chaolong Yang","Kai Yao","Yuyao Yan","Chenru Jiang","Weiguang Zhao","Jie Sun","Guangliang Cheng","Yifei Zhang","Bin Dong","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2503.12963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11269v2","updated":"2025-03-17T09:17:10Z","published":"2025-03-14T10:25:54Z","title":"Prof. Robot: Differentiable Robot Rendering Without Static and\n  Self-Collisions","summary":"  Differentiable rendering has gained significant attention in the field of\nrobotics, with differentiable robot rendering emerging as an effective paradigm\nfor learning robotic actions from image-space supervision. However, the lack of\nphysical world perception in this approach may lead to potential collisions\nduring action optimization. In this work, we introduce a novel improvement on\nprevious efforts by incorporating physical awareness of collisions through the\nlearning of a neural robotic collision classifier. This enables the\noptimization of actions that avoid collisions with static, non-interactable\nenvironments as well as the robot itself. To facilitate effective gradient\noptimization with the classifier, we identify the underlying issue and propose\nleveraging Eikonal regularization to ensure consistent gradients for\noptimization. Our solution can be seamlessly integrated into existing\ndifferentiable robot rendering frameworks, utilizing gradients for optimization\nand providing a foundation for future applications of differentiable rendering\nin robotics with improved reliability of interactions with the physical world.\nBoth qualitative and quantitative experiments demonstrate the necessity and\neffectiveness of our method compared to previous solutions.\n","authors":["Quanyuan Ruan","Jiabao Lei","Wenhao Yuan","Yanglin Zhang","Dekun Lu","Guiliang Liu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2503.11269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12955v1","updated":"2025-03-17T09:10:50Z","published":"2025-03-17T09:10:50Z","title":"HIS-GPT: Towards 3D Human-In-Scene Multimodal Understanding","summary":"  We propose a new task to benchmark human-in-scene understanding for embodied\nagents: Human-In-Scene Question Answering (HIS-QA). Given a human motion within\na 3D scene, HIS-QA requires the agent to comprehend human states and behaviors,\nreason about its surrounding environment, and answer human-related questions\nwithin the scene. To support this new task, we present HIS-Bench, a multimodal\nbenchmark that systematically evaluates HIS understanding across a broad\nspectrum, from basic perception to commonsense reasoning and planning. Our\nevaluation of various vision-language models on HIS-Bench reveals significant\nlimitations in their ability to handle HIS-QA tasks. To this end, we propose\nHIS-GPT, the first foundation model for HIS understanding. HIS-GPT integrates\n3D scene context and human motion dynamics into large language models while\nincorporating specialized mechanisms to capture human-scene interactions.\nExtensive experiments demonstrate that HIS-GPT sets a new state-of-the-art on\nHIS-QA tasks. We hope this work inspires future research on human behavior\nanalysis in 3D scenes, advancing embodied AI and world models.\n","authors":["Jiahe Zhao","Ruibing Hou","Zejie Tian","Hong Chang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2503.12955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00846v2","updated":"2025-03-17T09:09:49Z","published":"2023-12-01T07:04:47Z","title":"NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting\n  Guidance","summary":"  Existing neural implicit surface reconstruction methods have achieved\nimpressive performance in multi-view 3D reconstruction by leveraging explicit\ngeometry priors such as depth maps or point clouds as regularization. However,\nthe reconstruction results still lack fine details because of the over-smoothed\ndepth map or sparse point cloud. In this work, we propose a neural implicit\nsurface reconstruction pipeline with guidance from 3D Gaussian Splatting to\nrecover highly detailed surfaces. The advantage of 3D Gaussian Splatting is\nthat it can generate dense point clouds with detailed structure. Nonetheless, a\nnaive adoption of 3D Gaussian Splatting can fail since the generated points are\nthe centers of 3D Gaussians that do not necessarily lie on the surface. We thus\nintroduce a scale regularizer to pull the centers close to the surface by\nenforcing the 3D Gaussians to be extremely thin. Moreover, we propose to refine\nthe point cloud from 3D Gaussians Splatting with the normal priors from the\nsurface predicted by neural implicit models instead of using a fixed set of\npoints as guidance. Consequently, the quality of surface reconstruction\nimproves from the guidance of the more accurate 3D Gaussian splatting. By\njointly optimizing the 3D Gaussian Splatting and the neural implicit model, our\napproach benefits from both representations and generates complete surfaces\nwith intricate details. Experiments on Tanks and Temples verify the\neffectiveness of our proposed method.\n","authors":["Hanlin Chen","Chen Li","Yunsong Wang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2312.00846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17031v4","updated":"2025-03-17T09:07:22Z","published":"2023-03-29T21:26:23Z","title":"Visually Wired NFTs: Exploring the Role of Inspiration in Non-Fungible\n  Tokens","summary":"  The fervor for Non-Fungible Tokens (NFTs) attracted countless creators,\nleading to a Big Bang of digital assets driven by latent or explicit forms of\ninspiration, as in many creative processes. This work exploits Vision\nTransformers and graph-based modeling to delve into visual inspiration\nphenomena between NFTs over the years. Our goals include unveiling the main\nstructural traits that shape visual inspiration networks, exploring the\ninterrelation between visual inspiration and asset performances, investigating\ncrypto influence on inspiration processes, and explaining the inspiration\nrelationships among NFTs. Our findings unveil how the pervasiveness of\ninspiration led to a temporary saturation of the visual feature space, the\nimpact of the dichotomy between inspiring and inspired NFTs on their financial\nperformance, and an intrinsic self-regulatory mechanism between markets and\ninspiration waves. Our work can serve as a starting point for gaining a broader\nview of the evolution of Web3.\n","authors":["Lucio La Cava","Davide Costa","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2303.17031v4.pdf","comment":"Accepted for publication with ACM Trans. on the Web, Jan 2025.\n  https://dl.acm.org/doi/10.1145/3703411"},{"id":"http://arxiv.org/abs/2503.12953v1","updated":"2025-03-17T09:06:21Z","published":"2025-03-17T09:06:21Z","title":"Frame-wise Conditioning Adaptation for Fine-Tuning Diffusion Models in\n  Text-to-Video Prediction","summary":"  Text-video prediction (TVP) is a downstream video generation task that\nrequires a model to produce subsequent video frames given a series of initial\nvideo frames and text describing the required motion. In practice TVP methods\nfocus on a particular category of videos depicting manipulations of objects\ncarried out by human beings or robot arms. Previous methods adapt models\npre-trained on text-to-image tasks, and thus tend to generate video that lacks\nthe required continuity. A natural progression would be to leverage more recent\npre-trained text-to-video (T2V) models. This approach is rendered more\nchallenging by the fact that the most common fine-tuning technique, low-rank\nadaptation (LoRA), yields undesirable results. In this work, we propose an\nadaptation-based strategy we label Frame-wise Conditioning Adaptation (FCA).\nWithin the module, we devise a sub-module that produces frame-wise text\nembeddings from the input text, which acts as an additional text condition to\naid generation. We use FCA to fine-tune the T2V model, which incorporates the\ninitial frame(s) as an extra condition. We compare and discuss the more\neffective strategy for injecting such embeddings into the T2V model. We conduct\nextensive ablation studies on our design choices with quantitative and\nqualitative performance analysis. Our approach establishes a new\nstate-of-the-art for the task of TVP. The project page is at\nhttps://github.com/Cuberick-Orion/FCA .\n","authors":["Zheyuan Liu","Junyan Wang","Zicheng Duan","Cristian Rodriguez-Opazo","Anton van den Hengel"],"pdf_url":"https://arxiv.org/pdf/2503.12953v1.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2411.19488v2","updated":"2025-03-17T09:01:38Z","published":"2024-11-29T06:06:35Z","title":"Interleaved-Modal Chain-of-Thought","summary":"  Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.\n","authors":["Jun Gao","Yongqi Li","Ziqiang Cao","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2411.19488v2.pdf","comment":"CVPR 2025 Main Conference"},{"id":"http://arxiv.org/abs/2503.12947v1","updated":"2025-03-17T08:59:34Z","published":"2025-03-17T08:59:34Z","title":"DivCon-NeRF: Generating Augmented Rays with Diversity and Consistency\n  for Few-shot View Synthesis","summary":"  Neural Radiance Field (NeRF) has shown remarkable performance in novel view\nsynthesis but requires many multiview images, making it impractical for\nfew-shot scenarios. Ray augmentation was proposed to prevent overfitting for\nsparse training data by generating additional rays. However, existing methods,\nwhich generate augmented rays only near the original rays, produce severe\nfloaters and appearance distortion due to limited viewpoints and inconsistent\nrays obstructed by nearby obstacles and complex surfaces. To address these\nproblems, we propose DivCon-NeRF, which significantly enhances both diversity\nand consistency. It employs surface-sphere augmentation, which preserves the\ndistance between the original camera and the predicted surface point. This\nallows the model to compare the order of high-probability surface points and\nfilter out inconsistent rays easily without requiring the exact depth. By\nintroducing inner-sphere augmentation, DivCon-NeRF randomizes angles and\ndistances for diverse viewpoints, further increasing diversity. Consequently,\nour method significantly reduces floaters and visual distortions, achieving\nstate-of-the-art performance on the Blender, LLFF, and DTU datasets. Our code\nwill be publicly available.\n","authors":["Ingyun Lee","Jae Won Jang","Seunghyeon Seo","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2503.12947v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.12944v1","updated":"2025-03-17T08:58:33Z","published":"2025-03-17T08:58:33Z","title":"GIFT: Generated Indoor video frames for Texture-less point tracking","summary":"  Point tracking is becoming a powerful solver for motion estimation and video\nediting. Compared to classical feature matching, point tracking methods have\nthe key advantage of robustly tracking points under complex camera motion\ntrajectories and over extended periods. However, despite certain improvements\nin methodologies, current point tracking methods still struggle to track any\nposition in video frames, especially in areas that are texture-less or weakly\ntextured. In this work, we first introduce metrics for evaluating the texture\nintensity of a 3D object. Using these metrics, we classify the 3D models in\nShapeNet into three levels of texture intensity and create GIFT, a challenging\nsynthetic benchmark comprising 1800 indoor video sequences with rich\nannotations. Unlike existing datasets that assign ground truth points\narbitrarily, GIFT precisely anchors ground truth on classified target objects,\nensuring that each video corresponds to a specific texture intensity level.\nFurthermore, we comprehensively evaluate current methods on GIFT to assess\ntheir performance across different texture intensity levels and analyze the\nimpact of texture on point tracking.\n","authors":["Jianzheng Huang","Xianyu Mo","Ziling Liu","Jinyu Yang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.12944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12937v1","updated":"2025-03-17T08:51:44Z","published":"2025-03-17T08:51:44Z","title":"R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization","summary":"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.\n","authors":["Jingyi Zhang","Jiaxing Huang","Huanjin Yao","Shunyu Liu","Xikun Zhang","Shijian Lu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.12937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12935v1","updated":"2025-03-17T08:49:09Z","published":"2025-03-17T08:49:09Z","title":"L2HCount:Generalizing Crowd Counting from Low to High Crowd Density via\n  Density Simulation","summary":"  Since COVID-19, crowd-counting tasks have gained wide applications. While\nsupervised methods are reliable, annotation is more challenging in high-density\nscenes due to small head sizes and severe occlusion, whereas it's simpler in\nlow-density scenes. Interestingly, can we train the model in low-density scenes\nand generalize it to high-density scenes? Therefore, we propose a low- to\nhigh-density generalization framework (L2HCount) that learns the pattern\nrelated to high-density scenes from low-density ones, enabling it to generalize\nwell to high-density scenes. Specifically, we first introduce a High-Density\nSimulation Module and a Ground-Truth Generation Module to construct fake\nhigh-density images along with their corresponding ground-truth crowd\nannotations respectively by image-shifting technique, effectively simulating\nhigh-density crowd patterns. However, the simulated images have two issues:\nimage blurring and loss of low-density image characteristics. Therefore, we\nsecond propose a Head Feature Enhancement Module to extract clear features in\nthe simulated high-density scene. Third, we propose a Dual-Density Memory\nEncoding Module that uses two crowd memories to learn scene-specific patterns\nfrom low- and simulated high-density scenes, respectively. Extensive\nexperiments on four challenging datasets have shown the promising performance\nof L2HCount.\n","authors":["Guoliang Xu","Jianqin Yin","Ren Zhang","Yonghao Dang","Feng Zhou","Bo Yu"],"pdf_url":"https://arxiv.org/pdf/2503.12935v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15144v4","updated":"2025-03-17T08:45:27Z","published":"2023-12-23T02:54:41Z","title":"A Generically Contrastive Spatiotemporal Representation Enhancement for\n  3D Skeleton Action Recognition","summary":"  Skeleton-based action recognition is a central task in computer vision and\nhuman-robot interaction. However, most previous methods suffer from overlooking\nthe explicit exploitation of the latent data distributions (i.e., the\nintra-class variations and inter-class relations), thereby leading to confusion\nabout ambiguous samples and sub-optimum solutions of the skeleton encoders. To\nmitigate this, we propose a Contrastive Spatiotemporal Representation\nEnhancement (CSRE) framework to obtain more discriminative representations from\nthe sequences, which can be incorporated into various previous skeleton\nencoders and can be removed when testing. Specifically, we decompose the\nrepresentation into spatial- and temporal-specific features to explore\nfine-grained motion patterns along the corresponding dimensions. Furthermore,\nto explicitly exploit the latent data distributions, we employ the attentive\nfeatures to contrastive learning, which models the cross-sequence semantic\nrelations by pulling together the features from the positive pairs and pushing\naway the negative pairs. Extensive experiments show that CSRE with five various\nskeleton encoders (HCN, 2S-AGCN, CTR-GCN, Hyperformer, and BlockGCN) achieves\nsolid improvements on five benchmarks. The code will be released at\nhttps://github.com/zhshj0110/CSRE.\n","authors":["Shaojie Zhang","Jianqin Yin","Yonghao Dang"],"pdf_url":"https://arxiv.org/pdf/2312.15144v4.pdf","comment":"Accepted by PR 2025"},{"id":"http://arxiv.org/abs/2503.10631v2","updated":"2025-03-17T08:44:28Z","published":"2025-03-13T17:59:52Z","title":"HybridVLA: Collaborative Diffusion and Autoregression in a Unified\n  Vision-Language-Action Model","summary":"  Recent advancements in vision-language models (VLMs) for common-sense\nreasoning have led to the development of vision-language-action (VLA) models,\nenabling robots to perform generalized manipulation. Although existing\nautoregressive VLA methods leverage large-scale pretrained knowledge, they\ndisrupt the continuity of actions. Meanwhile, some VLA methods incorporate an\nadditional diffusion head to predict continuous actions, relying solely on\nVLM-extracted features, which limits their reasoning capabilities. In this\npaper, we introduce HybridVLA, a unified framework that seamlessly integrates\nthe strengths of both autoregressive and diffusion policies within a single\nlarge language model, rather than simply connecting them. To bridge the\ngeneration gap, a collaborative training recipe is proposed that injects the\ndiffusion modeling directly into the next-token prediction. With this recipe,\nwe find that these two forms of action prediction not only reinforce each other\nbut also exhibit varying performance across different tasks. Therefore, we\ndesign a collaborative action ensemble mechanism that adaptively fuses these\ntwo predictions, leading to more robust control. In experiments, HybridVLA\noutperforms previous state-of-the-art VLA methods across various simulation and\nreal-world tasks, including both single-arm and dual-arm robots, while\ndemonstrating stable manipulation in previously unseen configurations.\n","authors":["Jiaming Liu","Hao Chen","Pengju An","Zhuoyang Liu","Renrui Zhang","Chenyang Gu","Xiaoqi Li","Ziyu Guo","Sixiang Chen","Mengzhen Liu","Chengkai Hou","Mengdi Zhao","KC alex Zhou","Pheng-Ann Heng","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.10631v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12929v1","updated":"2025-03-17T08:39:10Z","published":"2025-03-17T08:39:10Z","title":"AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View\n  Prediction","summary":"  Novel view synthesis (NVS) is a cornerstone for image-to-3d creation.\nHowever, existing works still struggle to maintain consistency between the\ngenerated views and the input views, especially when there is a significant\ncamera pose difference, leading to poor-quality 3D geometries and textures. We\nattribute this issue to their treatment of all target views with equal priority\naccording to our empirical observation that the target views closer to the\ninput views exhibit higher fidelity. With this inspiration, we propose\nAR-1-to-3, a novel next-view prediction paradigm based on diffusion models that\nfirst generates views close to the input views, which are then utilized as\ncontextual information to progressively synthesize farther views. To encode the\ngenerated view subsequences as local and global conditions for the next-view\nprediction, we accordingly develop a stacked local feature encoding strategy\n(Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE).\nExtensive experiments demonstrate that our method significantly improves the\nconsistency between the generated views and the input views, producing\nhigh-fidelity 3D assets.\n","authors":["Xuying Zhang","Yupeng Zhou","Kai Wang","Yikai Wang","Zhen Li","Xiuli Shao","Daquan Zhou","Qibin Hou","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.12929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12927v1","updated":"2025-03-17T08:38:46Z","published":"2025-03-17T08:38:46Z","title":"MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification\n  Assisted with Textual Description Generation","summary":"  Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps://github.com/HovChen/MMLNB.\n","authors":["Huangwei Chen","Zhu Zhu","Zhenyu Yan","Yifei Chen","Mingyang Ding","Chenlei Li","Feiwei Qin"],"pdf_url":"https://arxiv.org/pdf/2503.12927v1.pdf","comment":"25 pages, 7 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.13275v1","updated":"2025-03-17T15:27:02Z","published":"2025-03-17T15:27:02Z","title":"Knowledge-Aware Iterative Retrieval for Multi-Agent Systems","summary":"  We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.\n","authors":["Seyoung Song"],"pdf_url":"https://arxiv.org/pdf/2503.13275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13254v1","updated":"2025-03-17T15:12:37Z","published":"2025-03-17T15:12:37Z","title":"Federated Mixture-of-Expert for Non-Overlapped Cross-Domain Sequential\n  Recommendation","summary":"  In the real world, users always have multiple interests while surfing\ndifferent services to enrich their daily lives, e.g., watching hot short\nvideos/live streamings. To describe user interests precisely for a better user\nexperience, the recent literature proposes cross-domain techniques by\ntransferring the other related services (a.k.a. domain) knowledge to enhance\nthe accuracy of target service prediction. In practice, naive cross-domain\ntechniques typically require there exist some overlapped users, and sharing\noverall information across domains, including user historical logs, user/item\nembeddings, and model parameter checkpoints. Nevertheless, other domain's\nuser-side historical logs and embeddings are not always available in real-world\nRecSys designing, since users may be totally non-overlapped across domains, or\nthe privacy-preserving policy limits the personalized information sharing\nacross domains. Thereby, a challenging but valuable problem is raised: How to\nempower target domain prediction accuracy by utilizing the other domain model\nparameters checkpoints only? To answer the question, we propose the FMoE-CDSR,\nwhich explores the non-overlapped cross-domain sequential recommendation\nscenario from the federated learning perspective.\n","authors":["Yu Liu","Hanbin Jiang","Lei Zhu","Yu Zhang","Yuqi Mao","Jiangxia Cao","Shuchao Pang"],"pdf_url":"https://arxiv.org/pdf/2503.13254v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.13018v1","updated":"2025-03-17T10:22:26Z","published":"2025-03-17T10:22:26Z","title":"Disentangling the Power Dynamics in Participatory Data Physicalisation","summary":"  Participatory data physicalisation (PDP) is recognised for its potential to\nsupport data-driven decisions among stakeholders who collaboratively construct\nphysical elements into commonly insightful visualisations. Like all\nparticipatory processes, PDP is however influenced by underlying power dynamics\nthat might lead to issues regarding extractive participation, marginalisation,\nor exclusion, among others. We first identified the decisions behind these\npower dynamics by developing an ontology that synthesises critical theoretical\ninsights from both visualisation and participatory design research, which were\nthen systematically applied unto a representative corpus of 23 PDP artefacts.\nBy revealing how shared decisions are guided by different agendas, this paper\npresents three contributions: 1) a cross-disciplinary ontology that facilitates\nthe systematic analysis of existing and novel PDP artefacts and processes;\nwhich leads to 2) six PDP agendas that reflect the key power dynamics in\ncurrent PDP practice, revealing the diversity of orientations towards\nstakeholder participation in PDP practice; and 3) a set of critical\nconsiderations that should guide how power dynamics can be balanced, such as by\nreflecting on how issues are represented, data is contextualised, participants\nexpress their meanings, and how participants can dissent with flexible artefact\nconstruction. Consequently, this study advances a feminist research agenda by\nguiding researchers and practitioners in openly reflecting on and sharing\nresponsibilities in data physicalisation and participatory data visualisation.\n","authors":["Silvia Cazacu","Georgia Panagiotidou","Therese Steenberghen","Andrew Vande Moere"],"pdf_url":"https://arxiv.org/pdf/2503.13018v1.pdf","comment":"In CHI'25, ACM (2025)"},{"id":"http://arxiv.org/abs/2503.12877v1","updated":"2025-03-17T07:16:22Z","published":"2025-03-17T07:16:22Z","title":"Leveraging the Dynamics of Leadership in Group Recommendation Systems","summary":"  In the field of group recommendation systems (GRS), effectively addressing\nthe diverse preferences of group members poses a significant challenge.\nTraditional GRS approaches often aggregate individual preferences into a\ncollective group preference to generate recommendations, which may overlook the\nintricate interactions between group members. We introduce a novel approach to\ngroup recommendation, with a specific focus on small groups sharing common\ninterests. In particular, we present a web-based restaurant recommendation\nsystem that enhances user satisfaction by modeling mutual interactions among\ngroup members. Drawing inspiration from group decision-making literature and\nleveraging graph theory, we propose a recommendation algorithm that emphasizes\nthe dynamics of relationships and trust within the group. By representing group\nmembers as nodes and their interactions as directed edges, the algorithm\ncaptures pairwise relationships to foster consensus and improve the alignment\nof recommendations with group preferences. This interaction-focused framework\nultimately seeks to enhance overall group satisfaction with the recommended\nchoices.\n","authors":["Peijin Yu","Shin'ichi Konomi"],"pdf_url":"https://arxiv.org/pdf/2503.12877v1.pdf","comment":"30 pages, 16 figures"},{"id":"http://arxiv.org/abs/2501.03072v2","updated":"2025-03-17T03:02:56Z","published":"2024-11-22T02:49:15Z","title":"OpenTable data with multi-criteria ratings","summary":"  With the development of recommender systems (RSs), several promising systems\nhave emerged, such as context-aware RS, multi-criteria RS, and group RS.\nMulti-criteria recommender systems (MCRSs) are designed to provide personalized\nrecommendations by considering user preferences in multiple attributes or\ncriteria simultaneously. Unlike traditional RSs that typically focus on a\nsingle rating, these systems help users make more informed decisions by\nconsidering their diverse preferences and needs across various dimensions. In\nthis article, we release the OpenTable data set which was crawled from\nOpenTable.com. The data set can be considered as a benchmark data set for\nmulti-criteria recommendations.\n","authors":["Yong Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.03072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13563v1","updated":"2025-03-17T08:09:42Z","published":"2025-03-17T08:09:42Z","title":"MES-RAG: Bringing Multi-modal, Entity-Storage, and Secure Enhancements\n  to RAG","summary":"  Retrieval-Augmented Generation (RAG) improves Large Language Models (LLMs) by\nusing external knowledge, but it struggles with precise entity information\nretrieval. In this paper, we proposed MES-RAG framework, which enhances\nentity-specific query handling and provides accurate, secure, and consistent\nresponses. MES-RAG introduces proactive security measures that ensure system\nintegrity by applying protections prior to data access. Additionally, the\nsystem supports real-time multi-modal outputs, including text, images, audio,\nand video, seamlessly integrating into existing RAG architectures. Experimental\nresults demonstrate that MES-RAG significantly improves both accuracy and\nrecall, highlighting its effectiveness in advancing the security and utility of\nquestion-answering, increasing accuracy to 0.83 (+0.25) on targeted task. Our\ncode and data are available at https://github.com/wpydcr/MES-RAG.\n","authors":["Pingyu Wu","Daiheng Gao","Jing Tang","Huimin Chen","Wenbo Zhou","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2503.13563v1.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2503.15549v1","updated":"2025-03-17T20:56:55Z","published":"2025-03-17T20:56:55Z","title":"Rendering Transparency to Ranking in Educational Assessment via Bayesian\n  Comparative Judgement","summary":"  Ensuring transparency in educational assessment is increasingly critical,\nparticularly post-pandemic, as demand grows for fairer and more reliable\nevaluation methods. Comparative Judgement (CJ) offers a promising alternative\nto traditional assessments, yet concerns remain about its perceived opacity.\nThis paper examines how Bayesian Comparative Judgement (BCJ) enhances\ntransparency by integrating prior information into the judgement process,\nproviding a structured, data-driven approach that improves interpretability and\naccountability.\n  BCJ assigns probabilities to judgement outcomes, offering quantifiable\nmeasures of uncertainty and deeper insights into decision confidence. By\nsystematically tracking how prior data and successive judgements inform final\nrankings, BCJ clarifies the assessment process and helps identify assessor\ndisagreements. Multi-criteria BCJ extends this by evaluating multiple learning\noutcomes (LOs) independently, preserving the richness of CJ while producing\ntransparent, granular rankings aligned with specific assessment goals. It also\nenables a holistic ranking derived from individual LOs, ensuring comprehensive\nevaluations without compromising detailed feedback.\n  Using a real higher education dataset with professional markers in the UK, we\ndemonstrate BCJ's quantitative rigour and ability to clarify ranking\nrationales. Through qualitative analysis and discussions with experienced CJ\npractitioners, we explore its effectiveness in contexts where transparency is\ncrucial, such as high-stakes national assessments. We highlight the benefits\nand limitations of BCJ, offering insights into its real-world application\nacross various educational settings.\n","authors":["Andy Gray","Alma Rahat","Stephen Lindsay","Jen Pearson","Tom Crick"],"pdf_url":"https://arxiv.org/pdf/2503.15549v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.13447v1","updated":"2025-03-17T17:59:54Z","published":"2025-03-17T17:59:54Z","title":"MetaScale: Test-Time Scaling with Evolving Meta-Thoughts","summary":"  One critical challenge for large language models (LLMs) for making complex\nreasoning is their reliance on matching reasoning patterns from training data,\ninstead of proactively selecting the most appropriate cognitive strategy to\nsolve a given task. Existing approaches impose fixed cognitive structures that\nenhance performance in specific tasks but lack adaptability across diverse\nscenarios. To address this limitation, we introduce METASCALE, a test-time\nscaling framework based on meta-thoughts -- adaptive thinking strategies\ntailored to each task. METASCALE initializes a pool of candidate meta-thoughts,\nthen iteratively selects and evaluates them using a multi-armed bandit\nalgorithm with upper confidence bound selection, guided by a reward model. To\nfurther enhance adaptability, a genetic algorithm evolves high-reward\nmeta-thoughts, refining and extending the strategy pool over time. By\ndynamically proposing and optimizing meta-thoughts at inference time, METASCALE\nimproves both accuracy and generalization across a wide range of tasks.\nExperimental results demonstrate that MetaScale consistently outperforms\nstandard inference approaches, achieving an 11% performance gain in win rate on\nArena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,\nMETASCALE scales more effectively with increasing sampling budgets and produces\nmore structured, expert-level responses.\n","authors":["Qin Liu","Wenxuan Zhou","Nan Xu","James Y. Huang","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2503.13447v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.13438v1","updated":"2025-03-17T17:58:45Z","published":"2025-03-17T17:58:45Z","title":"Deep Belief Markov Models for POMDP Inference","summary":"  This work introduces a novel deep learning-based architecture, termed the\nDeep Belief Markov Model (DBMM), which provides efficient, model-formulation\nagnostic inference in Partially Observable Markov Decision Process (POMDP)\nproblems. The POMDP framework allows for modeling and solving sequential\ndecision-making problems under observation uncertainty. In complex,\nhigh-dimensional, partially observable environments, existing methods for\ninference based on exact computations (e.g., via Bayes' theorem) or sampling\nalgorithms do not scale well. Furthermore, ground truth states may not be\navailable for learning the exact transition dynamics. DBMMs extend deep Markov\nmodels into the partially observable decision-making framework and allow\nefficient belief inference entirely based on available observation data via\nvariational inference methods. By leveraging the potency of neural networks,\nDBMMs can infer and simulate non-linear relationships in the system dynamics\nand naturally scale to problems with high dimensionality and discrete or\ncontinuous variables. In addition, neural network parameters can be dynamically\nupdated efficiently based on data availability. DBMMs can thus be used to infer\na belief variable, thus enabling the derivation of POMDP solutions over the\nbelief space. We evaluate the efficacy of the proposed methodology by\nevaluating the capability of model-formulation agnostic inference of DBMMs in\nbenchmark problems that include discrete and continuous variables.\n","authors":["Giacomo Arcieri","Konstantinos G. Papakonstantinou","Daniel Straub","Eleni Chatzi"],"pdf_url":"https://arxiv.org/pdf/2503.13438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13436v1","updated":"2025-03-17T17:58:30Z","published":"2025-03-17T17:58:30Z","title":"Unified Autoregressive Visual Generation and Understanding with\n  Continuous Tokens","summary":"  We present UniFluid, a unified autoregressive framework for joint visual\ngeneration and understanding leveraging continuous visual tokens. Our unified\nautoregressive architecture processes multimodal image and text inputs,\ngenerating discrete tokens for text and continuous tokens for image. We find\nthough there is an inherent trade-off between the image generation and\nunderstanding task, a carefully tuned training recipe enables them to improve\neach other. By selecting an appropriate loss balance weight, the unified model\nachieves results comparable to or exceeding those of single-task baselines on\nboth tasks. Furthermore, we demonstrate that employing stronger pre-trained\nLLMs and random-order generation during training is important to achieve\nhigh-fidelity image generation within this unified framework. Built upon the\nGemma model series, UniFluid exhibits competitive performance across both image\ngeneration and understanding, demonstrating strong transferability to various\ndownstream tasks, including image editing for generation, as well as visual\ncaptioning and question answering for understanding.\n","authors":["Lijie Fan","Luming Tang","Siyang Qin","Tianhong Li","Xuan Yang","Siyuan Qiao","Andreas Steiner","Chen Sun","Yuanzhen Li","Tao Zhu","Michael Rubinstein","Michalis Raptis","Deqing Sun","Radu Soricut"],"pdf_url":"https://arxiv.org/pdf/2503.13436v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2503.13432v1","updated":"2025-03-17T17:56:59Z","published":"2025-03-17T17:56:59Z","title":"Uncovering Utility Functions from Observed Outcomes","summary":"  Determining consumer preferences and utility is a foundational challenge in\neconomics. They are central in determining consumer behaviour through the\nutility-maximising consumer decision-making process. However, preferences and\nutilities are not observable and may not even be known to the individual making\nthe choice; only the outcome is observed in the form of demand. Without the\nability to observe the decision-making mechanism, demand estimation becomes a\nchallenging task and current methods fall short due to lack of scalability or\nability to identify causal effects. Estimating these effects is critical when\nconsidering changes in policy, such as pricing, the impact of taxes and\nsubsidies, and the effect of a tariff. To address the shortcomings of existing\nmethods, we combine revealed preference theory and inverse reinforcement\nlearning to present a novel algorithm, Preference Extraction and Reward\nLearning (PEARL) which, to the best of our knowledge, is the only algorithm\nthat can uncover a representation of the utility function that best\nrationalises observed consumer choice data given a specified functional form.\nWe introduce a flexible utility function, the Input-Concave Neural Network\nwhich captures complex relationships across goods, including cross-price\nelasticities. Results show PEARL outperforms the benchmark on both noise-free\nand noisy synthetic data.\n","authors":["Marta Grzeskiewicz"],"pdf_url":"https://arxiv.org/pdf/2503.13432v1.pdf","comment":"Working paper"},{"id":"http://arxiv.org/abs/2503.13431v1","updated":"2025-03-17T17:56:14Z","published":"2025-03-17T17:56:14Z","title":"Measuring In-Context Computation Complexity via Hidden State Prediction","summary":"  Detecting when a neural sequence model does \"interesting\" computation is an\nopen problem. The next token prediction loss is a poor indicator: Low loss can\nstem from trivially predictable sequences that are uninteresting, while high\nloss may reflect unpredictable but also irrelevant information that can be\nignored by the model. We propose a better metric: measuring the model's ability\nto predict its own future hidden states. We show empirically that this metric\n-- in contrast to the next token prediction loss -- correlates with the\nintuitive interestingness of the task. To measure predictability, we introduce\nthe architecture-agnostic \"prediction of hidden states\" (PHi) layer that serves\nas an information bottleneck on the main pathway of the network (e.g., the\nresidual stream in Transformers). We propose a novel learned predictive prior\nthat enables us to measure the novel information gained in each computation\nstep, which serves as our metric. We show empirically that our metric predicts\nthe description length of formal languages learned in-context, the complexity\nof mathematical reasoning problems, and the correctness of self-generated\nreasoning chains.\n","authors":["Vincent Herrmann","Róbert Csordás","Jürgen Schmidhuber"],"pdf_url":"https://arxiv.org/pdf/2503.13431v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13430v1","updated":"2025-03-17T17:55:32Z","published":"2025-03-17T17:55:32Z","title":"AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation\n  for Enhanced Vectorized Online HD Map Construction","summary":"  Autonomous driving requires an understanding of the infrastructure elements,\nsuch as lanes and crosswalks. To navigate safely, this understanding must be\nderived from sensor data in real-time and needs to be represented in vectorized\nform. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set\nof camera images from multiple views into one joint latent BEV grid.\nTraditionally, from this latent space, an intermediate raster map is predicted,\nproviding dense spatial supervision but requiring post-processing into the\ndesired vectorized form. More recent models directly derive infrastructure\nelements as polylines using vectorized map decoders, providing instance-level\ninformation. Our approach, Augmentation Map Network (AugMapNet), proposes\nlatent BEV grid augmentation, a novel technique that significantly enhances the\nlatent BEV representation. AugMapNet combines vector decoding and dense spatial\nsupervision more effectively than existing architectures while remaining as\nstraightforward to integrate and as generic as auxiliary supervision.\nExperiments on nuScenes and Argoverse2 datasets demonstrate significant\nimprovements in vectorized map prediction performance up to 13.3% over the\nStreamMapNet baseline on 60m range and greater improvements on larger ranges.\nWe confirm transferability by applying our method to another baseline and find\nsimilar improvements. A detailed analysis of the latent BEV grid confirms a\nmore structured latent space of AugMapNet and shows the value of our novel\nconcept beyond pure performance improvement. The code will be released soon.\n","authors":["Thomas Monninger","Md Zafar Anwar","Stanislaw Antol","Steffen Staab","Sihao Ding"],"pdf_url":"https://arxiv.org/pdf/2503.13430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13427v1","updated":"2025-03-17T17:54:55Z","published":"2025-03-17T17:54:55Z","title":"xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference","summary":"  Recent breakthroughs in solving reasoning, math and coding problems with\nLarge Language Models (LLMs) have been enabled by investing substantial\ncomputation budgets at inference time. Therefore, inference speed is one of the\nmost critical properties of LLM architectures, and there is a growing need for\nLLMs that are efficient and fast at inference. Recently, LLMs built on the\nxLSTM architecture have emerged as a powerful alternative to Transformers,\noffering linear compute scaling with sequence length and constant memory usage,\nboth highly desirable properties for efficient inference. However, such\nxLSTM-based LLMs have yet to be scaled to larger models and assessed and\ncompared with respect to inference speed and efficiency. In this work, we\nintroduce xLSTM 7B, a 7-billion-parameter LLM that combines xLSTM's\narchitectural benefits with targeted optimizations for fast and efficient\ninference. Our experiments demonstrate that xLSTM 7B achieves performance on\ndownstream tasks comparable to other similar-sized LLMs, while providing\nsignificantly faster inference speeds and greater efficiency compared to Llama-\nand Mamba-based LLMs. These results establish xLSTM 7B as the fastest and most\nefficient 7B LLM, offering a solution for tasks that require large amounts of\ntest-time computation. Our work highlights xLSTM's potential as a foundational\narchitecture for methods building on heavy use of LLM inference. Our model\nweights, model code and training code are open-source.\n","authors":["Maximilian Beck","Korbinian Pöppel","Phillip Lippe","Richard Kurle","Patrick M. Blies","Günter Klambauer","Sebastian Böck","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2503.13427v1.pdf","comment":"Code available at: https://github.com/NX-AI/xlstm and\n  https://github.com/NX-AI/xlstm-jax"},{"id":"http://arxiv.org/abs/2503.13423v1","updated":"2025-03-17T17:53:23Z","published":"2025-03-17T17:53:23Z","title":"SuperBPE: Space Travel for Language Models","summary":"  The assumption across nearly all language model (LM) tokenization schemes is\nthat tokens should be subwords, i.e., contained within word boundaries. While\nproviding a seemingly reasonable inductive bias, is this common practice\nlimiting the potential of modern LMs? Whitespace is not a reliable delimiter of\nmeaning, as evidenced by multi-word expressions (e.g., \"by the way\"),\ncrosslingual variation in the number of words needed to express a concept\n(e.g., \"spacesuit helmet\" in German is \"raumanzughelm\"), and languages that do\nnot use whitespace at all (e.g., Chinese). To explore the potential of\ntokenization beyond subwords, we introduce a \"superword\" tokenizer, SuperBPE,\nwhich incorporates a simple pretokenization curriculum into the byte-pair\nencoding (BPE) algorithm to first learn subwords, then superwords that bridge\nwhitespace. This brings dramatic improvements in encoding efficiency: when\nfixing the vocabulary size to 200k, SuperBPE encodes a fixed piece of text with\nup to 33% fewer tokens than BPE on average. In experiments, we pretrain 8B\ntransformer LMs from scratch while fixing the model size, vocabulary size, and\ntrain compute, varying *only* the algorithm for learning the vocabulary. Our\nmodel trained with SuperBPE achieves an average +4.0% absolute improvement over\nthe BPE baseline across 30 downstream tasks (including +8.2% on MMLU), while\nsimultaneously requiring 27% less compute at inference time. In analysis, we\nfind that SuperBPE results in segmentations of text that are more uniform in\nper-token difficulty. Qualitatively, this may be because SuperBPE tokens often\ncapture common multi-word expressions that function semantically as a single\nunit. SuperBPE is a straightforward, local modification to tokenization that\nimproves both encoding efficiency and downstream performance, yielding better\nlanguage models overall.\n","authors":["Alisa Liu","Jonathan Hayase","Valentin Hofmann","Sewoong Oh","Noah A. Smith","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2503.13423v1.pdf","comment":"preprint, code and artifacts will become available at\n  https://superbpe.github.io/"},{"id":"http://arxiv.org/abs/2503.13414v1","updated":"2025-03-17T17:42:54Z","published":"2025-03-17T17:42:54Z","title":"Reward Adaptation Via Q-Manipulation","summary":"  In this paper, we propose a new solution to reward adaptation (RA), the\nproblem where the learning agent adapts to a target reward function based on\none or multiple existing behaviors learned a priori under the same domain\ndynamics but different reward functions. Learning the target behavior from\nscratch is possible but often inefficient given the available source behaviors.\nOur work represents a new approach to RA via the manipulation of Q-functions.\nAssuming that the target reward function is a known function of the source\nreward functions, our approach to RA computes bounds of the Q function. We\nintroduce an iterative process to tighten the bounds, similar to value\niteration. This enables action pruning in the target domain before learning\neven starts. We refer to such a method as Q-Manipulation (Q-M). We formally\nprove that our pruning strategy does not affect the optimality of the returned\npolicy while empirically show that it improves the sample complexity. Q-M is\nevaluated in a variety of synthetic and simulation domains to demonstrate its\neffectiveness, generalizability, and practicality.\n","authors":["Kevin Vora","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13404v1","updated":"2025-03-17T17:34:34Z","published":"2025-03-17T17:34:34Z","title":"Fed-Joint: Joint Modeling of Nonlinear Degradation Signals and Failure\n  Events for Remaining Useful Life Prediction using Federated Learning","summary":"  Many failure mechanisms of machinery are closely related to the behavior of\ncondition monitoring (CM) signals. To achieve a cost-effective preventive\nmaintenance strategy, accurate remaining useful life (RUL) prediction based on\nthe signals is of paramount importance. However, the CM signals are often\nrecorded at different factories and production lines, with limited amounts of\ndata. Unfortunately, these datasets have rarely been shared between the sites\ndue to data confidentiality and ownership issues, a lack of computing and\nstorage power, and high communication costs associated with data transfer\nbetween sites and a data center. Another challenge in real applications is that\nthe CM signals are often not explicitly specified \\textit{a priori}, meaning\nthat existing methods, which often usually a parametric form, may not be\napplicable. To address these challenges, we propose a new prognostic framework\nfor RUL prediction using the joint modeling of nonlinear degradation signals\nand time-to-failure data within a federated learning scheme. The proposed\nmethod constructs a nonparametric degradation model using a federated\nmulti-output Gaussian process and then employs a federated survival model to\npredict failure times and probabilities for in-service machinery. The\nsuperiority of the proposed method over other alternatives is demonstrated\nthrough comprehensive simulation studies and a case study using turbofan engine\ndegradation signal data that include run-to-failure events.\n","authors":["Cheoljoon Jeong","Xubo Yue","Seokhyun Chung"],"pdf_url":"https://arxiv.org/pdf/2503.13404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13399v1","updated":"2025-03-17T17:33:10Z","published":"2025-03-17T17:33:10Z","title":"MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based\n  Scientific Research","summary":"  Scientific research demands sophisticated reasoning over multimodal data, a\nchallenge especially prevalent in biology. Despite recent advances in\nmultimodal large language models (MLLMs) for AI-assisted research, existing\nmultimodal reasoning benchmarks only target up to college-level difficulty,\nwhile research-level benchmarks emphasize lower-level perception, falling short\nof the complex multimodal reasoning needed for scientific discovery. To bridge\nthis gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark\ndesigned to assess three reasoning capabilities vital in research workflows:\nexpert image understanding, hypothesis generation, and experiment proposal.\nMicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology\nexperts across diverse microscopy modalities, ensuring VQA samples represent\nreal scientific practice. In constructing the benchmark, we find that standard\nMCQ generation methods induce language shortcuts, motivating a new two-stage\npipeline: an optimized LLM prompt structures question-answer pairs into MCQs;\nthen, an agent-based `RefineBot' updates them to remove shortcuts. Benchmarking\non state-of-the-art MLLMs reveal a peak performance of 53\\%; models with\nsmaller LLMs only slightly underperform top models, suggesting that\nlanguage-based reasoning is less challenging than multimodal reasoning; and\ntuning with scientific articles enhances performance. Expert analysis of\nchain-of-thought responses shows that perception errors are the most frequent,\nfollowed by knowledge errors and then overgeneralization errors. These insights\nhighlight the challenges in multimodal scientific reasoning, showing MicroVQA\nis a valuable resource advancing AI-driven biomedical research. MicroVQA is\navailable at https://huggingface.co/datasets/jmhb/microvqa, and project page at\nhttps://jmhb0.github.io/microvqa.\n","authors":["James Burgess","Jeffrey J Nirschl","Laura Bravo-Sánchez","Alejandro Lozano","Sanket Rajan Gupte","Jesus G. Galaz-Montoya","Yuhui Zhang","Yuchang Su","Disha Bhowmik","Zachary Coman","Sarina M. Hasan","Alexandra Johannesson","William D. Leineweber","Malvika G Nair","Ridhi Yarlagadda","Connor Zuraski","Wah Chiu","Sarah Cohen","Jan N. Hansen","Manuel D Leonetti","Chad Liu","Emma Lundberg","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2503.13399v1.pdf","comment":"CVPR 2025 (Conference on Computer Vision and Pattern Recognition)\n  Project page at https://jmhb0.github.io/microvqa Benchmark at\n  https://huggingface.co/datasets/jmhb/microvqa"},{"id":"http://arxiv.org/abs/2410.10986v2","updated":"2025-03-17T17:32:06Z","published":"2024-10-14T18:15:02Z","title":"What Does It Mean to Be a Transformer? Insights from a Theoretical\n  Hessian Analysis","summary":"  The Transformer architecture has inarguably revolutionized deep learning,\novertaking classical architectures like multi-layer perceptrons (MLPs) and\nconvolutional neural networks (CNNs). At its core, the attention block differs\nin form and functionality from most other architectural components in deep\nlearning--to the extent that, in comparison to MLPs/CNNs, Transformers are more\noften accompanied by adaptive optimizers, layer normalization, learning rate\nwarmup, etc. The root causes behind these outward manifestations and the\nprecise mechanisms that govern them remain poorly understood. In this work, we\nbridge this gap by providing a fundamental understanding of what distinguishes\nthe Transformer from the other architectures--grounded in a theoretical\ncomparison of the (loss) Hessian. Concretely, for a single self-attention\nlayer, (a) we first entirely derive the Transformer's Hessian and express it in\nmatrix derivatives; (b) we then characterize it in terms of data, weight, and\nattention moment dependencies; and (c) while doing so further highlight the\nimportant structural differences to the Hessian of classical networks. Our\nresults suggest that various common architectural and optimization choices in\nTransformers can be traced back to their highly non-linear dependencies on the\ndata and weight matrices, which vary heterogeneously across parameters.\nUltimately, our findings provide a deeper understanding of the Transformer's\nunique optimization landscape and the challenges it poses.\n","authors":["Weronika Ormaniec","Felix Dangel","Sidak Pal Singh"],"pdf_url":"https://arxiv.org/pdf/2410.10986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10240v2","updated":"2025-03-17T17:29:33Z","published":"2025-01-17T15:09:57Z","title":"Challenges and recommendations for Electronic Health Records data\n  extraction and preparation for dynamic prediction modelling in hospitalized\n  patients -- a practical guide","summary":"  Dynamic predictive modelling using electronic health record (EHR) data has\ngained significant attention in recent years. The reliability and\ntrustworthiness of such models depend heavily on the quality of the underlying\ndata, which is, in part, determined by the stages preceding the model\ndevelopment: data extraction from EHR systems and data preparation. In this\narticle, we identified over forty challenges encountered during these stages\nand provide actionable recommendations for addressing them. These challenges\nare organized into four categories: cohort definition, outcome definition,\nfeature engineering, and data cleaning. This comprehensive list serves as a\npractical guide for data extraction engineers and researchers, promoting best\npractices and improving the quality and real-world applicability of dynamic\nprediction models in clinical settings.\n","authors":["Elena Albu","Shan Gao","Pieter Stijnen","Frank E. Rademakers","Bas C T van Bussel","Taya Collyer","Tina Hernandez-Boussard","Laure Wynants","Ben Van Calster"],"pdf_url":"https://arxiv.org/pdf/2501.10240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13389v1","updated":"2025-03-17T17:22:15Z","published":"2025-03-17T17:22:15Z","title":"Investigating the effect of CPT in lateral spreading prediction using\n  Explainable AI","summary":"  This study proposes an autoencoder approach to extract latent features from\ncone penetration test profiles to evaluate the potential of incorporating CPT\ndata in an AI model. We employ autoencoders to compress 200 CPT profiles of\nsoil behavior type index (Ic) and normalized cone resistance (qc1Ncs) into ten\nlatent features while preserving critical information. We then utilize the\nextracted latent features with site parameters to train XGBoost models for\npredicting lateral spreading occurrences in the 2011 Christchurch earthquake.\nModels using the latent CPT features outperformed models with conventional CPT\nmetrics or no CPT data, achieving over 83% accuracy. Explainable AI revealed\nthe most crucial latent feature corresponding to soil behavior between 1-3\nmeter depths, highlighting this depth range's criticality for liquefaction\nevaluation. The autoencoder approach provides an automated technique for\ncondensing CPT profiles into informative latent features for machine-learning\nliquefaction models.\n","authors":["Cheng-Hsi Hsiao","Ellen Rathje","Krishna Kumar"],"pdf_url":"https://arxiv.org/pdf/2503.13389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13385v1","updated":"2025-03-17T17:13:43Z","published":"2025-03-17T17:13:43Z","title":"Scale Efficient Training for Large Datasets","summary":"  The rapid growth of dataset scales has been a key driver in advancing deep\nlearning research. However, as dataset scale increases, the training process\nbecomes increasingly inefficient due to the presence of low-value samples,\nincluding excessive redundant samples, overly challenging samples, and\ninefficient easy samples that contribute little to model improvement.To address\nthis challenge, we propose Scale Efficient Training (SeTa) for large datasets,\na dynamic sample pruning approach that losslessly reduces training time. To\nremove low-value samples, SeTa first performs random pruning to eliminate\nredundant samples, then clusters the remaining samples according to their\nlearning difficulty measured by loss. Building upon this clustering, a sliding\nwindow strategy is employed to progressively remove both overly challenging and\ninefficient easy clusters following an easy-to-hard curriculum.We conduct\nextensive experiments on large-scale synthetic datasets, including ToCa, SS1M,\nand ST+MJ, each containing over 3 million samples.SeTa reduces training costs\nby up to 50\\% while maintaining or improving performance, with minimal\ndegradation even at 70\\% cost reduction. Furthermore, experiments on various\nscale real datasets across various backbones (CNNs, Transformers, and Mambas)\nand diverse tasks (instruction tuning, multi-view stereo, geo-localization,\ncomposed image retrieval, referring image segmentation) demonstrate the\npowerful effectiveness and universality of our approach. Code is available at\nhttps://github.com/mrazhou/SeTa.\n","authors":["Qing Zhou","Junyu Gao","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13385v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.13383v1","updated":"2025-03-17T17:11:22Z","published":"2025-03-17T17:11:22Z","title":"Cream of the Crop: Harvesting Rich, Scalable and Transferable\n  Multi-Modal Data for Instruction Fine-Tuning","summary":"  The hypothesis that pretrained large language models (LLMs) necessitate only\nminimal supervision during the fine-tuning (SFT) stage (Zhou et al., 2024) has\nbeen substantiated by recent advancements in data curation and selection\nresearch. However, their stability and generalizability are compromised due to\nthe vulnerability to experimental setups and validation protocols, falling\nshort of surpassing random sampling (Diddee & Ippolito, 2024; Xia et al.,\n2024b). Built upon LLMs, multi-modal LLMs (MLLMs), combined with the sheer\ntoken volume and heightened heterogeneity of data sources, amplify both the\nsignificance and complexity of data selection.\n  To harvest multi-modal instructional data in a robust and efficient manner,\nwe re-define the granularity of the quality metric by decomposing it into 14\nvision-language-related capabilities, and introduce multi-modal rich scorers to\nevaluate the capabilities of each data candidate. To promote diversity, in\nlight of the inherent objective of the alignment stage, we take interaction\nstyle as diversity indicator and use a multi-modal rich styler to identify data\ninstruction patterns. In doing so, our multi-modal rich scorers and styler\n(mmSSR) guarantee that high-scoring information is conveyed to users in\ndiversified forms. Free from embedding-based clustering or greedy sampling,\nmmSSR efficiently scales to millions of data with varying budget constraints,\nsupports customization for general or specific capability acquisition, and\nfacilitates training-free generalization to new domains for curation. Across\n10+ experimental settings, validated by 14 multi-modal benchmarks, we\ndemonstrate consistent improvements over random sampling, baseline strategies\nand state-of-the-art selection methods, achieving 99.1% of full performance\nwith only 30% of the 2.6M data.\n","authors":["Mengyao Lyu","Yan Li","Huasong Zhong","Wenhao Yang","Hui Chen","Jungong Han","Guiguang Ding","Zhenheng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13383v1.pdf","comment":"update comparison with sota and analysis"},{"id":"http://arxiv.org/abs/2406.08267v2","updated":"2025-03-17T16:59:40Z","published":"2024-06-12T14:35:13Z","title":"A deep cut into Split Federated Self-supervised Learning","summary":"  Collaborative self-supervised learning has recently become feasible in highly\ndistributed environments by dividing the network layers between client devices\nand a central server. However, state-of-the-art methods, such as MocoSFL, are\noptimized for network division at the initial layers, which decreases the\nprotection of the client data and increases communication overhead. In this\npaper, we demonstrate that splitting depth is crucial for maintaining privacy\nand communication efficiency in distributed training. We also show that MocoSFL\nsuffers from a catastrophic quality deterioration for the minimal communication\noverhead. As a remedy, we introduce Momentum-Aligned contrastive Split\nFederated Learning (MonAcoSFL), which aligns online and momentum client models\nduring training procedure. Consequently, we achieve state-of-the-art accuracy\nwhile significantly reducing the communication overhead, making MonAcoSFL more\npractical in real-world scenarios.\n","authors":["Marcin Przewięźlikowski","Marcin Osial","Bartosz Zieliński","Marek Śmieja"],"pdf_url":"https://arxiv.org/pdf/2406.08267v2.pdf","comment":"European Conference on Machine Learning (ECML) 2024"},{"id":"http://arxiv.org/abs/2503.13371v1","updated":"2025-03-17T16:58:53Z","published":"2025-03-17T16:58:53Z","title":"SyncDiff: Diffusion-based Talking Head Synthesis with Bottlenecked\n  Temporal Visual Prior for Improved Synchronization","summary":"  Talking head synthesis, also known as speech-to-lip synthesis, reconstructs\nthe facial motions that align with the given audio tracks. The synthesized\nvideos are evaluated on mainly two aspects, lip-speech synchronization and\nimage fidelity. Recent studies demonstrate that GAN-based and diffusion-based\nmodels achieve state-of-the-art (SOTA) performance on this task, with\ndiffusion-based models achieving superior image fidelity but experiencing lower\nsynchronization compared to their GAN-based counterparts. To this end, we\npropose SyncDiff, a simple yet effective approach to improve diffusion-based\nmodels using a temporal pose frame with information bottleneck and\nfacial-informative audio features extracted from AVHuBERT, as conditioning\ninput into the diffusion process. We evaluate SyncDiff on two canonical talking\nhead datasets, LRS2 and LRS3 for direct comparison with other SOTA models.\nExperiments on LRS2/LRS3 datasets show that SyncDiff achieves a synchronization\nscore 27.7%/62.3% relatively higher than previous diffusion-based methods,\nwhile preserving their high-fidelity characteristics.\n","authors":["Xulin Fan","Heting Gao","Ziyi Chen","Peng Chang","Mei Han","Mark Hasegawa-Johnson"],"pdf_url":"https://arxiv.org/pdf/2503.13371v1.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2503.13366v1","updated":"2025-03-17T16:51:16Z","published":"2025-03-17T16:51:16Z","title":"Follow-the-Regularized-Leader with Adversarial Constraints","summary":"  Constrained Online Convex Optimization (COCO) can be seen as a generalization\nof the standard Online Convex Optimization (OCO) framework. At each round, a\ncost function and constraint function are revealed after a learner chooses an\naction. The goal is to minimize both the regret and cumulative constraint\nviolation (CCV) against an adaptive adversary. We show for the first time that\nis possible to obtain the optimal $O(\\sqrt{T})$ bound on both regret and CCV,\nimproving the best known bounds of $O \\left( \\sqrt{T} \\right)$ and $\\~{O}\n\\left( \\sqrt{T} \\right)$ for the regret and CCV, respectively.\n","authors":["Ricardo N. Ferreira","Cláudia Soares"],"pdf_url":"https://arxiv.org/pdf/2503.13366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13360v1","updated":"2025-03-17T16:45:12Z","published":"2025-03-17T16:45:12Z","title":"Mitigating Visual Forgetting via Take-along Visual Conditioning for\n  Multi-modal Long CoT Reasoning","summary":"  Recent advancements in Large Language Models (LLMs) have demonstrated\nenhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting\nto advanced, product-oriented solutions like OpenAI o1. During our\nre-implementation of this model, we noticed that in multimodal tasks requiring\nvisual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to\nmaintain focus on the visual information, in other words, MLLMs suffer from a\ngradual decline in attention to visual information as reasoning progresses,\ncausing text-over-relied outputs. To investigate this, we ablate image inputs\nduring long-chain reasoning. Concretely, we truncate the reasoning process\nmidway, then re-complete the reasoning process with the input image removed. We\nobserve only a ~2% accuracy drop on MathVista's test-hard subset, revealing the\nmodel's textual outputs dominate the following reasoning process. Motivated by\nthis, we propose Take-along Visual Conditioning (TVC), a strategy that shifts\nimage input to critical reasoning stages and compresses redundant visual tokens\nvia dynamic pruning. This methodology helps the model retain attention to the\nvisual components throughout the reasoning. Our approach achieves\nstate-of-the-art performance on average across five mathematical reasoning\nbenchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in\nenhancing multimodal reasoning systems.\n","authors":["Hai-Long Sun","Zhun Sun","Houwen Peng","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2503.13360v1.pdf","comment":"The project page is available at\n  https://sun-hailong.github.io/projects/TVC"},{"id":"http://arxiv.org/abs/2410.13643v2","updated":"2025-03-17T16:44:45Z","published":"2024-10-17T15:10:13Z","title":"Fine-Tuning Discrete Diffusion Models via Reward Optimization with\n  Applications to DNA and Protein Design","summary":"  Recent studies have demonstrated the strong empirical performance of\ndiffusion models on discrete sequences across domains from natural language to\nbiological sequence generation. For example, in the protein inverse folding\ntask, conditional diffusion models have achieved impressive results in\ngenerating natural-like sequences that fold back into the original structure.\nHowever, practical design tasks often require not only modeling a conditional\ndistribution but also optimizing specific task objectives. For instance, we may\nprefer protein sequences with high stability. To address this, we consider the\nscenario where we have pre-trained discrete diffusion models that can generate\nnatural-like sequences, as well as reward models that map sequences to task\nobjectives. We then formulate the reward maximization problem within discrete\ndiffusion models, analogous to reinforcement learning (RL), while minimizing\nthe KL divergence against pretrained diffusion models to preserve naturalness.\nTo solve this RL problem, we propose a novel algorithm, DRAKES, that enables\ndirect backpropagation of rewards through entire trajectories generated by\ndiffusion models, by making the originally non-differentiable trajectories\ndifferentiable using the Gumbel-Softmax trick. Our theoretical analysis\nindicates that our approach can generate sequences that are both natural-like\nand yield high rewards. While similar tasks have been recently explored in\ndiffusion models for continuous domains, our work addresses unique algorithmic\nand theoretical challenges specific to discrete diffusion models, which arise\nfrom their foundation in continuous-time Markov chains rather than Brownian\nmotion. Finally, we demonstrate the effectiveness of DRAKES in generating DNA\nand protein sequences that optimize enhancer activity and protein stability,\nrespectively, important tasks for gene therapies and protein-based\ntherapeutics.\n","authors":["Chenyu Wang","Masatoshi Uehara","Yichun He","Amy Wang","Tommaso Biancalani","Avantika Lal","Tommi Jaakkola","Sergey Levine","Hanchen Wang","Aviv Regev"],"pdf_url":"https://arxiv.org/pdf/2410.13643v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13356v1","updated":"2025-03-17T16:42:34Z","published":"2025-03-17T16:42:34Z","title":"Agents Play Thousands of 3D Video Games","summary":"  We present PORTAL, a novel framework for developing artificial intelligence\nagents capable of playing thousands of 3D video games through language-guided\npolicy generation. By transforming decision-making problems into language\nmodeling tasks, our approach leverages large language models (LLMs) to generate\nbehavior trees represented in domain-specific language (DSL). This method\neliminates the computational burden associated with traditional reinforcement\nlearning approaches while preserving strategic depth and rapid adaptability.\nOur framework introduces a hybrid policy structure that combines rule-based\nnodes with neural network components, enabling both high-level strategic\nreasoning and precise low-level control. A dual-feedback mechanism\nincorporating quantitative game metrics and vision-language model analysis\nfacilitates iterative policy improvement at both tactical and strategic levels.\nThe resulting policies are instantaneously deployable, human-interpretable, and\ncapable of generalizing across diverse gaming environments. Experimental\nresults demonstrate PORTAL's effectiveness across thousands of first-person\nshooter (FPS) games, showcasing significant improvements in development\nefficiency, policy generalization, and behavior diversity compared to\ntraditional approaches. PORTAL represents a significant advancement in game AI\ndevelopment, offering a practical solution for creating sophisticated agents\nthat can operate across thousands of commercial video games with minimal\ndevelopment overhead. Experiment results on the 3D video games are best viewed\non https://zhongwen.one/projects/portal .\n","authors":["Zhongwen Xu","Xianliang Wang","Siyi Li","Tao Yu","Liang Wang","Qiang Fu","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13352v1","updated":"2025-03-17T16:33:52Z","published":"2025-03-17T16:33:52Z","title":"Strain Problems got you in a Twist? Try StrainRelief: A Quantum-Accurate\n  Tool for Ligand Strain Calculations","summary":"  Ligand strain energy, the energy difference between the bound and unbound\nconformations of a ligand, is an important component of structure-based small\nmolecule drug design. A large majority of observed ligands in protein-small\nmolecule co-crystal structures bind in low-strain conformations, making strain\nenergy a useful filter for structure-based drug design. In this work we present\na tool for calculating ligand strain with a high accuracy. StrainRelief uses a\nMACE Neural Network Potential (NNP), trained on a large database of Density\nFunctional Theory (DFT) calculations to estimate ligand strain of neutral\nmolecules with quantum accuracy. We show that this tool estimates strain energy\ndifferences relative to DFT to within 1.4 kcal/mol, more accurately than\nalternative NNPs. These results highlight the utility of NNPs in drug\ndiscovery, and provide a useful tool for drug discovery teams.\n","authors":["Ewan R. S. Wallace","Nathan C. Frey","Joshua A. Rackers"],"pdf_url":"https://arxiv.org/pdf/2503.13352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04626v2","updated":"2025-03-17T16:32:24Z","published":"2024-12-05T21:41:20Z","title":"BigDocs: An Open Dataset for Training Multimodal Models on Document and\n  Code Tasks","summary":"  Multimodal AI has the potential to significantly enhance\ndocument-understanding tasks, such as processing receipts, understanding\nworkflows, extracting data from documents, and summarizing reports. Code\ngeneration tasks that require long-structured outputs can also be enhanced by\nmultimodality. Despite this, their use in commercial applications is often\nlimited due to limited access to training data and restrictive licensing, which\nhinders open access. To address these limitations, we introduce BigDocs-7.5M, a\nhigh-quality, open-access dataset comprising 7.5 million multimodal documents\nacross 30 tasks. We use an efficient data curation process to ensure our data\nis high-quality and license-permissive. Our process emphasizes accountability,\nresponsibility, and transparency through filtering rules, traceable metadata,\nand careful content analysis. Additionally, we introduce BigDocs-Bench, a\nbenchmark suite with 10 novel tasks where we create datasets that reflect\nreal-world use cases involving reasoning over Graphical User Interfaces (GUI)\nand code generation from images. Our experiments show that training with\nBigDocs-Bench improves average performance up to 25.8% over closed-source\nGPT-4o in document reasoning and structured output tasks such as\nScreenshot2HTML or Image2Latex generation. Finally, human evaluations showed a\npreference for outputs from models trained on BigDocs over GPT-4o. This\nsuggests that BigDocs can help both academics and the open-source community\nutilize and improve AI tools to enhance multimodal capabilities and document\nreasoning. The project is hosted at https://bigdocs.github.io .\n","authors":["Juan Rodriguez","Xiangru Jian","Siba Smarak Panigrahi","Tianyu Zhang","Aarash Feizi","Abhay Puri","Akshay Kalkunte","François Savard","Ahmed Masry","Shravan Nayak","Rabiul Awal","Mahsa Massoud","Amirhossein Abaskohi","Zichao Li","Suyuchen Wang","Pierre-André Noël","Mats Leon Richter","Saverio Vadacchino","Shubham Agarwal","Sanket Biswas","Sara Shanian","Ying Zhang","Noah Bolger","Kurt MacDonald","Simon Fauvel","Sathwik Tejaswi","Srinivas Sunkara","Joao Monteiro","Krishnamurthy DJ Dvijotham","Torsten Scholak","Nicolas Chapados","Sepideh Kharagani","Sean Hughes","M. Özsu","Siva Reddy","Marco Pedersoli","Yoshua Bengio","Christopher Pal","Issam Laradji","Spandana Gella","Perouz Taslakian","David Vazquez","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2412.04626v2.pdf","comment":"The project is hosted at https://bigdocs.github.io"},{"id":"http://arxiv.org/abs/2410.23996v2","updated":"2025-03-17T16:27:27Z","published":"2024-10-31T14:57:31Z","title":"An Information Criterion for Controlled Disentanglement of Multimodal\n  Data","summary":"  Multimodal representation learning seeks to relate and decompose information\ninherent in multiple modalities. By disentangling modality-specific information\nfrom information that is shared across modalities, we can improve\ninterpretability and robustness and enable downstream tasks such as the\ngeneration of counterfactual outcomes. Separating the two types of information\nis challenging since they are often deeply entangled in many real-world\napplications. We propose Disentangled Self-Supervised Learning\n(DisentangledSSL), a novel self-supervised approach for learning disentangled\nrepresentations. We present a comprehensive analysis of the optimality of each\ndisentangled representation, particularly focusing on the scenario not covered\nin prior work where the so-called Minimum Necessary Information (MNI) point is\nnot attainable. We demonstrate that DisentangledSSL successfully learns shared\nand modality-specific features on multiple synthetic and real-world datasets\nand consistently outperforms baselines on various downstream tasks, including\nprediction tasks for vision-language data, as well as molecule-phenotype\nretrieval tasks for biological data. The code is available at\nhttps://github.com/uhlerlab/DisentangledSSL.\n","authors":["Chenyu Wang","Sharut Gupta","Xinyi Zhang","Sana Tonekaboni","Stefanie Jegelka","Tommi Jaakkola","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2410.23996v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13335v1","updated":"2025-03-17T16:15:02Z","published":"2025-03-17T16:15:02Z","title":"Reliable and Efficient Amortized Model-based Evaluation","summary":"  Comprehensive evaluations of language models (LM) during both development and\ndeployment phases are necessary because these models possess numerous\ncapabilities (e.g., mathematical reasoning, legal support, or medical\ndiagnostic) as well as safety risks (e.g., racial bias, toxicity, or\nmisinformation). The average score across a wide range of benchmarks provides a\nsignal that helps guide the use of these LMs in practice. Currently, holistic\nevaluations are costly due to the large volume of benchmark questions, making\nfrequent evaluations impractical. A popular attempt to lower the cost is to\ncompute the average score on a subset of the benchmark. This approach,\nunfortunately, often renders an unreliable measure of LM performance because\nthe average score is often confounded with the difficulty of the questions in\nthe benchmark subset. Item response theory (IRT) was designed to address this\nchallenge, providing a reliable measurement by careful controlling for question\ndifficulty. Unfortunately, question difficulty is expensive to estimate. Facing\nthis challenge, we train a model that predicts question difficulty from its\ncontent, enabling a reliable measurement at a fraction of the cost. In\naddition, we leverage this difficulty predictor to further improve the\nevaluation efficiency through training a question generator given a difficulty\nlevel. This question generator is essential in adaptive testing, where, instead\nof using a random subset of the benchmark questions, informative questions are\nadaptively chosen based on the current estimation of LLM performance.\nExperiments on 22 common natural language benchmarks and 172 LMs show that this\napproach is more reliable and efficient compared to current common practice.\n","authors":["Sang Truong","Yuheng Tu","Percy Liang","Bo Li","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2503.13335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13329v1","updated":"2025-03-17T16:07:56Z","published":"2025-03-17T16:07:56Z","title":"PERC: a suite of software tools for the curation of cryoEM data with\n  application to simulation, modelling and machine learning","summary":"  Ease of access to data, tools and models expedites scientific research. In\nstructural biology there are now numerous open repositories of experimental and\nsimulated datasets. Being able to easily access and utilise these is crucial\nfor allowing researchers to make optimal use of their research effort. The\ntools presented here are useful for collating existing public cryoEM datasets\nand/or creating new synthetic cryoEM datasets to aid the development of novel\ndata processing and interpretation algorithms. In recent years, structural\nbiology has seen the development of a multitude of machine-learning based\nalgorithms for aiding numerous steps in the processing and reconstruction of\nexperimental datasets and the use of these approaches has become widespread.\nDeveloping such techniques in structural biology requires access to large\ndatasets which can be cumbersome to curate and unwieldy to make use of. In this\npaper we present a suite of Python software packages which we collectively\nrefer to as PERC (profet, EMPIARreader and CAKED). These are designed to reduce\nthe burden which data curation places upon structural biology research. The\nprotein structure fetcher (profet) package allows users to conveniently\ndownload and cleave sequences or structures from the Protein Data Bank or\nAlphafold databases. EMPIARreader allows lazy loading of Electron Microscopy\nPublic Image Archive datasets in a machine-learning compatible structure. The\nClass Aggregator for Key Electron-microscopy Data (CAKED) package is designed\nto seamlessly facilitate the training of machine learning models on electron\nmicroscopy data, including electron-cryo-microscopy-specific data augmentation\nand labelling. These packages may be utilised independently or as building\nblocks in workflows. All are available in open source repositories and designed\nto be easily extensible to facilitate more advanced workflows if required.\n","authors":["Beatriz Costa-Gomes","Joel Greer","Nikolai Juraschko","James Parkhurst","Jola Mirecka","Marjan Famili","Camila Rangel-Smith","Oliver Strickson","Alan Lowe","Mark Basham","Tom Burnley"],"pdf_url":"https://arxiv.org/pdf/2503.13329v1.pdf","comment":"22 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.13322v1","updated":"2025-03-17T15:59:20Z","published":"2025-03-17T15:59:20Z","title":"SMPR: A structure-enhanced multimodal drug-disease prediction model for\n  drug repositioning and cold start","summary":"  Repositioning drug-disease relationships has always been a hot field of\nresearch. However, actual cases of biologically validated drug relocation\nremain very limited, and existing models have not yet fully utilized the\nstructural information of the drug. Furthermore, most repositioning models are\nonly used to complete the relationship matrix, and their practicality is poor\nwhen dealing with drug cold start problems. This paper proposes a\nstructure-enhanced multimodal relationship prediction model (SMRP). SMPR is\nbased on the SMILE structure of the drug, using the Mol2VEC method to generate\ndrug embedded representations, and learn disease embedded representations\nthrough heterogeneous network graph neural networks. Ultimately, a drug-disease\nrelationship matrix is constructed. In addition, to reduce the difficulty of\nusers' use, SMPR also provides a cold start interface based on structural\nsimilarity based on reposition results to simply and quickly predict\ndrug-related diseases. The repositioning ability and cold start capability of\nthe model are verified from multiple perspectives. While the AUC and ACUPR\nscores of repositioning reach 99% and 61% respectively, the AUC of cold start\nachieve 80%. In particular, the cold start Recall indicator can reach more than\n70%, which means that SMPR is more sensitive to positive samples. Finally, case\nanalysis is used to verify the practical value of the model and visual analysis\ndirectly demonstrates the improvement of the structure to the model. For quick\nuse, we also provide local deployment of the model and package it into an\nexecutable program.\n","authors":["Xin Dong","Rui Miao","Suyan Zhang","Shuaibing Jia","Leifeng Zhang","Yong Liang","Jianhua Zhang","Yi Zhun Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.13322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13317v1","updated":"2025-03-17T15:54:57Z","published":"2025-03-17T15:54:57Z","title":"Do you understand epistemic uncertainty? Think again! Rigorous\n  frequentist epistemic uncertainty estimation in regression","summary":"  Quantifying model uncertainty is critical for understanding prediction\nreliability, yet distinguishing between aleatoric and epistemic uncertainty\nremains challenging. We extend recent work from classification to regression to\nprovide a novel frequentist approach to epistemic and aleatoric uncertainty\nestimation. We train models to generate conditional predictions by feeding\ntheir initial output back as an additional input. This method allows for a\nrigorous measurement of model uncertainty by observing how prediction responses\nchange when conditioned on the model's previous answer. We provide a complete\ntheoretical framework to analyze epistemic uncertainty in regression in a\nfrequentist way, and explain how it can be exploited in practice to gauge a\nmodel's uncertainty, with minimal changes to the original architecture.\n","authors":["Enrico Foglia","Benjamin Bobbia","Nikita Durasov","Michael Bauerheim","Pascal Fua","Stephane Moreau","Thierry Jardin"],"pdf_url":"https://arxiv.org/pdf/2503.13317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13316v1","updated":"2025-03-17T15:54:20Z","published":"2025-03-17T15:54:20Z","title":"RainScaleGAN: a Conditional Generative Adversarial Network for Rainfall\n  Downscaling","summary":"  To this day, accurately simulating local-scale precipitation and reliably\nreproducing its distribution remains a challenging task. The limited horizontal\nresolution of Global Climate Models is among the primary factors undermining\ntheir skill in this context. The physical mechanisms driving the onset and\ndevelopment of precipitation, especially in extreme events, operate at\nspatio-temporal scales smaller than those numerically resolved, thus struggling\nto be captured accurately. In order to circumvent this limitation, several\ndownscaling approaches have been developed over the last decades to address the\ndiscrepancy between the spatial resolution of models output and the resolution\nrequired by local-scale applications. In this paper, we introduce RainScaleGAN,\na conditional deep convolutional Generative Adversarial Network (GAN) for\nprecipitation downscaling. GANs have been effectively used in image\nsuper-resolution, an approach highly relevant for downscaling tasks.\nRainScaleGAN's capabilities are tested in a perfect-model setup, where the\nspatial resolution of a precipitation dataset is artificially degraded from\n0.25$^{\\circ}\\times$0.25$^{\\circ}$ to 2$^{\\circ}\\times$2$^\\circ$, and\nRainScaleGAN is used to restore it. The developed model outperforms one of the\nleading precipitation downscaling method found in the literature. RainScaleGAN\nnot only generates a synthetic dataset featuring plausible high-resolution\nspatial patterns and intensities, but also produces a precipitation\ndistribution with statistics closely mirroring those of the ground-truth\ndataset. Given that RainScaleGAN's approach is agnostic with respect to the\nunderlying physics, the method has the potential to be applied to other\nphysical variables such as surface winds or temperature.\n","authors":["Marcello Iotti","Paolo Davini","Jost von Hardenberg","Giuseppe Zappa"],"pdf_url":"https://arxiv.org/pdf/2503.13316v1.pdf","comment":"38 pages, 16 figures"},{"id":"http://arxiv.org/abs/2411.16738v2","updated":"2025-03-17T15:50:58Z","published":"2024-11-23T15:36:03Z","title":"Classifier-Free Guidance inside the Attraction Basin May Cause\n  Memorization","summary":"  Diffusion models are prone to exactly reproduce images from the training\ndata. This exact reproduction of the training data is concerning as it can lead\nto copyright infringement and/or leakage of privacy-sensitive information. In\nthis paper, we present a novel perspective on the memorization phenomenon and\npropose a simple yet effective approach to mitigate it. We argue that\nmemorization occurs because of an attraction basin in the denoising process\nwhich steers the diffusion trajectory towards a memorized image. However, this\ncan be mitigated by guiding the diffusion trajectory away from the attraction\nbasin by not applying classifier-free guidance until an ideal transition point\noccurs from which classifier-free guidance is applied. This leads to the\ngeneration of non-memorized images that are high in image quality and\nwell-aligned with the conditioning mechanism. To further improve on this, we\npresent a new guidance technique, opposite guidance, that escapes the\nattraction basin sooner in the denoising process. We demonstrate the existence\nof attraction basins in various scenarios in which memorization occurs, and we\nshow that our proposed approach successfully mitigates memorization.\n","authors":["Anubhav Jain","Yuya Kobayashi","Takashi Shibuya","Yuhta Takida","Nasir Memon","Julian Togelius","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2411.16738v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13304v1","updated":"2025-03-17T15:47:26Z","published":"2025-03-17T15:47:26Z","title":"GFSNetwork: Differentiable Feature Selection via Gumbel-Sigmoid\n  Relaxation","summary":"  Feature selection in deep learning remains a critical challenge, particularly\nfor high-dimensional tabular data where interpretability and computational\nefficiency are paramount. We present GFSNetwork, a novel neural architecture\nthat performs differentiable feature selection through temperature-controlled\nGumbel-Sigmoid sampling. Unlike traditional methods, where the user has to\ndefine the requested number of features, GFSNetwork selects it automatically\nduring an end-to-end process. Moreover, GFSNetwork maintains constant\ncomputational overhead regardless of the number of input features. We evaluate\nGFSNetwork on a series of classification and regression benchmarks, where it\nconsistently outperforms recent methods including DeepLasso, attention maps, as\nwell as traditional feature selectors, while using significantly fewer\nfeatures. Furthermore, we validate our approach on real-world metagenomic\ndatasets, demonstrating its effectiveness in high-dimensional biological data.\nConcluding, our method provides a scalable solution that bridges the gap\nbetween neural network flexibility and traditional feature selection\ninterpretability. We share our python implementation of GFSNetwork at\nhttps://github.com/wwydmanski/GFSNetwork, as well as a PyPi package\n(gfs_network).\n","authors":["Witold Wydmański","Marek Śmieja"],"pdf_url":"https://arxiv.org/pdf/2503.13304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05483v2","updated":"2025-03-17T15:43:14Z","published":"2024-11-08T11:21:31Z","title":"The Limits of Differential Privacy in Online Learning","summary":"  Differential privacy (DP) is a formal notion that restricts the privacy\nleakage of an algorithm when running on sensitive data, in which\nprivacy-utility trade-off is one of the central problems in private data\nanalysis. In this work, we investigate the fundamental limits of differential\nprivacy in online learning algorithms and present evidence that separates three\ntypes of constraints: no DP, pure DP, and approximate DP. We first describe a\nhypothesis class that is online learnable under approximate DP but not online\nlearnable under pure DP under the adaptive adversarial setting. This indicates\nthat approximate DP must be adopted when dealing with adaptive adversaries. We\nthen prove that any private online learner must make an infinite number of\nmistakes for almost all hypothesis classes. This essentially generalizes\nprevious results and shows a strong separation between private and non-private\nsettings since a finite mistake bound is always attainable (as long as the\nclass is online learnable) when there is no privacy requirement.\n","authors":["Bo Li","Wei Wang","Peng Ye"],"pdf_url":"https://arxiv.org/pdf/2411.05483v2.pdf","comment":"Correct some typos and add some comments"},{"id":"http://arxiv.org/abs/2503.13296v1","updated":"2025-03-17T15:41:39Z","published":"2025-03-17T15:41:39Z","title":"On Local Posterior Structure in Deep Ensembles","summary":"  Bayesian Neural Networks (BNNs) often improve model calibration and\npredictive uncertainty quantification compared to point estimators such as\nmaximum-a-posteriori (MAP). Similarly, deep ensembles (DEs) are also known to\nimprove calibration, and therefore, it is natural to hypothesize that deep\nensembles of BNNs (DE-BNNs) should provide even further improvements. In this\nwork, we systematically investigate this across a number of datasets, neural\nnetwork architectures, and BNN approximation methods and surprisingly find that\nwhen the ensembles grow large enough, DEs consistently outperform DE-BNNs on\nin-distribution data. To shine light on this observation, we conduct several\nsensitivity and ablation studies. Moreover, we show that even though DE-BNNs\noutperform DEs on out-of-distribution metrics, this comes at the cost of\ndecreased in-distribution performance. As a final contribution, we open-source\nthe large pool of trained models to facilitate further research on this topic.\n","authors":["Mikkel Jordahn","Jonas Vestergaard Jensen","Mikkel N. Schmidt","Michael Riis Andersen"],"pdf_url":"https://arxiv.org/pdf/2503.13296v1.pdf","comment":"Code and models available at\n  https://github.com/jonasvj/OnLocalPosteriorStructureInDeepEnsembles"},{"id":"http://arxiv.org/abs/2503.13288v1","updated":"2025-03-17T15:38:33Z","published":"2025-03-17T15:38:33Z","title":"$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time\n  Exploration and Exploitation","summary":"  Inference-time optimization scales computation to derive deliberate reasoning\nsteps for effective performance. While previous search-based strategies address\nthe short-sightedness of auto-regressive generation, the vast search space\nleads to excessive exploration and insufficient exploitation. To strike an\nefficient balance to derive the optimal step, we frame the decoding strategy as\nforesight sampling, leveraging simulated future steps to obtain globally\noptimal step estimation. Built on it, we propose a novel decoding strategy,\nnamed $\\phi$-Decoding. To provide a precise and expressive estimation of step\nvalue, $\\phi$-Decoding approximates two distributions via foresight and\nclustering. Sampling from the joint distribution, the optimal steps can be\nselected for exploitation. To support adaptive computation allocation, we\npropose in-width and in-depth pruning strategies, featuring a light-weight\nsolution to achieve inference efficiency. Extensive experiments across seven\nbenchmarks show $\\phi$-Decoding outperforms strong baselines in both\nperformance and efficiency. Additional analysis demonstrates its generalization\nacross various LLMs and scalability across a wide range of computing budgets.\nThe code will be released at https://github.com/xufangzhi/phi-Decoding, and the\nopen-source PyPI package is coming soon.\n","authors":["Fangzhi Xu","Hang Yan","Chang Ma","Haiteng Zhao","Jun Liu","Qika Lin","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2503.13288v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.07658v2","updated":"2025-03-17T15:37:35Z","published":"2024-12-10T16:45:03Z","title":"TraSCE: Trajectory Steering for Concept Erasure","summary":"  Recent advancements in text-to-image diffusion models have brought them to\nthe public spotlight, becoming widely accessible and embraced by everyday\nusers. However, these models have been shown to generate harmful content such\nas not-safe-for-work (NSFW) images. While approaches have been proposed to\nerase such abstract concepts from the models, jail-breaking techniques have\nsucceeded in bypassing such safety measures. In this paper, we propose TraSCE,\nan approach to guide the diffusion trajectory away from generating harmful\ncontent. Our approach is based on negative prompting, but as we show in this\npaper, a widely used negative prompting strategy is not a complete solution and\ncan easily be bypassed in some corner cases. To address this issue, we first\npropose using a specific formulation of negative prompting instead of the\nwidely used one. Furthermore, we introduce a localized loss-based guidance that\nenhances the modified negative prompting technique by steering the diffusion\ntrajectory. We demonstrate that our proposed method achieves state-of-the-art\nresults on various benchmarks in removing harmful content, including ones\nproposed by red teams, and erasing artistic styles and objects. Our proposed\napproach does not require any training, weight modifications, or training data\n(either image or prompt), making it easier for model owners to erase new\nconcepts.\n","authors":["Anubhav Jain","Yuya Kobayashi","Takashi Shibuya","Yuhta Takida","Nasir Memon","Julian Togelius","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2412.07658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13281v1","updated":"2025-03-17T15:31:55Z","published":"2025-03-17T15:31:55Z","title":"LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation","summary":"  Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets, n2c2, SIGIR, TREC 2021, and TREC 2022, using open-source models,\ncomparing it against TrialGPT, Zero-Shot, and GPT-4-based closed models.\nLLM-Match outperformed all baselines.\n","authors":["Xiaodi Li","Shaika Chowdhury","Chung Il Wi","Maria Vassilaki","Ken Liu","Terence T Sio","Owen Garrick","Young J Juhn","James R Cerhan","Cui Tao","Nansu Zong"],"pdf_url":"https://arxiv.org/pdf/2503.13281v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2503.13271v1","updated":"2025-03-17T15:23:21Z","published":"2025-03-17T15:23:21Z","title":"Graph Generative Models Evaluation with Masked Autoencoder","summary":"  In recent years, numerous graph generative models (GGMs) have been proposed.\nHowever, evaluating these models remains a considerable challenge, primarily\ndue to the difficulty in extracting meaningful graph features that accurately\nrepresent real-world graphs. The traditional evaluation techniques, which rely\non graph statistical properties like node degree distribution, clustering\ncoefficients, or Laplacian spectrum, overlook node features and lack\nscalability. There are newly proposed deep learning-based methods employing\ngraph random neural networks or contrastive learning to extract graph features,\ndemonstrating superior performance compared to traditional statistical methods,\nbut their experimental results also demonstrate that these methods do not\nalways working well across different metrics. Although there are overlaps among\nthese metrics, they are generally not interchangeable, each evaluating\ngenerative models from a different perspective. In this paper, we propose a\nnovel method that leverages graph masked autoencoders to effectively extract\ngraph features for GGM evaluations. We conduct extensive experiments on graphs\nand empirically demonstrate that our method can be more reliable and effective\nthan previously proposed methods across a number of GGM evaluation metrics,\nsuch as \"Fr\\'echet Distance (FD)\" and \"MMD Linear\". However, no single method\nstands out consistently across all metrics and datasets. Therefore, this study\nalso aims to raise awareness of the significance and challenges associated with\nGGM evaluation techniques, especially in light of recent advances in generative\nmodels.\n","authors":["Chengen Wang","Murat Kantarcioglu"],"pdf_url":"https://arxiv.org/pdf/2503.13271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10345v2","updated":"2025-03-17T15:16:47Z","published":"2025-03-13T13:23:43Z","title":"Mirror Online Conformal Prediction with Intermittent Feedback","summary":"  Online conformal prediction enables the runtime calibration of a pre-trained\nartificial intelligence model using feedback on its performance. Calibration is\nachieved through set predictions that are updated via online rules so as to\nensure long-term coverage guarantees. While recent research has demonstrated\nthe benefits of incorporating prior knowledge into the calibration process,\nthis has come at the cost of replacing coverage guarantees with less tangible\nregret guarantees based on the quantile loss. This work introduces intermittent\nmirror online conformal prediction (IM-OCP), a novel runtime calibration\nframework that integrates prior knowledge, while maintaining long-term coverage\nand achieving sub-linear regret. IM-OCP features closed-form updates with\nminimal memory complexity, and is designed to operate under potentially\nintermittent feedback.\n","authors":["Bowen Wang","Matteo Zecchin","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2503.10345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08597v2","updated":"2025-03-17T15:04:06Z","published":"2024-07-11T15:25:02Z","title":"Learning Program Behavioral Models from Synthesized Input-Output Pairs","summary":"  We introduce Modelizer - a novel framework that, given a black-box program,\nlearns a model from its input/output behavior using neural machine translation\nalgorithms. The resulting model mocks the original program: Given an input, the\nmodel predicts the output that would have been produced by the program.\nHowever, the model is also reversible - that is, the model can predict the\ninput that would have produced a given output. Finally, the model is\ndifferentiable and can be efficiently restricted to predict only a certain\naspect of the program behavior. Modelizer uses grammars to synthesize and\ninputs and unsupervised tokenizers to decompose the resulting outputs, allowing\nit to learn sequence-to-sequence associations between token streams. Other than\ninput grammars, Modelizer only requires the ability to execute the program. The\nresulting models are small, requiring fewer than 6.3 million parameters for\nlanguages such as Markdown or HTML; and they are accurate, achieving up to\n95.4% accuracy and a BLEU score of 0.98 with standard error 0.04 in mocking\nreal-world applications. As it learns from and predicts executions rather than\ncode, Modelizer departs from the LLM-centric research trend, opening new\nopportunities for program-specific models that are fully tuned towards\nindividual programs. Indeed, we foresee several applications of these models,\nespecially as the output of the program can be any aspect of program behavior.\nBeyond mocking and predicting program behavior, the models can also synthesize\ninputs that are likely to produce a particular behavior, such as failures or\ncoverage, thus assisting in program understanding and maintenance.\n","authors":["Tural Mammadov","Dietrich Klakow","Alexander Koller","Andreas Zeller"],"pdf_url":"https://arxiv.org/pdf/2407.08597v2.pdf","comment":"42 pages, 9 figures, 12 tables"},{"id":"http://arxiv.org/abs/2503.13248v1","updated":"2025-03-17T15:01:26Z","published":"2025-03-17T15:01:26Z","title":"Neural network-based Godunov corrections for approximate Riemann solvers\n  using bi-fidelity learning","summary":"  The Riemann problem is fundamental in the computational modeling of\nhyperbolic partial differential equations, enabling the development of stable\nand accurate upwind schemes. While exact solvers provide robust upwinding\nfluxes, their high computational cost necessitates approximate solvers.\nAlthough approximate solvers achieve accuracy in many scenarios, they produce\ninaccurate solutions in certain cases. To overcome this limitation, we propose\nconstructing neural network-based surrogate models, trained using supervised\nlearning, designed to map interior and exterior conservative state variables to\nthe corresponding exact flux. Specifically, we propose two distinct approaches:\none utilizing a vanilla neural network and the other employing a bi-fidelity\nneural network. The performance of the proposed approaches is demonstrated\nthrough applications to one-dimensional and two-dimensional partial\ndifferential equations, showcasing their robustness and accuracy.\n","authors":["Akshay Thakur","Matthew J. Zahr"],"pdf_url":"https://arxiv.org/pdf/2503.13248v1.pdf","comment":"22 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.13246v1","updated":"2025-03-17T14:58:22Z","published":"2025-03-17T14:58:22Z","title":"Highly Efficient Direct Analytics on Semantic-aware Time Series Data\n  Compression","summary":"  Semantic communication has emerged as a promising paradigm to tackle the\nchallenges of massive growing data traffic and sustainable data communication.\nIt shifts the focus from data fidelity to goal-oriented or task-oriented\nsemantic transmission. While deep learning-based methods are commonly used for\nsemantic encoding and decoding, they struggle with the sequential nature of\ntime series data and high computation cost, particularly in\nresource-constrained IoT environments. Data compression plays a crucial role in\nreducing transmission and storage costs, yet traditional data compression\nmethods fall short of the demands of goal-oriented communication systems. In\nthis paper, we propose a novel method for direct analytics on time series data\ncompressed by the SHRINK compression algorithm. Through experimentation using\noutlier detection as a case study, we show that our method outperforms\nbaselines running on uncompressed data in multiple cases, with merely 1%\ndifference in the worst case. Additionally, it achieves four times lower\nruntime on average and accesses approximately 10% of the data volume, which\nenables edge analytics with limited storage and computation power. These\nresults demonstrate that our approach offers reliable, high-speed outlier\ndetection analytics for diverse IoT applications while extracting semantics\nfrom time-series data, achieving high compression, and reducing data\ntransmission.\n","authors":["Guoyou Sun","Panagiotis Karras","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01109v2","updated":"2025-03-17T14:57:22Z","published":"2024-05-02T09:17:32Z","title":"Hypergraph $p$-Laplacian regularization on point clouds for data\n  interpolation","summary":"  As a generalization of graphs, hypergraphs are widely used to model\nhigher-order relations in data. This paper explores the benefit of the\nhypergraph structure for the interpolation of point cloud data that contain no\nexplicit structural information. We define the $\\varepsilon_n$-ball hypergraph\nand the $k_n$-nearest neighbor hypergraph on a point cloud and study the\n$p$-Laplacian regularization on the hypergraphs. We prove the variational\nconsistency between the hypergraph $p$-Laplacian regularization and the\ncontinuum $p$-Laplacian regularization in a semisupervised setting when the\nnumber of points $n$ goes to infinity while the number of labeled points\nremains fixed. A key improvement compared to the graph case is that the results\nrely on weaker assumptions on the upper bound of $\\varepsilon_n$ and $k_n$. To\nsolve the convex but non-differentiable large-scale optimization problem, we\nutilize the stochastic primal-dual hybrid gradient algorithm. Numerical\nexperiments on data interpolation verify that the hypergraph $p$-Laplacian\nregularization outperforms the graph $p$-Laplacian regularization in preventing\nthe development of spikes at the labeled points.\n","authors":["Kehan Shi","Martin Burger"],"pdf_url":"https://arxiv.org/pdf/2405.01109v2.pdf","comment":"34 pages"},{"id":"http://arxiv.org/abs/2411.07107v2","updated":"2025-03-17T14:51:27Z","published":"2024-11-11T16:33:25Z","title":"Training Neural Networks as Recognizers of Formal Languages","summary":"  Characterizing the computational power of neural network architectures in\nterms of formal language theory remains a crucial line of research, as it\ndescribes lower and upper bounds on the reasoning capabilities of modern AI.\nHowever, when empirically testing these bounds, existing work often leaves a\ndiscrepancy between experiments and the formal claims they are meant to\nsupport. The problem is that formal language theory pertains specifically to\nrecognizers: machines that receive a string as input and classify whether it\nbelongs to a language. On the other hand, it is common instead to evaluate\nlanguage models on proxy tasks, e.g., language modeling or sequence-to-sequence\ntransduction, that are similar in only an informal sense to the underlying\ntheory. We correct this mismatch by training and evaluating neural networks\ndirectly as binary classifiers of strings, using a general method that can be\napplied to a wide variety of languages. As part of this, we extend an algorithm\nrecently proposed by Sn{\\ae}bjarnarson et al. (2025) for efficient\nlength-controlled sampling of strings from regular languages. We provide\nresults on a variety of languages across the Chomsky hierarchy for three neural\narchitectures: a simple RNN, an LSTM, and a causally-masked transformer. We\nfind that the RNN and LSTM often outperform the transformer, and that auxiliary\ntraining objectives such as language modeling can help, although no single\nobjective uniformly improves performance across languages and architectures.\nOur contributions will facilitate theoretically sound empirical testing of\nlanguage recognition claims in future work. We have released our datasets as a\nbenchmark called FLaRe (Formal Language Recognition), along with our code.\n","authors":["Alexandra Butoi","Ghazal Khalighinejad","Anej Svete","Josef Valvoda","Ryan Cotterell","Brian DuSell"],"pdf_url":"https://arxiv.org/pdf/2411.07107v2.pdf","comment":"44 pages, 3 figures. ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13236v1","updated":"2025-03-17T14:48:57Z","published":"2025-03-17T14:48:57Z","title":"Gradient Extrapolation for Debiased Representation Learning","summary":"  Machine learning classification models trained with empirical risk\nminimization (ERM) often inadvertently rely on spurious correlations. When\nabsent in the test data, these unintended associations between non-target\nattributes and target labels lead to poor generalization. This paper addresses\nthis problem from a model optimization perspective and proposes a novel method,\nGradient Extrapolation for Debiased Representation Learning (GERNE), designed\nto learn debiased representations in both known and unknown attribute training\ncases. GERNE uses two distinct batches with different amounts of spurious\ncorrelations to define the target gradient as the linear extrapolation of two\ngradients computed from each batch's loss. It is demonstrated that the\nextrapolated gradient, if directed toward the gradient of the batch with fewer\namount of spurious correlation, can guide the training process toward learning\na debiased model. GERNE can serve as a general framework for debiasing with\nmethods, such as ERM, reweighting, and resampling, being shown as special\ncases. The theoretical upper and lower bounds of the extrapolation factor are\nderived to ensure convergence. By adjusting this factor, GERNE can be adapted\nto maximize the Group-Balanced Accuracy (GBA) or the Worst-Group Accuracy. The\nproposed approach is validated on five vision and one NLP benchmarks,\ndemonstrating competitive and often superior performance compared to\nstate-of-the-art baseline methods.\n","authors":["Ihab Asaad","Maha Shadaydeh","Joachim Denzler"],"pdf_url":"https://arxiv.org/pdf/2503.13236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13227v1","updated":"2025-03-17T14:41:51Z","published":"2025-03-17T14:41:51Z","title":"Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised\n  Learning Across Pseudo-Mismatch","summary":"  Federated Semi-Supervised Learning (FSSL) aims to leverage unlabeled data\nacross clients with limited labeled data to train a global model with strong\ngeneralization ability. Most FSSL methods rely on consistency regularization\nwith pseudo-labels, converting predictions from local or global models into\nhard pseudo-labels as supervisory signals. However, we discover that the\nquality of pseudo-label is largely deteriorated by data heterogeneity, an\nintrinsic facet of federated learning. In this paper, we study the problem of\nFSSL in-depth and show that (1) heterogeneity exacerbates pseudo-label\nmismatches, further degrading model performance and convergence, and (2) local\nand global models' predictive tendencies diverge as heterogeneity increases.\nMotivated by these findings, we propose a simple and effective method called\nSemi-supervised Aggregation for Globally-Enhanced Ensemble (SAGE), that can\nflexibly correct pseudo-labels based on confidence discrepancies. This strategy\neffectively mitigates performance degradation caused by incorrect pseudo-labels\nand enhances consensus between local and global models. Experimental results\ndemonstrate that SAGE outperforms existing FSSL methods in both performance and\nconvergence. Our code is available at https://github.com/Jay-Codeman/SAGE\n","authors":["Yijie Liu","Xinyi Shang","Yiqun Zhang","Yang Lu","Chen Gong","Jing-Hao Xue","Hanzi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13227v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.13224v1","updated":"2025-03-17T14:37:42Z","published":"2025-03-17T14:37:42Z","title":"ProDiF: Protecting Domain-Invariant Features to Secure Pre-Trained\n  Models Against Extraction","summary":"  Pre-trained models are valuable intellectual property, capturing both\ndomain-specific and domain-invariant features within their weight spaces.\nHowever, model extraction attacks threaten these assets by enabling\nunauthorized source-domain inference and facilitating cross-domain transfer via\nthe exploitation of domain-invariant features. In this work, we introduce\n**ProDiF**, a novel framework that leverages targeted weight space manipulation\nto secure pre-trained models against extraction attacks. **ProDiF** quantifies\nthe transferability of filters and perturbs the weights of critical filters in\nunsecured memory, while preserving actual critical weights in a Trusted\nExecution Environment (TEE) for authorized users. A bi-level optimization\nfurther ensures resilience against adaptive fine-tuning attacks. Experimental\nresults show that **ProDiF** reduces source-domain accuracy to near-random\nlevels and decreases cross-domain transferability by 74.65\\%, providing robust\nprotection for pre-trained models. This work offers comprehensive protection\nfor pre-trained DNN models and highlights the potential of weight space\nmanipulation as a novel approach to model security.\n","authors":["Tong Zhou","Shijin Duan","Gaowen Liu","Charles Fleming","Ramana Rao Kompella","Shaolei Ren","Xiaolin Xu"],"pdf_url":"https://arxiv.org/pdf/2503.13224v1.pdf","comment":"Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025"},{"id":"http://arxiv.org/abs/2503.13217v1","updated":"2025-03-17T14:28:08Z","published":"2025-03-17T14:28:08Z","title":"Dense Policy: Bidirectional Autoregressive Learning of Actions","summary":"  Mainstream visuomotor policies predominantly rely on generative models for\nholistic action prediction, while current autoregressive policies, predicting\nthe next token or chunk, have shown suboptimal results. This motivates a search\nfor more effective learning methods to unleash the potential of autoregressive\npolicies for robotic manipulation. This paper introduces a bidirectionally\nexpanded learning approach, termed Dense Policy, to establish a new paradigm\nfor autoregressive policies in action prediction. It employs a lightweight\nencoder-only architecture to iteratively unfold the action sequence from an\ninitial single frame into the target sequence in a coarse-to-fine manner with\nlogarithmic-time inference. Extensive experiments validate that our dense\npolicy has superior autoregressive learning capabilities and can surpass\nexisting holistic generative policies. Our policy, example data, and training\ncode will be publicly available upon publication. Project page: https:\n//selen-suyue.github.io/DspNet/.\n","authors":["Yue Su","Xinyu Zhan","Hongjie Fang","Han Xue","Hao-Shu Fang","Yong-Lu Li","Cewu Lu","Lixin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.13217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11601v3","updated":"2025-03-17T14:26:33Z","published":"2024-06-17T14:52:21Z","title":"Standardizing Structural Causal Models","summary":"  Synthetic datasets generated by structural causal models (SCMs) are commonly\nused for benchmarking causal structure learning algorithms. However, the\nvariances and pairwise correlations in SCM data tend to increase along the\ncausal ordering. Several popular algorithms exploit these artifacts, possibly\nleading to conclusions that do not generalize to real-world settings. Existing\nmetrics like $\\operatorname{Var}$-sortability and\n$\\operatorname{R^2}$-sortability quantify these patterns, but they do not\nprovide tools to remedy them. To address this, we propose\ninternally-standardized structural causal models (iSCMs), a modification of\nSCMs that introduces a standardization operation at each variable during the\ngenerative process. By construction, iSCMs are not\n$\\operatorname{Var}$-sortable. We also find empirical evidence that they are\nmostly not $\\operatorname{R^2}$-sortable for commonly-used graph families.\nMoreover, contrary to the post-hoc standardization of data generated by\nstandard SCMs, we prove that linear iSCMs are less identifiable from prior\nknowledge on the weights and do not collapse to deterministic relationships in\nlarge systems, which may make iSCMs a useful model in causal inference beyond\nthe benchmarking problem studied here. Our code is publicly available at:\nhttps://github.com/werkaaa/iscm.\n","authors":["Weronika Ormaniec","Scott Sussex","Lars Lorch","Bernhard Schölkopf","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2406.11601v3.pdf","comment":"Added additional benchmarks, including PC algorithm, GES, GOLEM.\n  Evaluated Var-sortability and R2-sortability of the heuristics for mitigating\n  variance accumulation"},{"id":"http://arxiv.org/abs/2503.13212v1","updated":"2025-03-17T14:23:04Z","published":"2025-03-17T14:23:04Z","title":"MAME: Multidimensional Adaptive Metamer Exploration with Human\n  Perceptual Feedback","summary":"  Alignment between human brain networks and artificial models is actively\nstudied in machine learning and neuroscience. A widely adopted approach to\nexplore their functional alignment is to identify metamers for both humans and\nmodels. Metamers refer to input stimuli that are physically different but\nequivalent within a given system. If a model's metameric space completely\nmatched the human metameric space, the model would achieve functional alignment\nwith humans. However, conventional methods lack direct ways to search for human\nmetamers. Instead, researchers first develop biologically inspired models and\nthen infer about human metamers indirectly by testing whether model metamers\nalso appear as metamers to humans. Here, we propose the Multidimensional\nAdaptive Metamer Exploration (MAME) framework, enabling direct high-dimensional\nexploration of human metameric space. MAME leverages online image generation\nguided by human perceptual feedback. Specifically, it modulates reference\nimages across multiple dimensions by leveraging hierarchical responses from\nconvolutional neural networks (CNNs). Generated images are presented to\nparticipants whose perceptual discriminability is assessed in a behavioral\ntask. Based on participants' responses, subsequent image generation parameters\nare adaptively updated online. Using our MAME framework, we successfully\nmeasured a human metameric space of over fifty dimensions within a single\nexperiment. Experimental results showed that human discrimination sensitivity\nwas lower for metameric images based on low-level features compared to\nhigh-level features, which image contrast metrics could not explain. The\nfinding suggests that the model computes low-level information not essential\nfor human perception. Our framework has the potential to contribute to\ndeveloping interpretable AI and understanding of brain function in\nneuroscience.\n","authors":["Mina Kamao","Hayato Ono","Ayumu Yamashita","Kaoru Amano","Masataka Sawayama"],"pdf_url":"https://arxiv.org/pdf/2503.13212v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.02500v3","updated":"2025-03-17T14:18:42Z","published":"2024-06-04T17:18:40Z","title":"Towards Efficient Mixture of Experts: A Holistic Study of Compression\n  Techniques","summary":"  Scaling large language models has driven remarkable advancements across\nvarious domains, yet the continual increase in model size presents significant\nchallenges for real-world deployment. The Mixture of Experts (MoE) architecture\noffers a promising solution by dynamically selecting and activating only a\nsubset of experts during inference, thus substantially reducing computational\ncosts while preserving high performance. Despite these benefits, MoE introduces\nnew inefficiencies, such as excessive parameters and communication overhead. In\nthis work, we present a holistic study of compression techniques for Mixture of\nExperts to enhance both efficiency and scalability. While recent efforts have\nfocused on Expert Trimming, which reduces the number of experts, these\napproaches still suffer from considerable communication and computational\ncosts. To address this, we propose more aggressive strategies, such as Layer\nDrop, which removes entire MoE layers, and Block Drop, which eliminates\ntransformer blocks. Surprisingly, these aggressive pruning techniques not only\npreserve model performance but also substantially improve computation and\nmemory efficiency. Furthermore, beyond Expert Trimming, we also introduce\nExpert Slimming, which compresses individual experts to further boost\nperformance and can be seamlessly integrated with Expert Trimming. Extensive\nexperimental results demonstrate the effectiveness of our proposed\nmethods-Layer Drop and Block Drop-along with the comprehensive recipe that\nintegrates Expert Slimming and Expert Trimming, achieving a 6.05x speedup with\n77.1% reduced memory usage while maintaining over 92% of performance on\nMixtral-8x7B. Our code is released at\nhttps://github.com/CASE-Lab-UMD/Unified-MoE-Compression.\n","authors":["Shwai He","Daize Dong","Liang Ding","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2406.02500v3.pdf","comment":"Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2402.12265v3","updated":"2025-03-17T14:08:19Z","published":"2024-02-19T16:26:40Z","title":"On the Byzantine-Resilience of Distillation-Based Federated Learning","summary":"  Federated Learning (FL) algorithms using Knowledge Distillation (KD) have\nreceived increasing attention due to their favorable properties with respect to\nprivacy, non-i.i.d. data and communication cost. These methods depart from\ntransmitting model parameters and instead communicate information about a\nlearning task by sharing predictions on a public dataset. In this work, we\nstudy the performance of such approaches in the byzantine setting, where a\nsubset of the clients act in an adversarial manner aiming to disrupt the\nlearning process. We show that KD-based FL algorithms are remarkably resilient\nand analyze how byzantine clients can influence the learning process. Based on\nthese insights, we introduce two new byzantine attacks and demonstrate their\nability to break existing byzantine-resilient methods. Additionally, we propose\na novel defence method which enhances the byzantine resilience of KD-based FL\nalgorithms. Finally, we provide a general framework to obfuscate attacks,\nmaking them significantly harder to detect, thereby improving their\neffectiveness. Our findings serve as an important building block in the\nanalysis of byzantine FL, contributing through the development of new attacks\nand new defence mechanisms, further advancing the robustness of KD-based FL\nalgorithms.\n","authors":["Christophe Roux","Max Zimmer","Sebastian Pokutta"],"pdf_url":"https://arxiv.org/pdf/2402.12265v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13200v1","updated":"2025-03-17T14:07:58Z","published":"2025-03-17T14:07:58Z","title":"Timing the Match: A Deep Reinforcement Learning Approach for\n  Ride-Hailing and Ride-Pooling Services","summary":"  Efficient timing in ride-matching is crucial for improving the performance of\nride-hailing and ride-pooling services, as it determines the number of drivers\nand passengers considered in each matching process. Traditional batched\nmatching methods often use fixed time intervals to accumulate ride requests\nbefore assigning matches. While this approach increases the number of available\ndrivers and passengers for matching, it fails to adapt to real-time\nsupply-demand fluctuations, often leading to longer passenger wait times and\ndriver idle periods. To address this limitation, we propose an adaptive\nride-matching strategy using deep reinforcement learning (RL) to dynamically\ndetermine when to perform matches based on real-time system conditions. Unlike\nfixed-interval approaches, our method continuously evaluates system states and\nexecutes matching at moments that minimize total passenger wait time.\nAdditionally, we incorporate a potential-based reward shaping (PBRS) mechanism\nto mitigate sparse rewards, accelerating RL training and improving decision\nquality. Extensive empirical evaluations using a realistic simulator trained on\nreal-world data demonstrate that our approach outperforms fixed-interval\nmatching strategies, significantly reducing passenger waiting times and detour\ndelays, thereby enhancing the overall efficiency of ride-hailing and\nride-pooling systems.\n","authors":["Yiman Bao","Jie Gao","Jinke He","Frans A. Oliehoek","Oded Cats"],"pdf_url":"https://arxiv.org/pdf/2503.13200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13195v1","updated":"2025-03-17T14:04:48Z","published":"2025-03-17T14:04:48Z","title":"Deep Learning Advancements in Anomaly Detection: A Comprehensive Survey","summary":"  The rapid expansion of data from diverse sources has made anomaly detection\n(AD) increasingly essential for identifying unexpected observations that may\nsignal system failures, security breaches, or fraud. As datasets become more\ncomplex and high-dimensional, traditional detection methods struggle to\neffectively capture intricate patterns. Advances in deep learning have made AD\nmethods more powerful and adaptable, improving their ability to handle\nhigh-dimensional and unstructured data. This survey provides a comprehensive\nreview of over 180 recent studies, focusing on deep learning-based AD\ntechniques. We categorize and analyze these methods into reconstruction-based\nand prediction-based approaches, highlighting their effectiveness in modeling\ncomplex data distributions. Additionally, we explore the integration of\ntraditional and deep learning methods, highlighting how hybrid approaches\ncombine the interpretability of traditional techniques with the flexibility of\ndeep learning to enhance detection accuracy and model transparency. Finally, we\nidentify open issues and propose future research directions to advance the\nfield of AD. This review bridges gaps in existing literature and serves as a\nvaluable resource for researchers and practitioners seeking to enhance AD\ntechniques using deep learning.\n","authors":["Haoqi Huang","Ping Wang","Jianhua Pei","Jiacheng Wang","Shahen Alexanian","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2503.13195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13194v1","updated":"2025-03-17T14:04:27Z","published":"2025-03-17T14:04:27Z","title":"A representational framework for learning and encoding structurally\n  enriched trajectories in complex agent environments","summary":"  The ability of artificial intelligence agents to make optimal decisions and\ngeneralise them to different domains and tasks is compromised in complex\nscenarios. One way to address this issue has focused on learning efficient\nrepresentations of the world and on how the actions of agents affect them, such\nas disentangled representations that exploit symmetries. Whereas such\nrepresentations are procedurally efficient, they are based on the compression\nof low-level state-action transitions, which lack structural richness. To\naddress this problem, we propose to enrich the agent's ontology and extend the\ntraditional conceptualisation of trajectories to provide a more nuanced view of\ntask execution. Structurally Enriched Trajectories (SETs) extend the encoding\nof sequences of states and their transitions by incorporating hierarchical\nrelations between objects, interactions and affordances. SETs are built as\nmulti-level graphs, providing a detailed representation of the agent dynamics\nand a transferable functional abstraction of the task. SETs are integrated into\nan architecture, Structurally Enriched Trajectory Learning and Encoding\n(SETLE), that employs a heterogeneous graph-based memory structure of\nmulti-level relational dependencies essential for generalisation. Using\nreinforcement learning as a data generation tool, we demonstrate that SETLE can\nsupport downstream tasks, enabling agents to recognise task-relevant structural\npatterns across diverse environments.\n","authors":["Corina Catarau-Cotutiu","Esther Mondragon","Eduardo Alonso"],"pdf_url":"https://arxiv.org/pdf/2503.13194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13180v1","updated":"2025-03-17T13:54:27Z","published":"2025-03-17T13:54:27Z","title":"GC-Fed: Gradient Centralized Federated Learning with Partial Client\n  Participation","summary":"  Multi-source information fusion (MSIF) leverages diverse data streams to\nenhance decision-making, situational awareness, and system resilience.\nFederated Learning (FL) enables MSIF while preserving privacy but suffers from\nclient drift under high data heterogeneity, leading to performance degradation.\nTraditional mitigation strategies rely on reference-based gradient adjustments,\nwhich can be unstable in partial participation settings. To address this, we\npropose Gradient Centralized Federated Learning (GC-Fed), a reference-free\ngradient correction method inspired by Gradient Centralization (GC). We\nintroduce Local GC and Global GC, applying GC during local training and global\naggregation, respectively. Our hybrid GC-Fed approach selectively applies GC at\nthe feature extraction layer locally and at the classifier layer globally,\nimproving training stability and model performance. Theoretical analysis and\nempirical results demonstrate that GC-Fed mitigates client drift and achieves\nstate-of-the-art accuracy gains of up to 20% in heterogeneous settings.\n","authors":["Jungwon Seo","Ferhat Ozgur Catak","Chunming Rong","Kibeom Hong","Minhoe Kim"],"pdf_url":"https://arxiv.org/pdf/2503.13180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13178v1","updated":"2025-03-17T13:53:57Z","published":"2025-03-17T13:53:57Z","title":"Rapfi: Distilling Efficient Neural Network for the Game of Gomoku","summary":"  Games have played a pivotal role in advancing artificial intelligence, with\nAI agents using sophisticated techniques to compete. Despite the success of\nneural network based game AIs, their performance often requires significant\ncomputational resources. In this paper, we present Rapfi, an efficient Gomoku\nagent that outperforms CNN-based agents in limited computation environments.\nRapfi leverages a compact neural network with a pattern-based codebook\ndistilled from CNNs, and an incremental update scheme that minimizes\ncomputation when input changes are minor. This new network uses computation\nthat is orders of magnitude less to reach a similar accuracy of much larger\nneural networks such as Resnet. Thanks to our incremental update scheme,\ndepth-first search methods such as the alpha-beta search can be significantly\naccelerated. With a carefully tuned evaluation and search, Rapfi reached\nstrength surpassing Katagomo, the strongest open-source Gomoku AI based on\nAlphaZero's algorithm, under limited computational resources where accelerators\nlike GPUs are absent. Rapfi ranked first among 520 Gomoku agents on Botzone and\nwon the championship in GomoCup 2024.\n","authors":["Zhanggen Jin","Haobin Duan","Zhiyang Hang"],"pdf_url":"https://arxiv.org/pdf/2503.13178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07638v2","updated":"2025-03-17T13:52:26Z","published":"2025-03-05T08:19:17Z","title":"Leveraging Taxonomy Similarity for Next Activity Prediction in Patient\n  Treatment","summary":"  The rapid progress in modern medicine presents physicians with complex\nchallenges when planning patient treatment. Techniques from the field of\nPredictive Business Process Monitoring, like Next-activity-prediction (NAP) can\nbe used as a promising technique to support physicians in treatment planning,\nby proposing a possible next treatment step. Existing patient data, often in\nthe form of electronic health records, can be analyzed to recommend the next\nsuitable step in the treatment process. However, the use of patient data poses\nmany challenges due to its knowledge-intensive character, high variability and\nscarcity of medical data. To overcome these challenges, this article examines\nthe use of the knowledge encoded in taxonomies to improve and explain the\nprediction of the next activity in the treatment process. This study proposes\nthe TS4NAP approach, which uses medical taxonomies (ICD-10-CM and ICD-10-PCS)\nin combination with graph matching to assess the similarities of medical codes\nto predict the next treatment step. The effectiveness of the proposed approach\nwill be evaluated using event logs that are derived from the MIMIC-IV dataset.\nThe results highlight the potential of using domain-specific knowledge held in\ntaxonomies to improve the prediction of the next activity, and thus can improve\ntreatment planning and decision-making by making the predictions more\nexplainable.\n","authors":["Martin Kuhn","Joscha Grüger","Tobias Geyer","Ralph Bergmann"],"pdf_url":"https://arxiv.org/pdf/2503.07638v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13173v1","updated":"2025-03-17T13:50:35Z","published":"2025-03-17T13:50:35Z","title":"PAUSE: Low-Latency and Privacy-Aware Active User Selection for Federated\n  Learning","summary":"  Federated learning (FL) enables multiple edge devices to collaboratively\ntrain a machine learning model without the need to share potentially private\ndata. Federated learning proceeds through iterative exchanges of model updates,\nwhich pose two key challenges: First, the accumulation of privacy leakage over\ntime, and second, communication latency. These two limitations are typically\naddressed separately: The former via perturbed updates to enhance privacy and\nthe latter using user selection to mitigate latency - both at the expense of\naccuracy. In this work, we propose a method that jointly addresses the\naccumulation of privacy leakage and communication latency via active user\nselection, aiming to improve the trade-off among privacy, latency, and model\nperformance. To achieve this, we construct a reward function that accounts for\nthese three objectives. Building on this reward, we propose a multi-armed\nbandit (MAB)-based algorithm, termed Privacy-aware Active User SElection\n(PAUSE) which dynamically selects a subset of users each round while ensuring\nbounded overall privacy leakage. We establish a theoretical analysis,\nsystematically showing that the reward growth rate of PAUSE follows that of the\nbest-known rate in MAB literature. To address the complexity overhead of active\nuser selection, we propose a simulated annealing-based relaxation of PAUSE and\nanalyze its ability to approximate the reward-maximizing policy under reduced\ncomplexity. We numerically validate the privacy leakage, associated improved\nlatency, and accuracy gains of our methods for the federated training in\nvarious scenarios.\n","authors":["Ori Peleg","Natalie Lang","Stefano Rini","Nir Shlezinger","Kobi Cohen"],"pdf_url":"https://arxiv.org/pdf/2503.13173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08722v2","updated":"2025-03-17T13:49:27Z","published":"2025-03-10T21:09:02Z","title":"A Recipe for Improving Remote Sensing VLM Zero Shot Generalization","summary":"  Foundation models have had a significant impact across various AI\napplications, enabling use cases that were previously impossible. Contrastive\nVisual Language Models (VLMs), in particular, have outperformed other\ntechniques in many tasks. However, their prevalence in remote sensing (RS) is\nstill limited, due to the scarcity of diverse remote-sensing visual-language\ndatasets. In this work we introduce two novel image-caption datasets for\ntraining of remote sensing foundation models. The first dataset pairs aerial\nand satellite imagery with captions generated by Gemini using landmarks\nextracted from Google Maps. The second dataset utilizes public web images and\ntheir corresponding alt-text, filtered for the remote sensing domain, resulting\nin a diverse dataset with greater breadth in image styles and subject matter.\nThese datasets are used to pre-train the\nMaMMUT~\\citep{kuo2023mammutsimplearchitecturejoint} VLM architecture, resulting\nin state-of-the-art generalization performance in zero-shot cross-modal\nretrieval on well-known public benchmarks. Finally, we present our ongoing\nresearch to distill image-level knowledge gained in the VLM contrastive\ntraining procedure to enhance the model's localization ability. Specifically,\nwe iteratively generate pseudo-labels for image regions based on the model's\nattention maps and use these labels for further training. To mitigate noisy\nattention maps and create robust segmentation masks, we introduce a novel\nattention-pooling mechanism called the Smooth-Attention-Operation.\n","authors":["Aviad Barzilai","Yotam Gigi","Amr Helmy","Vered Silverman","Yehonathan Refael","Bolous Jaber","Tomer Shekel","George Leifman","Genady Beryozkin"],"pdf_url":"https://arxiv.org/pdf/2503.08722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13850v4","updated":"2025-03-17T13:47:39Z","published":"2024-10-17T17:59:02Z","title":"Influence Functions for Scalable Data Attribution in Diffusion Models","summary":"  Diffusion models have led to significant advancements in generative\nmodelling. Yet their widespread adoption poses challenges regarding data\nattribution and interpretability. In this paper, we aim to help address such\nchallenges in diffusion models by developing an influence functions framework.\nInfluence function-based data attribution methods approximate how a model's\noutput would have changed if some training data were removed. In supervised\nlearning, this is usually used for predicting how the loss on a particular\nexample would change. For diffusion models, we focus on predicting the change\nin the probability of generating a particular example via several proxy\nmeasurements. We show how to formulate influence functions for such quantities\nand how previously proposed methods can be interpreted as particular design\nchoices in our framework. To ensure scalability of the Hessian computations in\ninfluence functions, we systematically develop K-FAC approximations based on\ngeneralised Gauss-Newton matrices specifically tailored to diffusion models. We\nrecast previously proposed methods as specific design choices in our framework\nand show that our recommended method outperforms previous data attribution\napproaches on common evaluations, such as the Linear Data-modelling Score (LDS)\nor retraining without top influences, without the need for method-specific\nhyperparameter tuning.\n","authors":["Bruno Mlodozeniec","Runa Eschenhagen","Juhan Bae","Alexander Immer","David Krueger","Richard Turner"],"pdf_url":"https://arxiv.org/pdf/2410.13850v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06027v2","updated":"2025-03-17T13:37:33Z","published":"2025-03-08T02:59:51Z","title":"Empowering Edge Intelligence: A Comprehensive Survey on On-Device AI\n  Models","summary":"  The rapid advancement of artificial intelligence (AI) technologies has led to\nan increasing deployment of AI models on edge and terminal devices, driven by\nthe proliferation of the Internet of Things (IoT) and the need for real-time\ndata processing. This survey comprehensively explores the current state,\ntechnical challenges, and future trends of on-device AI models. We define\non-device AI models as those designed to perform local data processing and\ninference, emphasizing their characteristics such as real-time performance,\nresource constraints, and enhanced data privacy. The survey is structured\naround key themes, including the fundamental concepts of AI models, application\nscenarios across various domains, and the technical challenges faced in edge\nenvironments. We also discuss optimization and implementation strategies, such\nas data preprocessing, model compression, and hardware acceleration, which are\nessential for effective deployment. Furthermore, we examine the impact of\nemerging technologies, including edge computing and foundation models, on the\nevolution of on-device AI models. By providing a structured overview of the\nchallenges, solutions, and future directions, this survey aims to facilitate\nfurther research and application of on-device AI, ultimately contributing to\nthe advancement of intelligent systems in everyday life.\n","authors":["Xubin Wang","Zhiqing Tang","Jianxiong Guo","Tianhui Meng","Chenhao Wang","Tian Wang","Weijia Jia"],"pdf_url":"https://arxiv.org/pdf/2503.06027v2.pdf","comment":"This paper has been accepted by ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2503.13162v1","updated":"2025-03-17T13:35:55Z","published":"2025-03-17T13:35:55Z","title":"Efficient Imitation Under Misspecification","summary":"  Interactive imitation learning (IL) is a powerful paradigm for learning to\nmake sequences of decisions from an expert demonstrating how to perform a task.\nPrior work in efficient imitation learning has focused on the realizable\nsetting, where the expert's policy lies within the learner's policy class (i.e.\nthe learner can perfectly imitate the expert in all states). However, in\npractice, perfect imitation of the expert is often impossible due to\ndifferences in state information and action space expressiveness (e.g.\nmorphological differences between robots and humans.) In this paper, we\nconsider the more general misspecified setting, where no assumptions are made\nabout the expert policy's realizability. We introduce a novel structural\ncondition, reward-agnostic policy completeness, and prove that it is sufficient\nfor interactive IL algorithms to efficiently avoid the quadratically\ncompounding errors that stymie offline approaches like behavioral cloning. We\naddress an additional practical constraint-the case of limited expert data-and\npropose a principled method for using additional offline data to further\nimprove the sample-efficiency of interactive IL algorithms. Finally, we\nempirically investigate the optimal reset distribution in efficient IL under\nmisspecification with a suite of continuous control tasks.\n","authors":["Nicolas Espinosa-Dice","Sanjiban Choudhury","Wen Sun","Gokul Swamy"],"pdf_url":"https://arxiv.org/pdf/2503.13162v1.pdf","comment":"37 pages, 5 figures. Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2412.13070v2","updated":"2025-03-17T13:32:12Z","published":"2024-12-17T16:34:32Z","title":"Learning of Patch-Based Smooth-Plus-Sparse Models for Image\n  Reconstruction","summary":"  We aim at the solution of inverse problems in imaging, by combining a\npenalized sparse representation of image patches with an unconstrained smooth\none. This allows for a straightforward interpretation of the reconstruction. We\nformulate the optimization as a bilevel problem. The inner problem deploys\nclassical algorithms while the outer problem optimizes the dictionary and the\nregularizer parameters through supervised learning. The process is carried out\nvia implicit differentiation and gradient-based optimization. We evaluate our\nmethod for denoising, super-resolution, and compressed-sensing\nmagnetic-resonance imaging. We compare it to other classical models as well as\ndeep-learning-based methods and show that it always outperforms the former and\nalso the latter in some instances.\n","authors":["Stanislas Ducotterd","Sebastian Neumayer","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2412.13070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13158v1","updated":"2025-03-17T13:31:12Z","published":"2025-03-17T13:31:12Z","title":"Laplace-Net: Learning Dynamical Systems with External Forcing","summary":"  Modelling forced dynamical systems - where an external input drives the\nsystem state - is critical across diverse domains such as engineering, finance,\nand the natural sciences. In this work, we propose Laplace-Net, a decoupled,\nsolver-free neural framework for learning forced and delay-aware systems. It\nleverages a Laplace transform-based approach to decompose internal dynamics,\nexternal inputs, and initial values into established theoretical concepts,\nenhancing interpretability. Laplace-Net promotes transferability since the\nsystem can be rapidly re-trained or fine-tuned for new forcing signals,\nproviding flexibility in applications ranging from controller adaptation to\nlong-horizon forecasting. Experimental results on eight benchmark datasets -\nincluding linear, non-linear, and delayed systems - demonstrate the method's\nimproved accuracy and robustness compared to state-of-the-art approaches,\nparticularly in handling complex and previously unseen inputs.\n","authors":["Bernd Zimmering","Cecília Coelho","Vaibhav Gupta","Maria Maleshkova","Oliver Niggemann"],"pdf_url":"https://arxiv.org/pdf/2503.13158v1.pdf","comment":"Preprint - under review"},{"id":"http://arxiv.org/abs/2410.19371v2","updated":"2025-03-17T13:23:33Z","published":"2024-10-25T08:18:49Z","title":"Noise-Aware Differentially Private Variational Inference","summary":"  Differential privacy (DP) provides robust privacy guarantees for statistical\ninference, but this can lead to unreliable results and biases in downstream\napplications. While several noise-aware approaches have been proposed which\nintegrate DP perturbation into the inference, they are limited to specific\ntypes of simple probabilistic models. In this work, we propose a novel method\nfor noise-aware approximate Bayesian inference based on stochastic gradient\nvariational inference which can also be applied to high-dimensional and\nnon-conjugate models. We also propose a more accurate evaluation method for\nnoise-aware posteriors. Empirically, our inference method has similar\nperformance to existing methods in the domain where they are applicable.\nOutside this domain, we obtain accurate coverages on high-dimensional Bayesian\nlinear regression and well-calibrated predictive probabilities on Bayesian\nlogistic regression with the UCI Adult dataset.\n","authors":["Talal Alrawajfeh","Joonas Jälkö","Antti Honkela"],"pdf_url":"https://arxiv.org/pdf/2410.19371v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17391v2","updated":"2025-03-17T13:20:52Z","published":"2025-02-24T18:16:23Z","title":"The Empirical Impact of Reducing Symmetries on the Performance of Deep\n  Ensembles and MoE","summary":"  Recent studies have shown that reducing symmetries in neural networks\nenhances linear mode connectivity between networks without requiring parameter\nspace alignment, leading to improved performance in linearly interpolated\nneural networks. However, in practical applications, neural network\ninterpolation is rarely used; instead, ensembles of networks are more common.\nIn this paper, we empirically investigate the impact of reducing symmetries on\nthe performance of deep ensembles and Mixture of Experts (MoE) across five\ndatasets. Additionally, to explore deeper linear mode connectivity, we\nintroduce the Mixture of Interpolated Experts (MoIE). Our results show that\ndeep ensembles built on asymmetric neural networks achieve significantly better\nperformance as ensemble size increases compared to their symmetric\ncounterparts. In contrast, our experiments do not provide conclusive evidence\non whether reducing symmetries affects both MoE and MoIE architectures.\n","authors":["Andrei Chernov","Oleg Novitskij"],"pdf_url":"https://arxiv.org/pdf/2502.17391v2.pdf","comment":"Accepted at the ICLR Workshop on Neural Network Weights as a New Data\n  Modality 2025"},{"id":"http://arxiv.org/abs/2407.20640v2","updated":"2025-03-17T13:19:11Z","published":"2024-07-30T08:35:26Z","title":"Improved Bounds for Pure Private Agnostic Learning: Item-Level and\n  User-Level Privacy","summary":"  Machine Learning has made remarkable progress in a wide range of fields. In\nmany scenarios, learning is performed on datasets involving sensitive\ninformation, in which privacy protection is essential for learning algorithms.\nIn this work, we study pure private learning in the agnostic model -- a\nframework reflecting the learning process in practice. We examine the number of\nusers required under item-level (where each user contributes one example) and\nuser-level (where each user contributes multiple examples) privacy and derive\nseveral improved upper bounds. For item-level privacy, our algorithm achieves a\nnear optimal bound for general concept classes. We extend this to the\nuser-level setting, rendering a tighter upper bound than the one proved by\nGhazi et al. (2023). Lastly, we consider the problem of learning thresholds\nunder user-level privacy and present an algorithm with a nearly tight user\ncomplexity.\n","authors":["Bo Li","Wei Wang","Peng Ye"],"pdf_url":"https://arxiv.org/pdf/2407.20640v2.pdf","comment":"Fix some typos"},{"id":"http://arxiv.org/abs/2503.13145v1","updated":"2025-03-17T13:16:25Z","published":"2025-03-17T13:16:25Z","title":"High-entropy Advantage in Neural Networks' Generalizability","summary":"  While the 2024 Nobel Prize in Physics ignites a worldwide discussion on the\norigins of neural networks and their foundational links to physics, modern\nmachine learning research predominantly focuses on computational and\nalgorithmic advancements, overlooking a picture of physics. Here we introduce\nthe concept of entropy into neural networks by reconceptualizing them as\nhypothetical physical systems where each parameter is a non-interacting\n'particle' within a one-dimensional space. By employing a Wang-Landau\nalgorithms, we construct the neural networks' (with up to 1 million parameters)\nentropy landscapes as functions of training loss and test accuracy (or loss)\nacross four distinct machine learning tasks, including arithmetic question,\nreal-world tabular data, image recognition, and language modeling. Our results\nreveal the existence of \\textit{entropy advantage}, where the high-entropy\nstates generally outperform the states reached via classical training optimizer\nlike stochastic gradient descent. We also find this advantage is more\npronounced in narrower networks, indicating a need of different training\noptimizers tailored to different sizes of neural networks.\n","authors":["Entao Yang","Xiaotian Zhang","Yue Shang","Ge Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20580v2","updated":"2025-03-17T12:41:58Z","published":"2025-02-27T22:45:41Z","title":"Training Large Neural Networks With Low-Dimensional Error Feedback","summary":"  Training deep neural networks typically relies on backpropagating high\ndimensional error signals a computationally intensive process with little\nevidence supporting its implementation in the brain. However, since most tasks\ninvolve low-dimensional outputs, we propose that low-dimensional error signals\nmay suffice for effective learning. To test this hypothesis, we introduce a\nnovel local learning rule based on Feedback Alignment that leverages indirect,\nlow-dimensional error feedback to train large networks. Our method decouples\nthe backward pass from the forward pass, enabling precise control over error\nsignal dimensionality while maintaining high-dimensional representations. We\nbegin with a detailed theoretical derivation for linear networks, which forms\nthe foundation of our learning framework, and extend our approach to nonlinear,\nconvolutional, and transformer architectures. Remarkably, we demonstrate that\neven minimal error dimensionality on the order of the task dimensionality can\nachieve performance matching that of traditional backpropagation. Furthermore,\nour rule enables efficient training of convolutional networks, which have\npreviously been resistant to Feedback Alignment methods, with minimal error.\nThis breakthrough not only paves the way toward more biologically accurate\nmodels of learning but also challenges the conventional reliance on\nhigh-dimensional gradient signals in neural network training. Our findings\nsuggest that low-dimensional error signals can be as effective as\nhigh-dimensional ones, prompting a reevaluation of gradient-based learning in\nhigh-dimensional systems. Ultimately, our work offers a fresh perspective on\nneural network optimization and contributes to understanding learning\nmechanisms in both artificial and biological systems.\n","authors":["Maher Hanut","Jonathan Kadmon"],"pdf_url":"https://arxiv.org/pdf/2502.20580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13116v1","updated":"2025-03-17T12:38:03Z","published":"2025-03-17T12:38:03Z","title":"VeriLeaky: Navigating IP Protection vs Utility in Fine-Tuning for\n  LLM-Driven Verilog Coding","summary":"  Large language models (LLMs) offer significant potential for coding, yet\nfine-tuning (FT) with curated data is essential for niche languages like\nVerilog. Using proprietary intellectual property (IP) for FT presents a serious\nrisk, as FT data can be leaked through LLM inference. This leads to a critical\ndilemma for design houses: seeking to build externally accessible LLMs offering\ncompetitive Verilog coding, how can they leverage in-house IP to enhance FT\nutility while ensuring IP protection?\n  For the first time in the literature, we study this dilemma. Using LLaMA\n3.1-8B, we conduct in-house FT on a baseline Verilog dataset (RTLCoder)\nsupplemented with our own in-house IP, which is validated through multiple\ntape-outs. To rigorously assess IP leakage, we quantify structural similarity\n(AST/Dolos) and functional equivalence (Synopsys Formality) between generated\ncodes and our in-house IP. We show that our IP can indeed be leaked, confirming\nthe threat. As defense, we evaluate logic locking of Verilog codes (ASSURE).\nThis offers some level of protection, yet reduces the IP's utility for FT and\ndegrades the LLM's performance. Our study shows the need for novel strategies\nthat are both effective and minimally disruptive to FT, an essential effort for\nenabling design houses to fully utilize their proprietary IP toward LLM-driven\nVerilog coding.\n","authors":["Zeng Wang","Minghao Shao","Mohammed Nabeel","Prithwish Basu Roy","Likhitha Mankali","Jitendra Bhandari","Ramesh Karri","Ozgur Sinanoglu","Muhammad Shafique","Johann Knechtel"],"pdf_url":"https://arxiv.org/pdf/2503.13116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13115v1","updated":"2025-03-17T12:37:53Z","published":"2025-03-17T12:37:53Z","title":"Beyond Propagation of Chaos: A Stochastic Algorithm for Mean Field\n  Optimization","summary":"  Gradient flow in the 2-Wasserstein space is widely used to optimize\nfunctionals over probability distributions and is typically implemented using\nan interacting particle system with $n$ particles. Analyzing these algorithms\nrequires showing (a) that the finite-particle system converges and/or (b) that\nthe resultant empirical distribution of the particles closely approximates the\noptimal distribution (i.e., propagation of chaos). However, establishing\nefficient sufficient conditions can be challenging, as the finite particle\nsystem may produce heavily dependent random variables.\n  In this work, we study the virtual particle stochastic approximation,\noriginally introduced for Stein Variational Gradient Descent. This method can\nbe viewed as a form of stochastic gradient descent in the Wasserstein space and\ncan be implemented efficiently. In popular settings, we demonstrate that our\nalgorithm's output converges to the optimal distribution under conditions\nsimilar to those for the infinite particle limit, and it produces i.i.d.\nsamples without the need to explicitly establish propagation of chaos bounds.\n","authors":["Chandan Tankala","Dheeraj M. Nagaraj","Anant Raj"],"pdf_url":"https://arxiv.org/pdf/2503.13115v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09315v2","updated":"2025-03-17T12:35:52Z","published":"2025-03-12T12:05:03Z","title":"ShuffleGate: An Efficient and Self-Polarizing Feature Selection Method\n  for Large-Scale Deep Models in Industry","summary":"  Deep models in industrial applications rely on thousands of features for\naccurate predictions, such as deep recommendation systems. While new features\nare introduced to capture evolving user behavior, outdated or redundant\nfeatures often remain, significantly increasing storage and computational\ncosts. To address this issue, feature selection methods are widely adopted to\nidentify and remove less important features. However, existing approaches face\ntwo major challenges: (1) they often require complex hyperparameter (Hp)\ntuning, making them difficult to employ in practice, and (2) they fail to\nproduce well-separated feature importance scores, which complicates\nstraightforward feature removal. Moreover, the impact of removing unimportant\nfeatures can only be evaluated through retraining the model, a time-consuming\nand resource-intensive process that severely hinders efficient feature\nselection.\n  To solve these challenges, we propose a novel feature selection approach,\nShuffleGate. In particular, it shuffles all feature values across instances\nsimultaneously and uses a gating mechanism that allows the model to dynamically\nlearn the weights for combining the original and shuffled inputs. Notably, it\ncan generate well-separated feature importance scores and estimate the\nperformance without retraining the model, while introducing only a single Hp.\nExperiments on four public datasets show that our approach outperforms\nstate-of-the-art methods in feature selection for model retraining. Moreover,\nit has been successfully integrated into the daily iteration of Bilibili's\nsearch models across various scenarios, where it significantly reduces feature\nset size (up to 60%+) and computational resource usage (up to 20%+), while\nmaintaining comparable performance.\n","authors":["Yihong Huang","Chen Chu","Fan Zhang","Fei Chen","Yu Lin","Ruiduan Li","Zhihao Li"],"pdf_url":"https://arxiv.org/pdf/2503.09315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13113v1","updated":"2025-03-17T12:34:55Z","published":"2025-03-17T12:34:55Z","title":"Exploring the Potential of Bilevel Optimization for Calibrating Neural\n  Networks","summary":"  Handling uncertainty is critical for ensuring reliable decision-making in\nintelligent systems. Modern neural networks are known to be poorly calibrated,\nresulting in predicted confidence scores that are difficult to use. This\narticle explores improving confidence estimation and calibration through the\napplication of bilevel optimization, a framework designed to solve hierarchical\nproblems with interdependent optimization levels. A self-calibrating bilevel\nneural-network training approach is introduced to improve a model's predicted\nconfidence scores. The effectiveness of the proposed framework is analyzed\nusing toy datasets, such as Blobs and Spirals, as well as more practical\nsimulated datasets, such as Blood Alcohol Concentration (BAC). It is compared\nwith a well-known and widely used calibration strategy, isotonic regression.\nThe reported experimental results reveal that the proposed bilevel optimization\napproach reduces the calibration error while preserving accuracy.\n","authors":["Gabriele Sanguin","Arjun Pakrashi","Marco Viola","Francesco Rinaldi"],"pdf_url":"https://arxiv.org/pdf/2503.13113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13111v1","updated":"2025-03-17T12:34:22Z","published":"2025-03-17T12:34:22Z","title":"MM-Spatial: Exploring 3D Spatial Understanding in Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) excel at 2D visual understanding but\nremain limited in their ability to reason about 3D space. In this work, we\nleverage large-scale high-quality 3D scene data with open-set annotations to\nintroduce 1) a novel supervised fine-tuning dataset and 2) a new evaluation\nbenchmark, focused on indoor scenes. Our Cubify Anything VQA (CA-VQA) data\ncovers diverse spatial tasks including spatial relationship prediction, metric\nsize and distance estimation, and 3D grounding. We show that CA-VQA enables us\nto train MM-Spatial, a strong generalist MLLM that also achieves\nstate-of-the-art performance on 3D spatial understanding benchmarks, including\nour own. We show how incorporating metric depth and multi-view inputs (provided\nin CA-VQA) can further improve 3D understanding, and demonstrate that data\nalone allows our model to achieve depth perception capabilities comparable to\ndedicated monocular depth estimation models. We will publish our SFT dataset\nand benchmark.\n","authors":["Erik Daxberger","Nina Wenzel","David Griffiths","Haiming Gang","Justin Lazarow","Gefen Kohavi","Kai Kang","Marcin Eichner","Yinfei Yang","Afshin Dehghan","Peter Grasch"],"pdf_url":"https://arxiv.org/pdf/2503.13111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15483v2","updated":"2025-03-17T12:33:30Z","published":"2025-02-21T14:12:44Z","title":"MoMa: A Modular Deep Learning Framework for Material Property Prediction","summary":"  Deep learning methods for material property prediction have been widely\nexplored to advance materials discovery. However, the prevailing pre-train then\nfine-tune paradigm often fails to address the inherent diversity and disparity\nof material tasks. To overcome these challenges, we introduce MoMa, a Modular\nframework for Materials that first trains specialized modules across a wide\nrange of tasks and then adaptively composes synergistic modules tailored to\neach downstream scenario. Evaluation across 17 datasets demonstrates the\nsuperiority of MoMa, with a substantial 14% average improvement over the\nstrongest baseline. Few-shot and continual learning experiments further\nhighlight MoMa's potential for real-world applications. Pioneering a new\nparadigm of modular material learning, MoMa will be open-sourced to foster\nbroader community collaboration.\n","authors":["Botian Wang","Yawen Ouyang","Yaohui Li","Yiqun Wang","Haorui Cui","Jianbing Zhang","Xiaonan Wang","Wei-Ying Ma","Hao Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.15483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10880v2","updated":"2025-03-17T12:29:05Z","published":"2024-10-09T15:36:42Z","title":"Fine-tuning can Help Detect Pretraining Data from Large Language Models","summary":"  In the era of large language models (LLMs), detecting pretraining data has\nbeen increasingly important due to concerns about fair evaluation and ethical\nrisks. Current methods differentiate members and non-members by designing\nscoring functions, like Perplexity and Min-k%. However, the diversity and\ncomplexity of training data magnifies the difficulty of distinguishing, leading\nto suboptimal performance in detecting pretraining data. In this paper, we\nfirst explore the benefits of unseen data, which can be easily collected after\nthe release of the LLM. We find that the perplexities of LLMs shift differently\nfor members and non-members, after fine-tuning with a small amount of\npreviously unseen data. In light of this, we introduce a novel and effective\nmethod termed Fine-tuned Score Deviation(FSD), which improves the performance\nof current scoring functions for pretraining data detection. In particular, we\npropose to measure the deviation distance of current scores after fine-tuning\non a small amount of unseen data within the same domain. In effect, using a few\nunseen data can largely decrease the scores of all non-members, leading to a\nlarger deviation distance than members. Extensive experiments demonstrate the\neffectiveness of our method, significantly improving the AUC score on common\nbenchmark datasets across various models.\n","authors":["Hengxiang Zhang","Songxin Zhang","Bingyi Jing","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2410.10880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15786v3","updated":"2025-03-17T12:01:18Z","published":"2024-04-24T10:19:25Z","title":"Rethinking model prototyping through the MedMNIST+ dataset collection","summary":"  The integration of deep learning based systems in clinical practice is often\nimpeded by challenges rooted in limited and heterogeneous medical datasets. In\naddition, the field has increasingly prioritized marginal performance gains on\na few, narrowly scoped benchmarks over clinical applicability, slowing down\nmeaningful algorithmic progress. This trend often results in excessive\nfine-tuning of existing methods on selected datasets rather than fostering\nclinically relevant innovations. In response, this work introduces a\ncomprehensive benchmark for the MedMNIST+ dataset collection, designed to\ndiversify the evaluation landscape across several imaging modalities,\nanatomical regions, classification tasks and sample sizes. We systematically\nreassess commonly used Convolutional Neural Networks (CNNs) and Vision\nTransformer (ViT) architectures across distinct medical datasets, training\nmethodologies, and input resolutions to validate and refine existing\nassumptions about model effectiveness and development. Our findings suggest\nthat computationally efficient training schemes and modern foundation models\noffer viable alternatives to costly end-to-end training. Additionally, we\nobserve that higher image resolutions do not consistently improve performance\nbeyond a certain threshold. This highlights the potential benefits of using\nlower resolutions, particularly in prototyping stages, to reduce computational\ndemands without sacrificing accuracy. Notably, our analysis reaffirms the\ncompetitiveness of CNNs compared to ViTs, emphasizing the importance of\ncomprehending the intrinsic capabilities of different architectures. Finally,\nby establishing a standardized evaluation framework, we aim to enhance\ntransparency, reproducibility, and comparability within the MedMNIST+ dataset\ncollection. Code is available at\nhttps://github.com/sdoerrich97/rethinking-model-prototyping-MedMNISTPlus .\n","authors":["Sebastian Doerrich","Francesco Di Salvo","Julius Brockmann","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2404.15786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13077v1","updated":"2025-03-17T11:32:28Z","published":"2025-03-17T11:32:28Z","title":"Towards Better Sample Efficiency in Multi-Agent Reinforcement Learning\n  via Exploration","summary":"  Multi-agent reinforcement learning has shown promise in learning cooperative\nbehaviors in team-based environments. However, such methods often demand\nextensive training time. For instance, the state-of-the-art method TiZero takes\n40 days to train high-quality policies for a football environment. In this\npaper, we hypothesize that better exploration mechanisms can improve the sample\nefficiency of multi-agent methods. We propose two different approaches for\nbetter exploration in TiZero: a self-supervised intrinsic reward and a random\nnetwork distillation bonus. Additionally, we introduce architectural\nmodifications to the original algorithm to enhance TiZero's computational\nefficiency. We evaluate the sample efficiency of these approaches through\nextensive experiments. Our results show that random network distillation\nimproves training sample efficiency by 18.8% compared to the original TiZero.\nFurthermore, we evaluate the qualitative behavior of the models produced by\nboth variants against a heuristic AI, with the self-supervised reward\nencouraging possession and random network distillation leading to a more\noffensive performance. Our results highlights the applicability of our random\nnetwork distillation variant in practical settings. Lastly, due to the nature\nof the proposed method, we acknowledge its use beyond football simulation,\nespecially in environments with strong multi-agent and strategic aspects.\n","authors":["Amir Baghi","Jens Sjölund","Joakim Bergdahl","Linus Gisslén","Alessandro Sestini"],"pdf_url":"https://arxiv.org/pdf/2503.13077v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2311.05589v2","updated":"2025-03-17T11:14:58Z","published":"2023-11-09T18:47:44Z","title":"A Coefficient Makes SVRG Effective","summary":"  Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang\n(2013), is a theoretically compelling optimization method. However, as Defazio\n& Bottou (2019) highlight, its effectiveness in deep learning is yet to be\nproven. In this work, we demonstrate the potential of SVRG in optimizing\nreal-world neural networks. Our empirical analysis finds that, for deeper\nneural networks, the strength of the variance reduction term in SVRG should be\nsmaller and decrease as training progresses. Inspired by this, we introduce a\nmultiplicative coefficient $\\alpha$ to control the strength and adjust it\nthrough a linear decay schedule. We name our method $\\alpha$-SVRG. Our results\nshow $\\alpha$-SVRG better optimizes models, consistently reducing training loss\ncompared to the baseline and standard SVRG across various model architectures\nand multiple image classification datasets. We hope our findings encourage\nfurther exploration into variance reduction techniques in deep learning. Code\nis available at github.com/davidyyd/alpha-SVRG.\n","authors":["Yida Yin","Zhiqiu Xu","Zhiyuan Li","Trevor Darrell","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.05589v2.pdf","comment":"Published in ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13057v1","updated":"2025-03-17T11:02:28Z","published":"2025-03-17T11:02:28Z","title":"MaskSDM with Shapley values to improve flexibility, robustness, and\n  explainability in species distribution modeling","summary":"  Species Distribution Models (SDMs) play a vital role in biodiversity\nresearch, conservation planning, and ecological niche modeling by predicting\nspecies distributions based on environmental conditions. The selection of\npredictors is crucial, strongly impacting both model accuracy and how well the\npredictions reflect ecological patterns. To ensure meaningful insights, input\nvariables must be carefully chosen to match the study objectives and the\necological requirements of the target species. However, existing SDMs,\nincluding both traditional and deep learning-based approaches, often lack key\ncapabilities for variable selection: (i) flexibility to choose relevant\npredictors at inference without retraining; (ii) robustness to handle missing\npredictor values without compromising accuracy; and (iii) explainability to\ninterpret and accurately quantify each predictor's contribution. To overcome\nthese limitations, we introduce MaskSDM, a novel deep learning-based SDM that\nenables flexible predictor selection by employing a masked training strategy.\nThis approach allows the model to make predictions with arbitrary subsets of\ninput variables while remaining robust to missing data. It also provides a\nclearer understanding of how adding or removing a given predictor affects model\nperformance and predictions. Additionally, MaskSDM leverages Shapley values for\nprecise predictor contribution assessments, improving upon traditional\napproximations. We evaluate MaskSDM on the global sPlotOpen dataset, modeling\nthe distributions of 12,738 plant species. Our results show that MaskSDM\noutperforms imputation-based methods and approximates models trained on\nspecific subsets of variables. These findings underscore MaskSDM's potential to\nincrease the applicability and adoption of SDMs, laying the groundwork for\ndeveloping foundation models in SDMs that can be readily applied to diverse\necological applications.\n","authors":["Robin Zbinden","Nina van Tiel","Gencer Sumbul","Chiara Vanalli","Benjamin Kellenberger","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2503.13057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13056v1","updated":"2025-03-17T11:02:23Z","published":"2025-03-17T11:02:23Z","title":"Deep Hedging of Green PPAs in Electricity Markets","summary":"  In power markets, Green Power Purchase Agreements have become an important\ncontractual tool of the energy transition from fossil fuels to renewable\nsources such as wind or solar radiation. Trading Green PPAs exposes agents to\nprice risks and weather risks. Also, developed electricity markets feature the\nso-called cannibalisation effect : large infeeds induce low prices and vice\nversa. As weather is a non-tradable entity the question arises how to hedge and\nrisk-manage in this highly incom-plete setting. We propose a ''deep hedging''\nframework utilising machine learning methods to construct hedging strategies.\nThe resulting strategies outperform static and dynamic benchmark strategies\nwith respect to different risk measures.\n","authors":["Richard Biegler-König","Daniel Oeltz"],"pdf_url":"https://arxiv.org/pdf/2503.13056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11066v2","updated":"2025-03-17T11:01:56Z","published":"2025-03-14T04:19:23Z","title":"Further Exploration of Precise Binding Energies from Physics Informed\n  Machine Learning and the Development of a Practical Ensemble Model","summary":"  Sixteen new physics informed machine learning models have been trained on\nbinding energy residuals from modern mass models that leverage shape parameters\nand other physical features. The models have been trained on a subset of AME\n2012 data and have been verified with a subset of the AME 2020 data. Among the\nmachine learning approaches tested in this work, the preferred approach is the\nleast squares boosted ensemble of trees which appears to have a superior\nability to both interpolate and extrapolate binding energy residuals. The\nmachine learning models for four mass models created from the ensemble of trees\napproach have been combined to create a composite model called the Four Model\nTree Ensemble (FMTE). The FMTE model predicts binding energy values from AME\n2020 with a standard deviation of 76 keV and a mean deviation of 34 keV for all\nnuclei with N > 7 and Z > 7. A comparison with new mass measurements for 33\nisotopes not included in AME 2012 or AME 2020 indicates that the FMTE performs\nbetter than all mass models that were tested.\n","authors":["I. Bentley","J. Tedder","M. Gebran","A. Paul"],"pdf_url":"https://arxiv.org/pdf/2503.11066v2.pdf","comment":"Submitted to PRC for review"},{"id":"http://arxiv.org/abs/2412.06646v2","updated":"2025-03-17T10:59:29Z","published":"2024-12-09T16:39:40Z","title":"The Narrow Gate: Localized Image-Text Communication in Vision-Language\n  Models","summary":"  Recent advances in multimodal training have significantly improved the\nintegration of image understanding and generation within a unified model. This\nstudy investigates how vision-language models (VLMs) handle image-understanding\ntasks, specifically focusing on how visual information is processed and\ntransferred to the textual domain. We compare VLMs that generate both images\nand text with those that output only text, highlighting key differences in\ninformation flow. We find that in models with multimodal outputs, image and\ntext embeddings are more separated within the residual stream. Additionally,\nmodels vary in how information is exchanged from visual to textual tokens. VLMs\nthat only output text exhibit a distributed communication pattern, where\ninformation is exchanged through multiple image tokens. In contrast, models\ntrained for image and text generation tend to rely on a single token that acts\nas a narrow gate for visual information. We demonstrate that ablating this\nsingle token significantly deteriorates performance on image understanding\ntasks. Furthermore, modifying this token enables effective steering of the\nimage semantics, showing that targeted, local interventions can reliably\ncontrol the model's global behavior.\n","authors":["Alessandro Serra","Francesco Ortu","Emanuele Panizon","Lucrezia Valeriani","Lorenzo Basile","Alessio Ansuini","Diego Doimo","Alberto Cazzaniga"],"pdf_url":"https://arxiv.org/pdf/2412.06646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13051v1","updated":"2025-03-17T10:55:55Z","published":"2025-03-17T10:55:55Z","title":"Permutation Learning with Only N Parameters: From SoftSort to\n  Self-Organizing Gaussians","summary":"  Sorting and permutation learning are key concepts in optimization and machine\nlearning, especially when organizing high-dimensional data into meaningful\nspatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N\nparameters to determine a full permutation matrix, making it computationally\nexpensive for large datasets. Low-rank matrix factorization approximations\nreduce memory requirements to 2MN (with M << N), but they still struggle with\nvery large problems. SoftSort, by providing a continuous relaxation of the\nargsort operator, allows differentiable 1D sorting, but it faces challenges\nwith multidimensional data and complex permutations. In this paper, we present\na novel method for learning permutations using only N parameters, which\ndramatically reduces storage costs. Our approach builds on SoftSort, but\nextends it by iteratively shuffling the N indices of the elements to be sorted\nthrough a separable learning process. This modification significantly improves\nsorting quality, especially for multidimensional data and complex optimization\ncriteria, and outperforms pure SoftSort. Our method offers improved memory\nefficiency and scalability compared to existing approaches, while maintaining\nhigh-quality permutation learning. Its dramatically reduced memory requirements\nmake it particularly well-suited for large-scale optimization tasks, such as\n\"Self-Organizing Gaussians\", where efficient and scalable permutation learning\nis critical.\n","authors":["Kai Uwe Barthel","Florian Barthel","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2503.13051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13050v1","updated":"2025-03-17T10:54:30Z","published":"2025-03-17T10:54:30Z","title":"E-Values Expand the Scope of Conformal Prediction","summary":"  Conformal prediction is a powerful framework for distribution-free\nuncertainty quantification. The standard approach to conformal prediction\nrelies on comparing the ranks of prediction scores: under exchangeability, the\nrank of a future test point cannot be too extreme relative to a calibration\nset. This rank-based method can be reformulated in terms of p-values. In this\npaper, we explore an alternative approach based on e-values, known as conformal\ne-prediction. E-values offer key advantages that cannot be achieved with\np-values, enabling new theoretical and practical capabilities. In particular,\nwe present three applications that leverage the unique strengths of e-values:\nbatch anytime-valid conformal prediction, fixed-size conformal sets with\ndata-dependent coverage, and conformal prediction under ambiguous ground truth.\nOverall, these examples demonstrate that e-value-based constructions provide a\nflexible expansion of the toolbox of conformal prediction.\n","authors":["Etienne Gauthier","Francis Bach","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2503.13050v1.pdf","comment":"Code available at: https://github.com/GauthierE/evalues-expand-cp"},{"id":"http://arxiv.org/abs/2503.07609v3","updated":"2025-03-17T10:48:02Z","published":"2025-03-10T17:59:44Z","title":"Preserving clusters and correlations: a dimensionality reduction method\n  for exceptionally high global structure preservation","summary":"  We present Preserving Clusters and Correlations (PCC), a novel dimensionality\nreduction (DR) method a novel dimensionality reduction (DR) method that\nachieves state-of-the-art global structure (GS) preservation while maintaining\ncompetitive local structure (LS) preservation. It optimizes two objectives: a\nGS preservation objective that preserves an approximation of Pearson and\nSpearman correlations between high- and low-dimensional distances, and an LS\npreservation objective that ensures clusters in the high-dimensional data are\nseparable in the low-dimensional data. PCC has a state-of-the-art ability to\npreserve the GS while having competitive LS preservation. In addition, we show\nthe correlation objective can be combined with UMAP to significantly improve\nits GS preservation with minimal degradation of the LS. We quantitatively\nbenchmark PCC against existing methods and demonstrate its utility in medical\nimaging, and show PCC is a competitive DR technique that demonstrates superior\nGS preservation in our benchmarks.\n","authors":["Jacob Gildenblat","Jens Pahnke"],"pdf_url":"https://arxiv.org/pdf/2503.07609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09483v2","updated":"2025-03-17T10:38:39Z","published":"2025-03-12T15:38:11Z","title":"Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization","summary":"  We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters.\n","authors":["Andreas Kofler","Luca Calatroni","Christoph Kolbitsch","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2503.09483v2.pdf","comment":"To be submitted to the EUSIPCO 2025 conference"},{"id":"http://arxiv.org/abs/2503.07671v2","updated":"2025-03-17T10:26:39Z","published":"2025-03-09T17:54:33Z","title":"Probabilistic Shielding for Safe Reinforcement Learning","summary":"  In real-life scenarios, a Reinforcement Learning (RL) agent aiming to\nmaximise their reward, must often also behave in a safe manner, including at\ntraining time. Thus, much attention in recent years has been given to Safe RL,\nwhere an agent aims to learn an optimal policy among all policies that satisfy\na given safety constraint. However, strict safety guarantees are often provided\nthrough approaches based on linear programming, and thus have limited scaling.\nIn this paper we present a new, scalable method, which enjoys strict formal\nguarantees for Safe RL, in the case where the safety dynamics of the Markov\nDecision Process (MDP) are known, and safety is defined as an undiscounted\nprobabilistic avoidance property. Our approach is based on state-augmentation\nof the MDP, and on the design of a shield that restricts the actions available\nto the agent. We show that our approach provides a strict formal safety\nguarantee that the agent stays safe at training and test time. Furthermore, we\ndemonstrate that our approach is viable in practice through experimental\nevaluation.\n","authors":["Edwin Hamel-De le Court","Francesco Belardinelli","Alex W. Goodall"],"pdf_url":"https://arxiv.org/pdf/2503.07671v2.pdf","comment":"13 pages, 3 figures, Conference: AAAI 2025"},{"id":"http://arxiv.org/abs/2305.17473v4","updated":"2025-03-17T10:18:52Z","published":"2023-05-27T13:23:21Z","title":"A Comprehensive Overview and Comparative Analysis on Deep Learning\n  Models: CNN, RNN, LSTM, GRU","summary":"  Deep learning (DL) has emerged as a powerful subset of machine learning (ML)\nand artificial intelligence (AI), outperforming traditional ML methods,\nespecially in handling unstructured and large datasets. Its impact spans across\nvarious domains, including speech recognition, healthcare, autonomous vehicles,\ncybersecurity, predictive analytics, and more. However, the complexity and\ndynamic nature of real-world problems present challenges in designing effective\ndeep learning models. Consequently, several deep learning models have been\ndeveloped to address different problems and applications. In this article, we\nconduct a comprehensive survey of various deep learning models, including\nConvolutional Neural Network (CNN), Recurrent Neural Network (RNN), Temporal\nConvolutional Networks (TCN), Transformer, Kolmogorov-Arnold networks (KAN),\nGenerative Models, Deep Reinforcement Learning (DRL), and Deep Transfer\nLearning. We examine the structure, applications, benefits, and limitations of\neach model. Furthermore, we perform an analysis using three publicly available\ndatasets: IMDB, ARAS, and Fruit-360. We compared the performance of six\nrenowned deep learning models: CNN, RNN, Long Short-Term Memory (LSTM),\nBidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU alongside\ntwo newer models, TCN and Transformer, using the IMDB and ARAS datasets.\nAdditionally, we evaluated the performance of eight CNN-based models, including\nVGG (Visual Geometry Group), Inception, ResNet (Residual Network),\nInceptionResNet, Xception (Extreme Inception), MobileNet, DenseNet (Dense\nConvolutional Network), and NASNet (Neural Architecture Search Network), for\nimage classification tasks using the Fruit-360 dataset.\n","authors":["Farhad Mortezapour Shiri","Thinagaran Perumal","Norwati Mustapha","Raihani Mohamed"],"pdf_url":"https://arxiv.org/pdf/2305.17473v4.pdf","comment":"62 pages, 37 figures"},{"id":"http://arxiv.org/abs/2308.12794v2","updated":"2025-03-17T10:18:45Z","published":"2023-08-24T13:49:48Z","title":"Job Shop Scheduling Benchmark: Environments and Instances for Learning\n  and Non-learning Methods","summary":"  Job shop scheduling problems address the routing and sequencing of tasks in a\njob shop setting. Despite significant interest from operations research and\nmachine learning communities over the years, a comprehensive platform for\ntesting and comparing solution methods has been notably lacking. To fill this\ngap, we introduce a unified implementation of job shop scheduling problems and\ntheir solution methods, addressing the long-standing need for a standardized\nbenchmarking platform in this domain. Our platform supports classic Job Shop\n(JSP), Flow Shop (FSP), Flexible Job Shop (FJSP), and Assembly Job Shop (AJSP),\nas well as variants featuring Sequence-Dependent Setup Times (SDST), variants\nwith online arrivals of jobs, and combinations of these problems (e.g.,\nFJSP-SDST and FAJSP). The platfrom provides a wide range of scheduling solution\nmethods, from heuristics, metaheuristics, and exact optimization to deep\nreinforcement learning. The implementation is available as an open-source\nGitHub repository, serving as a collaborative hub for researchers,\npractitioners, and those new to the field. Beyond enabling direct comparisons\nwith existing methods on widely studied benchmark problems, this resource\nserves as a robust starting point for addressing constrained and complex\nproblem variants. By establishing a comprehensive and unified foundation, this\nplatform is designed to consolidate existing knowledge and to inspire the\ndevelopment of next-generation algorithms in job shop scheduling research.\n","authors":["Robbert Reijnen","Igor G. Smit","Hongxiang Zhang","Yaoxin Wu","Zaharah Bukhsh","Yingqian Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.12794v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08555v2","updated":"2025-03-17T10:10:28Z","published":"2025-03-11T15:45:37Z","title":"An Analysis of Safety Guarantees in Multi-Task Bayesian Optimization","summary":"  This paper addresses the integration of additional information sources into a\nBayesian optimization framework while ensuring that safety constraints are\nsatisfied. The interdependencies between these information sources are modeled\nusing an unknown correlation matrix. We explore how uniform error bounds must\nbe adjusted to maintain constraint satisfaction throughout the optimization\nprocess, considering both Bayesian and frequentist statistical perspectives.\nThis is achieved by appropriately scaling the error bounds based on a\nconfidence interval that can be estimated from the data. Furthermore, the\nefficacy of the proposed approach is demonstrated through experiments on two\nbenchmark functions and a controller parameter optimization problem. Our\nresults highlight a significant improvement in sample efficiency, demonstrating\nthe methods suitability for optimizing expensive-to-evaluate functions.\n","authors":["Jannis O. Luebsen","Annika Eichler"],"pdf_url":"https://arxiv.org/pdf/2503.08555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13008v1","updated":"2025-03-17T10:07:50Z","published":"2025-03-17T10:07:50Z","title":"Knowledge Distillation: Enhancing Neural Network Compression with\n  Integrated Gradients","summary":"  Efficient deployment of deep neural networks on resource-constrained devices\ndemands advanced compression techniques that preserve accuracy and\ninteroperability. This paper proposes a machine learning framework that\naugments Knowledge Distillation (KD) with Integrated Gradients (IG), an\nattribution method, to optimise the compression of convolutional neural\nnetworks. We introduce a novel data augmentation strategy where IG maps,\nprecomputed from a teacher model, are overlaid onto training images to guide a\ncompact student model toward critical feature representations. This approach\nleverages the teacher's decision-making insights, enhancing the student's\nability to replicate complex patterns with reduced parameters. Experiments on\nCIFAR-10 demonstrate the efficacy of our method: a student model, compressed\n4.1-fold from the MobileNet-V2 teacher, achieves 92.5% classification accuracy,\nsurpassing the baseline student's 91.4% and traditional KD approaches, while\nreducing inference latency from 140 ms to 13 ms--a tenfold speedup. We perform\nhyperparameter optimisation for efficient learning. Comprehensive ablation\nstudies dissect the contributions of KD and IG, revealing synergistic effects\nthat boost both performance and model explainability. Our method's emphasis on\nfeature-level guidance via IG distinguishes it from conventional KD, offering a\ndata-driven solution for mining transferable knowledge in neural architectures.\nThis work contributes to machine learning by providing a scalable,\ninterpretable compression technique, ideal for edge computing applications\nwhere efficiency and transparency are paramount.\n","authors":["David E. Hernandez","Jose Ramon Chang","Torbjörn E. M. Nordling"],"pdf_url":"https://arxiv.org/pdf/2503.13008v1.pdf","comment":"15 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2503.01639v3","updated":"2025-03-17T10:01:57Z","published":"2025-03-03T15:19:16Z","title":"Cauchy-Schwarz Regularizers","summary":"  We introduce a novel class of regularization functions, called Cauchy-Schwarz\n(CS) regularizers, which can be designed to induce a wide range of properties\nin solution vectors of optimization problems. To demonstrate the versatility of\nCS regularizers, we derive regularization functions that promote\ndiscrete-valued vectors, eigenvectors of a given matrix, and orthogonal\nmatrices. The resulting CS regularizers are simple, differentiable, and can be\nfree of spurious stationary points, making them suitable for gradient-based\nsolvers and large-scale optimization problems. In addition, CS regularizers\nautomatically adapt to the appropriate scale, which is, for example, beneficial\nwhen discretizing the weights of neural networks. To demonstrate the efficacy\nof CS regularizers, we provide results for solving underdetermined systems of\nlinear equations and weight quantization in neural networks. Furthermore, we\ndiscuss specializations, variations, and generalizations, which lead to an even\nbroader class of new and possibly more powerful regularizers.\n","authors":["Sueda Taner","Ziyi Wang","Christoph Studer"],"pdf_url":"https://arxiv.org/pdf/2503.01639v3.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13001v1","updated":"2025-03-17T09:56:39Z","published":"2025-03-17T09:56:39Z","title":"Linear-Size Neural Network Representation of Piecewise Affine Functions\n  in $\\mathbb{R}^2$","summary":"  It is shown that any continuous piecewise affine (CPA) function\n$\\mathbb{R}^2\\to\\mathbb{R}$ with $p$ pieces can be represented by a ReLU neural\nnetwork with two hidden layers and $O(p)$ neurons. Unlike prior work, which\nfocused on convex pieces, this analysis considers CPA functions with connected\nbut potentially non-convex pieces.\n","authors":["Leo Zanotti"],"pdf_url":"https://arxiv.org/pdf/2503.13001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22311v2","updated":"2025-03-17T09:56:35Z","published":"2024-10-29T17:53:15Z","title":"Convex Formulations for Training Two-Layer ReLU Neural Networks","summary":"  Solving non-convex, NP-hard optimization problems is crucial for training\nmachine learning models, including neural networks. However, non-convexity\noften leads to black-box machine learning models with unclear inner workings.\nWhile convex formulations have been used for verifying neural network\nrobustness, their application to training neural networks remains less\nexplored. In response to this challenge, we reformulate the problem of training\ninfinite-width two-layer ReLU networks as a convex completely positive program\nin a finite-dimensional (lifted) space. Despite the convexity, solving this\nproblem remains NP-hard due to the complete positivity constraint. To overcome\nthis challenge, we introduce a semidefinite relaxation that can be solved in\npolynomial time. We then experimentally evaluate the tightness of this\nrelaxation, demonstrating its competitive performance in test accuracy across a\nrange of classification tasks.\n","authors":["Karthik Prakhya","Tolga Birdal","Alp Yurtsever"],"pdf_url":"https://arxiv.org/pdf/2410.22311v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2309.15604v2","updated":"2025-03-17T09:52:50Z","published":"2023-09-27T12:07:21Z","title":"Entropic Matching for Expectation Propagation of Markov Jump Processes","summary":"  We propose a novel, tractable latent state inference scheme for Markov jump\nprocesses, for which exact inference is often intractable. Our approach is\nbased on an entropic matching framework that can be embedded into the\nwell-known expectation propagation algorithm. We demonstrate the effectiveness\nof our method by providing closed-form results for a simple family of\napproximate distributions and apply it to the general class of chemical\nreaction networks, which are a crucial tool for modeling in systems biology.\nMoreover, we derive closed-form expressions for point estimation of the\nunderlying parameters using an approximate expectation maximization procedure.\nWe evaluate our method across various chemical reaction networks and compare it\nto multiple baseline approaches, demonstrating superior performance in\napproximating the mean of the posterior process. Finally, we discuss the\nlimitations of our method and potential avenues for future improvement,\nhighlighting its promising direction for addressing complex continuous-time\nBayesian inference problems.\n","authors":["Yannick Eich","Bastian Alt","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2309.15604v2.pdf","comment":"AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.12993v1","updated":"2025-03-17T09:47:42Z","published":"2025-03-17T09:47:42Z","title":"Robot Policy Transfer with Online Demonstrations: An Active\n  Reinforcement Learning Approach","summary":"  Transfer Learning (TL) is a powerful tool that enables robots to transfer\nlearned policies across different environments, tasks, or embodiments. To\nfurther facilitate this process, efforts have been made to combine it with\nLearning from Demonstrations (LfD) for more flexible and efficient policy\ntransfer. However, these approaches are almost exclusively limited to offline\ndemonstrations collected before policy transfer starts, which may suffer from\nthe intrinsic issue of covariance shift brought by LfD and harm the performance\nof policy transfer. Meanwhile, extensive work in the learning-from-scratch\nsetting has shown that online demonstrations can effectively alleviate\ncovariance shift and lead to better policy performance with improved sample\nefficiency. This work combines these insights to introduce online\ndemonstrations into a policy transfer setting. We present Policy Transfer with\nOnline Demonstrations, an active LfD algorithm for policy transfer that can\noptimize the timing and content of queries for online episodic expert\ndemonstrations under a limited demonstration budget. We evaluate our method in\neight robotic scenarios, involving policy transfer across diverse environment\ncharacteristics, task objectives, and robotic embodiments, with the aim to\ntransfer a trained policy from a source task to a related but different target\ntask. The results show that our method significantly outperforms all baselines\nin terms of average success rate and sample efficiency, compared to two\ncanonical LfD methods with offline demonstrations and one active LfD method\nwith online demonstrations. Additionally, we conduct preliminary sim-to-real\ntests of the transferred policy on three transfer scenarios in the real-world\nenvironment, demonstrating the policy effectiveness on a real robot\nmanipulator.\n","authors":["Muhan Hou","Koen Hindriks","A. E. Eiben","Kim Baraka"],"pdf_url":"https://arxiv.org/pdf/2503.12993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16944v2","updated":"2025-03-17T09:46:45Z","published":"2025-01-28T13:37:44Z","title":"Exact Computation of Any-Order Shapley Interactions for Graph Neural\n  Networks","summary":"  Albeit the ubiquitous use of Graph Neural Networks (GNNs) in machine learning\n(ML) prediction tasks involving graph-structured data, their interpretability\nremains challenging. In explainable artificial intelligence (XAI), the Shapley\nValue (SV) is the predominant method to quantify contributions of individual\nfeatures to a ML model's output. Addressing the limitations of SVs in complex\nprediction models, Shapley Interactions (SIs) extend the SV to groups of\nfeatures. In this work, we explain single graph predictions of GNNs with SIs\nthat quantify node contributions and interactions among multiple nodes. By\nexploiting the GNN architecture, we show that the structure of interactions in\nnode embeddings are preserved for graph prediction. As a result, the\nexponential complexity of SIs depends only on the receptive fields, i.e. the\nmessage-passing ranges determined by the connectivity of the graph and the\nnumber of convolutional layers. Based on our theoretical results, we introduce\nGraphSHAP-IQ, an efficient approach to compute any-order SIs exactly.\nGraphSHAP-IQ is applicable to popular message passing techniques in conjunction\nwith a linear global pooling and output layer. We showcase that GraphSHAP-IQ\nsubstantially reduces the exponential complexity of computing exact SIs on\nmultiple benchmark datasets. Beyond exact computation, we evaluate\nGraphSHAP-IQ's approximation of SIs on popular GNN architectures and compare\nwith existing baselines. Lastly, we visualize SIs of real-world water\ndistribution networks and molecule structures using a SI-Graph.\n","authors":["Maximilian Muschalik","Fabian Fumagalli","Paolo Frazzetto","Janine Strotherm","Luca Hermes","Alessandro Sperduti","Eyke Hüllermeier","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2501.16944v2.pdf","comment":"Preprint Version. Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12978v1","updated":"2025-03-17T09:36:07Z","published":"2025-03-17T09:36:07Z","title":"Enhancing Job Salary Prediction with Disentangled Composition Effect\n  Modeling: A Neural Prototyping Approach","summary":"  In the era of the knowledge economy, understanding how job skills influence\nsalary is crucial for promoting recruitment with competitive salary systems and\naligned salary expectations. Despite efforts on salary prediction based on job\npositions and talent demographics, there still lacks methods to effectively\ndiscern the set-structured skills' intricate composition effect on job salary.\nWhile recent advances in neural networks have significantly improved accurate\nset-based quantitative modeling, their lack of explainability hinders obtaining\ninsights into the skills' composition effects. Indeed, model explanation for\nset data is challenging due to the combinatorial nature, rich semantics, and\nunique format. To this end, in this paper, we propose a novel intrinsically\nexplainable set-based neural prototyping approach, namely \\textbf{LGDESetNet},\nfor explainable salary prediction that can reveal disentangled skill sets that\nimpact salary from both local and global perspectives. Specifically, we propose\na skill graph-enhanced disentangled discrete subset selection layer to identify\nmulti-faceted influential input subsets with varied semantics. Furthermore, we\npropose a set-oriented prototype learning method to extract globally\ninfluential prototypical sets. The resulting output is transparently derived\nfrom the semantic interplay between these input subsets and global prototypes.\nExtensive experiments on four real-world datasets demonstrate that our method\nachieves superior performance than state-of-the-art baselines in salary\nprediction while providing explainable insights into salary-influencing\npatterns.\n","authors":["Yang Ji","Ying Sun","Hengshu Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.12978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02136v2","updated":"2025-03-17T09:25:50Z","published":"2024-11-04T14:49:01Z","title":"Advanced computer vision for extracting georeferenced vehicle\n  trajectories from drone imagery","summary":"  This paper presents a framework for extracting georeferenced vehicle\ntrajectories from high-altitude drone imagery, addressing key challenges in\nurban traffic monitoring and the limitations of traditional ground-based\nsystems. Our approach integrates several novel contributions, including a\ntailored object detector optimized for high-altitude bird's-eye view\nperspectives, a unique track stabilization method that uses detected vehicle\nbounding boxes as exclusion masks during image registration, and an orthophoto\nand master frame-based georeferencing strategy that enhances consistent\nalignment across multiple drone viewpoints. Additionally, our framework\nfeatures robust vehicle dimension estimation and detailed road segmentation,\nenabling comprehensive traffic analysis. Conducted in the Songdo International\nBusiness District, South Korea, the study utilized a multi-drone experiment\ncovering 20 intersections, capturing approximately 12TB of 4K video data over\nfour days. The framework produced two high-quality datasets: the Songdo Traffic\ndataset, comprising approximately 700,000 unique vehicle trajectories, and the\nSongdo Vision dataset, containing over 5,000 human-annotated images with about\n300,000 vehicle instances in four classes. Comparisons with high-precision\nsensor data from an instrumented probe vehicle highlight the accuracy and\nconsistency of our extraction pipeline in dense urban environments. The public\nrelease of Songdo Traffic and Songdo Vision, and the complete source code for\nthe extraction pipeline, establishes new benchmarks in data quality,\nreproducibility, and scalability in traffic research. Results demonstrate the\npotential of integrating drone technology with advanced computer vision for\nprecise and cost-effective urban traffic monitoring, providing valuable\nresources for developing intelligent transportation systems and enhancing\ntraffic management strategies.\n","authors":["Robert Fonod","Haechan Cho","Hwasoo Yeo","Nikolas Geroliminis"],"pdf_url":"https://arxiv.org/pdf/2411.02136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12966v1","updated":"2025-03-17T09:22:14Z","published":"2025-03-17T09:22:14Z","title":"Optimal Denoising in Score-Based Generative Models: The Role of Data\n  Regularity","summary":"  Score-based generative models achieve state-of-the-art sampling performance\nby denoising a distribution perturbed by Gaussian noise. In this paper, we\nfocus on a single deterministic denoising step, and compare the optimal\ndenoiser for the quadratic loss, we name ''full-denoising'', to the alternative\n''half-denoising'' introduced by Hyv{\\\"a}rinen (2024). We show that looking at\nthe performances in term of distance between distribution tells a more nuanced\nstory, with different assumptions on the data leading to very different\nconclusions.We prove that half-denoising is better than full-denoising for\nregular enough densities, while full-denoising is better for singular densities\nsuch as mixtures of Dirac measures or densities supported on a low-dimensional\nsubspace. In the latter case, we prove that full-denoising can alleviate the\ncurse of dimensionality under a linear manifold hypothesis.\n","authors":["Eliot Beyler","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2503.12966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12964v1","updated":"2025-03-17T09:19:12Z","published":"2025-03-17T09:19:12Z","title":"Training Video Foundation Models with NVIDIA NeMo","summary":"  Video Foundation Models (VFMs) have recently been used to simulate the real\nworld to train physical AI systems and develop creative visual experiences.\nHowever, there are significant challenges in training large-scale, high quality\nVFMs that can generate high-quality videos. We present a scalable, open-source\nVFM training pipeline with NVIDIA NeMo, providing accelerated video dataset\ncuration, multimodal data loading, and parallelized video diffusion model\ntraining and inference. We also provide a comprehensive performance analysis\nhighlighting best practices for efficient VFM training and inference.\n","authors":["Zeeshan Patel","Ethan He","Parth Mannan","Xiaowei Ren","Ryan Wolf","Niket Agarwal","Jacob Huffman","Zhuoyao Wang","Carl Wang","Jack Chang","Yan Bai","Tommy Huang","Linnan Wang","Sahil Jain","Shanmugam Ramasamy","Joseph Jennings","Ekaterina Sirazitdinova","Oleg Sudakov","Mingyuan Ma","Bobby Chen","Forrest Lin","Hao Wang","Vasanth Rao Naik Sabavat","Sriharsha Niverty","Rong Ou","Pallab Bhattacharya","David Page","Nima Tajbakhsh","Ashwath Aithal"],"pdf_url":"https://arxiv.org/pdf/2503.12964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19488v2","updated":"2025-03-17T09:01:38Z","published":"2024-11-29T06:06:35Z","title":"Interleaved-Modal Chain-of-Thought","summary":"  Chain-of-Thought (CoT) prompting elicits large language models (LLMs) to\nproduce a series of intermediate reasoning steps before arriving at the final\nanswer. However, when transitioning to vision-language models (VLMs), their\ntext-only rationales struggle to express the fine-grained associations with the\noriginal image. In this paper, we propose an image-incorporated multimodal\nChain-of-Thought, named \\textbf{Interleaved-modal Chain-of-Thought (ICoT)},\nwhich generates sequential reasoning steps consisting of paired visual and\ntextual rationales to infer the final answer. Intuitively, the novel ICoT\nrequires VLMs to enable the generation of fine-grained interleaved-modal\ncontent, which is hard for current VLMs to fulfill. Considering that the\nrequired visual information is usually part of the input image, we propose\n\\textbf{Attention-driven Selection (ADS)} to realize ICoT over existing VLMs.\nADS intelligently inserts regions of the input image to generate the\ninterleaved-modal reasoning steps with ignorable additional latency. ADS relies\nsolely on the attention map of VLMs without the need for parameterization, and\ntherefore it is a plug-and-play strategy that can be generalized to a spectrum\nof VLMs. We apply ADS to realize ICoT on two popular VLMs of different\narchitectures. Extensive evaluations of three benchmarks have shown that ICoT\nprompting achieves substantial performance (up to 14\\%) and interpretability\nimprovements compared to existing multimodal CoT prompting methods.\n","authors":["Jun Gao","Yongqi Li","Ziqiang Cao","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2411.19488v2.pdf","comment":"CVPR 2025 Main Conference"},{"id":"http://arxiv.org/abs/2503.12941v1","updated":"2025-03-17T08:56:03Z","published":"2025-03-17T08:56:03Z","title":"HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of\n  Multimodal Large Language Model","summary":"  Instruction tuning is widely used to improve a pre-trained Multimodal Large\nLanguage Model (MLLM) by training it on curated task-specific datasets,\nenabling better comprehension of human instructions. However, it is infeasible\nto collect all possible instruction datasets simultaneously in real-world\nscenarios. Thus, enabling MLLM with continual instruction tuning is essential\nfor maintaining their adaptability. However, existing methods often trade off\nmemory efficiency for performance gains, significantly compromising overall\nefficiency. In this paper, we propose a task-specific expansion and\ntask-general fusion framework based on the variations in Centered Kernel\nAlignment (CKA) similarity across different model layers when trained on\ndiverse datasets. Furthermore, we analyze the information leakage present in\nthe existing benchmark and propose a new and more challenging benchmark to\nrationally evaluate the performance of different methods. Comprehensive\nexperiments showcase a significant performance improvement of our method\ncompared to existing state-of-the-art methods. Our code will be public\navailable.\n","authors":["Haiyang Guo","Fanhu Zeng","Ziwei Xiang","Fei Zhu","Da-Han Wang","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.12941v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.12937v1","updated":"2025-03-17T08:51:44Z","published":"2025-03-17T08:51:44Z","title":"R1-VL: Learning to Reason with Multimodal Large Language Models via\n  Step-wise Group Relative Policy Optimization","summary":"  Recent studies generally enhance MLLMs' reasoning capabilities via supervised\nfine-tuning on high-quality chain-of-thought reasoning data, which often leads\nmodels to merely imitate successful reasoning paths without understanding what\nthe wrong reasoning paths are. In this work, we aim to enhance the MLLMs'\nreasoning ability beyond passively imitating positive reasoning paths. To this\nend, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new\nonline reinforcement learning framework that enables MLLMs to self-improve\nreasoning ability via simple, effective and dense step-wise rewarding.\nSpecifically, StepGRPO introduces two novel rule-based reasoning rewards:\nStep-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity\nReward (StepRVR). StepRAR rewards the reasoning paths that contain necessary\nintermediate reasoning steps via a soft key-step matching technique, while\nStepRAR rewards reasoning paths that follow a well-structured and logically\nconsistent reasoning process through a reasoning completeness and logic\nevaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series\nof MLLMs with outstanding capabilities in step-by-step reasoning. Extensive\nexperiments over 8 benchmarks demonstrate the superiority of our methods.\n","authors":["Jingyi Zhang","Jiaxing Huang","Huanjin Yao","Shunyu Liu","Xikun Zhang","Shijian Lu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.12937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01684v2","updated":"2025-03-17T08:45:19Z","published":"2025-02-02T07:42:45Z","title":"Leveraging Joint Predictive Embedding and Bayesian Inference in Graph\n  Self Supervised Learning","summary":"  Graph representation learning has emerged as a cornerstone for tasks like\nnode classification and link prediction, yet prevailing self-supervised\nlearning (SSL) methods face challenges such as computational inefficiency,\nreliance on contrastive objectives, and representation collapse. Existing\napproaches often depend on feature reconstruction, negative sampling, or\ncomplex decoders, which introduce training overhead and hinder generalization.\nFurther, current techniques which address such limitations fail to account for\nthe contribution of node embeddings to a certain prediction in the absence of\nlabeled nodes. To address these limitations, we propose a novel joint embedding\npredictive framework for graph SSL that eliminates contrastive objectives and\nnegative sampling while preserving semantic and structural information.\nAdditionally, we introduce a semantic-aware objective term that incorporates\npseudo-labels derived from Gaussian Mixture Models (GMMs), enhancing node\ndiscriminability by evaluating latent feature contributions. Extensive\nexperiments demonstrate that our framework outperforms state-of-the-art graph\nSSL methods across benchmarks, achieving superior performance without\ncontrastive loss or complex decoders. Key innovations include (1) a\nnon-contrastive, view-invariant joint embedding predictive architecture, (2)\nLeveraging single context and multiple targets relationship between subgraphs,\nand (3) GMM-based pseudo-label scoring to capture semantic contributions. This\nwork advances graph SSL by offering a computationally efficient,\ncollapse-resistant paradigm that bridges spatial and semantic graph features\nfor downstream tasks. The code for our paper can be found at\nhttps://github.com/Deceptrax123/JPEB-GSSL\n","authors":["Srinitish Srinivasan","Omkumar CU"],"pdf_url":"https://arxiv.org/pdf/2502.01684v2.pdf","comment":"Preprit. Under Review"},{"id":"http://arxiv.org/abs/2503.12932v1","updated":"2025-03-17T08:41:43Z","published":"2025-03-17T08:41:43Z","title":"Efficient Action-Constrained Reinforcement Learning via\n  Acceptance-Rejection Method and Augmented MDPs","summary":"  Action-constrained reinforcement learning (ACRL) is a generic framework for\nlearning control policies with zero action constraint violation, which is\nrequired by various safety-critical and resource-constrained applications. The\nexisting ACRL methods can typically achieve favorable constraint satisfaction\nbut at the cost of either high computational burden incurred by the quadratic\nprograms (QP) or increased architectural complexity due to the use of\nsophisticated generative models. In this paper, we propose a generic and\ncomputationally efficient framework that can adapt a standard unconstrained RL\nmethod to ACRL through two modifications: (i) To enforce the action\nconstraints, we leverage the classic acceptance-rejection method, where we\ntreat the unconstrained policy as the proposal distribution and derive a\nmodified policy with feasible actions. (ii) To improve the acceptance rate of\nthe proposal distribution, we construct an augmented two-objective Markov\ndecision process (MDP), which include additional self-loop state transitions\nand a penalty signal for the rejected actions. This augmented MDP incentives\nthe learned policy to stay close to the feasible action sets. Through extensive\nexperiments in both robot control and resource allocation domains, we\ndemonstrate that the proposed framework enjoys faster training progress, better\nconstraint satisfaction, and a lower action inference time simultaneously than\nthe state-of-the-art ACRL methods. We have made the source code publicly\navailable to encourage further research in this direction.\n","authors":["Wei Hung","Shao-Hua Sun","Ping-Chun Hsieh"],"pdf_url":"https://arxiv.org/pdf/2503.12932v1.pdf","comment":"23 pages, 14 figures. Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12930v1","updated":"2025-03-17T08:40:50Z","published":"2025-03-17T08:40:50Z","title":"Augmented Invertible Koopman Autoencoder for long-term time series\n  forecasting","summary":"  Following the introduction of Dynamic Mode Decomposition and its numerous\nextensions, many neural autoencoder-based implementations of the Koopman\noperator have recently been proposed. This class of methods appears to be of\ninterest for modeling dynamical systems, either through direct long-term\nprediction of the evolution of the state or as a powerful embedding for\ndownstream methods. In particular, a recent line of work has developed\ninvertible Koopman autoencoders (IKAEs), which provide an exact reconstruction\nof the input state thanks to their analytically invertible encoder, based on\ncoupling layer normalizing flow models. We identify that the conservation of\nthe dimension imposed by the normalizing flows is a limitation for the IKAE\nmodels, and thus we propose to augment the latent state with a second,\nnon-invertible encoder network. This results in our new model: the Augmented\nInvertible Koopman AutoEncoder (AIKAE). We demonstrate the relevance of the\nAIKAE through a series of long-term time series forecasting experiments, on\nsatellite image time series as well as on a benchmark involving predictions\nbased on a large lookback window of observations.\n","authors":["Anthony Frion","Lucas Drumetz","Mauro Dalla Mura","Guillaume Tochon","Abdeldjalil Aïssa-El-Bey"],"pdf_url":"https://arxiv.org/pdf/2503.12930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12923v1","updated":"2025-03-17T08:36:16Z","published":"2025-03-17T08:36:16Z","title":"Lifelong Reinforcement Learning with Similarity-Driven Weighting by\n  Large Models","summary":"  Lifelong Reinforcement Learning (LRL) holds significant potential for\naddressing sequential tasks, but it still faces considerable challenges. A key\ndifficulty lies in effectively preventing catastrophic forgetting and\nfacilitating knowledge transfer while maintaining reliable decision-making\nperformance across subsequent tasks in dynamic environments. To tackle this, we\npropose a novel framework, SDW (Similarity-Driven Weighting Framework), which\nleverages large-language-model-generated dynamic functions to precisely control\nthe training process. The core of SDW lies in two functions pre-generated by\nlarge models: the task similarity function and the weight computation function.\nThe task similarity function extracts multidimensional features from task\ndescriptions to quantify the similarities and differences between tasks in\nterms of states, actions, and rewards. The weight computation function\ndynamically generates critical training parameters based on the similarity\ninformation, including the proportion of old task data stored in the Replay\nBuffer and the strategy consistency weight in the loss function, enabling an\nadaptive balance between learning new tasks and transferring knowledge from\nprevious tasks. By generating function code offline prior to training, rather\nthan relying on large-model inference during the training process, the SDW\nframework reduces computational overhead while maintaining efficiency in\nsequential task scenarios. Experimental results on Atari and MiniHack\nsequential tasks demonstrate that SDW significantly outperforms existing\nlifelong reinforcement learning methods.\n","authors":["Zhiyi Huang","Xiaohan Shan","Jianmin Li"],"pdf_url":"https://arxiv.org/pdf/2503.12923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08306v3","updated":"2025-03-17T08:35:32Z","published":"2025-03-11T11:16:47Z","title":"Reasoning in visual navigation of end-to-end trained agents: a dynamical\n  systems approach","summary":"  Progress in Embodied AI has made it possible for end-to-end-trained agents to\nnavigate in photo-realistic environments with high-level reasoning and\nzero-shot or language-conditioned behavior, but benchmarks are still dominated\nby simulation. In this work, we focus on the fine-grained behavior of\nfast-moving real robots and present a large-scale experimental study involving\n\\numepisodes{} navigation episodes in a real environment with a physical robot,\nwhere we analyze the type of reasoning emerging from end-to-end training. In\nparticular, we study the presence of realistic dynamics which the agent learned\nfor open-loop forecasting, and their interplay with sensing. We analyze the way\nthe agent uses latent memory to hold elements of the scene structure and\ninformation gathered during exploration. We probe the planning capabilities of\nthe agent, and find in its memory evidence for somewhat precise plans over a\nlimited horizon. Furthermore, we show in a post-hoc analysis that the value\nfunction learned by the agent relates to long-term planning. Put together, our\nexperiments paint a new picture on how using tools from computer vision and\nsequential decision making have led to new capabilities in robotics and\ncontrol. An interactive tool is available at\neurope.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.\n","authors":["Steeven Janny","Hervé Poirier","Leonid Antsfeld","Guillaume Bono","Gianluca Monaci","Boris Chidlovskii","Francesco Giuliari","Alessio Del Bue","Christian Wolf"],"pdf_url":"https://arxiv.org/pdf/2503.08306v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14598v2","updated":"2025-03-17T08:34:07Z","published":"2024-02-04T09:58:17Z","title":"EMN: Brain-inspired Elastic Memory Network for Quick Domain Adaptive\n  Feature Mapping","summary":"  Utilizing unlabeled data in the target domain to perform continuous\noptimization is critical to enhance the generalization ability of neural\nnetworks. Most domain adaptation methods focus on time-consuming optimization\nof deep feature extractors, which limits the deployment on lightweight edge\ndevices. Inspired by the memory mechanism and powerful generalization ability\nof biological neural networks in human brains, we propose a novel gradient-free\nElastic Memory Network, namely EMN, to support quick fine-tuning of the mapping\nbetween features and prediction without heavy optimization of deep features. In\nparticular, EMN adopts randomly connected neurons to memorize the association\nof features and labels, where the signals in the network are propagated as\nimpulses, and the prediction is made by associating the memories stored on\nneurons based on their confidence. More importantly, EMN supports reinforced\nmemorization of feature mapping based on unlabeled data to quickly adapt to a\nnew domain. Experiments based on four cross-domain real-world datasets show\nthat EMN can achieve up to 10% enhancement of performance while only needing\nless than 1% timing cost of traditional domain adaptation methods.\n","authors":["Jianming Lv","Chengjun Wang","Depin Liang","Qianli Ma","Wei Chen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2402.14598v2.pdf","comment":"15 pages,15 figures"},{"id":"http://arxiv.org/abs/2411.19951v4","updated":"2025-03-17T08:33:00Z","published":"2024-11-29T18:59:54Z","title":"Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation","summary":"  Recent years have witnessed the success of Multimodal Large Language Models\n(MLLMs) in the vision understanding domain. The success of these models can\nlargely be attributed to the dominant scaling law, which states that larger\nparameter sizes and data volumes contribute to better performance. Notably,\ndata scaling has mainly been powered by automatic data pipelines, which center\naround the self-instruction of LLMs. The paradigm has been taken for granted\nfor quite some time, but the study of the effectiveness of scaling with these\ndata has been neglected for a long time. In this context, this work revisits\nscaling with synthetic data and focuses on developing video-LLMs from a\ndata-centric perspective. Our main study approach is fine-tuning pre-trained\nimage-LLMs with video data and investigating learning efficiency through data\nscaling. Results from our preliminary experiments reveal a low learning\nefficiency phenomenon when simply scaling up video data samples, which, through\nour probing, can be ascribed to a lack of instruction diversity. Aiming at this\nissue, we propose a data augmentation method called Sparrow, which synthesizes\nvideo-like samples from pure text instruction data. Mixing these synthetic\nsamples with the video data enables a more efficient training scheme. Through\ncomprehensive experiments, we demonstrate that our proposed method achieves\nperformance comparable to or even superior to baselines trained with many more\nsamples. Meanwhile, we find that incorporating these synthetic samples can\nboost the performance of long video understanding without training with long\nvideo data. The code and data examples are available at\nhttps://github.com/VITA-MLLM/Sparrow.\n","authors":["Shukang Yin","Chaoyou Fu","Sirui Zhao","Yunhang Shen","Chunjiang Ge","Yan Yang","Zuwei Long","Yuhan Dai","Yongdong Luo","Haoyu Cao","Tong Xu","Xing Sun","Caifeng Shan","Ran He","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2411.19951v4.pdf","comment":"Project page: https://github.com/VITA-MLLM/Sparrow"},{"id":"http://arxiv.org/abs/2503.12919v1","updated":"2025-03-17T08:31:25Z","published":"2025-03-17T08:31:25Z","title":"COSMOS: Continuous Simplicial Neural Networks","summary":"  Simplicial complexes provide a powerful framework for modeling high-order\ninteractions in structured data, making them particularly suitable for\napplications such as trajectory prediction and mesh processing. However,\nexisting simplicial neural networks (SNNs), whether convolutional or\nattention-based, rely primarily on discrete filtering techniques, which can be\nrestrictive. In contrast, partial differential equations (PDEs) on simplicial\ncomplexes offer a principled approach to capture continuous dynamics in such\nstructures. In this work, we introduce COntinuous SiMplicial neural netwOrkS\n(COSMOS), a novel SNN architecture derived from PDEs on simplicial complexes.\nWe provide theoretical and experimental justifications of COSMOS's stability\nunder simplicial perturbations. Furthermore, we investigate the over-smoothing\nphenomenon, a common issue in geometric deep learning, demonstrating that\nCOSMOS offers better control over this effect than discrete SNNs. Our\nexperiments on real-world datasets of ocean trajectory prediction and\nregression on partial deformable shapes demonstrate that COSMOS achieves\ncompetitive performance compared to state-of-the-art SNNs in complex and noisy\nenvironments.\n","authors":["Aref Einizade","Dorina Thanou","Fragkiskos D. Malliaros","Jhony H. Giraldo"],"pdf_url":"https://arxiv.org/pdf/2503.12919v1.pdf","comment":"17 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.17912v2","updated":"2025-03-17T08:23:08Z","published":"2025-02-25T07:20:00Z","title":"Decoupled Graph Energy-based Model for Node Out-of-Distribution\n  Detection on Heterophilic Graphs","summary":"  Despite extensive research efforts focused on OOD detection on images, OOD\ndetection on nodes in graph learning remains underexplored. The dependence\namong graph nodes hinders the trivial adaptation of existing approaches on\nimages that assume inputs to be i.i.d. sampled, since many unique features and\nchallenges specific to graphs are not considered, such as the heterophily\nissue. Recently, GNNSafe, which considers node dependence, adapted energy-based\ndetection to the graph domain with state-of-the-art performance, however, it\nhas two serious issues: 1) it derives node energy from classification logits\nwithout specifically tailored training for modeling data distribution, making\nit less effective at recognizing OOD data; 2) it highly relies on energy\npropagation, which is based on homophily assumption and will cause significant\nperformance degradation on heterophilic graphs, where the node tends to have\ndissimilar distribution with its neighbors. To address the above issues, we\nsuggest training EBMs by MLE to enhance data distribution modeling and remove\nenergy propagation to overcome the heterophily issues. However, training EBMs\nvia MLE requires performing MCMC sampling on both node feature and node\nneighbors, which is challenging due to the node interdependence and discrete\ngraph topology. To tackle the sampling challenge, we introduce DeGEM, which\ndecomposes the learning process into two parts: a graph encoder that leverages\ntopology information for node representations and an energy head that operates\nin latent space. Extensive experiments validate that DeGEM, without OOD\nexposure during training, surpasses previous state-of-the-art methods,\nachieving an average AUROC improvement of 6.71% on homophilic graphs and 20.29%\non heterophilic graphs, and even outperform methods trained with OOD exposure.\nOur code is available at: https://github.com/draym28/DeGEM.\n","authors":["Yuhan Chen","Yihong Luo","Yifan Song","Pengwen Dai","Jing Tang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2502.17912v2.pdf","comment":"The first two authors contributed equally to this work; ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12912v1","updated":"2025-03-17T08:21:33Z","published":"2025-03-17T08:21:33Z","title":"Pose as a Modality: A Psychology-Inspired Network for Personality\n  Recognition with a New Multimodal Dataset","summary":"  In recent years, predicting Big Five personality traits from multimodal data\nhas received significant attention in artificial intelligence (AI). However,\nexisting computational models often fail to achieve satisfactory performance.\nPsychological research has shown a strong correlation between pose and\npersonality traits, yet previous research has largely ignored pose data in\ncomputational models. To address this gap, we develop a novel multimodal\ndataset that incorporates full-body pose data. The dataset includes video\nrecordings of 287 participants completing a virtual interview with 36\nquestions, along with self-reported Big Five personality scores as labels. To\neffectively utilize this multimodal data, we introduce the Psychology-Inspired\nNetwork (PINet), which consists of three key modules: Multimodal Feature\nAwareness (MFA), Multimodal Feature Interaction (MFI), and Psychology-Informed\nModality Correlation Loss (PIMC Loss). The MFA module leverages the Vision\nMamba Block to capture comprehensive visual features related to personality,\nwhile the MFI module efficiently fuses the multimodal features. The PIMC Loss,\ngrounded in psychological theory, guides the model to emphasize different\nmodalities for different personality dimensions. Experimental results show that\nthe PINet outperforms several state-of-the-art baseline models. Furthermore,\nthe three modules of PINet contribute almost equally to the model's overall\nperformance. Incorporating pose data significantly enhances the model's\nperformance, with the pose modality ranking mid-level in importance among the\nfive modalities. These findings address the existing gap in personality-related\ndatasets that lack full-body pose data and provide a new approach for improving\nthe accuracy of personality prediction models, highlighting the importance of\nintegrating psychological insights into AI frameworks.\n","authors":["Bin Tang","Keqi Pan","Miao Zheng","Ning Zhou","Jialu Sui","Dandan Zhu","Cheng-Long Deng","Shu-Guang Kuai"],"pdf_url":"https://arxiv.org/pdf/2503.12912v1.pdf","comment":"9 pages, 6 figures, AAAI 2025 Oral"},{"id":"http://arxiv.org/abs/2503.12902v1","updated":"2025-03-17T08:03:47Z","published":"2025-03-17T08:03:47Z","title":"Experiments with Optimal Model Trees","summary":"  Model trees provide an appealing way to perform interpretable machine\nlearning for both classification and regression problems. In contrast to\n``classic'' decision trees with constant values in their leaves, model trees\ncan use linear combinations of predictor variables in their leaf nodes to form\npredictions, which can help achieve higher accuracy and smaller trees. Typical\nalgorithms for learning model trees from training data work in a greedy\nfashion, growing the tree in a top-down manner by recursively splitting the\ndata into smaller and smaller subsets. Crucially, the selected splits are only\nlocally optimal, potentially rendering the tree overly complex and less\naccurate than a tree whose structure is globally optimal for the training data.\nIn this paper, we empirically investigate the effect of constructing globally\noptimal model trees for classification and regression with linear support\nvector machines at the leaf nodes. To this end, we present mixed-integer linear\nprogramming formulations to learn optimal trees, compute such trees for a large\ncollection of benchmark data sets, and compare their performance against\ngreedily grown model trees in terms of interpretability and accuracy. We also\ncompare to classic optimal and greedily grown decision trees, random forests,\nand support vector machines. Our results show that optimal model trees can\nachieve competitive accuracy with very small trees. We also investigate the\neffect on the accuracy of replacing axis-parallel splits with multivariate\nones, foregoing interpretability while potentially obtaining greater accuracy.\n","authors":["Sabino Francesco Roselli","Eibe Frank"],"pdf_url":"https://arxiv.org/pdf/2503.12902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12899v1","updated":"2025-03-17T07:59:42Z","published":"2025-03-17T07:59:42Z","title":"A Semantic-based Optimization Approach for Repairing LLMs: Case Study on\n  Code Generation","summary":"  Language Models (LMs) are widely used in software engineering for code\ngeneration, but they may produce code with errors. Rather than repairing the\ngenerated code, an alternative way is to address the underlying failures of\nmodels. LM repair offers a lightweight solution to this challenge: it requires\nminimal data, reduces computational costs, and reduces the side effects. Unlike\nretraining, LM repair focuses on applying tailored updates to targeted neurons,\nmaking it ideal for scenarios with limited resources, high-performance demands,\nor strict safety requirements. In this paper, we propose \\ul{S}emantic\n\\ul{T}argeting for \\ul{A}nalytical \\ul{R}epair (\\textsc{STAR}), a pioneering\nand novel semantic-based optimization approach for repairing LLMs.\n\\textsc{STAR} realizes main operations in LM repair methods in an optimization\nprocess, including locating ``buggy neurons'', solving ``neuron patches'', and\npatching ``buggy neurons''. Correspondingly, it computes the deltas of weight\nmatrix as the prior information to guide optimization; and attributes the\ntargeted layers and neurons leveraging statistical insights. The neuron patches\nare computed with a solid semantic-based analytical formula, which directly\nbridges the changes to logits with the deltas of neurons, by steering latent\nrepresentations. Compared to the prior work of LM repair (\\textsc{MINT}) and\noptimization methods (\\textsc{SGD}), \\textsc{STAR} integrates their strengths\nwhile mitigating their limitations. \\textsc{STAR} supports solving multiple\nfailures together, significantly improving the usefulness. Evaluated on three\ncode generation tasks using popular code LMs, \\textsc{STAR} demonstrates\nsuperior effectiveness. Additionally, \\textsc{STAR} exhibits better efficiency.\nIn terms of side effects, namely the balance between generalization and\nspecificity, \\textsc{STAR} outperforms prior work by a significant margin.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12899v1.pdf","comment":"12 pages, 6 figure, 6 tables, under peer-review"},{"id":"http://arxiv.org/abs/2503.12897v1","updated":"2025-03-17T07:58:06Z","published":"2025-03-17T07:58:06Z","title":"Federated Continual Instruction Tuning","summary":"  A vast amount of instruction tuning data is crucial for the impressive\nperformance of Large Multimodal Models (LMMs), but the associated computational\ncosts and data collection demands during supervised fine-tuning make it\nimpractical for most researchers. Federated learning (FL) has the potential to\nleverage all distributed data and training resources to reduce the overhead of\njoint training. However, most existing methods assume a fixed number of tasks,\nwhile in real-world scenarios, clients continuously encounter new knowledge and\noften struggle to retain old tasks due to memory constraints. In this work, we\nintroduce the Federated Continual Instruction Tuning (FCIT) benchmark to model\nthis real-world challenge. Our benchmark includes two realistic scenarios,\nencompassing four different settings and twelve carefully curated instruction\ntuning datasets. To address the challenges posed by FCIT, we propose dynamic\nknowledge organization to effectively integrate updates from different tasks\nduring training and subspace selective activation to allocate task-specific\noutput during inference. Extensive experimental results demonstrate that our\nproposed method significantly enhances model performance across varying levels\nof data heterogeneity and catastrophic forgetting. Our source code and dataset\nwill be made publicly available.\n","authors":["Haiyang Guo","Fanhu Zeng","Fei Zhu","Wenzhuo Liu","Da-Han Wang","Jian Xu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.12897v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.13356v4","updated":"2025-03-17T07:46:49Z","published":"2024-06-19T09:03:21Z","title":"Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via\n  Benign Relearning","summary":"  Machine unlearning is a promising approach to mitigate undesirable\nmemorization of training data in ML models. However, in this work we show that\nexisting approaches for unlearning in LLMs are surprisingly susceptible to a\nsimple set of $\\textit{benign relearning attacks}$. With access to only a small\nand potentially loosely related set of data, we find that we can ''jog'' the\nmemory of unlearned models to reverse the effects of unlearning. For example,\nwe show that relearning on public medical articles can lead an unlearned LLM to\noutput harmful knowledge about bioweapons, and relearning general wiki\ninformation about the book series Harry Potter can force the model to output\nverbatim memorized text. We formalize this unlearning-relearning pipeline,\nexplore the attack across three popular unlearning benchmarks, and discuss\nfuture directions and guidelines that result from our study. Our work indicates\nthat current approximate unlearning methods simply suppress the model outputs\nand fail to robustly forget target knowledge in the LLMs.\n","authors":["Shengyuan Hu","Yiwei Fu","Zhiwei Steven Wu","Virginia Smith"],"pdf_url":"https://arxiv.org/pdf/2406.13356v4.pdf","comment":"ICLR 2025, 32 pages, 8 figures, 9 tables"},{"id":"http://arxiv.org/abs/2503.12893v1","updated":"2025-03-17T07:46:10Z","published":"2025-03-17T07:46:10Z","title":"Edgeworth Expansion for Semi-hard Triplet Loss","summary":"  We develop a higher-order asymptotic analysis for the semi-hard triplet loss\nusing the Edgeworth expansion. It is known that this loss function enforces\nthat embeddings of similar samples are close while those of dissimilar samples\nare separated by a specified margin. By refining the classical central limit\ntheorem, our approach quantifies the impact of the margin parameter and the\nskewness of the underlying data distribution on the loss behavior. In\nparticular, we derive explicit Edgeworth expansions that reveal first-order\ncorrections in terms of the third cumulant, thereby characterizing non-Gaussian\neffects present in the distribution of distance differences between\nanchor-positive and anchor-negative pairs. Our findings provide detailed\ninsight into the sensitivity of the semi-hard triplet loss to its parameters\nand offer guidance for choosing the margin to ensure training stability.\n","authors":["Masanari Kimura"],"pdf_url":"https://arxiv.org/pdf/2503.12893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12883v1","updated":"2025-03-17T07:28:15Z","published":"2025-03-17T07:28:15Z","title":"Early Detection of Forest Calamities in Homogeneous Stands -- Deep\n  Learning Applied to Bark-Beetle Outbreaks","summary":"  Climate change has increased the vulnerability of forests to insect-related\ndamage, resulting in widespread forest loss in Central Europe and highlighting\nthe need for effective, continuous monitoring systems. Remote sensing based\nforest health monitoring, oftentimes, relies on supervised machine learning\nalgorithms that require labeled training data. Monitoring temporal patterns\nthrough time series analysis offers a potential alternative for earlier\ndetection of disturbance but requires substantial storage resources. This study\ninvestigates the potential of a Deep Learning algorithm based on a Long Short\nTerm Memory (LSTM) Autoencoder for the detection of anomalies in forest health\n(e.g. bark beetle outbreaks), utilizing Sentinel-2 time series data. This\napproach is an alternative to supervised machine learning methods, avoiding the\nnecessity for labeled training data. Furthermore, it is more memory-efficient\nthan other time series analysis approaches, as a robust model can be created\nusing only a 26-week-long time series as input. In this study, we monitored\npure stands of spruce in Thuringia, Germany, over a 7-year period from 2018 to\nthe end of 2024. Our best model achieved a detection accuracy of 87% on test\ndata and was able to detect 61% of all anomalies at a very early stage (more\nthan a month before visible signs of forest degradation). Compared to another\nwidely used time series break detection algorithm - BFAST (Breaks For Additive\nSeason and Trend), our approach consistently detected higher percentage of\nanomalies at an earlier stage. These findings suggest that LSTM-based\nAutoencoders could provide a promising, resource-efficient approach to forest\nhealth monitoring, enabling more timely responses to emerging threats.\n","authors":["Maximilian Kirsch","Jakob Wernicke","Pawan Datta","Christine Preisach"],"pdf_url":"https://arxiv.org/pdf/2503.12883v1.pdf","comment":"24 pages, 18 figures, submitted to IEEE: Journal of Selected Topics\n  in Applied Earth Observations and Remote Sensing"},{"id":"http://arxiv.org/abs/2503.12882v1","updated":"2025-03-17T07:25:32Z","published":"2025-03-17T07:25:32Z","title":"DAPI: Domain Adaptive Toxicity Probe Vector Intervention for\n  Fine-Grained Detoxification","summary":"  There have been attempts to utilize linear probe for detoxification, with\nexisting studies relying on a single toxicity probe vector to reduce toxicity.\nHowever, toxicity can be fine-grained into various subcategories, making it\ndifficult to remove certain types of toxicity by using a single toxicity probe\nvector. To address this limitation, we propose a category-specific toxicity\nprobe vector approach. First, we train multiple toxicity probe vectors for\ndifferent toxicity categories. During generation, we dynamically select the\nmost relevant toxicity probe vector based on the current context. Finally, the\nselected vector is dynamically scaled and subtracted from model. Our method\nsuccessfully mitigated toxicity from categories that the single probe vector\napproach failed to detoxify. Experiments demonstrate that our approach achieves\nup to a 78.52% reduction in toxicity on the evaluation dataset, while fluency\nremains nearly unchanged, with only a 0.052% drop compared to the unsteered\nmodel.\n","authors":["Cho Hyeonsu","Dooyoung Kim","Youngjoong Ko"],"pdf_url":"https://arxiv.org/pdf/2503.12882v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.03801v3","updated":"2025-03-17T07:17:43Z","published":"2024-10-04T08:14:24Z","title":"P1-KAN: an effective Kolmogorov-Arnold network with application to\n  hydraulic valley optimization","summary":"  A new Kolmogorov-Arnold network (KAN) is proposed to approximate potentially\nirregular functions in high dimensions. We provide error bounds for this\napproximation, assuming that the Kolmogorov-Arnold expansion functions are\nsufficiently smooth. When the function is only continuous, we also provide\nuniversal approximation theorems. We show that it outperforms multilayer\nperceptrons in terms of accuracy and convergence speed. We also compare it with\nseveral proposed KAN networks: it outperforms all networks for irregular\nfunctions and achieves similar accuracy to the original spline-based KAN\nnetwork for smooth functions. Finally, we compare some of the KAN networks in\noptimizing a French hydraulic valley.\n","authors":["Xavier Warin"],"pdf_url":"https://arxiv.org/pdf/2410.03801v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05894v3","updated":"2025-03-17T06:54:47Z","published":"2024-10-08T10:48:50Z","title":"DimOL: Dimensional Awareness as A New 'Dimension' in Operator Learning","summary":"  In the realm of computational physics, an enduring topic is the numerical\nsolutions to partial differential equations (PDEs). Recently, the attention of\nresearchers has shifted towards Neural Operator methods, renowned for their\ncapability to approximate ``operators'' -- mappings from functions to\nfunctions. Despite the universal approximation theorem within neural operators,\nensuring error bounds often requires employing numerous Fourier layers.\nHowever, what about lightweight models? In response to this question, we\nintroduce DimOL (Dimension-aware Operator Learning), drawing insights from\ndimensional analysis. To implement DimOL, we propose the ProdLayer, which can\nbe seamlessly integrated into FNO-based and Transformer-based PDE solvers,\nenhancing their ability to handle sum-of-products structures inherent in many\nphysical systems. Empirically, DimOL models achieve up to 48% performance gain\nwithin the PDE datasets. Furthermore, by analyzing Fourier components' weights,\nwe can symbolically discern the physical significance of each term. This sheds\nlight on the opaque nature of neural networks, unveiling underlying physical\nprinciples.\n","authors":["Yichen Song","Jiaming Wang","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.05894v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12858v1","updated":"2025-03-17T06:40:06Z","published":"2025-03-17T06:40:06Z","title":"Harnessing Test-time Adaptation for NLU tasks Involving Dialects of\n  English","summary":"  Test-time adaptation (TTA) is an excellent method which helps generalize\nmodels across domains, tasks, and distributions without the use of labeled\ndatasets. Thus, TTA is very useful in natural language processing (NLP) in the\ndialectal setting, since oftentimes, models are trained on Standard American\nEnglish (SAE), evaluated on Indian English or Nigerian English, of which\ndistribution differs significantly from the former. This is especially useful\nsince dialectal datasets are scarce. In this paper, we explore one of the most\nfamous TTA techniques, SHOT, in dialectal NLP. We finetune and evaluate SHOT on\ndifferent combinations of dialectal GLUE. Our findings show that SHOT is a\nviable technique when labeled datasets are unavailable. We also theoretically\npropose the concept of dialectal gap and show that it has a positive\ncorrelation with the effectiveness of SHOT. We also find that in many cases,\nfinetuning on SAE yields higher performance than finetuning on dialectal data.\nOur code is available at https://github.com/dukenguyenxyz/dialect-adaptation\n","authors":["Duke Nguyen","Aditya Joshi","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2503.12858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10265v2","updated":"2025-03-17T06:39:25Z","published":"2024-01-03T15:02:40Z","title":"The Best Time for an Update: Risk-Sensitive Minimization of Age-Based\n  Metrics","summary":"  Popular methods to quantify transmitted data quality are the Age of\nInformation (AoI), the Query Age of Information (QAoI), and the Age of\nIncorrect Information (AoII). We consider these metrics in a point-to-point\nwireless communication system, where the transmitter monitors a process and\nsends status updates to a receiver. The challenge is to decide on the best time\nfor an update, balancing the transmission energy and the age-based metric at\nthe receiver. Due to the inherent risk of high age-based metric values causing\ncomplications such as unstable system states, we introduce the new concept of\nrisky states to denote states with high age-based metric. We use this new\nnotion of risky states to quantify and minimize this risk of experiencing high\nage-based metrics by directly deriving the frequency of risky states as a novel\nrisk-metric. Building on this foundation, we introduce two risk-sensitive\nstrategies for AoI, QAoI and AoII. The first strategy uses system knowledge,\ni.e., channel quality and packet arrival probability, to find an optimal\nstrategy that transmits when the age-based metric exceeds a tunable threshold.\nA lower threshold leads to higher risk-sensitivity. The second strategy uses an\nenhanced Q-learning approach and balances the age-based metric, the\ntransmission energy and the frequency of risky states without requiring\nknowledge about the system. Numerical results affirm our risk-sensitive\nstrategies' high effectiveness.\n","authors":["Wanja de Sombre","Andrea Ortiz","Frank Aurzada","Anja Klein"],"pdf_url":"https://arxiv.org/pdf/2401.10265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12856v1","updated":"2025-03-17T06:35:59Z","published":"2025-03-17T06:35:59Z","title":"Island-Based Evolutionary Computation with Diverse Surrogates and\n  Adaptive Knowledge Transfer for High-Dimensional Data-Driven Optimization","summary":"  In recent years, there has been a growing interest in data-driven\nevolutionary algorithms (DDEAs) employing surrogate models to approximate the\nobjective functions with limited data. However, current DDEAs are primarily\ndesigned for lower-dimensional problems and their performance drops\nsignificantly when applied to large-scale optimization problems (LSOPs). To\naddress the challenge, this paper proposes an offline DDEA named DSKT-DDEA.\nDSKT-DDEA leverages multiple islands that utilize different data to establish\ndiverse surrogate models, fostering diverse subpopulations and mitigating the\nrisk of premature convergence. In the intra-island optimization phase, a\nsemi-supervised learning method is devised to fine-tune the surrogates. It not\nonly facilitates data argumentation, but also incorporates the distribution\ninformation gathered during the search process to align the surrogates with the\nevolving local landscapes. Then, in the inter-island knowledge transfer phase,\nthe algorithm incorporates an adaptive strategy that periodically transfers\nindividual information and evaluates the transfer effectiveness in the new\nenvironment, facilitating global optimization efficacy. Experimental results\ndemonstrate that our algorithm is competitive with state-of-the-art DDEAs on\nproblems with up to 1000 dimensions, while also exhibiting decent parallelism\nand scalability. Our DSKT-DDEA is open-source and accessible at:\nhttps://github.com/LabGong/DSKT-DDEA.\n","authors":["Xian-Rong Zhang","Yue-Jiao Gong","Zhiguang Cao","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12856v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2503.12853v1","updated":"2025-03-17T06:27:43Z","published":"2025-03-17T06:27:43Z","title":"Adaptive Transformer Attention and Multi-Scale Fusion for Spine 3D\n  Segmentation","summary":"  This study proposes a 3D semantic segmentation method for the spine based on\nthe improved SwinUNETR to improve segmentation accuracy and robustness. Aiming\nat the complex anatomical structure of spinal images, this paper introduces a\nmulti-scale fusion mechanism to enhance the feature extraction capability by\nusing information of different scales, thereby improving the recognition\naccuracy of the model for the target area. In addition, the introduction of the\nadaptive attention mechanism enables the model to dynamically adjust the\nattention to the key area, thereby optimizing the boundary segmentation effect.\nThe experimental results show that compared with 3D CNN, 3D U-Net, and 3D U-Net\n+ Transformer, the model of this study has achieved significant improvements in\nmIoU, mDice, and mAcc indicators, and has better segmentation performance. The\nablation experiment further verifies the effectiveness of the proposed improved\nmethod, proving that multi-scale fusion and adaptive attention mechanism have a\npositive effect on the segmentation task. Through the visualization analysis of\nthe inference results, the model can better restore the real anatomical\nstructure of the spinal image. Future research can further optimize the\nTransformer structure and expand the data scale to improve the generalization\nability of the model. This study provides an efficient solution for the task of\nmedical image segmentation, which is of great significance to intelligent\nmedical image analysis.\n","authors":["Yanlin Xiang","Qingyuan He","Ting Xu","Ran Hao","Jiacheng Hu","Hanchao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16848v2","updated":"2025-03-17T06:25:26Z","published":"2024-12-22T04:18:02Z","title":"ACL-QL: Adaptive Conservative Level in Q-Learning for Offline\n  Reinforcement Learning","summary":"  Offline Reinforcement Learning (RL), which operates solely on static datasets\nwithout further interactions with the environment, provides an appealing\nalternative to learning a safe and promising control policy. The prevailing\nmethods typically learn a conservative policy to mitigate the problem of\nQ-value overestimation, but it is prone to overdo it, leading to an overly\nconservative policy. Moreover, they optimize all samples equally with fixed\nconstraints, lacking the nuanced ability to control conservative levels in a\nfine-grained manner. Consequently, this limitation results in a performance\ndecline. To address the above two challenges in a united way, we propose a\nframework, Adaptive Conservative Level in Q-Learning (ACL-QL), which limits the\nQ-values in a mild range and enables adaptive control on the conservative level\nover each state-action pair, i.e., lifting the Q-values more for good\ntransitions and less for bad transitions. We theoretically analyze the\nconditions under which the conservative level of the learned Q-function can be\nlimited in a mild range and how to optimize each transition adaptively.\nMotivated by the theoretical analysis, we propose a novel algorithm, ACL-QL,\nwhich uses two learnable adaptive weight functions to control the conservative\nlevel over each transition. Subsequently, we design a monotonicity loss and\nsurrogate losses to train the adaptive weight functions, Q-function, and policy\nnetwork alternatively. We evaluate ACL-QL on the commonly used D4RL benchmark\nand conduct extensive ablation studies to illustrate the effectiveness and\nstate-of-the-art performance compared to existing offline DRL baselines.\n","authors":["Kun Wu","Yinuo Zhao","Zhiyuan Xu","Zhengping Che","Chengxiang Yin","Chi Harold Liu","Feiferi Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2412.16848v2.pdf","comment":"19 pages, 4 figures, IEEE Transactions on Neural Networks and\n  Learning Systems (2024)"},{"id":"http://arxiv.org/abs/2406.17714v3","updated":"2025-03-17T05:36:58Z","published":"2024-06-25T16:56:17Z","title":"Compositional Models for Estimating Causal Effects","summary":"  Many real-world systems can be usefully represented as sets of interacting\ncomponents. Examples include computational systems, such as query processors\nand compilers, natural systems, such as cells and ecosystems, and social\nsystems, such as families and organizations. However, current approaches to\nestimating potential outcomes and causal effects typically treat such systems\nas single units, represent them with a fixed set of variables, and assume a\nhomogeneous data-generating process. In this work, we study a compositional\napproach for estimating individual-level potential outcomes and causal effects\nin structured systems, where each unit is represented by an instance-specific\ncomposition of multiple heterogeneous components. The compositional approach\ndecomposes unit-level causal queries into more fine-grained queries, explicitly\nmodeling how unit-level interventions affect component-level outcomes to\ngenerate a unit's outcome. We demonstrate this approach using modular neural\nnetwork architectures and show that it provides benefits for causal effect\nestimation from observational data, such as accurate causal effect estimation\nfor structured units, increased sample efficiency, improved overlap between\ntreatment and control groups, and compositional generalization to units with\nunseen combinations of components. Remarkably, our results show that\ncompositional modeling can improve the accuracy of causal estimation even when\ncomponent-level outcomes are unobserved. We also create and use a set of\nreal-world evaluation environments for the empirical evaluation of\ncompositional approaches for causal effect estimation and demonstrate the role\nof composition structure, varying amounts of component-level data access, and\ncomponent heterogeneity in the performance of compositional models as compared\nto the non-compositional approaches.\n","authors":["Purva Pruthi","David Jensen"],"pdf_url":"https://arxiv.org/pdf/2406.17714v3.pdf","comment":"Accepted at the Fourth Conference on Causal Learning and Reasoning\n  (CLeaR), 2025"},{"id":"http://arxiv.org/abs/2405.12419v2","updated":"2025-03-17T05:35:35Z","published":"2024-05-20T23:53:42Z","title":"GeoMask3D: Geometrically Informed Mask Selection for Self-Supervised\n  Point Cloud Learning in 3D","summary":"  We introduce a pioneering approach to self-supervised learning for point\nclouds, employing a geometrically informed mask selection strategy called\nGeoMask3D (GM3D) to boost the efficiency of Masked Auto Encoders (MAE). Unlike\nthe conventional method of random masking, our technique utilizes a\nteacher-student model to focus on intricate areas within the data, guiding the\nmodel's focus toward regions with higher geometric complexity. This strategy is\ngrounded in the hypothesis that concentrating on harder patches yields a more\nrobust feature representation, as evidenced by the improved performance on\ndownstream tasks. Our method also presents a complete-to-partial feature-level\nknowledge distillation technique designed to guide the prediction of geometric\ncomplexity utilizing a comprehensive context from feature-level information.\nExtensive experiments confirm our method's superiority over State-Of-The-Art\n(SOTA) baselines, demonstrating marked improvements in classification, and\nfew-shot tasks.\n","authors":["Ali Bahri","Moslem Yazdanpanah","Mehrdad Noori","Milad Cheraghalikhani","Gustavo Adolfo Vargas Hakim","David Osowiechi","Farzad Beizaee","Ismail Ben Ayed","Christian Desrosiers"],"pdf_url":"https://arxiv.org/pdf/2405.12419v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12822v1","updated":"2025-03-17T05:05:05Z","published":"2025-03-17T05:05:05Z","title":"An Optimization Framework for Differentially Private Sparse Fine-Tuning","summary":"  Differentially private stochastic gradient descent (DP-SGD) is broadly\nconsidered to be the gold standard for training and fine-tuning neural networks\nunder differential privacy (DP). With the increasing availability of\nhigh-quality pre-trained model checkpoints (e.g., vision and language models),\nfine-tuning has become a popular strategy. However, despite recent progress in\nunderstanding and applying DP-SGD for private transfer learning tasks,\nsignificant challenges remain -- most notably, the performance gap between\nmodels fine-tuned with DP-SGD and their non-private counterparts. Sparse\nfine-tuning on private data has emerged as an alternative to full-model\nfine-tuning; recent work has shown that privately fine-tuning only a small\nsubset of model weights and keeping the rest of the weights fixed can lead to\nbetter performance. In this work, we propose a new approach for sparse\nfine-tuning of neural networks under DP. Existing work on private sparse\nfinetuning often used fixed choice of trainable weights (e.g., updating only\nthe last layer), or relied on public model's weights to choose the subset of\nweights to modify. Such choice of weights remains suboptimal. In contrast, we\nexplore an optimization-based approach, where our selection method makes use of\nthe private gradient information, while using off the shelf privacy accounting\ntechniques. Our numerical experiments on several computer vision models and\ndatasets show that our selection method leads to better prediction accuracy,\ncompared to full-model private fine-tuning or existing private sparse\nfine-tuning approaches.\n","authors":["Mehdi Makni","Kayhan Behdin","Gabriel Afriat","Zheng Xu","Sergei Vassilvitskii","Natalia Ponomareva","Hussein Hazimeh","Rahul Mazumder"],"pdf_url":"https://arxiv.org/pdf/2503.12822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12813v1","updated":"2025-03-17T04:41:26Z","published":"2025-03-17T04:41:26Z","title":"Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN LSTM\n  With WOA GWO Optimization: Global COVID-19 Case Study","summary":"  Effective epidemic modeling is essential for managing public health crises,\nrequiring robust methods to predict disease spread and optimize resource\nallocation. This study introduces a novel deep learning framework that advances\ntime series forecasting for infectious diseases, with its application to COVID\n19 data as a critical case study. Our hybrid approach integrates Convolutional\nNeural Networks (CNNs) and Long Short Term Memory (LSTM) models to capture\nspatial and temporal dynamics of disease transmission across diverse regions.\nThe CNN extracts spatial features from raw epidemiological data, while the LSTM\nmodels temporal patterns, yielding precise and adaptable predictions. To\nmaximize performance, we employ a hybrid optimization strategy combining the\nWhale Optimization Algorithm (WOA) and Gray Wolf Optimization (GWO) to fine\ntune hyperparameters, such as learning rates, batch sizes, and training epochs\nenhancing model efficiency and accuracy. Applied to COVID 19 case data from 24\ncountries across six continents, our method outperforms established benchmarks,\nincluding ARIMA and standalone LSTM models, with statistically significant\ngains in predictive accuracy (e.g., reduced RMSE). This framework demonstrates\nits potential as a versatile method for forecasting epidemic trends, offering\ninsights for resource planning and decision making in both historical contexts,\nlike the COVID 19 pandemic, and future outbreaks.\n","authors":["Mousa Alizadeh","Mohammad Hossein Samaei","Azam Seilsepour","Mohammad TH Beheshti"],"pdf_url":"https://arxiv.org/pdf/2503.12813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12811v1","updated":"2025-03-17T04:36:45Z","published":"2025-03-17T04:36:45Z","title":"A Multi-Power Law for Loss Curve Prediction Across Learning Rate\n  Schedules","summary":"  Training large models is both resource-intensive and time-consuming, making\nit crucial to understand the quantitative relationship between model\nperformance and hyperparameters. In this paper, we present an empirical law\nthat describes how the pretraining loss of large language models evolves under\ndifferent learning rate schedules, such as constant, cosine, and step decay\nschedules. Our proposed law takes a multi-power form, combining a power law\nbased on the sum of learning rates and additional power laws to account for a\nloss reduction effect induced by learning rate decay. We extensively validate\nthis law on various model sizes and architectures, and demonstrate that after\nfitting on a few learning rate schedules, the law accurately predicts the loss\ncurves for unseen schedules of different shapes and horizons. Moreover, by\nminimizing the predicted final pretraining loss across learning rate schedules,\nwe are able to find a schedule that outperforms the widely used cosine learning\nrate schedule. Interestingly, this automatically discovered schedule bears some\nresemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (Hu et\nal, 2024) but achieves a slightly lower final loss. We believe these results\ncould offer valuable insights for understanding the dynamics of pretraining and\ndesigning learning rate schedules to improve efficiency.\n","authors":["Kairong Luo","Haodong Wen","Shengding Hu","Zhenbo Sun","Zhiyuan Liu","Maosong Sun","Kaifeng Lyu","Wenguang Chen"],"pdf_url":"https://arxiv.org/pdf/2503.12811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09468v3","updated":"2025-03-17T04:30:03Z","published":"2024-12-12T17:15:49Z","title":"STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized\n  Variational Autoencoders for Financial Trading","summary":"  In financial trading, factor models are widely used to price assets and\ncapture excess returns from mispricing. Recently, we have witnessed the rise of\nvariational autoencoder-based latent factor models, which learn latent factors\nself-adaptively. While these models focus on modeling overall market\nconditions, they often fail to effectively capture the temporal patterns of\nindividual stocks. Additionally, representing multiple factors as single values\nsimplifies the model but limits its ability to capture complex relationships\nand dependencies. As a result, the learned factors are of low quality and lack\ndiversity, reducing their effectiveness and robustness across different trading\nperiods. To address these issues, we propose a Spatio-Temporal factOR Model\nbased on dual vector quantized variational autoencoders, named STORM, which\nextracts features of stocks from temporal and spatial perspectives, then fuses\nand aligns these features at the fine-grained and semantic level, and\nrepresents the factors as multi-dimensional embeddings. The discrete codebooks\ncluster similar factor embeddings, ensuring orthogonality and diversity, which\nhelps distinguish between different factors and enables factor selection in\nfinancial trading. To show the performance of the proposed factor model, we\napply it to two downstream experiments: portfolio management on two stock\ndatasets and individual trading tasks on six specific stocks. The extensive\nexperiments demonstrate STORM's flexibility in adapting to downstream tasks and\nsuperior performance over baseline models.\n","authors":["Yilei Zhao","Wentao Zhang","Tingran Yang","Yong Jiang","Fei Huang","Wei Yang Bryan Lim"],"pdf_url":"https://arxiv.org/pdf/2412.09468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12808v1","updated":"2025-03-17T04:24:21Z","published":"2025-03-17T04:24:21Z","title":"Estimating stationary mass, frequency by frequency","summary":"  Suppose we observe a trajectory of length $n$ from an $\\alpha$-mixing\nstochastic process over a finite but potentially large state space. We consider\nthe problem of estimating the probability mass placed by the stationary\ndistribution of any such process on elements that occur with a certain\nfrequency in the observed sequence. We estimate this vector of probabilities in\ntotal variation distance, showing universal consistency in $n$ and recovering\nknown results for i.i.d. sequences as special cases. Our proposed methodology\ncarefully combines the plug-in (or empirical) estimator with a\nrecently-proposed modification of the Good--Turing estimator called\n\\textsc{WingIt}, which was originally developed for Markovian sequences. En\nroute to controlling the error of our estimator, we develop new performance\nbounds on \\textsc{WingIt} and the plug-in estimator for $\\alpha$-mixing\nstochastic processes. Importantly, the extensively used method of\nPoissonization can no longer be applied in our non i.i.d. setting, and so we\ndevelop complementary tools -- including concentration inequalities for a\nnatural self-normalized statistic of mixing sequences -- that may prove\nindependently useful in the design and analysis of estimators for related\nproblems.\n","authors":["Milind Nakul","Vidya Muthukumar","Ashwin Pananjady"],"pdf_url":"https://arxiv.org/pdf/2503.12808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12803v1","updated":"2025-03-17T04:19:20Z","published":"2025-03-17T04:19:20Z","title":"Leveraging Deep Neural Networks for Aspect-Based Sentiment\n  Classification","summary":"  Aspect-based sentiment analysis seeks to determine sentiment with a high\nlevel of detail. While graph convolutional networks (GCNs) are commonly used\nfor extracting sentiment features, their straightforward use in syntactic\nfeature extraction can lead to a loss of crucial information. This paper\npresents a novel edge-enhanced GCN, called EEGCN, which improves performance by\npreserving feature integrity as it processes syntactic graphs. We incorporate a\nbidirectional long short-term memory (Bi-LSTM) network alongside a\nself-attention-based transformer for effective text encoding, ensuring the\nretention of long-range dependencies. A bidirectional GCN (Bi-GCN) with message\npassing then captures the relationships between entities, while an\naspect-specific masking technique removes extraneous information. Extensive\nevaluations and ablation studies on four benchmark datasets show that EEGCN\nsignificantly enhances aspect-based sentiment analysis, overcoming issues with\nsyntactic feature extraction and advancing the field's methodologies.\n","authors":["Chen Li","Debo Cheng","Yasuhiko Morimoto"],"pdf_url":"https://arxiv.org/pdf/2503.12803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12801v1","updated":"2025-03-17T04:15:47Z","published":"2025-03-17T04:15:47Z","title":"BLIA: Detect model memorization in binary classification model through\n  passive Label Inference attack","summary":"  Model memorization has implications for both the generalization capacity of\nmachine learning models and the privacy of their training data. This paper\ninvestigates label memorization in binary classification models through two\nnovel passive label inference attacks (BLIA). These attacks operate passively,\nrelying solely on the outputs of pre-trained models, such as confidence scores\nand log-loss values, without interacting with or modifying the training\nprocess. By intentionally flipping 50% of the labels in controlled subsets,\ntermed \"canaries,\" we evaluate the extent of label memorization under two\nconditions: models trained without label differential privacy (Label-DP) and\nthose trained with randomized response-based Label-DP. Despite the application\nof varying degrees of Label-DP, the proposed attacks consistently achieve\nsuccess rates exceeding 50%, surpassing the baseline of random guessing and\nconclusively demonstrating that models memorize training labels, even when\nthese labels are deliberately uncorrelated with the features.\n","authors":["Mohammad Wahiduzzaman Khan","Sheng Chen","Ilya Mironov","Leizhen Zhang","Rabib Noor"],"pdf_url":"https://arxiv.org/pdf/2503.12801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11390v2","updated":"2025-03-17T04:11:54Z","published":"2024-12-16T02:37:38Z","title":"A3E: Aligned and Augmented Adversarial Ensemble for Accurate, Robust and\n  Privacy-Preserving EEG Decoding","summary":"  An electroencephalogram (EEG) based brain-computer interface (BCI) enables\ndirect communication between the brain and external devices. However, EEG-based\nBCIs face at least three major challenges in real-world applications: data\nscarcity and individual differences, adversarial vulnerability, and data\nprivacy. While previous studies have addressed one or two of these issues,\nsimultaneous accommodation of all three challenges remains challenging and\nunexplored. This paper fills this gap, by proposing an Aligned and Augmented\nAdversarial Ensemble (A3E) algorithm and integrating it into three privacy\nprotection scenarios (centralized source-free transfer, federated source-free\ntransfer, and source data perturbation), achieving simultaneously accurate\ndecoding, adversarial robustness, and privacy protection of EEG-based BCIs.\nExperiments on three public EEG datasets demonstrated that our proposed\napproach outperformed over 10 classic and state-of-the-art approaches in both\naccuracy and robustness in all three privacy-preserving scenarios, even\noutperforming state-of-the-art transfer learning approaches that do not\nconsider privacy protection at all. This is the first time that three major\nchallenges in EEG-based BCIs can be addressed simultaneously, significantly\nimproving the practicalness of EEG decoding in real-world BCIs.\n","authors":["Xiaoqing Chen","Tianwang Jia","Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2412.11390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19243v5","updated":"2025-03-17T04:11:01Z","published":"2024-03-28T08:58:20Z","title":"Efficient Learning With Sine-Activated Low-rank Matrices","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v5.pdf","comment":"The first two authors contributed equally. Paper accepted at ICLR\n  2025"},{"id":"http://arxiv.org/abs/2503.12796v1","updated":"2025-03-17T04:06:10Z","published":"2025-03-17T04:06:10Z","title":"A Reinforcement Learning-Driven Transformer GAN for Molecular Generation","summary":"  Generating molecules with desired chemical properties presents a critical\nchallenge in fields such as chemical synthesis and drug discovery. Recent\nadvancements in artificial intelligence (AI) and deep learning have\nsignificantly contributed to data-driven molecular generation. However,\nchallenges persist due to the inherent sensitivity of simplified molecular\ninput line entry system (SMILES) representations and the difficulties in\napplying generative adversarial networks (GANs) to discrete data. This study\nintroduces RL-MolGAN, a novel Transformer-based discrete GAN framework designed\nto address these challenges. Unlike traditional Transformer architectures,\nRL-MolGAN utilizes a first-decoder-then-encoder structure, facilitating the\ngeneration of drug-like molecules from both $de~novo$ and scaffold-based\ndesigns. In addition, RL-MolGAN integrates reinforcement learning (RL) and\nMonte Carlo tree search (MCTS) techniques to enhance the stability of GAN\ntraining and optimize the chemical properties of the generated molecules. To\nfurther improve the model's performance, RL-MolWGAN, an extension of RL-MolGAN,\nincorporates Wasserstein distance and mini-batch discrimination, which together\nenhance the stability of the GAN. Experimental results on two widely used\nmolecular datasets, QM9 and ZINC, validate the effectiveness of our models in\ngenerating high-quality molecular structures with diverse and desirable\nchemical properties.\n","authors":["Chen Li","Huidong Tang","Ye Zhu","Yoshihiro Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2503.12796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12793v1","updated":"2025-03-17T04:01:37Z","published":"2025-03-17T04:01:37Z","title":"Improving Generalization of Universal Adversarial Perturbation via\n  Dynamic Maximin Optimization","summary":"  Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.\n","authors":["Yechao Zhang","Yingzhe Xu","Junyu Shi","Leo Yu Zhang","Shengshan Hu","Minghui Li","Yanjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12793v1.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2503.12784v1","updated":"2025-03-17T03:43:00Z","published":"2025-03-17T03:43:00Z","title":"Causal Feature Learning in the Social Sciences","summary":"  Variable selection poses a significant challenge in causal modeling,\nparticularly within the social sciences, where constructs often rely on\ninter-related factors such as age, socioeconomic status, gender, and race.\nIndeed, it has been argued that such attributes must be modeled as macro-level\nabstractions of lower-level manipulable features, in order to preserve the\nmodularity assumption essential to causal inference. This paper accordingly\nextends the theoretical framework of Causal Feature Learning (CFL).\nEmpirically, we apply the CFL algorithm to diverse social science datasets,\nevaluating how CFL-derived macrostates compare with traditional microstates in\ndownstream modeling tasks.\n","authors":["Jingzhou Huang","Jiuyao Lu","Alexander Williams Tolbert"],"pdf_url":"https://arxiv.org/pdf/2503.12784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12780v1","updated":"2025-03-17T03:33:28Z","published":"2025-03-17T03:33:28Z","title":"LangDA: Building Context-Awareness via Language for Domain Adaptive\n  Semantic Segmentation","summary":"  Unsupervised domain adaptation for semantic segmentation (DASS) aims to\ntransfer knowledge from a label-rich source domain to a target domain with no\nlabels. Two key approaches in DASS are (1) vision-only approaches using masking\nor multi-resolution crops, and (2) language-based approaches that use generic\nclass-wise prompts informed by target domain (e.g. \"a {snowy} photo of a\n{class}\"). However, the former is susceptible to noisy pseudo-labels that are\nbiased to the source domain. The latter does not fully capture the intricate\nspatial relationships of objects -- key for dense prediction tasks. To this\nend, we propose LangDA. LangDA addresses these challenges by, first, learning\ncontextual relationships between objects via VLM-generated scene descriptions\n(e.g. \"a pedestrian is on the sidewalk, and the street is lined with\nbuildings.\"). Second, LangDA aligns the entire image features with text\nrepresentation of this context-aware scene caption and learns generalized\nrepresentations via text. With this, LangDA sets the new state-of-the-art\nacross three DASS benchmarks, outperforming existing methods by 2.6%, 1.4% and\n3.9%.\n","authors":["Chang Liu","Bavesh Balaji","Saad Hossain","C Thomas","Kwei-Herng Lai","Raviteja Vemulapalli","Alexander Wong","Sirisha Rambhatla"],"pdf_url":"https://arxiv.org/pdf/2503.12780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11039v4","updated":"2025-03-17T03:16:23Z","published":"2025-01-19T13:14:53Z","title":"Beyond Any-Shot Adaptation: Predicting Optimization Outcome for\n  Robustness Gains without Extra Pay","summary":"  Foundation models have revolutionized general-purpose problem-solving,\noffering rapid task adaptation through pretraining, meta-training, and\nfinetuning. Recent crucial advances in these paradigms reveal the importance of\nchallenging task prioritized sampling to enhance adaptation robustness under\ndistribution shifts. However, ranking task difficulties over iteration as a\npreliminary step typically requires exhaustive task evaluation, which is\npractically unaffordable in computation and data-annotation. This study\nprovides a novel perspective to illuminate the possibility of leveraging the\ndual importance of adaptation robustness and learning efficiency, particularly\nin scenarios where task evaluation is risky or costly, such as iterative\nagent-environment interactions for robotic policy evaluation or computationally\nintensive inference steps for finetuning foundation models. Firstly, we\nintroduce Model Predictive Task Sampling (MPTS), a framework that bridges the\ntask space and adaptation risk landscape, providing a theoretical foundation\nfor robust active task sampling. MPTS employs a generative model to\ncharacterize the episodic optimization process and predicts task-specific\nadaptation risk via posterior inference. The resulting risk learner amortizes\nthe costly evaluation of task adaptation performance and provably approximates\ntask difficulty rankings. MPTS seamlessly integrates into zero-shot, few-shot,\nand supervised finetuning settings. Empirically, we conduct extensive\nexperiments in pattern recognition using foundation models and sequential\ndecision-making. Our results demonstrate that MPTS significantly enhances\nadaptation robustness for tail or out-of-distribution (OOD) tasks and improves\nlearning efficiency compared to state-of-the-art (SOTA) methods. The code is\navailable at the project site https://github.com/thu-rllab/MPTS.\n","authors":["Qi Cheems Wang","Zehao Xiao","Yixiu Mao","Yun Qu","Jiayi Shen","Yiqin Lv","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2501.11039v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02198v2","updated":"2025-03-17T03:15:01Z","published":"2024-11-04T15:53:45Z","title":"Metric properties of partial and robust Gromov-Wasserstein distances","summary":"  The Gromov-Wasserstein (GW) distances define a family of metrics, based on\nideas from optimal transport, which enable comparisons between probability\nmeasures defined on distinct metric spaces. They are particularly useful in\nareas such as network analysis and geometry processing, as computation of a GW\ndistance involves solving for registration between the objects which minimizes\ngeometric distortion. Although GW distances have proven useful for various\napplications in the recent machine learning literature, it has been observed\nthat they are inherently sensitive to outlier noise and cannot accommodate\npartial matching. This has been addressed by various constructions building on\nthe GW framework; in this article, we focus specifically on a natural\nrelaxation of the GW optimization problem, introduced by Chapel et al., which\nis aimed at addressing exactly these shortcomings. Our goal is to understand\nthe theoretical properties of this relaxed optimization problem, from the\nviewpoint of metric geometry. While the relaxed problem fails to induce a\nmetric, we derive precise characterizations of how it fails the axioms of\nnon-degeneracy and triangle inequality. These observations lead us to define a\nnovel family of distances, whose construction is inspired by the Prokhorov and\nKy Fan distances, as well as by the recent work of Raghvendra et al.\\ on robust\nversions of classical Wasserstein distance. We show that our new distances\ndefine true metrics, that they induce the same topology as the GW distances,\nand that they enjoy additional robustness to perturbations. These results\nprovide a mathematically rigorous basis for using our robust partial GW\ndistances in applications where outliers and partial matching are concerns.\n","authors":["Jannatul Chhoa","Michael Ivanitskiy","Fushuai Jiang","Shiying Li","Daniel McBride","Tom Needham","Kaiying O'Hare"],"pdf_url":"https://arxiv.org/pdf/2411.02198v2.pdf","comment":"V2: Improved robustness result and added numerics"},{"id":"http://arxiv.org/abs/2409.10831v2","updated":"2025-03-17T03:08:29Z","published":"2024-09-17T01:48:42Z","title":"PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music\n  Processing","summary":"  The recent explosion of generative AI-Music systems has raised numerous\nconcerns over data copyright, licensing music from musicians, and the conflict\nbetween open-source AI and large prestige companies. Such issues highlight the\nneed for publicly available, copyright-free musical data, in which there is a\nlarge shortage, particularly for symbolic music data. To alleviate this issue,\nwe present PDMX: a large-scale open-source dataset of over 250K public domain\nMusicXML scores collected from the score-sharing forum MuseScore, making it the\nlargest available copyright-free symbolic music dataset to our knowledge. PDMX\nadditionally includes a wealth of both tag and user interaction metadata,\nallowing us to efficiently analyze the dataset and filter for high quality\nuser-generated scores. Given the additional metadata afforded by our data\ncollection process, we conduct multitrack music generation experiments\nevaluating how different representative subsets of PDMX lead to different\nbehaviors in downstream models, and how user-rating statistics can be used as\nan effective measure of data quality. Examples can be found at\nhttps://pnlong.github.io/PDMX.demo/.\n","authors":["Phillip Long","Zachary Novack","Taylor Berg-Kirkpatrick","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2409.10831v2.pdf","comment":"Accepted to 2025 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2503.12770v1","updated":"2025-03-17T03:07:38Z","published":"2025-03-17T03:07:38Z","title":"Asynchronous Predictive Counterfactual Regret Minimization$^+$ Algorithm\n  in Solving Extensive-Form Games","summary":"  Counterfactual Regret Minimization (CFR) algorithms are widely used to\ncompute a Nash equilibrium (NE) in two-player zero-sum imperfect-information\nextensive-form games (IIGs). Among them, Predictive CFR$^+$ (PCFR$^+$) is\nparticularly powerful, achieving an exceptionally fast empirical convergence\nrate via the prediction in many games. However, the empirical convergence rate\nof PCFR$^+$ would significantly degrade if the prediction is inaccurate,\nleading to unstable performance on certain IIGs. To enhance the robustness of\nPCFR$^+$, we propose a novel variant, Asynchronous PCFR$^+$ (APCFR$^+$), which\nemploys an adaptive asynchronization of step-sizes between the updates of\nimplicit and explicit accumulated counterfactual regrets to mitigate the impact\nof the prediction inaccuracy on convergence. We present a theoretical analysis\ndemonstrating why APCFR$^+$ can enhance the robustness. Finally, we propose a\nsimplified version of APCFR$^+$ called Simple APCFR$^+$ (SAPCFR$^+$), which\nuses a fixed asynchronization of step-sizes to simplify the implementation that\nonly needs a single-line modification of the original PCFR+. Interestingly,\nSAPCFR$^+$ achieves a constant-factor lower theoretical regret bound than\nPCFR$^+$ in the worst case. Experimental results demonstrate that (i) both\nAPCFR$^+$ and SAPCFR$^+$ outperform PCFR$^+$ in most of the tested games, as\nwell as (ii) SAPCFR$^+$ achieves a comparable empirical convergence rate with\nAPCFR$^+$.\n","authors":["Linjian Meng","Youzhi Zhang","Zhenxing Ge","Tianpei Yang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2503.12770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01516v2","updated":"2025-03-17T03:01:25Z","published":"2024-10-02T13:05:09Z","title":"Bounds on Lp errors in density ratio estimation via f-divergence loss\n  functions","summary":"  Density ratio estimation (DRE) is a core technique in machine learning used\nto capture relationships between two probability distributions. $f$-divergence\nloss functions, which are derived from variational representations of\n$f$-divergence, have become a standard choice in DRE for achieving cutting-edge\nperformance. This study provides novel theoretical insights into DRE by\nderiving upper and lower bounds on the $L_p$ errors through $f$-divergence loss\nfunctions. These bounds apply to any estimator belonging to a class of\nLipschitz continuous estimators, irrespective of the specific $f$-divergence\nloss function employed. The derived bounds are expressed as a product involving\nthe data dimensionality and the expected value of the density ratio raised to\nthe $p$-th power. Notably, the lower bound includes an exponential term that\ndepends on the Kullback--Leibler (KL) divergence, revealing that the $L_p$\nerror increases significantly as the KL divergence grows when $p > 1$. This\nincrease becomes even more pronounced as the value of $p$ grows. The\ntheoretical insights are validated through numerical experiments.\n","authors":["Yoshiaki Kitazawa"],"pdf_url":"https://arxiv.org/pdf/2410.01516v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.13434v1","updated":"2025-03-17T17:58:05Z","published":"2025-03-17T17:58:05Z","title":"BlobCtrl: A Unified and Flexible Framework for Element-level Image\n  Generation and Editing","summary":"  Element-level visual manipulation is essential in digital content creation,\nbut current diffusion-based methods lack the precision and flexibility of\ntraditional tools. In this work, we introduce BlobCtrl, a framework that\nunifies element-level generation and editing using a probabilistic blob-based\nrepresentation. By employing blobs as visual primitives, our approach\neffectively decouples and represents spatial location, semantic content, and\nidentity information, enabling precise element-level manipulation. Our key\ncontributions include: 1) a dual-branch diffusion architecture with\nhierarchical feature fusion for seamless foreground-background integration; 2)\na self-supervised training paradigm with tailored data augmentation and score\nfunctions; and 3) controllable dropout strategies to balance fidelity and\ndiversity. To support further research, we introduce BlobData for large-scale\ntraining and BlobBench for systematic evaluation. Experiments show that\nBlobCtrl excels in various element-level manipulation tasks while maintaining\ncomputational efficiency, offering a practical solution for precise and\nflexible visual content creation. Project page:\nhttps://liyaowei-stu.github.io/project/BlobCtrl/\n","authors":["Yaowei Li","Lingen Li","Zhaoyang Zhang","Xiaoyu Li","Guangzhi Wang","Hongxiang Li","Xiaodong Cun","Ying Shan","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2503.13434v1.pdf","comment":"Project Webpage: https://liyaowei-stu.github.io/project/BlobCtrl/"},{"id":"http://arxiv.org/abs/2411.17698v4","updated":"2025-03-17T17:44:37Z","published":"2024-11-26T18:59:58Z","title":"Video-Guided Foley Sound Generation with Multimodal Controls","summary":"  Generating sound effects for videos often requires creating artistic sound\neffects that diverge significantly from real-life sources and flexible control\nin the sound design. To address this problem, we introduce MultiFoley, a model\ndesigned for video-guided sound generation that supports multimodal\nconditioning through text, audio, and video. Given a silent video and a text\nprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels\nspinning without wind noise) or more whimsical sounds (e.g., making a lion's\nroar sound like a cat's meow). MultiFoley also allows users to choose reference\naudio from sound effects (SFX) libraries or partial videos for conditioning. A\nkey novelty of our model lies in its joint training on both internet video\ndatasets with low-quality audio and professional SFX recordings, enabling\nhigh-quality, full-bandwidth (48kHz) audio generation. Through automated\nevaluations and human studies, we demonstrate that MultiFoley successfully\ngenerates synchronized high-quality sounds across varied conditional inputs and\noutperforms existing methods. Please see our project page for video results:\nhttps://ificl.github.io/MultiFoley/\n","authors":["Ziyang Chen","Prem Seetharaman","Bryan Russell","Oriol Nieto","David Bourgin","Andrew Owens","Justin Salamon"],"pdf_url":"https://arxiv.org/pdf/2411.17698v4.pdf","comment":"Accepted at CVPR 2025. Project site:\n  https://ificl.github.io/MultiFoley/"},{"id":"http://arxiv.org/abs/2411.06742v2","updated":"2025-03-17T07:08:58Z","published":"2024-11-11T06:40:06Z","title":"Loss-tolerant neural video codec aware congestion control for real time\n  video communication","summary":"  Because of reinforcement learning's (RL) ability to automatically create more\nadaptive controlling logics beyond the hand-crafted heuristics, numerous effort\nhas been made to apply RL to congestion control (CC) design for real time video\ncommunication (RTC) applications and has successfully shown promising benefits\nover the rule-based RTC CCs. Online reinforcement learning is often adopted to\ntrain the RL models so the models can directly adapt to real network\nenvironments. However, its trail-and-error manner can also cause catastrophic\ndegradation of the quality of experience (QoE) of RTC application at run time.\nThus, safeguard strategies such as falling back to hand-crafted heuristics can\nbe used to run along with RL models to guarantee the actions explored in the\ntraining sensible, despite that these safeguard strategies interrupt the\nlearning process and make it more challenging to discover optimal RL policies.\n  The recent emergence of loss-tolerant neural video codecs (NVC) naturally\nprovides a layer of protection for the online learning of RL-based congestion\ncontrol because of its resilience to packet losses, but such packet loss\nresilience have not been fully exploited in prior works yet. In this paper, we\npresent a reinforcement learning (RL) based congestion control which can be\naware of and takes advantage of packet loss tolerance characteristic of NVCs\nvia reward in online RL learning. Through extensive evaluation on various\nvideos and network traces in a simulated environment, we demonstrate that our\nNVC-aware CC running with the loss-tolerant NVC reduces the training time by\n41\\% compared to other prior RL-based CCs. It also boosts the mean video\nquality by 0.3 to 1.6dB, lower the tail frame delay by 3 to 200ms, and reduces\nthe video stalls by 20\\% to 77\\% in comparison with other baseline RTC CCs.\n","authors":["Zhengxu Xia","Hanchen Li","Junchen Jiang"],"pdf_url":"https://arxiv.org/pdf/2411.06742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03465v2","updated":"2025-03-17T06:29:41Z","published":"2025-02-05T18:59:52Z","title":"Seeing World Dynamics in a Nutshell","summary":"  We consider the problem of efficiently representing casually captured\nmonocular videos in a spatially- and temporally-coherent manner. While existing\napproaches predominantly rely on 2D/2.5D techniques treating videos as\ncollections of spatiotemporal pixels, they struggle with complex motions,\nocclusions, and geometric consistency due to absence of temporal coherence and\nexplicit 3D structure. Drawing inspiration from monocular video as a projection\nof the dynamic 3D world, we explore representing videos in their intrinsic 3D\nform through continuous flows of Gaussian primitives in space-time. In this\npaper, we propose NutWorld, a novel framework that efficiently transforms\nmonocular videos into dynamic 3D Gaussian representations in a single forward\npass. At its core, NutWorld introduces a structured spatial-temporal aligned\nGaussian (STAG) representation, enabling optimization-free scene modeling with\neffective depth and flow regularization. Through comprehensive experiments, we\ndemonstrate that NutWorld achieves high-fidelity video reconstruction quality\nwhile enabling various downstream applications in real-time. Demos and code\nwill be available at https://github.com/Nut-World/NutWorld.\n","authors":["Qiuhong Shen","Xuanyu Yi","Mingbao Lin","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12852v1","updated":"2025-03-17T06:12:36Z","published":"2025-03-17T06:12:36Z","title":"ACT360: An Efficient 360-Degree Action Detection and Summarization\n  Framework for Mission-Critical Training and Debriefing","summary":"  Effective training and debriefing are critical in high-stakes,\nmission-critical environments such as disaster response, military simulations,\nand industrial safety, where precision and minimizing errors are paramount. The\ntraditional post-training analysis relies on manually reviewing 2D videos, a\ntime-consuming process that lacks comprehensive situational awareness. To\naddress these limitations, we introduce ACT360, a system that leverages\n360-degree videos and machine learning for automated action detection and\nstructured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch\nOnce (YOWO) model with spatial attention and equirectangular-aware convolution\n(EAC) to mitigate panoramic video distortions. To enable deployment in\nresource-constrained environments, we apply quantization and model pruning,\nreducing the model size by 74% while maintaining robust accuracy (mAP drop of\nonly 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our\napproach on a publicly available dataset of 55 labeled 360-degree videos\ncovering seven key operational actions, recorded across various real-world\ntraining sessions and environmental conditions. Additionally, ACT360 integrates\n360AIE (Action Insight Explorer), a web-based interface for automatic action\ndetection, retrieval, and textual summarization using large language models\n(LLMs), significantly enhancing post-incident analysis efficiency. ACT360\nserves as a generalized framework for mission-critical debriefing,\nincorporating EAC, spatial attention, summarization, and model optimization.\nThese innovations apply to any training environment requiring lightweight\naction detection and structured post-exercise analysis.\n","authors":["Aditi Tiwari","Klara Nahrstedt"],"pdf_url":"https://arxiv.org/pdf/2503.12852v1.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.12799v1","updated":"2025-03-17T04:07:47Z","published":"2025-03-17T04:07:47Z","title":"Grounded Chain-of-Thought for Multimodal Large Language Models","summary":"  Despite great progress, existing multimodal large language models (MLLMs) are\nprone to visual hallucination, greatly impeding their trustworthy applications.\nIn this paper, we study this problem from the perspective of visual-spatial\nreasoning, and propose a new learning task for MLLMs, termed Grounded\nChain-of-Thought (GCoT). Different from recent visual CoT studies, which focus\nmore on visual knowledge reasoning, GCoT is keen to helping MLLMs to recognize\nand ground the relevant visual cues step by step, thereby predicting the\ncorrect answer with grounding coordinates as the intuitive basis. To facilitate\nthis task, we also carefully design and construct a dataset called multimodal\ngrounded chain-of-thought (MM-GCoT) consisting of 24,022 GCoT examples for\n5,033 images. Besides, a comprehensive consistency evaluation system is also\nintroduced, including the metrics of answer accuracy, grounding accuracy and\nanswer-grounding consistency. We further design and conduct a bunch of\nexperiments on 12 advanced MLLMs, and reveal some notable findings: i. most\nMLLMs performs poorly on the consistency evaluation, indicating obvious visual\nhallucination; ii. visual hallucination is not directly related to the\nparameter size and general multimodal performance, i.e., a larger and stronger\nMLLM is not less affected by this issue. Lastly, we also demonstrate that the\nproposed dataset can help existing MLLMs to well cultivate their GCoT\ncapability and reduce the inconsistent answering significantly. Moreover, their\nGCoT can be also generalized to exiting multimodal tasks, such as open-world QA\nand REC.\n","authors":["Qiong Wu","Xiangcong Yang","Yiyi Zhou","Chenxin Fang","Baiyang Song","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2503.12799v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10831v2","updated":"2025-03-17T03:08:29Z","published":"2024-09-17T01:48:42Z","title":"PDMX: A Large-Scale Public Domain MusicXML Dataset for Symbolic Music\n  Processing","summary":"  The recent explosion of generative AI-Music systems has raised numerous\nconcerns over data copyright, licensing music from musicians, and the conflict\nbetween open-source AI and large prestige companies. Such issues highlight the\nneed for publicly available, copyright-free musical data, in which there is a\nlarge shortage, particularly for symbolic music data. To alleviate this issue,\nwe present PDMX: a large-scale open-source dataset of over 250K public domain\nMusicXML scores collected from the score-sharing forum MuseScore, making it the\nlargest available copyright-free symbolic music dataset to our knowledge. PDMX\nadditionally includes a wealth of both tag and user interaction metadata,\nallowing us to efficiently analyze the dataset and filter for high quality\nuser-generated scores. Given the additional metadata afforded by our data\ncollection process, we conduct multitrack music generation experiments\nevaluating how different representative subsets of PDMX lead to different\nbehaviors in downstream models, and how user-rating statistics can be used as\nan effective measure of data quality. Examples can be found at\nhttps://pnlong.github.io/PDMX.demo/.\n","authors":["Phillip Long","Zachary Novack","Taylor Berg-Kirkpatrick","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2409.10831v2.pdf","comment":"Accepted to 2025 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2503.12806v1","updated":"2025-03-17T04:22:53Z","published":"2025-03-17T04:22:53Z","title":"AV-Surf: Surface-Enhanced Geometry-Aware Novel-View Acoustic Synthesis","summary":"  Accurately modeling sound propagation with complex real-world environments is\nessential for Novel View Acoustic Synthesis (NVAS). While previous studies have\nleveraged visual perception to estimate spatial acoustics, the combined use of\nsurface normal and structural details from 3D representations in acoustic\nmodeling has been underexplored. Given their direct impact on sound wave\nreflections and propagation, surface normals should be jointly modeled with\nstructural details to achieve accurate spatial acoustics. In this paper, we\npropose a surface-enhanced geometry-aware approach for NVAS to improve spatial\nacoustic modeling. To achieve this, we exploit geometric priors, such as image,\ndepth map, surface normals, and point clouds obtained using a 3D Gaussian\nSplatting (3DGS) based framework. We introduce a dual cross-attention-based\ntransformer integrating geometrical constraints into frequency query to\nunderstand the surroundings of the emitter. Additionally, we design a\nConvNeXt-based spectral features processing network called Spectral Refinement\nNetwork (SRN) to synthesize realistic binaural audio. Experimental results on\nthe RWAVS and SoundSpace datasets highlight the necessity of our approach, as\nit surpasses existing methods in novel view acoustic synthesis.\n","authors":["Hadam Baek","Hannie Shin","Jiyoung Seo","Chanwoo Kim","Saerom Kim","Hyeongbok Kim","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12806v1.pdf","comment":null}]},"2025-03-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.09644v3","updated":"2025-03-16T23:56:11Z","published":"2024-10-12T20:45:24Z","title":"Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?","summary":"  Vocabulary adaptation, which integrates new vocabulary into pre-trained\nlanguage models, enables expansion to new languages and mitigates token\nover-fragmentation. However, existing approaches are limited by their reliance\non heuristics or external embeddings. We propose VocADT, a novel method for\nvocabulary adaptation using adapter modules that are trained to learn the\noptimal linear combination of existing embeddings while keeping the model's\nweights fixed. VocADT offers a flexible and scalable solution without depending\non external resources or language constraints. Across 11 languages-with diverse\nscripts, resource availability, and fragmentation-we demonstrate that VocADT\noutperforms the original Mistral model and other baselines across various\nmultilingual tasks including natural language understanding and machine\ntranslation. We find that Latin-script languages and highly fragmented\nlanguages benefit the most from vocabulary adaptation. We further fine-tune the\nadapted model on the generative task of machine translation and find that\nvocabulary adaptation is still beneficial after fine-tuning and that VocADT is\nthe most effective.\n","authors":["HyoJung Han","Akiko Eriguchi","Haoran Xu","Hieu Hoang","Marine Carpuat","Huda Khayrallah"],"pdf_url":"https://arxiv.org/pdf/2410.09644v3.pdf","comment":"ICLR2025"},{"id":"http://arxiv.org/abs/2407.06483v2","updated":"2025-03-16T23:22:35Z","published":"2024-07-09T01:17:44Z","title":"Composable Interventions for Language Models","summary":"  Test-time interventions for language models can enhance factual accuracy,\nmitigate harmful outputs, and improve model efficiency without costly\nretraining. But despite a flood of new methods, different types of\ninterventions are largely developing independently. In practice, multiple\ninterventions must be applied sequentially to the same model, yet we lack\nstandardized ways to study how interventions interact. We fill this gap by\nintroducing composable interventions, a framework to study the effects of using\nmultiple interventions on the same language models, featuring new metrics and a\nunified codebase. Using our framework, we conduct extensive experiments and\ncompose popular methods from three emerging intervention categories --\nKnowledge Editing, Model Compression, and Machine Unlearning. Our results from\n310 different compositions uncover meaningful interactions: compression hinders\nediting and unlearning, composing interventions hinges on their order of\napplication, and popular general-purpose metrics are inadequate for assessing\ncomposability. Taken together, our findings showcase clear gaps in\ncomposability, suggesting a need for new multi-objective interventions. All of\nour code is public:\nhttps://github.com/hartvigsen-group/composable-interventions.\n","authors":["Arinbjorn Kolbeinsson","Kyle O'Brien","Tianjin Huang","Shanghua Gao","Shiwei Liu","Jonathan Richard Schwarz","Anurag Vaidya","Faisal Mahmood","Marinka Zitnik","Tianlong Chen","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2407.06483v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12667v1","updated":"2025-03-16T21:55:17Z","published":"2025-03-16T21:55:17Z","title":"Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility","summary":"  Despite advances in language modelling, distributional methods that build\nsemantic representations from co-occurrences fail to discriminate between\nplausible and implausible events. In this work, we investigate how plausibility\nprediction can be improved by injecting latent knowledge prompted from large\nlanguage models using parameter-efficient fine-tuning. We train 12 task\nadapters to learn various physical properties and association measures and\nperform adapter fusion to compose latent semantic knowledge from each task on\ntop of pre-trained AlBERT embeddings. We automate auxiliary task data\ngeneration, which enables us to scale our approach and fine-tune our learned\nrepresentations across two plausibility datasets. Our code is available at\nhttps://github.com/Jacob-Chmura/plausibility-vaccine.\n","authors":["Jacob Chmura","Jonah Dauvet","Sebastian Sabry"],"pdf_url":"https://arxiv.org/pdf/2503.12667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04844v3","updated":"2025-03-16T21:53:10Z","published":"2025-03-05T18:29:15Z","title":"Universal Narrative Model: an Author-centric Storytelling Framework for\n  Generative AI","summary":"  Generative AI promises to finally realize dynamic, personalized storytelling\ntechnologies across a range of media. To date, experimentation with generative\nAI in the field of procedural narrative generation has been quite promising\nfrom a technical perspective. However, fundamental narrative dilemmas remain,\nsuch as the balance between player agency and narrative coherence, and no\nrigorous narrative standard has been proposed to specifically leverage the\nstrengths of generative AI. In this paper, we propose the Universal Narrative\nModel (UNM), an open and extensible standard designed to place writers at the\ncenter of future narrative design workflows and enable interoperability across\nauthoring platforms. By encoding an author's intent according to an objective\nnarrative model, the UNM enables narrative portability as well as intent-based\nconstraints for generative systems.\n","authors":["Hank Gerba"],"pdf_url":"https://arxiv.org/pdf/2503.04844v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10601v3","updated":"2025-03-16T21:45:21Z","published":"2024-02-16T11:37:05Z","title":"When \"Competency\" in Reasoning Opens the Door to Vulnerability:\n  Jailbreaking LLMs via Novel Complex Ciphers","summary":"  Recent advancements in Large Language Model (LLM) safety have primarily\nfocused on mitigating attacks crafted in natural language or common ciphers\n(e.g. Base64), which are likely integrated into newer models' safety training.\nHowever, we reveal a paradoxical vulnerability: as LLMs advance in reasoning,\nthey inadvertently become more susceptible to novel jailbreaking attacks.\nEnhanced reasoning enables LLMs to interpret complex instructions and decode\ncomplex user-defined ciphers, creating an exploitable security gap. To study\nthis vulnerability, we introduce Attacks using Custom Encryptions (ACE), a\njailbreaking technique that encodes malicious queries with novel ciphers.\nExtending ACE, we introduce Layered Attacks using Custom Encryptions (LACE),\nwhich applies multi-layer ciphers to amplify attack complexity. Furthermore, we\ndevelop CipherBench, a benchmark designed to evaluate LLMs' accuracy in\ndecoding encrypted benign text. Our experiments reveal a critical trade-off:\nLLMs that are more capable of decoding ciphers are more vulnerable to these\njailbreaking attacks, with success rates on GPT-4o escalating from 40% under\nACE to 78% with LACE. These findings highlight a critical insight: as LLMs\nbecome more adept at deciphering complex user ciphers--many of which cannot be\npreemptively included in safety training--they become increasingly exploitable.\n","authors":["Divij Handa","Zehua Zhang","Amir Saeidi","Shrinidhi Kumbhar","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2402.10601v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12663v1","updated":"2025-03-16T21:36:36Z","published":"2025-03-16T21:36:36Z","title":"Logic-RAG: Augmenting Large Multimodal Models with Visual-Spatial\n  Knowledge for Road Scene Understanding","summary":"  Large multimodal models (LMMs) are increasingly integrated into autonomous\ndriving systems for user interaction. However, their limitations in\nfine-grained spatial reasoning pose challenges for system interpretability and\nuser trust. We introduce Logic-RAG, a novel Retrieval-Augmented Generation\n(RAG) framework that improves LMMs' spatial understanding in driving scenarios.\nLogic-RAG constructs a dynamic knowledge base (KB) about object-object\nrelationships in first-order logic (FOL) using a perception module, a\nquery-to-logic embedder, and a logical inference engine. We evaluated Logic-RAG\non visual-spatial queries using both synthetic and real-world driving videos.\nWhen using popular LMMs (GPT-4V, Claude 3.5) as proxies for an autonomous\ndriving system, these models achieved only 55% accuracy on synthetic driving\nscenes and under 75% on real-world driving scenes. Augmenting them with\nLogic-RAG increased their accuracies to over 80% and 90%, respectively. An\nablation study showed that even without logical inference, the fact-based\ncontext constructed by Logic-RAG alone improved accuracy by 15%. Logic-RAG is\nextensible: it allows seamless replacement of individual components with\nimproved versions and enables domain experts to compose new knowledge in both\nFOL and natural language. In sum, Logic-RAG addresses critical spatial\nreasoning deficiencies in LMMs for autonomous driving applications. Code and\ndata are available at https://github.com/Imran2205/LogicRAG.\n","authors":["Imran Kabir","Md Alimoor Reza","Syed Billah"],"pdf_url":"https://arxiv.org/pdf/2503.12663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12651v1","updated":"2025-03-16T21:11:18Z","published":"2025-03-16T21:11:18Z","title":"VeriLA: A Human-Centered Evaluation Framework for Interpretable\n  Verification of LLM Agent Failures","summary":"  AI practitioners increasingly use large language model (LLM) agents in\ncompound AI systems to solve complex reasoning tasks, these agent executions\noften fail to meet human standards, leading to errors that compromise the\nsystem's overall performance. Addressing these failures through human\nintervention is challenging due to the agents' opaque reasoning processes,\nmisalignment with human expectations, the complexity of agent dependencies, and\nthe high cost of manual inspection. This paper thus introduces a human-centered\nevaluation framework for Verifying LLM Agent failures (VeriLA), which\nsystematically assesses agent failures to reduce human effort and make these\nagent failures interpretable to humans. The framework first defines clear\nexpectations of each agent by curating human-designed agent criteria. Then, it\ndevelops a human-aligned agent verifier module, trained with human gold\nstandards, to assess each agent's execution output. This approach enables\ngranular evaluation of each agent's performance by revealing failures from a\nhuman standard, offering clear guidelines for revision, and reducing human\ncognitive load. Our case study results show that VeriLA is both interpretable\nand efficient in helping practitioners interact more effectively with the\nsystem. By upholding accountability in human-agent collaboration, VeriLA paves\nthe way for more trustworthy and human-aligned compound AI systems.\n","authors":["Yoo Yeon Sung","Hannah Kim","Dan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00025v2","updated":"2025-03-16T21:05:53Z","published":"2025-02-24T12:08:26Z","title":"Evaluating Large Language Models on the Spanish Medical Intern Resident\n  (MIR) Examination 2024/2025:A Comparative Analysis of Clinical Reasoning and\n  Knowledge Application","summary":"  This study presents a comparative evaluation of 22 large language models LLMs\non the Spanish Medical Intern Resident MIR examinations for 2024 and 2025 with\na focus on clinical reasoning domain specific expertise and multimodal\nprocessing capabilities The MIR exam consisting of 210 multiple choice\nquestions some requiring image interpretation serves as a stringent benchmark\nfor assessing both factual recall and complex clinical problem solving skills\nOur investigation encompasses general purpose models such as GPT4 Claude LLaMA\nand Gemini as well as specialized fine tuned systems like Miri Pro which\nleverages proprietary Spanish healthcare data to excel in medical contexts\n  Recent market entries Deepseek and Grok have further enriched the evaluation\nlandscape particularly for tasks that demand advanced visual and semantic\nanalysis The findings indicate that while general purpose LLMs perform robustly\noverall fine tuned models consistently achieve superior accuracy especially in\naddressing nuanced domain specific challenges A modest performance decline\nobserved between the two exam cycles appears attributable to the implementation\nof modified questions designed to mitigate reliance on memorization\n  The results underscore the transformative potential of domain specific fine\ntuning and multimodal integration in advancing medical AI applications They\nalso highlight critical implications for the future integration of LLMs into\nmedical education training and clinical decision making emphasizing the\nimportance of balancing automated reasoning with ethical and context aware\njudgment\n","authors":["Carlos Luengo Vera","Ignacio Ferro Picon","M. Teresa del Val Nunez","Jose Andres Gomez Gandia","Antonio de Lucas Ancillo","Victor Ramos Arroyo","Carlos Milan Figueredo"],"pdf_url":"https://arxiv.org/pdf/2503.00025v2.pdf","comment":"26 pages, 1 table, 7 figures"},{"id":"http://arxiv.org/abs/2503.10617v2","updated":"2025-03-16T20:15:27Z","published":"2025-03-13T17:57:04Z","title":"Compositional Subspace Representation Fine-tuning for Adaptive Large\n  Language Models","summary":"  Adapting large language models to multiple tasks can cause cross-skill\ninterference, where improvements for one skill degrade another. While methods\nsuch as LoRA impose orthogonality constraints at the weight level, they do not\nfully address interference in hidden-state representations. We propose\nCompositional Subspace Representation Fine-tuning (CS-ReFT), a novel\nrepresentation-based approach that learns multiple orthonormal subspace\ntransformations, each specializing in a distinct skill, and composes them via a\nlightweight router. By isolating these subspace edits in the hidden state,\nrather than weight matrices, CS-ReFT prevents cross-task conflicts more\neffectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B\nachieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring\nonly 0.0098% of model parameters. These findings show that specialized\nrepresentation edits, composed via a simple router, significantly enhance\nmulti-task instruction following with minimal overhead.\n","authors":["Andy Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.10617v2.pdf","comment":"Accepted to ICLR 2025 SCOPE"},{"id":"http://arxiv.org/abs/2503.10619v2","updated":"2025-03-16T20:14:05Z","published":"2025-03-13T17:57:32Z","title":"Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with\n  Tree Search","summary":"  We introduce Siege, a multi-turn adversarial framework that models the\ngradual erosion of Large Language Model (LLM) safety through a tree search\nperspective. Unlike single-turn jailbreaks that rely on one meticulously\nengineered prompt, Siege expands the conversation at each turn in a\nbreadth-first fashion, branching out multiple adversarial prompts that exploit\npartial compliance from previous responses. By tracking these incremental\npolicy leaks and re-injecting them into subsequent queries, Siege reveals how\nminor concessions can accumulate into fully disallowed outputs. Evaluations on\nthe JailbreakBench dataset show that Siege achieves a 100% success rate on\nGPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries\nthan baselines such as Crescendo or GOAT. This tree search methodology offers\nan in-depth view of how model safeguards degrade over successive dialogue\nturns, underscoring the urgency of robust multi-turn testing procedures for\nlanguage models.\n","authors":["Andy Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.10619v2.pdf","comment":"Accepted to ICLR 2025 Trustworthy LLM"},{"id":"http://arxiv.org/abs/2503.12627v1","updated":"2025-03-16T19:43:25Z","published":"2025-03-16T19:43:25Z","title":"Online Misinformation Detection in Live Streaming Videos","summary":"  Online misinformation detection is an important issue and methods are\nproposed to detect and curb misinformation in various forms. However, previous\nstudies are conducted in an offline manner. We claim a realistic misinformation\ndetection setting that has not been studied yet is online misinformation\ndetection in live streaming videos (MDLS). In the proposal, we formulate the\nproblem of MDLS and illustrate the importance and the challenge of the task.\nBesides, we propose feasible ways of developing the problem into AI challenges\nas well as potential solutions to the problem.\n","authors":["Rui Cao"],"pdf_url":"https://arxiv.org/pdf/2503.12627v1.pdf","comment":"First prize winner in the Smart City Challenge in the 16th ACM\n  international WSDM conference(WSDM), 2023"},{"id":"http://arxiv.org/abs/2412.04606v2","updated":"2025-03-16T19:19:05Z","published":"2024-12-05T20:43:39Z","title":"Semantic Consistency-Based Uncertainty Quantification for Factuality in\n  Radiology Report Generation","summary":"  Radiology report generation (RRG) has shown great potential in assisting\nradiologists by automating the labor-intensive task of report writing. While\nrecent advancements have improved the quality and coherence of generated\nreports, ensuring their factual correctness remains a critical challenge.\nAlthough generative medical Vision Large Language Models (VLLMs) have been\nproposed to address this issue, these models are prone to hallucinations and\ncan produce inaccurate diagnostic information. To address these concerns, we\nintroduce a novel Semantic Consistency-Based Uncertainty Quantification\nframework that provides both report-level and sentence-level uncertainties.\nUnlike existing approaches, our method does not require modifications to the\nunderlying model or access to its inner state, such as output token logits,\nthus serving as a plug-and-play module that can be seamlessly integrated with\nstate-of-the-art models. Extensive experiments demonstrate the efficacy of our\nmethod in detecting hallucinations and enhancing the factual accuracy of\nautomatically generated radiology reports. By abstaining from high-uncertainty\nreports, our approach improves factuality scores by $10$\\%, achieved by\nrejecting $20$\\% of reports using the \\texttt{Radialog} model on the MIMIC-CXR\ndataset. Furthermore, sentence-level uncertainty flags the lowest-precision\nsentence in each report with an $82.9$\\% success rate. Our implementation is\nopen-source and available at https://github.com/BU-DEPEND-Lab/SCUQ-RRG.\n","authors":["Chenyu Wang","Weichao Zhou","Shantanu Ghosh","Kayhan Batmanghelich","Wenchao Li"],"pdf_url":"https://arxiv.org/pdf/2412.04606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12608v1","updated":"2025-03-16T18:44:06Z","published":"2025-03-16T18:44:06Z","title":"UniBERTs: Adversarial Training for Language-Universal Representations","summary":"  This paper presents UniBERT, a compact multilingual language model that\nleverages an innovative training framework integrating three components: masked\nlanguage modeling, adversarial training, and knowledge distillation.\nPre-trained on a meticulously curated Wikipedia corpus spanning 107 languages,\nUniBERT is designed to reduce the computational demands of large-scale models\nwhile maintaining competitive performance across various natural language\nprocessing tasks. Comprehensive evaluations on four tasks -- named entity\nrecognition, natural language inference, question answering, and semantic\ntextual similarity -- demonstrate that our multilingual training strategy\nenhanced by an adversarial objective significantly improves cross-lingual\ngeneralization. Specifically, UniBERT models show an average relative\nimprovement of 7.72% over traditional baselines, which achieved an average\nrelative improvement of only 1.17%, with statistical analysis confirming the\nsignificance of these gains (p-value = 0.0181). This work highlights the\nbenefits of combining adversarial training and knowledge distillation to build\nscalable and robust language models, thereby advancing the field of\nmultilingual and cross-lingual natural language processing.\n","authors":["Andrei-Marius Avram","Marian Lupaşcu","Dumitru-Clementin Cercel","Ionuţ Mironică","Ştefan Trăuşan-Matu"],"pdf_url":"https://arxiv.org/pdf/2503.12608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19801v2","updated":"2025-03-16T18:22:15Z","published":"2024-09-29T21:53:18Z","title":"CRScore: Grounding Automated Evaluation of Code Review Comments in Code\n  Claims and Smells","summary":"  The task of automated code review has recently gained a lot of attention from\nthe machine learning community. However, current review comment evaluation\nmetrics rely on comparisons with a human-written reference for a given code\nchange (also called a diff). Furthermore, code review is a one-to-many problem,\nlike generation and summarization, with many \"valid reviews\" for a diff. Thus,\nwe develop CRScore - a reference-free metric to measure dimensions of review\nquality like conciseness, comprehensiveness, and relevance. We design CRScore\nto evaluate reviews in a way that is grounded in claims and potential issues\ndetected in the code by LLMs and static analyzers. We demonstrate that CRScore\ncan produce valid, fine-grained scores of review quality that have the greatest\nalignment with human judgment among open source metrics (0.54 Spearman\ncorrelation) and are more sensitive than reference-based metrics. We also\nrelease a corpus of 2.9k human-annotated review quality scores for\nmachine-generated and GitHub review comments to support the development of\nautomated metrics.\n","authors":["Atharva Naik","Marcus Alenius","Daniel Fried","Carolyn Rose"],"pdf_url":"https://arxiv.org/pdf/2409.19801v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10660v2","updated":"2025-03-16T18:20:37Z","published":"2025-02-15T03:57:52Z","title":"User Profile with Large Language Models: Construction, Updating, and\n  Benchmarking","summary":"  User profile modeling plays a key role in personalized systems, as it\nrequires building accurate profiles and updating them with new information. In\nthis paper, we present two high-quality open-source user profile datasets: one\nfor profile construction and another for profile updating. These datasets offer\na strong basis for evaluating user profile modeling techniques in dynamic\nsettings. We also show a methodology that uses large language models (LLMs) to\ntackle both profile construction and updating. Our method uses a probabilistic\nframework to predict user profiles from input text, allowing for precise and\ncontext-aware profile generation. Our experiments demonstrate that models like\nMistral-7b and Llama2-7b perform strongly in both tasks. LLMs improve the\nprecision and recall of the generated profiles, and high evaluation scores\nconfirm the effectiveness of our approach.\n","authors":["Nusrat Jahan Prottasha","Md Kowsher","Hafijur Raman","Israt Jahan Anny","Prakash Bhat","Ivan Garibay","Ozlem Garibay"],"pdf_url":"https://arxiv.org/pdf/2502.10660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12634v4","updated":"2025-03-16T17:50:27Z","published":"2023-12-19T22:33:17Z","title":"MotionScript: Natural Language Descriptions for Expressive 3D Human\n  Motions","summary":"  We introduce MotionScript, a novel framework for generating highly detailed,\nnatural language descriptions of 3D human motions. Unlike existing motion\ndatasets that rely on broad action labels or generic captions, MotionScript\nprovides fine-grained, structured descriptions that capture the full complexity\nof human movement including expressive actions (e.g., emotions, stylistic\nwalking) and interactions beyond standard motion capture datasets. MotionScript\nserves as both a descriptive tool and a training resource for text-to-motion\nmodels, enabling the synthesis of highly realistic and diverse human motions\nfrom text. By augmenting motion datasets with MotionScript captions, we\ndemonstrate significant improvements in out-of-distribution motion generation,\nallowing large language models (LLMs) to generate motions that extend beyond\nexisting data. Additionally, MotionScript opens new applications in animation,\nvirtual human simulation, and robotics, providing an interpretable bridge\nbetween intuitive descriptions and motion synthesis. To the best of our\nknowledge, this is the first attempt to systematically translate 3D motion into\nstructured natural language without requiring training data.\n","authors":["Payam Jome Yazdian","Rachel Lagasse","Hamid Mohammadi","Eric Liu","Li Cheng","Angelica Lim"],"pdf_url":"https://arxiv.org/pdf/2312.12634v4.pdf","comment":"Project webpage: https://pjyazdian.github.io/MotionScript"},{"id":"http://arxiv.org/abs/2408.06621v3","updated":"2025-03-16T17:36:12Z","published":"2024-08-13T04:18:32Z","title":"Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs","summary":"  Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose two novel techniques for robust and efficient unlearning for LLMs.\nFirst, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while\nmaintaining fluency by boosting the probability of the next most likely token.\nSecond, we develop a data-adaptive initialization for LoRA adapters via\nlow-rank approximation weighted with relative Fisher information, thereby\nfocusing updates on parameters critical for removing targeted knowledge.\nExperiments on the Training Data Extraction Challenge dataset using GPT-Neo\nmodels as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models\ndemonstrate that our approach effectively removes sensitive information while\nmaintaining reasoning and generative capabilities with minimal impact. Our\nimplementation can be found in\nhttps://github.com/csm9493/efficient-llm-unlearning.\n","authors":["Sungmin Cha","Sungjun Cho","Dasol Hwang","Moontae Lee"],"pdf_url":"https://arxiv.org/pdf/2408.06621v3.pdf","comment":"ICLR 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2503.12576v1","updated":"2025-03-16T17:16:36Z","published":"2025-03-16T17:16:36Z","title":"RaSA: Rank-Sharing Low-Rank Adaptation","summary":"  Low-rank adaptation (LoRA) has been prominently employed for\nparameter-efficient fine-tuning of large language models (LLMs). However, the\nlimited expressive capacity of LoRA, stemming from the low-rank constraint, has\nbeen recognized as a bottleneck, particularly in rigorous tasks like code\ngeneration and mathematical reasoning. To address this limitation, we introduce\nRank-Sharing Low-Rank Adaptation (RaSA), an innovative extension that enhances\nthe expressive capacity of LoRA by leveraging partial rank sharing across\nlayers. By forming a shared rank pool and applying layer-specific weighting,\nRaSA effectively increases the number of ranks without augmenting parameter\noverhead. Our theoretically grounded and empirically validated approach\ndemonstrates that RaSA not only maintains the core advantages of LoRA but also\nsignificantly boosts performance in challenging code and math tasks. Code, data\nand scripts are available at: https://github.com/zwhe99/RaSA.\n","authors":["Zhiwei He","Zhaopeng Tu","Xing Wang","Xingyu Chen","Zhijie Wang","Jiahao Xu","Tian Liang","Wenxiang Jiao","Zhuosheng Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2503.12576v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2407.01082v3","updated":"2025-03-16T17:12:44Z","published":"2024-07-01T08:37:25Z","title":"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs","summary":"  Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. However,\npopular sampling methods like top-p (nucleus sampling) often struggle to\nbalance quality and diversity, especially at higher temperatures, leading to\nincoherent or repetitive outputs. To address this challenge, we propose min-p\nsampling, a dynamic truncation method that adjusts the sampling threshold based\non the model's confidence by scaling according to the top token's probability.\nWe conduct extensive experiments on benchmarks including GPQA, GSM8K, and\nAlpacaEval Creative Writing, demonstrating that min-p sampling improves both\nthe quality and diversity of generated text, particularly at high temperatures.\nMoreover, human evaluations reveal a clear preference for min-p sampling in\nterms of both text quality and diversity. Min-p sampling has been adopted by\nmultiple open-source LLM implementations, highlighting its practical utility\nand potential impact.\n","authors":["Minh Nguyen","Andrew Baker","Clement Neo","Allen Roush","Andreas Kirsch","Ravid Shwartz-Ziv"],"pdf_url":"https://arxiv.org/pdf/2407.01082v3.pdf","comment":"Improvements: 1. Added results from refined human evaluation using\n  VLLM and better survey methodology 2. Added independent evaluations (e.g.\n  EQ-Bench) 3. Added citations to recent papers that have adopted/replicated\n  min-p 4. Revised community adoption metrics for greater verifiability,\n  focusing on major frameworks 5. Reorganised appendix"},{"id":"http://arxiv.org/abs/2412.20504v4","updated":"2025-03-16T16:25:31Z","published":"2024-12-29T15:42:24Z","title":"ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding","summary":"  Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.\n","authors":["Xiao Wang","Qingyi Si","Jianlong Wu","Shiyu Zhu","Li Cao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2412.20504v4.pdf","comment":"Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata"},{"id":"http://arxiv.org/abs/2503.04556v2","updated":"2025-03-16T16:22:47Z","published":"2025-03-06T15:47:19Z","title":"Compositional Causal Reasoning Evaluation in Language Models","summary":"  Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1.\n","authors":["Jacqueline R. M. A. Maasch","Alihan Hüyük","Xinnuo Xu","Aditya V. Nori","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2503.04556v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09292v2","updated":"2025-03-16T16:21:57Z","published":"2025-01-16T04:56:33Z","title":"To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation","summary":"  Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.09292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12560v1","updated":"2025-03-16T16:16:53Z","published":"2025-03-16T16:16:53Z","title":"Multi-Granular Multimodal Clue Fusion for Meme Understanding","summary":"  With the continuous emergence of various social media platforms frequently\nused in daily life, the multimodal meme understanding (MMU) task has been\ngarnering increasing attention. MMU aims to explore and comprehend the meanings\nof memes from various perspectives by performing tasks such as metaphor\nrecognition, sentiment analysis, intention detection, and offensiveness\ndetection. Despite making progress, limitations persist due to the loss of\nfine-grained metaphorical visual clue and the neglect of multimodal text-image\nweak correlation. To overcome these limitations, we propose a multi-granular\nmultimodal clue fusion model (MGMCF) to advance MMU. Firstly, we design an\nobject-level semantic mining module to extract object-level image feature\nclues, achieving fine-grained feature clue extraction and enhancing the model's\nability to capture metaphorical details and semantics. Secondly, we propose a\nbrand-new global-local cross-modal interaction model to address the weak\ncorrelation between text and images. This model facilitates effective\ninteraction between global multimodal contextual clues and local unimodal\nfeature clues, strengthening their representations through a bidirectional\ncross-modal attention mechanism. Finally, we devise a dual-semantic guided\ntraining strategy to enhance the model's understanding and alignment of\nmultimodal representations in the semantic space. Experiments conducted on the\nwidely-used MET-MEME bilingual dataset demonstrate significant improvements\nover state-of-the-art baselines. Specifically, there is an 8.14% increase in\nprecision for offensiveness detection task, and respective accuracy\nenhancements of 3.53%, 3.89%, and 3.52% for metaphor recognition, sentiment\nanalysis, and intention detection tasks. These results, underpinned by in-depth\nanalyses, underscore the effectiveness and potential of our approach for\nadvancing MMU.\n","authors":["Li Zheng","Hao Fei","Ting Dai","Zuquan Peng","Fei Li","Huisheng Ma","Chong Teng","Donghong Ji"],"pdf_url":"https://arxiv.org/pdf/2503.12560v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2503.12559v1","updated":"2025-03-16T16:14:52Z","published":"2025-03-16T16:14:52Z","title":"AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for\n  Video-language Understanding","summary":"  Multimodal Large Language Models (MLLMs) have revolutionized video\nunderstanding, yet are still limited by context length when processing long\nvideos. Recent methods compress videos by leveraging visual redundancy\nuniformly, yielding promising results. Nevertheless, our quantitative analysis\nshows that redundancy varies significantly across time and model layers,\nnecessitating a more flexible compression strategy. We propose AdaReTaKe, a\ntraining-free method that flexibly reduces visual redundancy by allocating\ncompression ratios among time and layers with theoretical guarantees.\nIntegrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity\nfrom 256 to 2048 frames while preserving critical information. Experiments on\nVideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe\noutperforms existing methods by 2.3% and 2.8% for 7B and 72B models,\nrespectively, with even greater improvements of 5.9% and 6.0% on the longest\nLVBench. Our code is available at\nhttps://github.com/SCZwangxiao/video-FlexReduc.git.\n","authors":["Xiao Wang","Qingyi Si","Jianlong Wu","Shiyu Zhu","Li Cao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.12559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12556v1","updated":"2025-03-16T15:55:29Z","published":"2025-03-16T15:55:29Z","title":"From Guessing to Asking: An Approach to Resolving the Persona Knowledge\n  Gap in LLMs during Multi-Turn Conversations","summary":"  In multi-turn dialogues, large language models (LLM) face a critical\nchallenge of ensuring coherence while adapting to user-specific information.\nThis study introduces the persona knowledge gap, the discrepancy between a\nmodel's internal understanding and the knowledge required for coherent,\npersonalized conversations. While prior research has recognized these gaps,\ncomputational methods for their identification and resolution remain\nunderexplored. We propose Conversation Preference Elicitation and\nRecommendation (CPER), a novel framework that dynamically detects and resolves\npersona knowledge gaps using intrinsic uncertainty quantification and\nfeedback-driven refinement. CPER consists of three key modules: a Contextual\nUnderstanding Module for preference extraction, a Dynamic Feedback Module for\nmeasuring uncertainty and refining persona alignment, and a Persona-Driven\nResponse Generation module for adapting responses based on accumulated user\ncontext. We evaluate CPER on two real-world datasets: CCPE-M for preferential\nmovie recommendations and ESConv for mental health support. Using A/B testing,\nhuman evaluators preferred CPER's responses 42% more often than baseline models\nin CCPE-M and 27% more often in ESConv. A qualitative human evaluation confirms\nthat CPER's responses are preferred for maintaining contextual relevance and\ncoherence, particularly in longer (12+ turn) conversations.\n","authors":["Sarvesh Baskar","Tanmay Tulsidas Verelakar","Srinivasan Parthasarathy","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2503.12556v1.pdf","comment":"12 pages, 1 Figure, Oral Presentation at NAACL 2025"},{"id":"http://arxiv.org/abs/2503.12530v1","updated":"2025-03-16T14:50:54Z","published":"2025-03-16T14:50:54Z","title":"Basic Category Usage in Vision Language Models","summary":"  The field of psychology has long recognized a basic level of categorization\nthat humans use when labeling visual stimuli, a term coined by Rosch in 1976.\nThis level of categorization has been found to be used most frequently, to have\nhigher information density, and to aid in visual language tasks with priming in\nhumans. Here, we investigate basic level categorization in two recently\nreleased, open-source vision-language models (VLMs). This paper demonstrates\nthat Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic level\ncategorization consistent with human behavior. Moreover, the models'\npreferences are consistent with nuanced human behaviors like the biological\nversus non-biological basic level effects and the well established expert basic\nlevel shift, further suggesting that VLMs acquire cognitive categorization\nbehaviors from the human data on which they are trained.\n","authors":["Hunter Sawyer","Jesse Roberts","Kyle Moore"],"pdf_url":"https://arxiv.org/pdf/2503.12530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12528v1","updated":"2025-03-16T14:45:43Z","published":"2025-03-16T14:45:43Z","title":"Investigating Human-Aligned Large Language Model Uncertainty","summary":"  Recent work has sought to quantify large language model uncertainty to\nfacilitate model control and modulate user trust. Previous works focus on\nmeasures of uncertainty that are theoretically grounded or reflect the average\novert behavior of the model. In this work, we investigate a variety of\nuncertainty measures, in order to identify measures that correlate with human\ngroup-level uncertainty. We find that Bayesian measures and a variation on\nentropy measures, top-k entropy, tend to agree with human behavior as a\nfunction of model size. We find that some strong measures decrease in\nhuman-similarity with model size, but, by multiple linear regression, we find\nthat combining multiple uncertainty measures provide comparable human-alignment\nwith reduced size-dependency.\n","authors":["Kyle Moore","Jesse Roberts","Daryl Watson","Pamela Wisniewski"],"pdf_url":"https://arxiv.org/pdf/2503.12528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12524v1","updated":"2025-03-16T14:39:33Z","published":"2025-03-16T14:39:33Z","title":"EXAONE Deep: Reasoning Enhanced Language Models","summary":"  We present EXAONE Deep series, which exhibits superior capabilities in\nvarious reasoning tasks, including math and coding benchmarks. We train our\nmodels mainly on the reasoning-specialized dataset that incorporates long\nstreams of thought processes. Evaluation results show that our smaller models,\nEXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while\nthe largest model, EXAONE Deep 32B, demonstrates competitive performance\nagainst leading open-weight models. All EXAONE Deep models are openly available\nfor research purposes and can be downloaded from\nhttps://huggingface.co/LGAI-EXAONE\n","authors":["LG AI Research","Kyunghoon Bae","Eunbi Choi","Kibong Choi","Stanley Jungkyu Choi","Yemuk Choi","Seokhee Hong","Junwon Hwang","Hyojin Jeon","Kijeong Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Hyosang Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Yongil Kim","Youchul Kim","Edward Hwayoung Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Sangha Park","Yongmin Park","Sihoon Yang","Heuiyeen Yeen","Sihyuk Yi","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2503.12524v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2412.04862,\n  arXiv:2408.03541"},{"id":"http://arxiv.org/abs/2411.12537v4","updated":"2025-03-16T14:35:24Z","published":"2024-11-19T14:35:38Z","title":"Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues","summary":"  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers for long\nsequences. However, both Transformers and LRNNs struggle to perform\nstate-tracking, which may impair performance in tasks such as code evaluation.\nIn one forward pass, current architectures are unable to solve even parity, the\nsimplest state-tracking task, which non-linear RNNs can handle effectively.\nRecently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like\nMamba to solve parity stems from restricting the value range of their diagonal\nstate-transition matrices to $[0, 1]$ and that incorporating negative values\ncan resolve this issue. We extend this result to non-diagonal LRNNs such as\nDeltaNet. We prove that finite precision LRNNs with state-transition matrices\nhaving only positive eigenvalues cannot solve parity, while non-triangular\nmatrices are needed to count modulo $3$. Notably, we also prove that LRNNs can\nlearn any regular language when their state-transition matrices are products of\nidentity minus vector outer product matrices, each with eigenvalues in the\nrange $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of\nMamba and DeltaNet to include negative values not only enables them to solve\nparity but consistently improves their performance on state-tracking tasks. We\nalso show that state-tracking enabled LRNNs can be pretrained stably and\nefficiently at scale (1.3B parameters), achieving competitive performance on\nlanguage modeling and showing promise on code and math tasks.\n","authors":["Riccardo Grazzi","Julien Siems","Jörg K. H. Franke","Arber Zela","Frank Hutter","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2411.12537v4.pdf","comment":"V2: Correction to Theorem 1 and 2 and to point 3 of Proposition 1.\n  V3: ICLR Camera Ready, V4: ICLR Camera Ready, added figures to theory\n  section, updated modular arithmetic with brackets results because previous\n  results did not contain multiplication"},{"id":"http://arxiv.org/abs/2503.09313v2","updated":"2025-03-16T14:15:20Z","published":"2025-03-12T12:04:05Z","title":"xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using\n  Self-Knowledge Distillation","summary":"  In the current literature, most embedding models are based on the\nencoder-only transformer architecture to extract a dense and meaningful\nrepresentation of the given input, which can be a text, an image, and more.\nWith the recent advances in language modeling thanks to the introduction of\nLarge Language Models, the possibility of extracting embeddings from these\nlarge and extensively trained models has been explored. However, current\nstudies focus on textual embeddings in English, which is also the main language\non which these models have been trained. Furthermore, there are very few models\nthat consider multimodal and multilingual input. In light of this, we propose\nan adaptation methodology for Large Vision-Language Models trained on English\nlanguage data to improve their performance in extracting multilingual and\nmultimodal embeddings. Finally, we design and introduce a benchmark to evaluate\nthe effectiveness of multilingual and multimodal embedding models.\n","authors":["Elio Musacchio","Lucia Siciliani","Pierpaolo Basile","Giovanni Semeraro"],"pdf_url":"https://arxiv.org/pdf/2503.09313v2.pdf","comment":"fix typo in number of tasks in MMEB; fix url for source code; added\n  missing reference to XTD10"},{"id":"http://arxiv.org/abs/2503.12491v1","updated":"2025-03-16T12:49:44Z","published":"2025-03-16T12:49:44Z","title":"CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences","summary":"  Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.\n","authors":["Ziran Qin","Yuchen Cao","Mingbao Lin","Wen Hu","Shixuan Fan","Ke Cheng","Weiyao Lin","Jianguo Li"],"pdf_url":"https://arxiv.org/pdf/2503.12491v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2412.04703v2","updated":"2025-03-16T11:57:25Z","published":"2024-12-06T01:29:24Z","title":"Transformers Struggle to Learn to Search","summary":"  Search is an ability foundational in many important tasks, and recent studies\nhave shown that large language models (LLMs) struggle to perform search\nrobustly. It is unknown whether this inability is due to a lack of data,\ninsufficient model parameters, or fundamental limitations of the transformer\narchitecture. In this work, we use the foundational graph connectivity problem\nas a testbed to generate effectively limitless high-coverage data to train\nsmall transformers and test whether they can learn to perform search. We find\nthat, when given the right training distribution, the transformer is able to\nlearn to search.\n  We analyze the algorithm that the transformer has learned through a novel\nmechanistic interpretability technique that enables us to extract the\ncomputation graph from the trained model. We find that transformers perform\nsearch at every vertex in parallel: For each vertex in the input graph,\ntransformers compute the set of vertices reachable from that vertex. Each layer\nthen progressively expands these sets, allowing the model to search over a\nnumber of vertices exponential in $n_{\\text{layers}}$.\n  However, we find that as the input graph size increases, the transformer has\ngreater difficulty in learning the task. This difficulty is not resolved even\nas the number of parameters is increased, suggesting that increasing model\nscale will not lead to robust search abilities. We also find that performing\nsearch in-context (i.e., chain-of-thought) does not resolve this inability to\nlearn to search on larger graphs.\n","authors":["Abulhair Saparov","Srushti Pawar","Shreyas Pimpalgaonkar","Nitish Joshi","Richard Yuanzhe Pang","Vishakh Padmakumar","Seyed Mehran Kazemi","Najoung Kim","He He"],"pdf_url":"https://arxiv.org/pdf/2412.04703v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.12440v1","updated":"2025-03-16T10:26:24Z","published":"2025-03-16T10:26:24Z","title":"HKCanto-Eval: A Benchmark for Evaluating Cantonese Language\n  Understanding and Cultural Comprehension in LLMs","summary":"  The ability of language models to comprehend and interact in diverse\nlinguistic and cultural landscapes is crucial. The Cantonese language used in\nHong Kong presents unique challenges for natural language processing due to its\nrich cultural nuances and lack of dedicated evaluation datasets. The\nHKCanto-Eval benchmark addresses this gap by evaluating the performance of\nlarge language models (LLMs) on Cantonese language understanding tasks,\nextending to English and Written Chinese for cross-lingual evaluation.\nHKCanto-Eval integrates cultural and linguistic nuances intrinsic to Hong Kong,\nproviding a robust framework for assessing language models in realistic\nscenarios. Additionally, the benchmark includes questions designed to tap into\nthe underlying linguistic metaknowledge of the models. Our findings indicate\nthat while proprietary models generally outperform open-weight models,\nsignificant limitations remain in handling Cantonese-specific linguistic and\ncultural knowledge, highlighting the need for more targeted training data and\nevaluation methods. The code can be accessed at\nhttps://github.com/hon9kon9ize/hkeval2025\n","authors":["Tsz Chung Cheng","Chung Shing Cheng","Chaak Ming Lau","Eugene Tin-Ho Lam","Chun Yat Wong","Hoi On Yu","Cheuk Hei Chong"],"pdf_url":"https://arxiv.org/pdf/2503.12440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12436v1","updated":"2025-03-16T10:16:21Z","published":"2025-03-16T10:16:21Z","title":"CorpusStudio: Surfacing Emergent Patterns in a Corpus of Prior Work\n  while Writing","summary":"  Many communities, including the scientific community, develop implicit\nwriting norms. Understanding them is crucial for effective communication with\nthat community. Writers gradually develop an implicit understanding of norms by\nreading papers and receiving feedback on their writing. However, it is\ndifficult to both externalize this knowledge and apply it to one's own writing.\nWe propose two new writing support concepts that reify document and\nsentence-level patterns in a given text corpus: (1) an ordered distribution\nover section titles and (2) given the user's draft and cursor location, many\nretrieved contextually relevant sentences. Recurring words in the latter are\nalgorithmically highlighted to help users see any emergent norms. Study results\n(N=16) show that participants revised the structure and content using these\nconcepts, gaining confidence in aligning with or breaking norms after reviewing\nmany examples. These results demonstrate the value of reifying distributions\nover other authors' writing choices during the writing process.\n","authors":["Hai Dang","Chelse Swoopes","Daniel Buschek","Elena L. Glassman"],"pdf_url":"https://arxiv.org/pdf/2503.12436v1.pdf","comment":"19 pages, 12 figures, 1 table, ACM CHI 2025"},{"id":"http://arxiv.org/abs/2410.21533v3","updated":"2025-03-16T10:12:08Z","published":"2024-10-28T21:02:13Z","title":"L3Ms -- Lagrange Large Language Models","summary":"  Supervised fine-tuning (SFT) and alignment of large language models (LLMs)\nare key steps in providing a good user experience. However, the concept of an\nappropriate alignment is inherently application-dependent, and current methods\noften rely on heuristic choices to drive optimization. In this work, we\nformulate SFT and alignment as a constrained optimization problem: the LLM is\nfine-tuned on a task while being required to meet application-specific\nrequirements, without resorting to heuristics. To solve this, we propose\nLagrange Large Language Models (L3Ms), which employ logarithmic barriers to\nenforce the constraints. This approach allows for the customization of L3Ms\nacross diverse applications while avoiding heuristic-driven processes. We\nexperimentally demonstrate the versatility and efficacy of L3Ms in achieving\ntailored alignments for various applications.\n","authors":["Guneet S. Dhillon","Xingjian Shi","Yee Whye Teh","Alex Smola"],"pdf_url":"https://arxiv.org/pdf/2410.21533v3.pdf","comment":"International Conference on Learning Representations (ICLR), 2025"},{"id":"http://arxiv.org/abs/2412.11605v2","updated":"2025-03-16T09:43:15Z","published":"2024-12-16T09:47:43Z","title":"SPaR: Self-Play with Tree-Search Refinement to Improve\n  Instruction-Following in Large Language Models","summary":"  Instruction-following is a fundamental capability of language models,\nrequiring the model to recognize even the most subtle requirements in the\ninstructions and accurately reflect them in its output. Such an ability is\nwell-suited for and often optimized by preference learning. However, existing\nmethods often directly sample multiple independent responses from the model\nwhen creating preference pairs. Such practice can introduce content variations\nirrelevant to whether the instruction is precisely followed (e.g., different\nexpressions about the same semantic), interfering with the goal of teaching\nmodels to recognize the key differences that lead to improved instruction\nfollowing. In light of this, we introduce SPaR, a self-play framework\nintegrating tree-search self-refinement to yield valid and comparable\npreference pairs free from distractions. By playing against itself, an LLM\nemploys a tree-search strategy to refine its previous responses with respect to\nthe instruction while minimizing unnecessary variations. Our experiments show\nthat a LLaMA3-8B model, trained over three iterations guided by SPaR, surpasses\nGPT-4-Turbo on the IFEval benchmark without losing general capabilities.\nFurthermore, SPaR demonstrates promising scalability, greatly enhancing models\nlike GLM-4-9B and LLaMA3-70B. We also identify how inference scaling in tree\nsearch would impact model performance. Our code and data are publicly available\nat https://github.com/thu-coai/SPaR.\n","authors":["Jiale Cheng","Xiao Liu","Cunxiang Wang","Xiaotao Gu","Yida Lu","Dan Zhang","Yuxiao Dong","Jie Tang","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2412.11605v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.15654v2","updated":"2025-03-16T08:58:25Z","published":"2025-02-21T18:22:36Z","title":"Machine-generated text detection prevents language model collapse","summary":"  As Large Language Models (LLMs) become increasingly prevalent, their\ngenerated outputs are proliferating across the web, risking a future where\nmachine-generated content dilutes human-authored text. Since online data is the\nprimary resource for LLM pre-training, subsequent models could be trained on an\nunknown portion of synthetic samples. This will lead to model collapse, a\ndegenerative process whereby LLMs reinforce their own errors, and ultimately\nyield a declining performance. In this study, we investigate the impact of\ndecoding strategy on model collapse, analysing the characteristics of text at\neach model generation, the similarity to human references, and the resulting\nmodel performance. Using the decoding strategies that lead to the most\nsignificant degradation, we evaluate model collapse in more realistic scenarios\nwhere the origin of the data (human or synthetic) is unknown. We train a\nmachine-generated text detector and propose an importance sampling approach to\nalleviate model collapse. Our method is validated on two LLM variants (GPT-2\nand SmolLM2) on the open-ended text generation task. We demonstrate that it can\nnot only prevent model collapse but also improve performance when sufficient\nhuman-authored samples are present.\n","authors":["George Drayson","Vasileios Lampos"],"pdf_url":"https://arxiv.org/pdf/2502.15654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04811v3","updated":"2025-03-16T08:42:00Z","published":"2024-08-09T01:45:39Z","title":"h4rm3l: A language for Composable Jailbreak Attack Synthesis","summary":"  Despite their demonstrated valuable capabilities, state-of-the-art (SOTA)\nwidely deployed large language models (LLMs) still have the potential to cause\nharm to society due to the ineffectiveness of their safety filters, which can\nbe bypassed by prompt transformations called jailbreak attacks. Current\napproaches to LLM safety assessment, which employ datasets of templated prompts\nand benchmarking pipelines, fail to cover sufficiently large and diverse sets\nof jailbreak attacks, leading to the widespread deployment of unsafe LLMs.\nRecent research showed that novel jailbreak attacks could be derived by\ncomposition; however, a formal composable representation for jailbreak attacks,\nwhich, among other benefits, could enable the exploration of a large\ncompositional space of jailbreak attacks through program synthesis methods, has\nnot been previously proposed. We introduce h4rm3l, a novel approach that\naddresses this gap with a human-readable domain-specific language (DSL). Our\nframework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak\nattacks as compositions of parameterized string transformation primitives. (2)\nA synthesizer with bandit algorithms that efficiently generates jailbreak\nattacks optimized for a target black box LLM. (3) The h4rm3l red-teaming\nsoftware toolkit that employs the previous two components and an automated\nharmful LLM behavior classifier that is strongly aligned with human judgment.\nWe demonstrate h4rm3l's efficacy by synthesizing a dataset of 2656 successful\nnovel jailbreak attacks targeting 6 SOTA open-source and proprietary LLMs, and\nby benchmarking those models against a subset of these synthesized attacks. Our\nresults show that h4rm3l's synthesized attacks are diverse and more successful\nthan existing jailbreak attacks in literature, with success rates exceeding 90%\non SOTA LLMs.\n","authors":["Moussa Koulako Bala Doumbouya","Ananjan Nandi","Gabriel Poesia","Davide Ghilardi","Anna Goldie","Federico Bianchi","Dan Jurafsky","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2408.04811v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17390v2","updated":"2025-03-16T08:21:34Z","published":"2024-07-24T16:14:15Z","title":"$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM\n  Generated Multi-Document Topic Sets","summary":"  Using LLMs for Multi-Document Topic Extraction has recently gained popularity\ndue to their apparent high-quality outputs, expressiveness, and ease of use.\nHowever, most existing evaluation practices are not designed for LLM-generated\ntopics and result in low inter-annotator agreement scores, hindering the\nreliable use of LLMs for the task. To address this, we introduce $T^5Score$, an\nevaluation methodology that decomposes the quality of a topic set into\nquantifiable aspects, measurable through easy-to-perform annotation tasks. This\nframing enables a convenient, manual or automatic, evaluation procedure\nresulting in a strong inter-annotator agreement score. To substantiate our\nmethodology and claims, we perform extensive experimentation on multiple\ndatasets and report the results.\n","authors":["Itamar Trainin","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2407.17390v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20771v2","updated":"2025-03-16T07:22:10Z","published":"2024-10-28T06:14:12Z","title":"MrT5: Dynamic Token Merging for Efficient Byte-level Language Models","summary":"  Models that rely on subword tokenization have significant drawbacks, such as\nsensitivity to character-level noise like spelling errors and inconsistent\ncompression rates across different languages and scripts. While character- or\nbyte-level models like ByT5 attempt to address these concerns, they have not\ngained widespread adoption -- processing raw byte streams without tokenization\nresults in significantly longer sequence lengths, making training and inference\ninefficient. This work introduces MrT5 (MergeT5), a more efficient variant of\nByT5 that integrates a token deletion mechanism in its encoder to dynamically\nshorten the input sequence length. After processing through a fixed number of\nencoder layers, a learned delete gate determines which tokens are to be removed\nand which are to be retained for subsequent layers. MrT5 effectively \"merges\"\ncritical information from deleted tokens into a more compact sequence,\nleveraging contextual information from the remaining tokens. In continued\npre-training experiments, we find that MrT5 can achieve significant gains in\ninference runtime with minimal effect on performance, as measured by\nbits-per-byte. Additionally, with multilingual training, MrT5 adapts to the\northographic characteristics of each language, learning language-specific\ncompression rates. Furthermore, MrT5 shows comparable accuracy to ByT5 on\ndownstream evaluations such as XNLI, TyDi QA, and character-level tasks while\nreducing sequence lengths by up to 75%. Our approach presents a solution to the\npractical limitations of existing byte-level models.\n","authors":["Julie Kallini","Shikhar Murty","Christopher D. Manning","Christopher Potts","Róbert Csordás"],"pdf_url":"https://arxiv.org/pdf/2410.20771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13125v2","updated":"2025-03-16T06:33:02Z","published":"2025-01-21T10:20:39Z","title":"Generating Plausible Distractors for Multiple-Choice Questions via\n  Student Choice Prediction","summary":"  In designing multiple-choice questions (MCQs) in education, creating\nplausible distractors is crucial for identifying students' misconceptions and\ngaps in knowledge and accurately assessing their understanding. However, prior\nstudies on distractor generation have not paid sufficient attention to\nenhancing the difficulty of distractors, resulting in reduced effectiveness of\nMCQs. This study presents a pipeline for training a model to generate\ndistractors that are more likely to be selected by students. First, we train a\npairwise ranker to reason about students' misconceptions and assess the\nrelative plausibility of two distractors. Using this model, we create a dataset\nof pairwise distractor ranks and then train a distractor generator via Direct\nPreference Optimization (DPO) to generate more plausible distractors.\nExperiments on computer science subjects (Python, DB, MLDL) demonstrate that\nour pairwise ranker effectively identifies students' potential\nmisunderstandings and achieves ranking accuracy comparable to human experts.\nFurthermore, our distractor generator outperforms several baselines in\ngenerating plausible distractors and produces questions with a higher item\ndiscrimination index (DI).\n","authors":["Yooseop Lee","Suin Kim","Yohan Jo"],"pdf_url":"https://arxiv.org/pdf/2501.13125v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12547v2","updated":"2025-03-16T06:25:03Z","published":"2025-01-21T23:54:17Z","title":"Human-like conceptual representations emerge from language prediction","summary":"  Recent advances in large language models (LLMs) provide a new opportunity to\naddress the long-standing question of how concepts are represented and\norganized in the mind, which is central to unravelling the nature of human\ncognition. Here, we reframed the classic reverse dictionary task to simulate\nhuman concept inference in context and investigated the emergence of human-like\nconceptual representations within LLMs. We found that LLMs were able to infer\nconcepts from definitional descriptions and construct representation spaces\nthat converge towards a shared, context-independent structure. These\nrepresentations effectively predicted human behavioural judgments and aligned\nwell with neural activity patterns in the human brain, offering evidence for\nbiological plausibility. These findings demonstrate that human-like conceptual\nrepresentations and organization can naturally emerge from language prediction,\neven without real-world grounding. Our work supports the view that LLMs serve\nas valuable tools for understanding complex human cognition and paves the way\nfor better alignment between artificial and human intelligence.\n","authors":["Ningyu Xu","Qi Zhang","Chao Du","Qiang Luo","Xipeng Qiu","Xuanjing Huang","Menghan Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.12547v2.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2402.07927v2","updated":"2025-03-16T06:23:34Z","published":"2024-02-05T19:49:13Z","title":"A Systematic Survey of Prompt Engineering in Large Language Models:\n  Techniques and Applications","summary":"  Prompt engineering has emerged as an indispensable technique for extending\nthe capabilities of large language models (LLMs) and vision-language models\n(VLMs). This approach leverages task-specific instructions, known as prompts,\nto enhance model efficacy without modifying the core model parameters. Rather\nthan updating the model parameters, prompts allow seamless integration of\npre-trained models into downstream tasks by eliciting desired model behaviors\nsolely based on the given prompt. Prompts can be natural language instructions\nthat provide context to guide the model or learned vector representations that\nactivate relevant knowledge. This burgeoning field has enabled success across\nvarious applications, from question-answering to commonsense reasoning.\nHowever, there remains a lack of systematic organization and understanding of\nthe diverse prompt engineering methods and techniques. This survey paper\naddresses the gap by providing a structured overview of recent advancements in\nprompt engineering, categorized by application area. For each prompting\napproach, we provide a summary detailing the prompting methodology, its\napplications, the models involved, and the datasets utilized. We also delve\ninto the strengths and limitations of each approach and include a taxonomy\ndiagram and table summarizing datasets, models, and critical points of each\nprompting technique. This systematic analysis enables a better understanding of\nthis rapidly developing field and facilitates future research by illuminating\nopen challenges and opportunities for prompt engineering.\n","authors":["Pranab Sahoo","Ayush Kumar Singh","Sriparna Saha","Vinija Jain","Samrat Mondal","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2402.07927v2.pdf","comment":"12 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.12370v1","updated":"2025-03-16T06:19:44Z","published":"2025-03-16T06:19:44Z","title":"Understanding Common Ground Misalignment in Goal-Oriented Dialog: A\n  Case-Study with Ubuntu Chat Logs","summary":"  While it is commonly accepted that maintaining common ground plays a role in\nconversational success, little prior research exists connecting conversational\ngrounding to success in task-oriented conversations. We study failures of\ngrounding in the Ubuntu IRC dataset, where participants use text-only\ncommunication to resolve technical issues. We find that disruptions in\nconversational flow often stem from a misalignment in common ground, driven by\na divergence in beliefs and assumptions held by participants. These\ndisruptions, which we call conversational friction, significantly correlate\nwith task success. We find that although LLMs can identify overt cases of\nconversational friction, they struggle with subtler and more context-dependent\ninstances requiring pragmatic or domain-specific reasoning.\n","authors":["Rupak Sarkar","Neha Srikanth","Taylor Hudson","Rachel Rudinger","Claire Bonial","Philip Resnik"],"pdf_url":"https://arxiv.org/pdf/2503.12370v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.17605v2","updated":"2025-03-16T06:12:20Z","published":"2025-02-24T19:48:00Z","title":"PICASO: Permutation-Invariant Context Composition with State Space\n  Models","summary":"  Providing Large Language Models with relevant contextual knowledge at\ninference time has been shown to greatly improve the quality of their\ngenerations. This is often achieved by prepending informative passages of text,\nor 'contexts', retrieved from external knowledge bases to their input. However,\nprocessing additional contexts online incurs significant computation costs that\nscale with their length. State Space Models (SSMs) offer a promising solution\nby allowing a database of contexts to be mapped onto fixed-dimensional states\nfrom which to start the generation. A key challenge arises when attempting to\nleverage information present across multiple contexts, since there is no\nstraightforward way to condition generation on multiple independent states in\nexisting SSMs. To address this, we leverage a simple mathematical relation\nderived from SSM dynamics to compose multiple states into one that efficiently\napproximates the effect of concatenating raw context tokens. Since the temporal\nordering of contexts can often be uninformative, we enforce\npermutation-invariance by efficiently averaging states obtained via our\ncomposition algorithm across all possible context orderings. We evaluate our\nresulting method on WikiText and MSMARCO in both zero-shot and fine-tuned\nsettings, and show that we can match the strongest performing baseline while\nenjoying on average 5.4x speedup.\n","authors":["Tian Yu Liu","Alessandro Achille","Matthew Trager","Aditya Golatkar","Luca Zancato","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2502.17605v2.pdf","comment":"Published in The Thirteenth International Conference on Learning\n  Representations, ICLR 2025"},{"id":"http://arxiv.org/abs/2503.09639v2","updated":"2025-03-16T06:03:01Z","published":"2025-03-12T02:54:15Z","title":"Can A Society of Generative Agents Simulate Human Behavior and Inform\n  Public Health Policy? A Case Study on Vaccine Hesitancy","summary":"  Can we simulate a sandbox society with generative agents to model human\nbehavior, thereby reducing the over-reliance on real human trials for assessing\npublic policies? In this work, we investigate the feasibility of simulating\nhealth-related decision-making, using vaccine hesitancy, defined as the delay\nin acceptance or refusal of vaccines despite the availability of vaccination\nservices (MacDonald, 2015), as a case study. To this end, we introduce the\nVacSim framework with 100 generative agents powered by Large Language Models\n(LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1)\ninstantiate a population of agents with demographics based on census data; 2)\nconnect the agents via a social network and model vaccine attitudes as a\nfunction of social dynamics and disease-related information; 3) design and\nevaluate various public health interventions aimed at mitigating vaccine\nhesitancy. To align with real-world results, we also introduce simulation\nwarmup and attitude modulation to adjust agents' attitudes. We propose a series\nof evaluations to assess the reliability of various LLM simulations.\nExperiments indicate that models like Llama and Qwen can simulate aspects of\nhuman behavior but also highlight real-world alignment challenges, such as\ninconsistent responses with demographic profiles. This early exploration of\nLLM-driven simulations is not meant to serve as definitive policy guidance;\ninstead, it serves as a call for action to examine social simulation for policy\ndevelopment.\n","authors":["Abe Bohan Hou","Hongru Du","Yichen Wang","Jingyu Zhang","Zixiao Wang","Paul Pu Liang","Daniel Khashabi","Lauren Gardner","Tianxing He"],"pdf_url":"https://arxiv.org/pdf/2503.09639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16367v3","updated":"2025-03-16T05:23:12Z","published":"2024-04-25T07:10:29Z","title":"Learning Syntax Without Planting Trees: Understanding Hierarchical\n  Generalization in Transformers","summary":"  Transformers trained on natural language data have been shown to learn its\nhierarchical structure and generalize to sentences with unseen syntactic\nstructures without explicitly encoding any structural bias. In this work, we\ninvestigate sources of inductive bias in transformer models and their training\nthat could cause such generalization behavior to emerge. We extensively\nexperiment with transformer models trained on multiple synthetic datasets and\nwith different training objectives and show that while other objectives e.g.\nsequence-to-sequence modeling, prefix language modeling, often failed to lead\nto hierarchical generalization, models trained with the language modeling\nobjective consistently learned to generalize hierarchically. We then conduct\npruning experiments to study how transformers trained with the language\nmodeling objective encode hierarchical structure. When pruned, we find joint\nexistence of subnetworks within the model with different generalization\nbehaviors (subnetworks corresponding to hierarchical structure and linear\norder). Finally, we take a Bayesian perspective to further uncover\ntransformers' preference for hierarchical generalization: We establish a\ncorrelation between whether transformers generalize hierarchically on a dataset\nand whether the simplest explanation of that dataset is provided by a\nhierarchical grammar compared to regular grammars exhibiting linear\ngeneralization.\n","authors":["Kabir Ahuja","Vidhisha Balachandran","Madhur Panwar","Tianxing He","Noah A. Smith","Navin Goyal","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2404.16367v3.pdf","comment":"Accepted in TACL Code now available:\n  https://github.com/kabirahuja2431/transformers-hg"},{"id":"http://arxiv.org/abs/2503.12358v1","updated":"2025-03-16T04:53:38Z","published":"2025-03-16T04:53:38Z","title":"IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation","summary":"  Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.\n","authors":["In-Chang Baek","Sung-Hyun Kim","Seo-yung Lee","Dong-Hyun Lee","Kyung-Joong Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12358v1.pdf","comment":"9 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.12357v1","updated":"2025-03-16T04:53:23Z","published":"2025-03-16T04:53:23Z","title":"Numerical Words and Linguistic Loops: The Perpetual Four-Letter Routine","summary":"  This study presents a fascinating linguistic property related to the number\nof letters in words and their corresponding numerical values. By selecting any\narbitrary word, counting its constituent letters, and subsequently spelling out\nthe resulting count and tallying the letters anew, an unanticipated pattern is\nobserved. Remarkably, this iterative sequence, conducted on a dataset of\n100,000 random words, invariably converges to the numeral four (4), termed the\nLinguistic Loop (LL) constant. Examining 73 languages utilizing the Latin\nalphabet, this research reveals distinctive patterns. Among them, 28 languages\nexhibit LL-positive behavior adhering to the established property, while 31\nlanguages deviate as LL-negative. Additionally, 13 languages display nuanced\ntendencies: eight feature two LL constants (bi-positivity), and five feature\nthree constants (tri-positivity). This discovery highlights a linguistic quirk\nwithin Latin alphabet-based language number-word representations, uncovering an\nintriguing facet across diverse alphabetic systems. It also raises questions\nabout the underlying linguistic and cognitive mechanisms responsible for this\nphenomenon.\n","authors":["Krishna Chaitanya Polavaram"],"pdf_url":"https://arxiv.org/pdf/2503.12357v1.pdf","comment":"9 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2411.19921v2","updated":"2025-03-16T04:09:27Z","published":"2024-11-29T18:36:15Z","title":"SIMS: Simulating Stylized Human-Scene Interactions with\n  Retrieval-Augmented Script Generation","summary":"  Simulating stylized human-scene interactions (HSI) in physical environments\nis a challenging yet fascinating task. Prior works emphasize long-term\nexecution but fall short in achieving both diverse style and physical\nplausibility. To tackle this challenge, we introduce a novel hierarchical\nframework named SIMS that seamlessly bridges highlevel script-driven intent\nwith a low-level control policy, enabling more expressive and diverse\nhuman-scene interactions. Specifically, we employ Large Language Models with\nRetrieval-Augmented Generation (RAG) to generate coherent and diverse long-form\nscripts, providing a rich foundation for motion planning. A versatile\nmulticondition physics-based control policy is also developed, which leverages\ntext embeddings from the generated scripts to encode stylistic cues,\nsimultaneously perceiving environmental geometries and accomplishing task\ngoals. By integrating the retrieval-augmented script generation with the\nmulti-condition controller, our approach provides a unified solution for\ngenerating stylized HSI motions. We further introduce a comprehensive planning\ndataset produced by RAG and a stylized motion dataset featuring diverse\nlocomotions and interactions. Extensive experiments demonstrate SIMS's\neffectiveness in executing various tasks and generalizing across different\nscenarios, significantly outperforming previous methods.\n","authors":["Wenjia Wang","Liang Pan","Zhiyang Dou","Jidong Mei","Zhouyingcheng Liao","Yuke Lou","Yifan Wu","Lei Yang","Jingbo Wang","Taku Komura"],"pdf_url":"https://arxiv.org/pdf/2411.19921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12347v1","updated":"2025-03-16T04:00:32Z","published":"2025-03-16T04:00:32Z","title":"Synthesizing Privacy-Preserving Text Data via Finetuning without\n  Finetuning Billion-Scale LLMs","summary":"  Synthetic data offers a promising path to train models while preserving data\nprivacy. Differentially private (DP) finetuning of large language models (LLMs)\nas data generator is effective, but is impractical when computation resources\nare limited. Meanwhile, prompt-based methods such as private evolution, depend\nheavily on the manual prompts, and ineffectively use private information in\ntheir iterative data selection process. To overcome these limitations, we\npropose CTCL (Data Synthesis with ConTrollability and CLustering), a novel\nframework for generating privacy-preserving synthetic data without extensive\nprompt engineering or billion-scale LLM finetuning. CTCL pretrains a\nlightweight 140M conditional generator and a clustering-based topic model on\nlarge-scale public data. To further adapt to the private domain, the generator\nis DP finetuned on private data for fine-grained textual information, while the\ntopic model extracts a DP histogram representing distributional information.\nThe DP generator then samples according to the DP histogram to synthesize a\ndesired number of data examples. Evaluation across five diverse domains\ndemonstrates the effectiveness of our framework, particularly in the strong\nprivacy regime. Systematic ablation validates the design of each framework\ncomponent and highlights the scalability of our approach.\n","authors":["Bowen Tan","Zheng Xu","Eric Xing","Zhiting Hu","Shanshan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.12347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12345v1","updated":"2025-03-16T03:51:06Z","published":"2025-03-16T03:51:06Z","title":"General Table Question Answering via Answer-Formula Joint Generation","summary":"  Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperations, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely-used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nFormula as the logical form for solving complex reasoning on the tables with\ndifferent structures. Specifically, we construct a large Formula-annotated\nTableQA dataset \\texttt{FromulaQA} from existing datasets. In addition, we\npropose \\texttt{TabAF}, a general table answering framework to solve multiple\ntypes of tasks over multiple types of tables simultaneously. Unlike existing\nmethods, \\texttt{TabAF} decodes answers and Formulas with a single LLM\nbackbone, demonstrating great versatility and generalization. \\texttt{TabAF}\nbased on Llama3.1-70B achieves new state-of-the-art performance on the\nWikiTableQuestion, HiTab and TabFact.\n","authors":["Zhongyuan Wang","Richong Zhang","Zhijie Nie"],"pdf_url":"https://arxiv.org/pdf/2503.12345v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2502.07164v2","updated":"2025-03-16T03:45:42Z","published":"2025-02-11T01:03:33Z","title":"Does Training on Synthetic Data Make Models Less Robust?","summary":"  An increasingly common practice is to train large language models (LLMs)\nusing synthetic data. Often this synthetic data is produced by the same or\nsimilar LLMs as those it is being used to train. This raises the question of\nwhether the synthetic data might in fact exacerbate certain \"blindspots\" by\nreinforcing heuristics that the LLM already encodes. In this paper, we conduct\nsimulated experiments on the natural language inference (NLI) task with\nLlama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted\nevaluation set designed to measure the presence of specific heuristic\nstrategies for NLI, as our \"blindspot\" task. Our goal is to determine whether\nperformance disparities between the general and blind spot tasks emerge. Our\nresults indicate that synthetic data does not reinforce blindspots in the way\nwe expected. Specifically, we see that, while fine-tuning with synthetic data\ndoesn't necessarily reduce the use of the heuristic, it also does not make it\nworse as we hypothesized.\n","authors":["Lingze Zhang","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2502.07164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07378v5","updated":"2025-03-16T03:27:33Z","published":"2024-03-12T07:31:18Z","title":"SVD-LLM: Truncation-aware Singular Value Decomposition for Large\n  Language Model Compression","summary":"  The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitates LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the compressed weights after SVD\ntruncation. In this work, we propose SVD-LLM, a SVD-based post-training LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening technique to ensure a direct\nmapping between singular values and compression loss. Moreover, SVD-LLM adopts\na parameter update with sequential low-rank approximation to compensate for the\naccuracy degradation after SVD compression. We evaluate SVD-LLM on 10 datasets\nand seven models from three different LLM families at three different scales.\nOur results demonstrate the superiority of SVD-LLM over state-of-the-arts,\nespecially at high model compression ratios. Our code is available at\nhttps://github.com/AIoT-MLSys-Lab/SVD-LLM\n","authors":["Xin Wang","Yu Zheng","Zhongwei Wan","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.07378v5.pdf","comment":"ICLR 2025; Code available at:\n  https://github.com/AIoT-MLSys-Lab/SVD-LLM"},{"id":"http://arxiv.org/abs/2503.12340v1","updated":"2025-03-16T03:27:12Z","published":"2025-03-16T03:27:12Z","title":"SVD-LLM V2: Optimizing Singular Value Truncation for Large Language\n  Model Compression","summary":"  Despite significant advancements, the practical deployment of Large Language\nModels (LLMs) is often hampered by their immense sizes, highlighting the need\nfor effective compression techniques. Singular Value Decomposition (SVD) is a\npromising LLM compression technique. However, existing SVD-based compression\nmethods fall short in reducing truncation losses, leading to less competitive\nperformance in compressed models. In this work, we introduce SVD-LLM V2, a\nSVD-based LLM compression method that optimizes singular value truncation in\nSVD compression with two techniques. First, SVD-LLM V2 proposes to use\ntheoretical truncation loss of weight matrices to assign a unique compression\nratio to each weight matrix at different layers to accommodate weight\nredundancy heterogeneity. Second, SVD-LLM V2 proposes loss-optimized weight\ntruncation to ensure that the truncated singular values result in a lower and\nmore stable truncation loss in practice. We evaluate SVD-LLM V2 on ten datasets\nand five LLMs at various scales. Our results show SVD-LLM V2 outperforms\nstate-of-the-art SVD-based LLM compression methods. Our code is available at\nhttps://github.com/AIoT-MLSys-Lab/SVD-LLM\n","authors":["Xin Wang","Samiul Alam","Zhongwei Wan","Hui Shen","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12340v1.pdf","comment":"NAACL 2025; Code available at\n  https://github.com/AIoT-MLSys-Lab/SVD-LLM"},{"id":"http://arxiv.org/abs/2503.12329v1","updated":"2025-03-16T02:56:09Z","published":"2025-03-16T02:56:09Z","title":"CapArena: Benchmarking and Analyzing Detailed Image Captioning in the\n  LLM Era","summary":"  Image captioning has been a longstanding challenge in vision-language\nresearch. With the rise of LLMs, modern Vision-Language Models (VLMs) generate\ndetailed and comprehensive image descriptions. However, benchmarking the\nquality of such captions remains unresolved. This paper addresses two key\nquestions: (1) How well do current VLMs actually perform on image captioning,\nparticularly compared to humans? We built CapArena, a platform with over 6000\npairwise caption battles and high-quality human preference votes. Our\narena-style evaluation marks a milestone, showing that leading models like\nGPT-4o achieve or even surpass human performance, while most open-source models\nlag behind. (2) Can automated metrics reliably assess detailed caption quality?\nUsing human annotations from CapArena, we evaluate traditional and recent\ncaptioning metrics, as well as VLM-as-a-Judge. Our analysis reveals that while\nsome metrics (e.g., METEOR) show decent caption-level agreement with humans,\ntheir systematic biases lead to inconsistencies in model ranking. In contrast,\nVLM-as-a-Judge demonstrates robust discernment at both the caption and model\nlevels. Building on these insights, we release CapArena-Auto, an accurate and\nefficient automated benchmark for detailed captioning, achieving 94.3%\ncorrelation with human rankings at just $4 per test. Data and resources will be\nopen-sourced at https://caparena.github.io.\n","authors":["Kanzhi Cheng","Wenpo Song","Jiaxin Fan","Zheng Ma","Qiushi Sun","Fangzhi Xu","Chenyang Yan","Nuo Chen","Jianbing Zhang","Jiajun Chen"],"pdf_url":"https://arxiv.org/pdf/2503.12329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05662v2","updated":"2025-03-16T02:28:32Z","published":"2025-01-10T02:28:04Z","title":"Cascaded Self-Evaluation Augmented Training for Lightweight Multimodal\n  LLMs","summary":"  Efficient Multimodal Large Language Models (EMLLMs) can improve performance\nthrough Chain-of-Thought (CoT) reasoning, but they have poor self-evaluation\ncapabilities during the CoT reasoning process. This is due to their tendency to\nsimplify the reasoning process and the degradation of self-evaluation ability\nduring downstream task fine-tuning. To address this, we intuitively propose\n\\textit{Self-Evaluation Augmented Training (SEAT)}, which uses more powerful\nEMLLMs to evaluate CoT reasoning data. The evaluation data is then used to\ntrain EMLLMs. However, due to the difficulties EMLLMs face with processing long\ntoken input-output sequences, and the degradation of self-evaluation ability as\na basis for CoT reasoning, the SEAT method is not fully adapted. Therefore, we\nfurther propose \\textit{Cascaded Self-Evaluation Augmented Training\n(Cas-SEAT)}, which converts long prompts into cascaded short prompts, each\nfocusing on a specific task. Additionally, we mix CoT reasoning and\nself-evaluation data to preserve its CoT reasoning ability while enhancing the\nself-evaluation capability of EMLLMs. We also conduct \\textit{Double-level Data\nFiltering (DDF)}, which includes source data filtering and labeled data\nfiltering, using both manual selection and MLLMs for filtering. Cas-SEAT and\nDDF work together to improve the performance of EMLLMs. Experiments show that\nCas-SEAT achieves an average improvement of 22.16% across multiple datasets,\nand DDF significantly reduces the resource consumption of training\n","authors":["Zheqi Lv","Wenkai Wang","Jiawei Wang","Shengyu Zhang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.05662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06378v2","updated":"2025-03-16T02:28:10Z","published":"2025-03-09T01:13:56Z","title":"General Scales Unlock AI Evaluation with Explanatory and Predictive\n  Power","summary":"  Ensuring safe and effective use of AI requires understanding and anticipating\nits performance on novel tasks, from advanced scientific challenges to\ntransformed workplace activities. So far, benchmarking has guided progress in\nAI, but it has offered limited explanatory and predictive power for\ngeneral-purpose AI systems, given the low transferability across diverse tasks.\nIn this paper, we introduce general scales for AI evaluation that can explain\nwhat common AI benchmarks really measure, extract ability profiles of AI\nsystems, and predict their performance for new task instances, in- and\nout-of-distribution. Our fully-automated methodology builds on 18 newly-crafted\nrubrics that place instance demands on general scales that do not saturate.\nIllustrated for 15 large language models and 63 tasks, high explanatory power\nis unleashed from inspecting the demand and ability profiles, bringing insights\non the sensitivity and specificity exhibited by different benchmarks, and how\nknowledge, metacognition and reasoning are affected by model size,\nchain-of-thought and distillation. Surprisingly, high predictive power at the\ninstance level becomes possible using these demand levels, providing superior\nestimates over black-box baseline predictors based on embeddings or finetuning,\nespecially in out-of-distribution settings (new tasks and new benchmarks). The\nscales, rubrics, battery, techniques and results presented here represent a\nmajor step for AI evaluation, underpinning the reliable deployment of AI in the\nyears ahead. (Collaborative platform:\nhttps://kinds-of-intelligence-cfi.github.io/ADELE.)\n","authors":["Lexin Zhou","Lorenzo Pacchiardi","Fernando Martínez-Plumed","Katherine M. Collins","Yael Moros-Daval","Seraphina Zhang","Qinlin Zhao","Yitian Huang","Luning Sun","Jonathan E. Prunty","Zongqian Li","Pablo Sánchez-García","Kexin Jiang Chen","Pablo A. M. Casares","Jiyun Zu","John Burden","Behzad Mehrbakhsh","David Stillwell","Manuel Cebrian","Jindong Wang","Peter Henderson","Sherry Tongshuang Wu","Patrick C. Kyllonen","Lucy Cheke","Xing Xie","José Hernández-Orallo"],"pdf_url":"https://arxiv.org/pdf/2503.06378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12301v1","updated":"2025-03-16T00:22:00Z","published":"2025-03-16T00:22:00Z","title":"One Goal, Many Challenges: Robust Preference Optimization Amid\n  Content-Aware and Multi-Source Noise","summary":"  Large Language Models (LLMs) have made significant strides in generating\nhuman-like responses, largely due to preference alignment techniques. However,\nthese methods often assume unbiased human feedback, which is rarely the case in\nreal-world scenarios. This paper introduces Content-Aware Noise-Resilient\nPreference Optimization (CNRPO), a novel framework that addresses multiple\nsources of content-dependent noise in preference learning. CNRPO employs a\nmulti-objective optimization approach to separate true preferences from\ncontent-aware noises, effectively mitigating their impact. We leverage backdoor\nattack mechanisms to efficiently learn and control various noise sources within\na single model. Theoretical analysis and extensive experiments on different\nsynthetic noisy datasets demonstrate that CNRPO significantly improves\nalignment with primary human preferences while controlling for secondary noises\nand biases, such as response length and harmfulness.\n","authors":["Amirabbas Afzali","Amirhossein Afsharrad","Seyed Shahabeddin Mousavi","Sanjay Lall"],"pdf_url":"https://arxiv.org/pdf/2503.12301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18771v2","updated":"2025-03-16T00:07:06Z","published":"2024-03-27T17:20:39Z","title":"CheckEval: A reliable LLM-as-a-Judge framework for evaluating text\n  generation using checklists","summary":"  Existing LLM-as-a-Judge approaches for evaluating text generation suffer from\nrating inconsistencies, with low agreement and high rating variance across\ndifferent evaluator models. We attribute this to subjective evaluation criteria\ncombined with Likert scale scoring in existing protocols. To address this\nissue, we introduce CheckEval, a checklist-based evaluation framework that\nimproves rating reliability via decomposed binary questions. Through\nexperiments with 12 evaluator models across multiple datasets, we first\ndemonstrate that CheckEval strongly correlates with human judgments, improving\nthe average correlation with human judgments by 0.10. More importantly,\nCheckEval dramatically improves the average agreement across evaluator models\nby 0.45 and reduces the score variance. CheckEval scores furthermore have the\nbenefit of being more interpretable because it decomposes evaluation criteria\ninto traceable binary decisions, allowing analyses of specific attributes\ndriving quality judgments.\n","authors":["Yukyung Lee","Joonghoon Kim","Jaehee Kim","Hyowon Cho","Jaewook Kang","Pilsung Kang","Najoung Kim"],"pdf_url":"https://arxiv.org/pdf/2403.18771v2.pdf","comment":"Extended version currently under review (Workshop version: HEAL at\n  CHI 2024)"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.04008v2","updated":"2025-03-16T21:12:51Z","published":"2024-12-13T09:26:04Z","title":"A Generative AI-driven Metadata Modelling Approach","summary":"  Since decades, the modelling of metadata has been core to the functioning of\nany academic library. Its importance has only enhanced with the increasing\npervasiveness of Generative Artificial Intelligence (AI)-driven information\nactivities and services which constitute a library's outreach. However, with\nthe rising importance of metadata, there arose several outstanding problems\nwith the process of designing a library metadata model impacting its\nreusability, crosswalk and interoperability with other metadata models. This\npaper posits that the above problems stem from an underlying thesis that there\nshould only be a few core metadata models which would be necessary and\nsufficient for any information service using them, irrespective of the\nheterogeneity of intra-domain or inter-domain settings. To that end, this paper\nadvances a contrary view of the above thesis and substantiates its argument in\nthree key steps. First, it introduces a novel way of thinking about a library\nmetadata model as an ontology-driven composition of five functionally\ninterlinked representation levels from perception to its intensional definition\nvia properties. Second, it introduces the representational manifoldness\nimplicit in each of the five levels which cumulatively contributes to a\nconceptually entangled library metadata model. Finally, and most importantly,\nit proposes a Generative AI-driven Human-Large Language Model (LLM)\ncollaboration based metadata modelling approach to disentangle the entanglement\ninherent in each representation level leading to the generation of a\nconceptually disentangled metadata model. Throughout the paper, the arguments\nare exemplified by motivating scenarios and examples from representative\nlibraries handling cancer information.\n","authors":["Mayukh Bagchi"],"pdf_url":"https://arxiv.org/pdf/2501.04008v2.pdf","comment":"Accepted for publication @ Special Issue on \"Generative AI and\n  Libraries\" - Library Trends Journal, Johns Hopkins University Press,\n  Maryland, USA"},{"id":"http://arxiv.org/abs/2501.09292v2","updated":"2025-03-16T16:21:57Z","published":"2025-01-16T04:56:33Z","title":"To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation","summary":"  Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.09292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12547v1","updated":"2025-03-16T15:32:30Z","published":"2025-03-16T15:32:30Z","title":"LLMSeR: Enhancing Sequential Recommendation via LLM-based Data\n  Augmentation","summary":"  Sequential Recommender Systems (SRS) have become a cornerstone of online\nplatforms, leveraging users' historical interaction data to forecast their next\npotential engagement. Despite their widespread adoption, SRS often grapple with\nthe long-tail user dilemma, resulting in less effective recommendations for\nindividuals with limited interaction records. The advent of Large Language\nModels (LLMs), with their profound capability to discern semantic relationships\namong items, has opened new avenues for enhancing SRS through data\naugmentation. Nonetheless, current methodologies encounter obstacles, including\nthe absence of collaborative signals and the prevalence of hallucination\nphenomena.In this work, we present LLMSeR, an innovative framework that\nutilizes Large Language Models (LLMs) to generate pseudo-prior items, thereby\nimproving the efficacy of Sequential Recommender Systems (SRS). To alleviate\nthe challenge of insufficient collaborative signals, we introduce the Semantic\nInteraction Augmentor (SIA), a method that integrates both semantic and\ncollaborative information to comprehensively augment user interaction data.\nMoreover, to weaken the adverse effects of hallucination in SRS, we develop the\nAdaptive Reliability Validation (ARV), a validation technique designed to\nassess the reliability of the generated pseudo items. Complementing these\nadvancements, we also devise a Dual-Channel Training strategy, ensuring\nseamless integration of data augmentation into the SRS training\nprocess.Extensive experiments conducted with three widely-used SRS models\ndemonstrate the generalizability and efficacy of LLMSeR.\n","authors":["Yuqi Sun","Qidong Liu","Haiping Zhu","Feng Tian"],"pdf_url":"https://arxiv.org/pdf/2503.12547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09313v2","updated":"2025-03-16T14:15:20Z","published":"2025-03-12T12:04:05Z","title":"xVLM2Vec: Adapting LVLM-based embedding models to multilinguality using\n  Self-Knowledge Distillation","summary":"  In the current literature, most embedding models are based on the\nencoder-only transformer architecture to extract a dense and meaningful\nrepresentation of the given input, which can be a text, an image, and more.\nWith the recent advances in language modeling thanks to the introduction of\nLarge Language Models, the possibility of extracting embeddings from these\nlarge and extensively trained models has been explored. However, current\nstudies focus on textual embeddings in English, which is also the main language\non which these models have been trained. Furthermore, there are very few models\nthat consider multimodal and multilingual input. In light of this, we propose\nan adaptation methodology for Large Vision-Language Models trained on English\nlanguage data to improve their performance in extracting multilingual and\nmultimodal embeddings. Finally, we design and introduce a benchmark to evaluate\nthe effectiveness of multilingual and multimodal embedding models.\n","authors":["Elio Musacchio","Lucia Siciliani","Pierpaolo Basile","Giovanni Semeraro"],"pdf_url":"https://arxiv.org/pdf/2503.09313v2.pdf","comment":"fix typo in number of tasks in MMEB; fix url for source code; added\n  missing reference to XTD10"},{"id":"http://arxiv.org/abs/2503.12321v1","updated":"2025-03-16T02:13:37Z","published":"2025-03-16T02:13:37Z","title":"A novel association and ranking approach identifies factors affecting\n  educational outcomes of STEM majors","summary":"  Improving undergraduate success in STEM requires identifying actionable\nfactors that impact student outcomes, allowing institutions to prioritize key\nleverage points for change. We examined academic, demographic, and\ninstitutional factors that might be associated with graduation rates at two\nfour-year colleges in the northeastern United States using a novel association\nalgorithm called D-basis to rank attributes associated with graduation.\nImportantly, the data analyzed included tracking data from the National Student\nClearinghouse on students who left their original institutions to determine\noutcomes following transfer.\n  Key predictors of successful graduation include performance in introductory\nSTEM courses, the choice of first mathematics class, and flexibility in major\nselection. High grades in introductory biology, general chemistry, and\nmathematics courses were strongly correlated with graduation. At the same time,\nstudents who switched majors - especially from STEM to non-STEM - had higher\noverall graduation rates. Additionally, Pell eligibility and demographic\nfactors, though less predictive overall, revealed disparities in time to\ngraduation and retention rates.\n  The findings highlight the importance of early academic support in STEM\ngateway courses and the implementation of institutional policies that provide\nflexibility in major selection. Enhancing student success in introductory\nmathematics, biology, and chemistry courses could greatly influence graduation\nrates. Furthermore, customized mathematics pathways and focused support for\nSTEM courses may assist institutions in optimizing student outcomes. This study\noffers data-driven insights to guide strategies to increase STEM degree\ncompletion.\n","authors":["Kira Adaricheva","Jonathan T. Brockman","Gillian Z. Elston","Lawrence Hobbie","Skylar Homan","Mohamad Khalefa","Jiyun V. Kim","Rochelle K. Nelson","Sarah Samad","Oren Segal"],"pdf_url":"https://arxiv.org/pdf/2503.12321v1.pdf","comment":"34 pages, 7 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.12623v1","updated":"2025-03-16T19:32:32Z","published":"2025-03-16T19:32:32Z","title":"MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network","summary":"  This paper introduces MAVEN (Multi-modal Attention for Valence-Arousal\nEmotion Network), a novel architecture for dynamic emotion recognition through\ndimensional modeling of affect. The model uniquely integrates visual, audio,\nand textual modalities via a bi-directional cross-modal attention mechanism\nwith six distinct attention pathways, enabling comprehensive interactions\nbetween all modality pairs. Our proposed approach employs modality-specific\nencoders to extract rich feature representations from synchronized video\nframes, audio segments, and transcripts. The architecture's novelty lies in its\ncross-modal enhancement strategy, where each modality representation is refined\nthrough weighted attention from other modalities, followed by self-attention\nrefinement through modality-specific encoders. Rather than directly predicting\nvalence-arousal values, MAVEN predicts emotions in a polar coordinate form,\naligning with psychological models of the emotion circumplex. Experimental\nevaluation on the Aff-Wild2 dataset demonstrates the effectiveness of our\napproach, with performance measured using Concordance Correlation Coefficient\n(CCC). The multi-stage architecture demonstrates superior ability to capture\nthe complex, nuanced nature of emotional expressions in conversational videos,\nadvancing the state-of-the-art (SOTA) in continuous emotion recognition\nin-the-wild. Code can be found at:\nhttps://github.com/Vrushank-Ahire/MAVEN_8th_ABAW.\n","authors":["Vrushank Ahire","Kunal Shah","Mudasir Nazir Khan","Nikhil Pakhale","Lownish Rai Sookha","M. A. Ganaie","Abhinav Dhall"],"pdf_url":"https://arxiv.org/pdf/2503.12623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20504v4","updated":"2025-03-16T16:25:31Z","published":"2024-12-29T15:42:24Z","title":"ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding","summary":"  Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.\n","authors":["Xiao Wang","Qingyi Si","Jianlong Wu","Shiyu Zhu","Li Cao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2412.20504v4.pdf","comment":"Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata"},{"id":"http://arxiv.org/abs/2503.12559v1","updated":"2025-03-16T16:14:52Z","published":"2025-03-16T16:14:52Z","title":"AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for\n  Video-language Understanding","summary":"  Multimodal Large Language Models (MLLMs) have revolutionized video\nunderstanding, yet are still limited by context length when processing long\nvideos. Recent methods compress videos by leveraging visual redundancy\nuniformly, yielding promising results. Nevertheless, our quantitative analysis\nshows that redundancy varies significantly across time and model layers,\nnecessitating a more flexible compression strategy. We propose AdaReTaKe, a\ntraining-free method that flexibly reduces visual redundancy by allocating\ncompression ratios among time and layers with theoretical guarantees.\nIntegrated into state-of-the-art MLLMs, AdaReTaKe improves processing capacity\nfrom 256 to 2048 frames while preserving critical information. Experiments on\nVideoMME, MLVU, LongVideoBench, and LVBench datasets demonstrate that AdaReTaKe\noutperforms existing methods by 2.3% and 2.8% for 7B and 72B models,\nrespectively, with even greater improvements of 5.9% and 6.0% on the longest\nLVBench. Our code is available at\nhttps://github.com/SCZwangxiao/video-FlexReduc.git.\n","authors":["Xiao Wang","Qingyi Si","Jianlong Wu","Shiyu Zhu","Li Cao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.12559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05782v2","updated":"2025-03-16T13:36:14Z","published":"2024-07-08T09:45:20Z","title":"Sequential Contrastive Audio-Visual Learning","summary":"  Contrastive learning has emerged as a powerful technique in audio-visual\nrepresentation learning, leveraging the natural co-occurrence of audio and\nvisual modalities in webscale video datasets. However, conventional contrastive\naudio-visual learning (CAV) methodologies often rely on aggregated\nrepresentations derived through temporal aggregation, neglecting the intrinsic\nsequential nature of the data. This oversight raises concerns regarding the\nability of standard approaches to capture and utilize fine-grained information\nwithin sequences. In response to this limitation, we propose sequential\ncontrastive audiovisual learning (SCAV), which contrasts examples based on\ntheir non-aggregated representation space using multidimensional sequential\ndistances. Audio-visual retrieval experiments with the VGGSound and Music\ndatasets demonstrate the effectiveness of SCAV, with up to 3.5x relative\nimprovements in recall against traditional aggregation-based contrastive\nlearning and other previously proposed methods, which utilize more parameters\nand data. We also show that models trained with SCAV exhibit a significant\ndegree of flexibility regarding the metric employed for retrieval, allowing us\nto use a hybrid retrieval approach that is both effective and efficient.\n","authors":["Ioannis Tsiamas","Santiago Pascual","Chunghsin Yeh","Joan Serrà"],"pdf_url":"https://arxiv.org/pdf/2407.05782v2.pdf","comment":"ICASSP 2025. Version 1 contains more details"},{"id":"http://arxiv.org/abs/2411.06976v2","updated":"2025-03-16T12:12:03Z","published":"2024-11-11T13:34:24Z","title":"A Hierarchical Compression Technique for 3D Gaussian Splatting\n  Compression","summary":"  3D Gaussian Splatting (GS) demonstrates excellent rendering quality and\ngeneration speed in novel view synthesis. However, substantial data size poses\nchallenges for storage and transmission, making 3D GS compression an essential\ntechnology. Current 3D GS compression research primarily focuses on developing\nmore compact scene representations, such as converting explicit 3D GS data into\nimplicit forms. In contrast, compression of the GS data itself has hardly been\nexplored. To address this gap, we propose a Hierarchical GS Compression (HGSC)\ntechnique. Initially, we prune unimportant Gaussians based on importance scores\nderived from both global and local significance, effectively reducing\nredundancy while maintaining visual quality. An Octree structure is used to\ncompress 3D positions. Based on the 3D GS Octree, we implement a hierarchical\nattribute compression strategy by employing a KD-tree to partition the 3D GS\ninto multiple blocks. We apply farthest point sampling to select anchor\nprimitives within each block and others as non-anchor primitives with varying\nLevels of Details (LoDs). Anchor primitives serve as reference points for\npredicting non-anchor primitives across different LoDs to reduce spatial\nredundancy. For anchor primitives, we use the region adaptive hierarchical\ntransform to achieve near-lossless compression of various attributes. For\nnon-anchor primitives, each is predicted based on the k-nearest anchor\nprimitives. To further minimize prediction errors, the reconstructed LoD and\nanchor primitives are combined to form new anchor primitives to predict the\nnext LoD. Our method notably achieves superior compression quality and a\nsignificant data size reduction of over 4.5 times compared to the\nstate-of-the-art compression method on small scenes datasets.\n","authors":["He Huang","Wenjie Huang","Qi Yang","Yiling Xu","Zhu li"],"pdf_url":"https://arxiv.org/pdf/2411.06976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12381v1","updated":"2025-03-16T07:01:29Z","published":"2025-03-16T07:01:29Z","title":"Deepfake Detection with Optimized Hybrid Model: EAR Biometric Descriptor\n  via Improved RCNN","summary":"  Deepfake is a widely used technology employed in recent years to create\npernicious content such as fake news, movies, and rumors by altering and\nsubstituting facial information from various sources. Given the ongoing\nevolution of deepfakes investigation of continuous identification and\nprevention is crucial. Due to recent technological advancements in AI\n(Artificial Intelligence) distinguishing deepfakes and artificially altered\nimages has become challenging. This approach introduces the robust detection of\nsubtle ear movements and shape changes to generate ear descriptors. Further, we\nalso propose a novel optimized hybrid deepfake detection model that considers\nthe ear biometric descriptors via enhanced RCNN (Region-Based Convolutional\nNeural Network). Initially, the input video is converted into frames and\npreprocessed through resizing, normalization, grayscale conversion, and\nfiltering processes followed by face detection using the Viola-Jones technique.\nNext, a hybrid model comprising DBN (Deep Belief Network) and Bi-GRU\n(Bidirectional Gated Recurrent Unit) is utilized for deepfake detection based\non ear descriptors. The output from the detection phase is determined through\nimproved score-level fusion. To enhance the performance, the weights of both\ndetection models are optimally tuned using the SU-JFO (Self-Upgraded Jellyfish\nOptimization method). Experimentation is conducted based on four scenarios:\ncompression, noise, rotation, pose, and illumination on three different\ndatasets. The performance results affirm that our proposed method outperforms\ntraditional models such as CNN (Convolution Neural Network), SqueezeNet, LeNet,\nLinkNet, LSTM (Long Short-Term Memory), DFP (Deepfake Predictor) [1], and\nResNext+CNN+LSTM [2] in terms of various performance metrics viz. accuracy,\nspecificity, and precision.\n","authors":["Ruchika Sharma","Rudresh Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2503.12381v1.pdf","comment":"Submiited to journal"}]},"2025-03-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.12294v1","updated":"2025-03-15T23:20:45Z","published":"2025-03-15T23:20:45Z","title":"The Lucie-7B LLM and the Lucie Training Dataset: Open resources for\n  multilingual language generation","summary":"  We present both the Lucie Training Dataset and the Lucie-7B foundation model.\nThe Lucie Training Dataset is a multilingual collection of textual corpora\ncentered around French and designed to offset anglo-centric biases found in\nmany datasets for large language model pretraining. Its French data is pulled\nnot only from traditional web sources, but also from French cultural heritage\ndocuments, filling an important gap in modern datasets. Beyond French, which\nmakes up the largest share of the data, we added documents to support several\nother European languages, including English, Spanish, German, and Italian.\nApart from its value as a resource for French language and culture, an\nimportant feature of this dataset is that it prioritizes data rights by\nminimizing copyrighted material. In addition, building on the philosophy of\npast open projects, it is redistributed in the form used for training and its\nprocessing is described on Hugging Face and GitHub. The Lucie-7B foundation\nmodel is trained on equal amounts of data in French and English -- roughly 33%\neach -- in an effort to better represent cultural aspects of French-speaking\ncommunities. We also describe two instruction fine-tuned models,\nLucie-7B-Instruct-v1.1 and Lucie-7B-Instruct-human-data, which we release as\ndemonstrations of Lucie-7B in use. These models achieve promising results\ncompared to state-of-the-art models, demonstrating that an open approach\nprioritizing data rights can still deliver strong performance. We see these\nmodels as an initial step toward developing more performant, aligned models in\nthe near future. Model weights for Lucie-7B and the Lucie instruct models,\nalong with intermediate checkpoints for the former, are published on Hugging\nFace, while model training and data preparation code is available on GitHub.\nThis makes Lucie-7B one of the first OSI compliant language models according to\nthe new OSI definition.\n","authors":["Olivier Gouvert","Julie Hunter","Jérôme Louradour","Christophe Cerisara","Evan Dufraisse","Yaya Sy","Laura Rivière","Jean-Pierre Lorré","OpenLLM-France community"],"pdf_url":"https://arxiv.org/pdf/2503.12294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12286v1","updated":"2025-03-15T22:57:31Z","published":"2025-03-15T22:57:31Z","title":"Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances\n  Rare Disease Diagnosis from Clinical Notes","summary":"  Background: Several studies show that large language models (LLMs) struggle\nwith phenotype-driven gene prioritization for rare diseases. These studies\ntypically use Human Phenotype Ontology (HPO) terms to prompt foundation models\nlike GPT and LLaMA to predict candidate genes. However, in real-world settings,\nfoundation models are not optimized for domain-specific tasks like clinical\ndiagnosis, yet inputs are unstructured clinical notes rather than standardized\nterms. How LLMs can be instructed to predict candidate genes or disease\ndiagnosis from unstructured clinical notes remains a major challenge. Methods:\nWe introduce RAG-driven CoT and CoT-driven RAG, two methods that combine\nChain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze\nclinical notes. A five-question CoT protocol mimics expert reasoning, while RAG\nretrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in\nMan). We evaluated these approaches on rare disease datasets, including 5,980\nPhenopacket-derived notes, 255 literature-based narratives, and 220 in-house\nclinical notes from Childrens Hospital of Philadelphia. Results: We found that\nrecent foundations models, including Llama 3.3-70B-Instruct and\nDeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2\nand GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both\noutperform foundation models in candidate gene prioritization from clinical\nnotes; in particular, both methods with DeepSeek backbone resulted in a top-10\ngene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT\nworks better for high-quality notes, where early retrieval can anchor the\nsubsequent reasoning steps in domain-specific evidence, while CoT-driven RAG\nhas advantage when processing lengthy and noisy notes.\n","authors":["Da Wu","Zhanliang Wang","Quan Nguyen","Kai Wang"],"pdf_url":"https://arxiv.org/pdf/2503.12286v1.pdf","comment":"31 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.08600v2","updated":"2025-03-15T21:25:43Z","published":"2025-03-11T16:35:08Z","title":"NSF-SciFy: Mining the NSF Awards Database for Scientific Claims","summary":"  We present NSF-SciFy, a large-scale dataset for scientific claim extraction\nderived from the National Science Foundation (NSF) awards database, comprising\nover 400K grant abstracts spanning five decades. While previous datasets relied\non published literature, we leverage grant abstracts which offer a unique\nadvantage: they capture claims at an earlier stage in the research lifecycle\nbefore publication takes effect. We also introduce a new task to distinguish\nbetween existing scientific claims and aspirational research intentions in\nproposals. Using zero-shot prompting with frontier large language models, we\njointly extract 114K scientific claims and 145K investigation proposals from\n16K grant abstracts in the materials science domain to create a focused subset\ncalled NSF-SciFy-MatSci. We use this dataset to evaluate 3 three key tasks: (1)\ntechnical to non-technical abstract generation, where models achieve high\nBERTScore (0.85+ F1); (2) scientific claim extraction, where fine-tuned models\noutperform base models by 100% relative improvement; and (3) investigation\nproposal extraction, showing 90%+ improvement with fine-tuning. We introduce\nnovel LLM-based evaluation metrics for robust assessment of claim/proposal\nextraction quality. As the largest scientific claim dataset to date -- with an\nestimated 2.8 million claims across all STEM disciplines funded by the NSF --\nNSF-SciFy enables new opportunities for claim verification and meta-scientific\nresearch. We publicly release all datasets, trained models, and evaluation code\nto facilitate further research.\n","authors":["Delip Rao","Weiqiu You","Eric Wong","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2503.08600v2.pdf","comment":"11 pages, 3 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.04636v2","updated":"2025-03-15T20:03:47Z","published":"2025-03-06T17:24:06Z","title":"Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking","summary":"  As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.\n","authors":["Yijie Xu","Aiwei Liu","Xuming Hu","Lijie Wen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04636v2.pdf","comment":"Accepted by the ICLR 2025 Workshop on GenAI Watermarking"},{"id":"http://arxiv.org/abs/2412.06858v2","updated":"2025-03-15T19:35:05Z","published":"2024-12-08T21:46:22Z","title":"Taming Sensitive Weights : Noise Perturbation Fine-tuning for Robust LLM\n  Quantization","summary":"  Quantization is a critical step to enable efficient LLM serving under limited\nresource. However, previous research observes that certain weights in the LLM,\nknown as outliers, are significantly sensitive to quantization noises. Existing\nquantization methods leave these outliers as floating points or higher\nprecisions to retain performance, posting challenges on the efficient hardware\ndeployment of the mixed-precision model. This work investigates an alternative\nway to tame the sensitive weights' impact on the quantization error, by\nreducing the loss Hessian trace with respect to outliers through an efficient\nfine-tuning process. We propose Noise Perturbation Fine-tuning (NPFT), which\nidentifies outlier weights and add random weight perturbations on the outliers\nas the model going through a PEFT optimization. NPFT tames the sensitivity of\noutlier weights so that the quantized model performance can be improved without\nspecial treatment to the outliers. When applied to OPT and LLaMA models, our\nNPFT method achieves stable performance improvements for both uniform and\nnon-uniform quantizers, while also offering better inference efficiency.\nNotably, the simplest RTN can achieve performance on par with GPTQ using our\nNPFT on LLaMA2-7B-4bits benchmark.\n","authors":["Dongwei Wang","Huanrui Yang"],"pdf_url":"https://arxiv.org/pdf/2412.06858v2.pdf","comment":"Accepted as poster by CPAL2025"},{"id":"http://arxiv.org/abs/2307.10442v2","updated":"2025-03-15T19:14:13Z","published":"2023-07-19T20:16:46Z","title":"Thrust: Adaptively Propels Large Language Models with External Knowledge","summary":"  Although large-scale pre-trained language models (PTLMs) are shown to encode\nrich knowledge in their model parameters, the inherent knowledge in PTLMs can\nbe opaque or static, making external knowledge necessary. However, the existing\ninformation retrieval techniques could be costly and may even introduce noisy\nand sometimes misleading knowledge. To address these challenges, we propose the\ninstance-level adaptive propulsion of external knowledge (IAPEK), where we only\nconduct the retrieval when necessary. To achieve this goal, we propose\nmeasuring whether a PTLM contains enough knowledge to solve an instance with a\nnovel metric, Thrust, which leverages the representation distribution of a\nsmall number of seen instances. Extensive experiments demonstrate that thrust\nis a good measurement of PTLM models' instance-level knowledgeability.\nMoreover, we can achieve significantly higher cost-efficiency with the Thrust\nscore as the retrieval indicator than the naive usage of external knowledge on\n88% of the evaluated tasks with 26% average performance improvement. Such\nfindings shed light on the real-world practice of knowledge-enhanced LMs with a\nlimited knowledge-seeking budget due to computation latency or costs.\n","authors":["Xinran Zhao","Hongming Zhang","Xiaoman Pan","Wenlin Yao","Dong Yu","Jianshu Chen"],"pdf_url":"https://arxiv.org/pdf/2307.10442v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.12225v1","updated":"2025-03-15T18:43:13Z","published":"2025-03-15T18:43:13Z","title":"Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents","summary":"  This article explores the gaps that can manifest when using a large language\nmodel (LLM) to obtain simplified interpretations of data practices from a\ncomplex privacy policy. We exemplify these gaps to showcase issues in accuracy,\ncompleteness, clarity and representation, while advocating for continued\nresearch to realize an LLM's true potential in revolutionizing privacy\nmanagement through personal assistants and automated compliance checking.\n","authors":["Rinku Dewri"],"pdf_url":"https://arxiv.org/pdf/2503.12225v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.12225v1","updated":"2025-03-15T18:43:13Z","published":"2025-03-15T18:43:13Z","title":"Interpretation Gaps in LLM-Assisted Comprehension of Privacy Documents","summary":"  This article explores the gaps that can manifest when using a large language\nmodel (LLM) to obtain simplified interpretations of data practices from a\ncomplex privacy policy. We exemplify these gaps to showcase issues in accuracy,\ncompleteness, clarity and representation, while advocating for continued\nresearch to realize an LLM's true potential in revolutionizing privacy\nmanagement through personal assistants and automated compliance checking.\n","authors":["Rinku Dewri"],"pdf_url":"https://arxiv.org/pdf/2503.12225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12183v1","updated":"2025-03-15T15:54:44Z","published":"2025-03-15T15:54:44Z","title":"Bridging Textual-Collaborative Gap through Semantic Codes for Sequential\n  Recommendation","summary":"  In recent years, substantial research efforts have been devoted to enhancing\nsequential recommender systems by integrating abundant side information with\nID-based collaborative information. This study specifically focuses on\nleveraging the textual metadata (e.g., titles and brands) associated with\nitems. While existing methods have achieved notable success by combining text\nand ID representations, they often struggle to strike a balance between textual\ninformation embedded in text representations and collaborative information from\nsequential patterns of user behavior. In light of this, we propose CoCoRec, a\nnovel Code-based textual and Collaborative semantic fusion method for\nsequential Recommendation. The key idea behind our approach is to bridge the\ngap between textual and collaborative information using semantic codes.\nSpecifically, we generate fine-grained semantic codes from multi-view text\nembeddings through vector quantization techniques. Subsequently, we develop a\ncode-guided semantic-fusion module based on the cross-attention mechanism to\nflexibly extract and integrate relevant information from text representations.\nIn order to further enhance the fusion of textual and collaborative semantics,\nwe introduce an optimization strategy that employs code masking with two\nspecific objectives: masked code modeling and masked sequence alignment. The\nmerit of these objectives lies in leveraging mask prediction tasks and\naugmented item representations to capture code correlations within individual\nitems and enhance the sequence modeling of the recommendation backbone.\nExtensive experiments conducted on four public datasets demonstrate the\nsuperiority of CoCoRec, showing significant improvements over various\nsequential recommendation models. Our code is available at\nhttps://anonymous.4open.science/r/CoCoRec-6E41.\n","authors":["Enze Liu","Bowen Zheng","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2503.12183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01001v3","updated":"2025-03-15T14:45:21Z","published":"2025-03-02T19:43:35Z","title":"Towards An Efficient LLM Training Paradigm for CTR Prediction","summary":"  Large Language Models (LLMs) have demonstrated tremendous potential as the\nnext-generation ranking-based recommendation system. Many recent works have\nshown that LLMs can significantly outperform conventional click-through-rate\n(CTR) prediction approaches. Despite such promising results, the computational\ninefficiency inherent in the current training paradigm makes it particularly\nchallenging to train LLMs for ranking-based recommendation tasks on large\ndatasets. To train LLMs for CTR prediction, most existing studies adopt the\nprevalent ''sliding-window'' paradigm. Given a sequence of $m$ user\ninteractions, a unique training prompt is constructed for each interaction by\ndesignating it as the prediction target along with its preceding $n$\ninteractions serving as context. In turn, the sliding-window paradigm results\nin an overall complexity of $O(mn^2)$ that scales linearly with the length of\nuser interactions. Consequently, a direct adoption to train LLMs with such\nstrategy can result in prohibitively high training costs as the length of\ninteractions grows. To alleviate the computational inefficiency, we propose a\nnovel training paradigm, namely Dynamic Target Isolation (DTI), that\nstructurally parallelizes the training of $k$ (where $k >> 1$) target\ninteractions. Furthermore, we identify two major bottlenecks - hidden-state\nleakage and positional bias overfitting - that limit DTI to only scale up to a\nsmall value of $k$ (e.g., 5) then propose a computationally light solution to\neffectively tackle each. Through extensive experiments on three widely adopted\npublic CTR datasets, we empirically show that DTI reduces training time by an\naverage of $\\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without\ncompromising CTR prediction performance.\n","authors":["Allen Lin","Renqin Cai","Yun He","Hanchao Yu","Jing Qian","Rui Li","Qifan Wang","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2503.01001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03524v2","updated":"2025-03-15T14:18:53Z","published":"2025-03-05T14:08:53Z","title":"Intrinsic and Extrinsic Factor Disentanglement for Recommendation in\n  Various Context Scenarios","summary":"  In recommender systems, the patterns of user behaviors (e.g., purchase,\nclick) may vary greatly in different contexts (e.g., time and location). This\nis because user behavior is jointly determined by two types of factors:\nintrinsic factors, which reflect consistent user preference, and extrinsic\nfactors, which reflect external incentives that may vary in different contexts.\nDifferentiating between intrinsic and extrinsic factors helps learn user\nbehaviors better. However, existing studies have only considered\ndifferentiating them from a single, pre-defined context (e.g., time or\nlocation), ignoring the fact that a user's extrinsic factors may be influenced\nby the interplay of various contexts at the same time. In this paper, we\npropose the Intrinsic-Extrinsic Disentangled Recommendation (IEDR) model, a\ngeneric framework that differentiates intrinsic from extrinsic factors\nconsidering various contexts simultaneously, enabling more accurate\ndifferentiation of factors and hence the improvement of recommendation\naccuracy. IEDR contains a context-invariant contrastive learning component to\ncapture intrinsic factors, and a disentanglement component to extract extrinsic\nfactors under the interplay of various contexts. The two components work\ntogether to achieve effective factor learning. Extensive experiments on\nreal-world datasets demonstrate IEDR's effectiveness in learning disentangled\nfactors and significantly improving recommendation accuracy by up to 4% in\nNDCG.\n","authors":["Yixin Su","Wei Jiang","Fangquan Lin","Cheng Yang","Sarah M. Erfani","Junhao Gan","Yunxiang Zhao","Ruixuan Li","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03524v2.pdf","comment":"32 pages, 13 figures, 11 tables. Published on Transactions of\n  Information Systems"},{"id":"http://arxiv.org/abs/2503.12062v1","updated":"2025-03-15T09:27:59Z","published":"2025-03-15T09:27:59Z","title":"Genicious: Contextual Few-shot Prompting for Insights Discovery","summary":"  Data and insights discovery is critical for decision-making in modern\norganizations. We present Genicious, an LLM-aided interface that enables users\nto interact with tabular datasets and ask complex queries in natural language.\nBy benchmarking various prompting strategies and language models, we have\ndeveloped an end-to-end tool that leverages contextual few-shot prompting,\nachieving superior performance in terms of latency, accuracy, and scalability.\nGenicious empowers stakeholders to explore, analyze and visualize their\ndatasets efficiently while ensuring data security through role-based access\ncontrol and a Text-to-SQL approach.\n","authors":["Vineet Kumar","Ronald Tony","Darshita Rathore","Vipasha Rana","Bhuvanesh Mandora"," Kanishka","Chetna Bansal","Anindya Moitra"],"pdf_url":"https://arxiv.org/pdf/2503.12062v1.pdf","comment":"5 pages, 3 figures, CODS-COMAD Dec 24, Jodhpur, India"},{"id":"http://arxiv.org/abs/2407.08227v3","updated":"2025-03-15T06:25:38Z","published":"2024-07-11T07:01:50Z","title":"DALL-M: Context-Aware Clinical Data Augmentation with LLMs","summary":"  X-ray images are vital in medical diagnostics, but their effectiveness is\nlimited without clinical context. Radiologists often find chest X-rays\ninsufficient for diagnosing underlying diseases, necessitating the integration\nof structured clinical features with radiology reports.\n  To address this, we introduce DALL-M, a novel framework that enhances\nclinical datasets by generating contextual synthetic data. DALL-M augments\nstructured patient data, including vital signs (e.g., heart rate, oxygen\nsaturation), radiology findings (e.g., lesion presence), and demographic\nfactors. It integrates this tabular data with contextual knowledge extracted\nfrom radiology reports and domain-specific resources (e.g., Radiopaedia,\nWikipedia), ensuring clinical consistency and reliability.\n  DALL-M follows a three-phase process: (i) clinical context storage, (ii)\nexpert query generation, and (iii) context-aware feature augmentation. Using\nlarge language models (LLMs), it generates both contextual synthetic values for\nexisting clinical features and entirely new, clinically relevant features.\n  Applied to 799 cases from the MIMIC-IV dataset, DALL-M expanded the original\n9 clinical features to 91. Empirical validation with machine learning models\n(including Decision Trees, Random Forests, XGBoost, and TabNET) demonstrated a\n16.5% improvement in F1 score and a 25% increase in Precision and Recall.\n  DALL-M bridges an important gap in clinical data augmentation by preserving\ndata integrity while enhancing predictive modeling in healthcare. Our results\nshow that integrating LLM-generated synthetic features significantly improves\nmodel performance, making DALL-M a scalable and practical approach for\nAI-driven medical diagnostics.\n","authors":["Chihcheng Hsieh","Catarina Moreira","Isabel Blanco Nobre","Sandra Costa Sousa","Chun Ouyang","Margot Brereton","Joaquim Jorge","Jacinto C. Nascimento"],"pdf_url":"https://arxiv.org/pdf/2407.08227v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.12173v1","updated":"2025-03-15T15:31:04Z","published":"2025-03-15T15:31:04Z","title":"LAPIG: Language Guided Projector Image Generation with Surface\n  Adaptation and Stylization","summary":"  We propose LAPIG, a language guided projector image generation method with\nsurface adaptation and stylization. LAPIG consists of a projector-camera system\nand a target textured projection surface. LAPIG takes the user text prompt as\ninput and aims to transform the surface style using the projector. LAPIG's key\nchallenge is that due to the projector's physical brightness limitation and the\nsurface texture, the viewer's perceived projection may suffer from color\nsaturation and artifacts in both dark and bright regions, such that even with\nthe state-of-the-art projector compensation techniques, the viewer may see\nclear surface texture-related artifacts. Therefore, how to generate a projector\nimage that follows the user's instruction while also displaying minimum surface\nartifacts is an open problem. To address this issue, we propose projection\nsurface adaptation (PSA) that can generate compensable surface stylization. We\nfirst train two networks to simulate the projector compensation and\nproject-and-capture processes, this allows us to find a satisfactory projector\nimage without real project-and-capture and utilize gradient descent for fast\nconvergence. Then, we design content and saturation losses to guide the\nprojector image generation, such that the generated image shows no clearly\nperceivable artifacts when projected. Finally, the generated image is projected\nfor visually pleasing surface style morphing effects. The source code and video\nare available on the project page: https://Yu-chen-Deng.github.io/LAPIG/.\n","authors":["Yuchen Deng","Haibin Ling","Bingyao Huang"],"pdf_url":"https://arxiv.org/pdf/2503.12173v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.12149v1","updated":"2025-03-15T14:10:25Z","published":"2025-03-15T14:10:25Z","title":"Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm\n  Perception in Large Vision-Language Models","summary":"  With the advent of large vision-language models (LVLMs) demonstrating\nincreasingly human-like abilities, a pivotal question emerges: do different\nLVLMs interpret multimodal sarcasm differently, and can a single model grasp\nsarcasm from multiple perspectives like humans? To explore this, we introduce\nan analytical framework using systematically designed prompts on existing\nmultimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409\nsamples, we examine interpretive variations within and across models, focusing\non confidence levels, alignment with dataset labels, and recognition of\nambiguous \"neutral\" cases. Our findings reveal notable discrepancies -- across\nLVLMs and within the same model under varied prompts. While\nclassification-oriented prompts yield higher internal consistency, models\ndiverge markedly when tasked with interpretive reasoning. These results\nchallenge binary labeling paradigms by highlighting sarcasm's subjectivity. We\nadvocate moving beyond rigid annotation schemes toward multi-perspective,\nuncertainty-aware modeling, offering deeper insights into multimodal sarcasm\ncomprehension. Our code and data are available at:\nhttps://github.com/CoderChen01/LVLMSarcasmAnalysis\n","authors":["Junjie Chen","Xuyang Liu","Subin Huang","Linfeng Zhang","Hang Yu"],"pdf_url":"https://arxiv.org/pdf/2503.12149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12127v1","updated":"2025-03-15T13:18:04Z","published":"2025-03-15T13:18:04Z","title":"Hyperbolic Safety-Aware Vision-Language Models","summary":"  Addressing the retrieval of unsafe content from vision-language models such\nas CLIP is an important step towards real-world integration. Current efforts\nhave relied on unlearning techniques that try to erase the model's knowledge of\nunsafe concepts. While effective in reducing unwanted outputs, unlearning\nlimits the model's capacity to discern between safe and unsafe content. In this\nwork, we introduce a novel approach that shifts from unlearning to an awareness\nparadigm by leveraging the inherent hierarchical properties of the hyperbolic\nspace. We propose to encode safe and unsafe content as an entailment hierarchy,\nwhere both are placed in different regions of hyperbolic space. Our HySAC,\nHyperbolic Safety-Aware CLIP, employs entailment loss functions to model the\nhierarchical and asymmetrical relations between safe and unsafe image-text\npairs. This modelling, ineffective in standard vision-language models due to\ntheir reliance on Euclidean embeddings, endows the model with awareness of\nunsafe content, enabling it to serve as both a multimodal unsafe classifier and\na flexible content retriever, with the option to dynamically redirect unsafe\nqueries toward safer alternatives or retain the original output. Extensive\nexperiments show that our approach not only enhances safety recognition but\nalso establishes a more adaptable and interpretable framework for content\nmoderation in vision-language models. Our source code is available at\nhttps://github.com/aimagelab/HySAC.\n","authors":["Tobia Poppi","Tejaswi Kasarla","Pascal Mettes","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.12127v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2501.09291v2","updated":"2025-03-15T12:38:50Z","published":"2025-01-16T04:53:29Z","title":"LAVCap: LLM-based Audio-Visual Captioning using Optimal Transport","summary":"  Automated audio captioning is a task that generates textual descriptions for\naudio content, and recent studies have explored using visual information to\nenhance captioning quality. However, current methods often fail to effectively\nfuse audio and visual data, missing important semantic cues from each modality.\nTo address this, we introduce LAVCap, a large language model (LLM)-based\naudio-visual captioning framework that effectively integrates visual\ninformation with audio to improve audio captioning performance. LAVCap employs\nan optimal transport-based alignment loss to bridge the modality gap between\naudio and visual features, enabling more effective semantic extraction.\nAdditionally, we propose an optimal transport attention module that enhances\naudio-visual fusion using an optimal transport assignment map. Combined with\nthe optimal training strategy, experimental results demonstrate that each\ncomponent of our framework is effective. LAVCap outperforms existing\nstate-of-the-art methods on the AudioCaps dataset, without relying on large\ndatasets or post-processing. Code is available at\nhttps://github.com/NAVER-INTEL-Co-Lab/gaudi-lavcap.\n","authors":["Kyeongha Rho","Hyeongkeun Lee","Valentio Iverson","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2501.09291v2.pdf","comment":"5 pages, 2 figures; Accepted to ICASSP 2025"}]},"2025-03-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.14495v1","updated":"2025-03-18T17:58:28Z","published":"2025-03-18T17:58:28Z","title":"Temporal Consistency for LLM Reasoning Process Error Identification","summary":"  Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency\n","authors":["Jiacheng Guo","Yue Wu","Jiahao Qiu","Kaixuan Huang","Xinzhe Juan","Ling Yang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14484v1","updated":"2025-03-18T17:54:14Z","published":"2025-03-18T17:54:14Z","title":"Gricean Norms as a Basis for Effective Collaboration","summary":"  Effective human-AI collaboration hinges not only on the AI agent's ability to\nfollow explicit instructions but also on its capacity to navigate ambiguity,\nincompleteness, invalidity, and irrelevance in communication. Gricean\nconversational and inference norms facilitate collaboration by aligning unclear\ninstructions with cooperative principles. We propose a normative framework that\nintegrates Gricean norms and cognitive frameworks -- common ground, relevance\ntheory, and theory of mind -- into large language model (LLM) based agents. The\nnormative framework adopts the Gricean maxims of quantity, quality, relation,\nand manner, along with inference, as Gricean norms to interpret unclear\ninstructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within\nthis framework, we introduce Lamoids, GPT-4 powered agents designed to\ncollaborate with humans. To assess the influence of Gricean norms in human-AI\ncollaboration, we evaluate two versions of a Lamoid: one with norms and one\nwithout. In our experiments, a Lamoid collaborates with a human to achieve\nshared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear\nand unclear natural language instructions. Our results reveal that the Lamoid\nwith Gricean norms achieves higher task accuracy and generates clearer, more\naccurate, and contextually relevant responses than the Lamoid without norms.\nThis improvement stems from the normative framework, which enhances the agent's\npragmatic reasoning, fostering effective human-AI collaboration and enabling\ncontext-aware communication in LLM-based agents.\n","authors":["Fardin Saad","Pradeep K. Murukannaiah","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2503.14484v1.pdf","comment":"Accepted to AAMAS 2025. 8 pages (excl. references), 9 figures/tables.\n  (Appendix: 5 pages, 6 figures/tables). Code available at:\n  https://github.com/fardinsaad/Gricean-Norms"},{"id":"http://arxiv.org/abs/2503.14481v1","updated":"2025-03-18T17:53:20Z","published":"2025-03-18T17:53:20Z","title":"Don't lie to your friends: Learning what you know from collaborative\n  self-play","summary":"  To be helpful assistants, AI agents must be aware of their own capabilities\nand limitations. This includes knowing when to answer from parametric knowledge\nversus using tools, when to trust tool outputs, and when to abstain or hedge.\nSuch capabilities are hard to teach through supervised fine-tuning because they\nrequire constructing examples that reflect the agent's specific capabilities.\nWe therefore propose a radically new approach to teaching agents what they\nknow: \\emph{collaborative self-play}. We construct multi-agent collaborations\nin which the group is rewarded for collectively arriving at correct answers.\nThe desired meta-knowledge emerges from the incentives built into the structure\nof the interaction. We focus on small societies of agents that have access to\nheterogeneous tools (corpus-specific retrieval), and therefore must collaborate\nto maximize their success while minimizing their effort. Experiments show that\ngroup-level rewards for multi-agent communities can induce policies that\n\\emph{transfer} to improve tool use and selective prediction in settings where\nindividual agents are deployed in isolation.\n","authors":["Jacob Eisenstein","Reza Aghajani","Adam Fisch","Dheeru Dua","Fantine Huot","Mirella Lapata","Vicky Zayats","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2503.14481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04382v2","updated":"2025-03-18T17:51:56Z","published":"2025-02-05T18:58:02Z","title":"Sparse Autoencoders for Hypothesis Generation","summary":"  We describe HypotheSAEs, a general method to hypothesize interpretable\nrelationships between text data (e.g., headlines) and a target variable (e.g.,\nclicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text\nembeddings to produce interpretable features describing the data distribution,\n(2) select features that predict the target variable, and (3) generate a\nnatural language interpretation of each feature (e.g., \"mentions being\nsurprised or shocked\") using an LLM. Each interpretation serves as a hypothesis\nabout what predicts the target variable. Compared to baselines, our method\nbetter identifies reference hypotheses on synthetic datasets (at least +0.06 in\nF1) and produces more predictive hypotheses on real datasets (~twice as many\nsignificant findings), despite requiring 1-2 orders of magnitude less compute\nthan recent LLM-based methods. HypotheSAEs also produces novel discoveries on\ntwo well-studied tasks: explaining partisan differences in Congressional\nspeeches and identifying drivers of engagement with online headlines.\n","authors":["Rajiv Movva","Kenny Peng","Nikhil Garg","Jon Kleinberg","Emma Pierson"],"pdf_url":"https://arxiv.org/pdf/2502.04382v2.pdf","comment":"First two authors contributed equally; working paper. Code is\n  available at https://github.com/rmovva/HypotheSAEs"},{"id":"http://arxiv.org/abs/2503.14477v1","updated":"2025-03-18T17:51:04Z","published":"2025-03-18T17:51:04Z","title":"Calibrating Verbal Uncertainty as a Linear Feature to Reduce\n  Hallucinations","summary":"  LLMs often adopt an assertive language style also when making false claims.\nSuch ``overconfident hallucinations'' mislead users and erode trust. Achieving\nthe ability to express in language the actual degree of uncertainty around a\nclaim is therefore of great importance. We find that ``verbal uncertainty'' is\ngoverned by a single linear feature in the representation space of LLMs, and\nshow that this has only moderate correlation with the actual ``semantic\nuncertainty'' of the model. We apply this insight and show that (1) the\nmismatch between semantic and verbal uncertainty is a better predictor of\nhallucinations than semantic uncertainty alone and (2) we can intervene on\nverbal uncertainty at inference time and reduce hallucinations on short-form\nanswers, achieving an average relative reduction of 32%.\n","authors":["Ziwei Ji","Lei Yu","Yeskendir Koishekenov","Yejin Bang","Anthony Hartshorn","Alan Schelten","Cheng Zhang","Pascale Fung","Nicola Cancedda"],"pdf_url":"https://arxiv.org/pdf/2503.14477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13803v2","updated":"2025-03-18T17:49:06Z","published":"2024-06-19T20:07:37Z","title":"LLMs as Models for Analogical Reasoning","summary":"  Analogical reasoning-the capacity to identify and map structural\nrelationships between different domains-is fundamental to human cognition and\nlearning. Recent studies have shown that large language models (LLMs) can\nsometimes match humans in analogical reasoning tasks, opening the possibility\nthat analogical reasoning might emerge from domain general processes. However,\nit is still debated whether these emergent capacities are largely superficial\nand limited to simple relations seen during training or whether they rather\nencompass the flexible representational and mapping capabilities which are the\nfocus of leading cognitive models of analogy. In this study, we introduce novel\nanalogical reasoning tasks that require participants to map between\nsemantically contentful words and sequences of letters and other abstract\ncharacters. This task necessitates the ability to flexibly re-represent rich\nsemantic information-an ability which is known to be central to human analogy\nbut which is thus far not well-captured by existing cognitive theories and\nmodels. We assess the performance of both human participants and LLMs on tasks\nfocusing on reasoning from semantic structure and semantic content, introducing\nvariations that test the robustness of their analogical inferences. Advanced\nLLMs match human performance across several conditions, though humans and LLMs\nrespond differently to certain task variations and semantic distractors. Our\nresults thus provide new evidence that LLMs might offer a how-possibly\nexplanation of human analogical reasoning in contexts that are not yet well\nmodeled by existing theories, but that even today's best models are unlikely to\nyield how-actually explanations.\n","authors":["Sam Musker","Alex Duchnowski","Raphaël Millière","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2406.13803v2.pdf","comment":"The title has been changed from Semantic Structure-Mapping in LLM and\n  Human Analogical Reasoning to LLMs as Models for Analogical Reasoning to\n  improve clarity and accuracy"},{"id":"http://arxiv.org/abs/2503.14476v1","updated":"2025-03-18T17:49:06Z","published":"2025-03-18T17:49:06Z","title":"DAPO: An Open-Source LLM Reinforcement Learning System at Scale","summary":"  Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.\n","authors":["Qiying Yu","Zheng Zhang","Ruofei Zhu","Yufeng Yuan","Xiaochen Zuo","Yu Yue","Tiantian Fan","Gaohong Liu","Lingjun Liu","Xin Liu","Haibin Lin","Zhiqi Lin","Bole Ma","Guangming Sheng","Yuxuan Tong","Chi Zhang","Mofan Zhang","Wang Zhang","Hang Zhu","Jinhua Zhu","Jiaze Chen","Jiangjie Chen","Chengyi Wang","Hongli Yu","Weinan Dai","Yuxuan Song","Xiangpeng Wei","Hao Zhou","Jingjing Liu","Wei-Ying Ma","Ya-Qin Zhang","Lin Yan","Mu Qiao","Yonghui Wu","Mingxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14476v1.pdf","comment":"Project Page: https://dapo-sia.github.io/"},{"id":"http://arxiv.org/abs/2503.14456v1","updated":"2025-03-18T17:31:05Z","published":"2025-03-18T17:31:05Z","title":"RWKV-7 \"Goose\" with Expressive Dynamic State Evolution","summary":"  We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.\n","authors":["Bo Peng","Ruichong Zhang","Daniel Goldstein","Eric Alcaide","Haowen Hou","Janna Lu","William Merrill","Guangyu Song","Kaifeng Tan","Saiteja Utpala","Nathan Wilce","Johan S. Wind","Tianyi Wu","Daniel Wuttke","Christian Zhou-Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.14456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15378v5","updated":"2025-03-18T17:14:43Z","published":"2024-01-27T10:50:11Z","title":"A RAG-based Question Answering System Proposal for Understanding Islam:\n  MufassirQAS LLM","summary":"  Challenges exist in learning and understanding religions, such as the\ncomplexity and depth of religious doctrines and teachings. Chatbots as\nquestion-answering systems can help in solving these challenges. LLM chatbots\nuse NLP techniques to establish connections between topics and accurately\nrespond to complex questions. These capabilities make it perfect for\nenlightenment on religion as a question-answering chatbot. However, LLMs also\ntend to generate false information, known as hallucination. Also, the chatbots'\nresponses can include content that insults personal religious beliefs,\ninterfaith conflicts, and controversial or sensitive topics. It must avoid such\ncases without promoting hate speech or offending certain groups of people or\ntheir beliefs. This study uses a vector database-based Retrieval Augmented\nGeneration (RAG) approach to enhance the accuracy and transparency of LLMs. Our\nquestion-answering system is called \"MufassirQAS\". We created a database\nconsisting of several open-access books that include Turkish context. These\nbooks contain Turkish translations and interpretations of Islam. This database\nis utilized to answer religion-related questions and ensure our answers are\ntrustworthy. The relevant part of the dataset, which LLM also uses, is\npresented along with the answer. We have put careful effort into creating\nsystem prompts that give instructions to prevent harmful, offensive, or\ndisrespectful responses to respect people's values and provide reliable\nresults. The system answers and shares additional information, such as the page\nnumber from the respective book and the articles referenced for obtaining the\ninformation. MufassirQAS and ChatGPT are also tested with sensitive questions.\nWe got better performance with our system. Study and enhancements are still in\nprogress. Results and future works are given.\n","authors":["Ahmet Yusuf Alan","Enis Karaarslan","Ömer Aydin"],"pdf_url":"https://arxiv.org/pdf/2401.15378v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08640v2","updated":"2025-03-18T17:13:42Z","published":"2025-03-11T17:30:58Z","title":"Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention","summary":"  Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.\n","authors":["Emily Xiao","Chin-Jou Li","Yilin Zhang","Graham Neubig","Amanda Bertsch"],"pdf_url":"https://arxiv.org/pdf/2503.08640v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.14434v1","updated":"2025-03-18T17:11:24Z","published":"2025-03-18T17:11:24Z","title":"LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers","summary":"  Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.\n","authors":["Nikhil Abhyankar","Parshin Shojaee","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2503.14434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14433v1","updated":"2025-03-18T17:11:09Z","published":"2025-03-18T17:11:09Z","title":"Splintering Nonconcatenative Languages for Better Tokenization","summary":"  Common subword tokenization algorithms like BPE and UnigramLM assume that\ntext can be split into meaningful units by concatenative measures alone. This\nis not true for languages such as Hebrew and Arabic, where morphology is\nencoded in root-template patterns, or Malay and Georgian, where split affixes\nare common. We present SPLINTER, a pre-processing step which rearranges text\ninto a linear form that better represents such nonconcatenative morphologies,\nenabling meaningful contiguous segments to be found by the tokenizer. We\ndemonstrate SPLINTER's merit using both intrinsic measures evaluating token\nvocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using\nBERT-architecture models trained for Hebrew.\n","authors":["Bar Gazit","Shaltiel Shmidman","Avi Shmidman","Yuval Pinter"],"pdf_url":"https://arxiv.org/pdf/2503.14433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14432v1","updated":"2025-03-18T17:09:57Z","published":"2025-03-18T17:09:57Z","title":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play","summary":"  Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.\n","authors":["Wei Fang","Yang Zhang","Kaizhi Qian","James Glass","Yada Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10460v2","updated":"2025-03-18T17:07:21Z","published":"2025-03-13T15:29:22Z","title":"Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond","summary":"  This paper introduces Light-R1, an open-source suite for training long\nreasoning models using reproducible and cost-effective methodology. Given the\nproprietary nature of data used in the DeepSeek-R1 series, we develop an\nalternative approach leveraging exclusively public data and models. Our\ncurriculum training progressively increases data difficulty, combined with\nmulti-staged post-training. Our Light-R1-32B model, trained from\nQwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math\nreasoning.\n  Experimental results show that this curriculum approach becomes more\neffective when distinct, diverse datasets are available for different training\nstages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on\nproprietary data) with 3,000 challenging examples from our curriculum dataset\nyielded state-of-the-art 7B and 14B models, while the 32B model,\nLight-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying GRPO on long reasoning models.\nOur final Light-R1-14B-DS achieves SOTA performance among 14B models in math,\nwith AIME24 \\& 25 scores of 74.0 and 60.2 respectively, surpassing many 32B\nmodels and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training,\nLight-R1-14B-DS demonstrates strong cross-domain generalization.\n  Light-R1 represents a significant advancement in making sophisticated\nreasoning models more accessible and implementable in real-world applications.\nOur models, training data and code have been made available at\nhttps://github.com/Qihoo360/Light-R1.\n","authors":["Liang Wen","Yunke Cai","Fenrui Xiao","Xin He","Qi An","Zhenyu Duan","Yimin Du","Junchen Liu","Lifu Tang","Xiaowei Lv","Haosheng Zou","Yongchao Deng","Shousheng Jia","Xiangzheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.10460v2.pdf","comment":"v2: better writing & format for later submission; all release at\n  https://github.com/Qihoo360/Light-R1"},{"id":"http://arxiv.org/abs/2406.18841v4","updated":"2025-03-18T16:57:17Z","published":"2024-05-14T15:03:05Z","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions","summary":"  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n","authors":["Junfeng Jiao","Saleh Afroogh","Yiming Xu","Connor Phillips"],"pdf_url":"https://arxiv.org/pdf/2406.18841v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14421v1","updated":"2025-03-18T16:55:07Z","published":"2025-03-18T16:55:07Z","title":"ExDDV: A New Dataset for Explainable Deepfake Detection in Video","summary":"  The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.\n","authors":["Vlad Hondru","Eduard Hogea","Darian Onchis","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2503.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14411v1","updated":"2025-03-18T16:50:10Z","published":"2025-03-18T16:50:10Z","title":"Unifying Text Semantics and Graph Structures for Temporal\n  Text-attributed Graphs with Large Language Models","summary":"  Temporal graph neural networks (TGNNs) have shown remarkable performance in\ntemporal graph modeling. However, real-world temporal graphs often possess rich\ntextual information, giving rise to temporal text-attributed graphs (TTAGs).\nSuch combination of dynamic text semantics and evolving graph structures\nintroduces heightened complexity. Existing TGNNs embed texts statically and\nrely heavily on encoding mechanisms that biasedly prioritize structural\ninformation, overlooking the temporal evolution of text semantics and the\nessential interplay between semantics and structures for synergistic\nreinforcement. To tackle these issues, we present \\textbf{{Cross}}, a novel\nframework that seamlessly extends existing TGNNs for TTAG modeling. The key\nidea is to employ the advanced large language models (LLMs) to extract the\ndynamic semantics in text space and then generate expressive representations\nunifying both semantics and structures. Specifically, we propose a Temporal\nSemantics Extractor in the {Cross} framework, which empowers the LLM to offer\nthe temporal semantic understanding of node's evolving contexts of textual\nneighborhoods, facilitating semantic dynamics. Subsequently, we introduce the\nSemantic-structural Co-encoder, which collaborates with the above Extractor for\nsynthesizing illuminating representations by jointly considering both semantic\nand structural information while encouraging their mutual reinforcement.\nExtensive experimental results on four public datasets and one practical\nindustrial dataset demonstrate {Cross}'s significant effectiveness and\nrobustness.\n","authors":["Siwei Zhang","Yun Xiong","Yateng Tang","Xi Chen","Zian Jia","Zehao Gu","Jiarong Xu","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14411v1.pdf","comment":"Submit to ICML2025"},{"id":"http://arxiv.org/abs/2503.14408v1","updated":"2025-03-18T16:49:56Z","published":"2025-03-18T16:49:56Z","title":"Large Language Models for Virtual Human Gesture Selection","summary":"  Co-speech gestures convey a wide variety of meanings and play an important\nrole in face-to-face human interactions. These gestures significantly influence\nthe addressee's engagement, recall, comprehension, and attitudes toward the\nspeaker. Similarly, they impact interactions between humans and embodied\nvirtual agents. The process of selecting and animating meaningful gestures has\nthus become a key focus in the design of these agents. However, automating this\ngesture selection process poses a significant challenge. Prior gesture\ngeneration techniques have varied from fully automated, data-driven methods,\nwhich often struggle to produce contextually meaningful gestures, to more\nmanual approaches that require crafting specific gesture expertise and are\ntime-consuming and lack generalizability. In this paper, we leverage the\nsemantic capabilities of Large Language Models to develop a gesture selection\napproach that suggests meaningful, appropriate co-speech gestures. We first\ndescribe how information on gestures is encoded into GPT-4. Then, we conduct a\nstudy to evaluate alternative prompting approaches for their ability to select\nmeaningful, contextually relevant gestures and to align them appropriately with\nthe co-speech utterance. Finally, we detail and demonstrate how this approach\nhas been implemented within a virtual agent system, automating the selection\nand subsequent animation of the selected gestures for enhanced human-agent\ninteractions.\n","authors":["Parisa Ghanad Torshizi","Laura B. Hensel","Ari Shapiro","Stacy C. Marsella"],"pdf_url":"https://arxiv.org/pdf/2503.14408v1.pdf","comment":"9 pages, 6 figures, Accepted at the AAMAS 2025 conference"},{"id":"http://arxiv.org/abs/2501.02063v3","updated":"2025-03-18T16:45:54Z","published":"2025-01-03T19:16:36Z","title":"AGGA: A Dataset of Academic Guidelines for Generative AI and Large\n  Language Models","summary":"  This study introduces AGGA, a dataset comprising 80 academic guidelines for\nthe use of Generative AIs (GAIs) and Large Language Models (LLMs) in academic\nsettings, meticulously collected from official university websites. The dataset\ncontains 188,674 words and serves as a valuable resource for natural language\nprocessing tasks commonly applied in requirements engineering, such as model\nsynthesis, abstraction identification, and document structure assessment.\nAdditionally, AGGA can be further annotated to function as a benchmark for\nvarious tasks, including ambiguity detection, requirements categorization, and\nthe identification of equivalent requirements. Our methodologically rigorous\napproach ensured a thorough examination, with a selection of universities that\nrepresent a diverse range of global institutions, including top-ranked\nuniversities across six continents. The dataset captures perspectives from a\nvariety of academic fields, including humanities, technology, and both public\nand private institutions, offering a broad spectrum of insights into the\nintegration of GAIs and LLMs in academia.\n","authors":["Junfeng Jiao","Saleh Afroogh","Kevin Chen","David Atkinson","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2501.02063v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.18842,\n  arXiv:2501.00959"},{"id":"http://arxiv.org/abs/2406.04746v2","updated":"2025-03-18T16:45:09Z","published":"2024-06-07T08:46:19Z","title":"PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance\n  Prediction","summary":"  Text-to-image generation has recently emerged as a viable alternative to\ntext-to-image retrieval, driven by the visually impressive results of\ngenerative diffusion models. Although query performance prediction is an active\nresearch topic in information retrieval, to the best of our knowledge, there is\nno prior study that analyzes the difficulty of queries (referred to as prompts)\nin text-to-image generation, based on human judgments. To this end, we\nintroduce the first dataset of prompts which are manually annotated in terms of\nimage generation performance. Additionally, we extend these evaluations to\ntext-to-image retrieval by collecting manual annotations that represent\nretrieval performance. We thus establish the first joint benchmark for prompt\nand query performance prediction (PQPP) across both tasks, comprising over 10K\nqueries. Our benchmark enables (i) the comparative assessment of prompt/query\ndifficulty in both image generation and image retrieval, and (ii) the\nevaluation of prompt/query performance predictors addressing both generation\nand retrieval. We evaluate several pre- and post-generation/retrieval\nperformance predictors, thus providing competitive baselines for future\nresearch. Our benchmark and code are publicly available at\nhttps://github.com/Eduard6421/PQPP.\n","authors":["Eduard Poesina","Adriana Valentina Costache","Adrian-Gabriel Chifu","Josiane Mothe","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2406.04746v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2406.18842v3","updated":"2025-03-18T16:42:30Z","published":"2024-05-26T15:28:24Z","title":"The global landscape of academic guidelines for generative AI and Large\n  Language Models","summary":"  The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.\n","authors":["Junfeng Jiao","Saleh Afroogh","Kevin Chen","David Atkinson","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.18842v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09292v3","updated":"2025-03-18T16:42:17Z","published":"2025-01-16T04:56:33Z","title":"To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation","summary":"  Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.09292v3.pdf","comment":"1st workshop of \"Quantify Uncertainty and Hallucination in Foundation\n  Models: The Next Frontier in Reliable AI\" at ICLR 2025"},{"id":"http://arxiv.org/abs/2407.16970v3","updated":"2025-03-18T16:34:14Z","published":"2024-07-24T03:32:05Z","title":"Towards Aligning Language Models with Textual Feedback","summary":"  We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.\n","authors":["Saüc Abadal Lloret","Shehzaad Dhuliawala","Keerthiram Murugesan","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.16970v3.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2503.14392v1","updated":"2025-03-18T16:27:01Z","published":"2025-03-18T16:27:01Z","title":"From \"Hallucination\" to \"Suture\": Insights from Language Philosophy to\n  Enhance Large Language Models","summary":"  This paper explores hallucination phenomena in large language models (LLMs)\nthrough the lens of language philosophy and psychoanalysis. By incorporating\nLacan's concepts of the \"chain of signifiers\" and \"suture points,\" we propose\nthe Anchor-RAG framework as a novel approach to mitigate hallucinations. In\ncontrast to the predominant reliance on trial-and-error experiments, constant\nadjustments of mathematical formulas, or resource-intensive methods that\nemphasize quantity over quality, our approach returns to the fundamental\nprinciples of linguistics to analyze the root causes of hallucinations in LLMs.\nDrawing from robust theoretical foundations, we derive algorithms and models\nthat are not only effective in reducing hallucinations but also enhance LLM\nperformance and improve output quality. This paper seeks to establish a\ncomprehensive theoretical framework for understanding hallucinations in LLMs\nand aims to challenge the prevalent \"guess-and-test\" approach and rat race\nmentality in the field. We aspire to pave the way for a new era of\ninterpretable LLMs, offering deeper insights into the inner workings of\nlanguage-based AI systems.\n","authors":["Qiantong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14392v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2503.14391v1","updated":"2025-03-18T16:26:29Z","published":"2025-03-18T16:26:29Z","title":"How much do LLMs learn from negative examples?","summary":"  Large language models (LLMs) undergo a three-phase training process:\nunsupervised pre-training, supervised fine-tuning (SFT), and learning from\nhuman feedback (RLHF/DPO). Notably, it is during the final phase that these\nmodels are exposed to negative examples -- incorrect, rejected, or suboptimal\nresponses to queries. This paper delves into the role of negative examples in\nthe training of LLMs, using a likelihood-ratio (Likra) model on multiple-choice\nquestion answering benchmarks to precisely manage the influence and the volume\nof negative examples. Our findings reveal three key insights: (1) During a\ncritical phase in training, Likra with negative examples demonstrates a\nsignificantly larger improvement per training example compared to SFT using\nonly positive examples. This leads to a sharp jump in the learning curve for\nLikra unlike the smooth and gradual improvement of SFT; (2) negative examples\nthat are plausible but incorrect (near-misses) exert a greater influence; and\n(3) while training with positive examples fails to significantly decrease the\nlikelihood of plausible but incorrect answers, training with negative examples\nmore accurately identifies them. These results indicate a potentially\nsignificant role for negative examples in improving accuracy and reducing\nhallucinations for LLMs.\n","authors":["Shadi Hamdan","Deniz Yuret"],"pdf_url":"https://arxiv.org/pdf/2503.14391v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.06663v5","updated":"2025-03-18T16:21:04Z","published":"2024-08-13T06:28:43Z","title":"Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models","summary":"  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n","authors":["Kaiser Sun","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2408.06663v5.pdf","comment":"Rep4NLP Camera Ready"},{"id":"http://arxiv.org/abs/2503.14382v1","updated":"2025-03-18T16:15:55Z","published":"2025-03-18T16:15:55Z","title":"Good/Evil Reputation Judgment of Celebrities by LLMs via Retrieval\n  Augmented Generation","summary":"  The purpose of this paper is to examine whether large language models (LLMs)\ncan understand what is good and evil with respect to judging good/evil\nreputation of celebrities. Specifically, we first apply a large language model\n(namely, ChatGPT) to the task of collecting sentences that mention the target\ncelebrity from articles about celebrities on Web pages. Next, the collected\nsentences are categorized based on their contents by ChatGPT, where ChatGPT\nassigns a category name to each of those categories. Those assigned category\nnames are referred to as \"aspects\" of each celebrity. Then, by applying the\nframework of retrieval augmented generation (RAG), we show that the large\nlanguage model is quite effective in the task of judging good/evil reputation\nof aspects and descriptions of each celebrity. Finally, also in terms of\nproving the advantages of the proposed method over existing services\nincorporating RAG functions, we show that the proposed method of judging\ngood/evil of aspects/descriptions of each celebrity significantly outperform an\nexisting service incorporating RAG functions.\n","authors":["Rikuto Tsuchida","Hibiki Yokoyama","Takehito Utsuro"],"pdf_url":"https://arxiv.org/pdf/2503.14382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07917v2","updated":"2025-03-18T15:49:08Z","published":"2024-11-12T16:49:51Z","title":"CryptoLLM: Unleashing the Power of Prompted LLMs for SmartQnA and\n  Classification of Crypto Posts","summary":"  The rapid growth of social media has resulted in an large volume of\nuser-generated content, particularly in niche domains such as cryptocurrency.\nThis task focuses on developing robust classification models to accurately\ncategorize cryptocurrency-related social media posts into predefined classes,\nincluding but not limited to objective, positive, negative, etc. Additionally,\nthe task requires participants to identify the most relevant answers from a set\nof posts in response to specific questions. By leveraging advanced LLMs, this\nresearch aims to enhance the understanding and filtering of cryptocurrency\ndiscourse, thereby facilitating more informed decision-making in this volatile\nsector. We have used a prompt-based technique to solve the classification task\nfor reddit posts and twitter posts. Also, we have used 64-shot technique along\nwith prompts on GPT-4-Turbo model to determine whether a answer is relevant to\na question or not.\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2411.07917v2.pdf","comment":"Updated and Final Version"},{"id":"http://arxiv.org/abs/2411.06946v2","updated":"2025-03-18T15:36:28Z","published":"2024-11-11T12:54:22Z","title":"Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models","summary":"  Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.\n","authors":["Aniket Deroy","Subhankar Maity"],"pdf_url":"https://arxiv.org/pdf/2411.06946v2.pdf","comment":"Updated and Final Version"},{"id":"http://arxiv.org/abs/2503.14350v1","updated":"2025-03-18T15:31:12Z","published":"2025-03-18T15:31:12Z","title":"VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded\n  Generation","summary":"  Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.\n","authors":["Shoubin Yu","Difan Liu","Ziqiao Ma","Yicong Hong","Yang Zhou","Hao Tan","Joyce Chai","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2503.14350v1.pdf","comment":"First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/"},{"id":"http://arxiv.org/abs/2503.14345v1","updated":"2025-03-18T15:25:08Z","published":"2025-03-18T15:25:08Z","title":"MoonCast: High-Quality Zero-Shot Podcast Generation","summary":"  Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.\n","authors":["Zeqian Ju","Dongchao Yang","Jianwei Yu","Kai Shen","Yichong Leng","Zhengtao Wang","Xu Tan","Xinyu Zhou","Tao Qin","Xiangyang Li"],"pdf_url":"https://arxiv.org/pdf/2503.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14341v1","updated":"2025-03-18T15:21:27Z","published":"2025-03-18T15:21:27Z","title":"Spatio-Temporal Graph Neural Networks for Infant Language Acquisition\n  Prediction","summary":"  Predicting the words that a child is going to learn next can be useful for\nboosting language acquisition, and such predictions have been shown to be\npossible with both neural network techniques (looking at changes in the\nvocabulary state over time) and graph model (looking at data pertaining to the\nrelationships between words). However, these models do not fully capture the\ncomplexity of the language learning process of an infant when used in\nisolation. In this paper, we examine how a model of language acquisition for\ninfants and young children can be constructed and adapted for use in a\nSpatio-Temporal Graph Convolutional Network (STGCN), taking into account the\ndifferent types of linguistic relationships that occur during child language\nlearning. We introduce a novel approach for predicting child vocabulary\nacquisition, and evaluate the efficacy of such a model with respect to the\ndifferent types of linguistic relationships that occur during language\nacquisition, resulting in insightful observations on model calibration and norm\nselection. An evaluation of this model found that the mean accuracy of models\nfor predicting new words when using sensorimotor relationships (0.733) and\nsemantic relationships (0.729) were found to be superior to that observed with\na 2-layer Feed-forward neural network. Furthermore, the high recall for some\nrelationships suggested that some relationships (e.g. visual) were superior in\nidentifying a larger proportion of relevant words that a child should\nsubsequently learn than others (such as auditory).\n","authors":["Andrew Roxburgh","Floriana Grasso","Terry R. Payne"],"pdf_url":"https://arxiv.org/pdf/2503.14341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16657v3","updated":"2025-03-18T15:19:15Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v3.pdf","comment":"Project website: https://zunwang1.github.io/DreamRunner"},{"id":"http://arxiv.org/abs/2503.14337v1","updated":"2025-03-18T15:14:14Z","published":"2025-03-18T15:14:14Z","title":"PENCIL: Long Thoughts with Short Memory","summary":"  While recent works (e.g. o1, DeepSeek R1) have demonstrated great promise of\nusing long Chain-of-Thought (CoT) to improve reasoning capabilities of language\nmodels, scaling it up during test-time is challenging due to inefficient memory\nusage -- intermediate computations accumulate indefinitely in context even no\nlonger needed for future thoughts. We propose PENCIL, which incorporates a\nreduction mechanism into the autoregressive generation process, allowing the\nmodel to recursively clean up intermediate thoughts based on patterns learned\nfrom training. With this reduction mechanism, PENCIL significantly reduces the\nmaximal context length required during generation, and thus can generate longer\nthoughts with limited memory, solving larger-scale problems given more thinking\ntime. For example, we demonstrate PENCIL achieves 97\\% accuracy on the\nchallenging Einstein's puzzle -- a task even large models like GPT-4 struggle\nwith -- using only a small 25M-parameter transformer with 2048 context length.\nTheoretically, we prove PENCIL can perform universal space-efficient\ncomputation by simulating Turing machines with optimal time and space\ncomplexity, and thus can solve arbitrary computational tasks that would\notherwise be intractable given context window constraints.\n","authors":["Chenxiao Yang","Nathan Srebro","David McAllester","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.14337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15977v5","updated":"2025-03-18T15:03:25Z","published":"2024-09-24T11:18:09Z","title":"TCSinger: Zero-Shot Singing Voice Synthesis with Style Transfer and\n  Multi-Level Style Control","summary":"  Zero-shot singing voice synthesis (SVS) with style transfer and style control\naims to generate high-quality singing voices with unseen timbres and styles\n(including singing method, emotion, rhythm, technique, and pronunciation) from\naudio and text prompts. However, the multifaceted nature of singing styles\nposes a significant challenge for effective modeling, transfer, and control.\nFurthermore, current SVS models often fail to generate singing voices rich in\nstylistic nuances for unseen singers. To address these challenges, we introduce\nTCSinger, the first zero-shot SVS model for style transfer across cross-lingual\nspeech and singing styles, along with multi-level style control. Specifically,\nTCSinger proposes three primary modules: 1) the clustering style encoder\nemploys a clustering vector quantization model to stably condense style\ninformation into a compact latent space; 2) the Style and Duration Language\nModel (S\\&D-LM) concurrently predicts style information and phoneme duration,\nwhich benefits both; 3) the style adaptive decoder uses a novel mel-style\nadaptive normalization method to generate singing voices with enhanced details.\nExperimental results show that TCSinger outperforms all baseline models in\nsynthesis quality, singer similarity, and style controllability across various\ntasks, including zero-shot style transfer, multi-level style control,\ncross-lingual style transfer, and speech-to-singing style transfer. Singing\nvoice samples can be accessed at https://aaronz345.github.io/TCSingerDemo/.\n","authors":["Yu Zhang","Ziyue Jiang","Ruiqi Li","Changhao Pan","Jinzheng He","Rongjie Huang","Chuxin Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.15977v5.pdf","comment":"Accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2503.14324v1","updated":"2025-03-18T14:56:46Z","published":"2025-03-18T14:56:46Z","title":"DualToken: Towards Unifying Visual Understanding and Generation with\n  Dual Visual Vocabularies","summary":"  The differing representation spaces required for visual understanding and\ngeneration pose a challenge in unifying them within the autoregressive paradigm\nof large language models. A vision tokenizer trained for reconstruction excels\nat capturing low-level perceptual details, making it well-suited for visual\ngeneration but lacking high-level semantic representations for understanding\ntasks. Conversely, a vision encoder trained via contrastive learning aligns\nwell with language but struggles to decode back into the pixel space for\ngeneration tasks. To bridge this gap, we propose DualToken, a method that\nunifies representations for both understanding and generation within a single\ntokenizer. However, directly integrating reconstruction and semantic objectives\nin a single tokenizer creates conflicts, leading to degraded performance in\nboth reconstruction quality and semantic performance. Instead of forcing a\nsingle codebook to handle both semantic and perceptual information, DualToken\ndisentangles them by introducing separate codebooks for high and low-level\nfeatures, effectively transforming their inherent conflict into a synergistic\nrelationship. As a result, DualToken achieves state-of-the-art performance in\nboth reconstruction and semantic tasks while demonstrating remarkable\neffectiveness in downstream MLLM understanding and generation tasks. Notably,\nwe also show that DualToken, as a unified tokenizer, surpasses the naive\ncombination of two distinct types vision encoders, providing superior\nperformance within a unified MLLM.\n","authors":["Wei Song","Yuran Wang","Zijia Song","Yadong Li","Haoze Sun","Weipeng Chen","Zenan Zhou","Jianhua Xu","Jiaqi Wang","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13281v2","updated":"2025-03-18T14:56:41Z","published":"2025-03-17T15:31:55Z","title":"LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation","summary":"  Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines.\n","authors":["Xiaodi Li","Shaika Chowdhury","Chung Il Wi","Maria Vassilaki","Ken Liu","Terence T Sio","Owen Garrick","Young J Juhn","James R Cerhan","Cui Tao","Nansu Zong"],"pdf_url":"https://arxiv.org/pdf/2503.13281v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2503.13262v2","updated":"2025-03-18T14:41:59Z","published":"2025-03-17T15:16:59Z","title":"TablePilot: Recommending Human-Preferred Tabular Data Analysis with\n  Large Language Models","summary":"  Tabular data analysis is crucial in many scenarios, yet efficiently\nidentifying the most relevant data analysis queries and results for a new table\nremains a significant challenge. The complexity of tabular data, diverse\nanalytical operations, and the demand for high-quality analysis make the\nprocess tedious. To address these challenges, we aim to recommend\nquery-code-result triplets tailored for new tables in tabular data analysis\nworkflows. In this paper, we present TablePilot, a pioneering tabular data\nanalysis framework leveraging large language models to autonomously generate\ncomprehensive and superior analytical results without relying on user profiles\nor prior interactions. The framework incorporates key designs in analysis\npreparation and analysis optimization to enhance accuracy. Additionally, we\npropose Rec-Align, a novel method to further improve recommendation quality and\nbetter align with human preferences. Experiments on DART, a dataset\nspecifically designed for comprehensive tabular data analysis recommendation,\ndemonstrate the effectiveness of our framework. Based on GPT-4o, the tuned\nTablePilot achieves 77.0% top-5 recommendation recall. Human evaluations\nfurther highlight its effectiveness in optimizing tabular data analysis\nworkflows.\n","authors":["Deyin Yi","Yihao Liu","Lang Cao","Mengyu Zhou","Haoyu Dong","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11516v2","updated":"2025-03-18T14:40:54Z","published":"2024-10-15T11:37:21Z","title":"TopoLM: brain-like spatio-functional organization in a topographic\n  language model","summary":"  Neurons in the brain are spatially organized such that neighbors on tissue\noften exhibit similar response profiles. In the human language system,\nexperimental studies have observed clusters for syntactic and semantic\ncategories, but the mechanisms underlying this functional organization remain\nunclear. Here, building on work from the vision literature, we develop TopoLM,\na transformer language model with an explicit two-dimensional spatial\nrepresentation of model units. By combining a next-token prediction objective\nwith a spatial smoothness loss, representations in this model assemble into\nclusters that correspond to semantically interpretable groupings of text and\nclosely match the functional organization in the brain's language system.\nTopoLM successfully predicts the emergence of the spatio-functional\norganization of a cortical language system as well as the organization of\nfunctional clusters selective for fine-grained linguistic features empirically\nobserved in human cortex. Our results suggest that the functional organization\nof the human language system is driven by a unified spatial objective, and\nprovide a functionally and spatially aligned model of language processing in\nthe brain.\n","authors":["Neil Rathi","Johannes Mehrer","Badr AlKhamissi","Taha Binhuraib","Nicholas M. Blauch","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2410.11516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13788v2","updated":"2025-03-18T14:17:47Z","published":"2024-10-17T17:29:04Z","title":"Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying\n  Questions","summary":"  Large language models (LLMs) must often respond to highly ambiguous user\nrequests. In such cases, the LLM's best response may be to ask a clarifying\nquestion to elicit more information. Existing LLMs often respond by\npresupposing a single interpretation of such ambiguous requests, frustrating\nusers who intended a different interpretation. We speculate this is caused by\ncurrent preference data labeling practice, where LLM responses are evaluated\nonly on their prior contexts. To address this, we assign preference labels by\nsimulating their expected outcomes in future turns. This allows LLMs to learn\nto ask clarifying questions when it can generate responses that are tailored to\neach user interpretation in future turns. On open-domain QA datasets with\nmultiple annotations, we evaluate systems based on their ability to ask\nclarifying questions to recover each user's interpretation and expected answer.\nWe compare systems trained using our proposed preference labeling methods\nagainst standard methods, which assign preferences based on only prior context.\nOur method achieves a 5% improvement in F1 measured against the answer set from\ndifferent interpretations of each query, showing the value of modeling future\nconversation turns. We further demonstrate that our method can be used to train\nmodels to judiciously determine when to ask clarifying questions, directly\nanswering the question when clarification is unnecessary. In our experiments,\nwe find that our method achieves a 3% improvement in accuracy of such judgments\nover existing methods.\n","authors":["Michael J. Q. Zhang","W. Bradley Knox","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2410.13788v2.pdf","comment":"Presented at ICLR 2025"},{"id":"http://arxiv.org/abs/2407.20899v3","updated":"2025-03-18T14:13:40Z","published":"2024-07-30T15:17:15Z","title":"Faithful and Plausible Natural Language Explanations for Image\n  Classification: A Pipeline Approach","summary":"  Existing explanation methods for image classification struggle to provide\nfaithful and plausible explanations. This paper addresses this issue by\nproposing a post-hoc natural language explanation method that can be applied to\nany CNN-based classifier without altering its training process or affecting\npredictive performance. By analysing influential neurons and the corresponding\nactivation maps, the method generates a faithful description of the\nclassifier's decision process in the form of a structured meaning\nrepresentation, which is then converted into text by a language model. Through\nthis pipeline approach, the generated explanations are grounded in the neural\nnetwork architecture, providing accurate insight into the classification\nprocess while remaining accessible to non-experts. Experimental results show\nthat the NLEs constructed by our method are significantly more plausible and\nfaithful. In particular, user interventions in the neural network structure\n(masking of neurons) are three times more effective than the baselines.\n","authors":["Adam Wojciechowski","Mateusz Lango","Ondrej Dusek"],"pdf_url":"https://arxiv.org/pdf/2407.20899v3.pdf","comment":"Findings of EMNLP 2024"},{"id":"http://arxiv.org/abs/2503.14269v1","updated":"2025-03-18T14:02:59Z","published":"2025-03-18T14:02:59Z","title":"DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by\n  Adaptive Tree Traversal","summary":"  Large Language Models (LLMs) have revolutionized various domains, including\nnatural language processing, data analysis, and software development, by\nenabling automation. In software engineering, LLM-powered coding agents have\ngarnered significant attention due to their potential to automate complex\ndevelopment tasks, assist in debugging, and enhance productivity. However,\nexisting approaches often struggle with sub-optimal decision-making, requiring\neither extensive manual intervention or inefficient compute scaling strategies.\nTo improve coding agent performance, we present Dynamic Action Re-Sampling\n(DARS), a novel inference time compute scaling approach for coding agents, that\nis faster and more effective at recovering from sub-optimal decisions compared\nto baselines. While traditional agents either follow linear trajectories or\nrely on random sampling for scaling compute, our approach DARS works by\nbranching out a trajectory at certain key decision points by taking an\nalternative action given the history of the trajectory and execution feedback\nof the previous attempt from that point. We evaluate our approach on SWE-Bench\nLite benchmark, demonstrating that this scaling strategy achieves a pass@k\nscore of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of\n47%, outperforming state-of-the-art (SOTA) open-source frameworks.\n","authors":["Vaibhav Aggarwal","Ojasv Kamal","Abhinav Japesh","Zhijing Jin","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2503.14269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11356v2","updated":"2025-03-18T14:01:54Z","published":"2023-12-18T17:12:35Z","title":"The Problem of Coherence in Natural Language Explanations of\n  Recommendations","summary":"  Providing natural language explanations for recommendations is particularly\nuseful from the perspective of a non-expert user. Although several methods for\nproviding such explanations have recently been proposed, we argue that an\nimportant aspect of explanation quality has been overlooked in their\nexperimental evaluation. Specifically, the coherence between generated text and\npredicted rating, which is a necessary condition for an explanation to be\nuseful, is not properly captured by currently used evaluation measures. In this\npaper, we highlight the issue of explanation and prediction coherence by 1)\npresenting results from a manual verification of explanations generated by one\nof the state-of-the-art approaches 2) proposing a method of automatic coherence\nevaluation 3) introducing a new transformer-based method that aims to produce\nmore coherent explanations than the state-of-the-art approaches 4) performing\nan experimental evaluation which demonstrates that this method significantly\nimproves the explanation coherence without affecting the other aspects of\nrecommendation performance.\n","authors":["Jakub Raczyński","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2312.11356v2.pdf","comment":"ECAI 2023"},{"id":"http://arxiv.org/abs/2503.14258v1","updated":"2025-03-18T13:48:18Z","published":"2025-03-18T13:48:18Z","title":"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System","summary":"  This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.\n","authors":["Weihang Su","Baoqing Yue","Qingyao Ai","Yiran Hu","Jiaqi Li","Changyue Wang","Kaiyuan Zhang","Yueyue Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21113v2","updated":"2025-03-18T13:30:27Z","published":"2024-10-28T15:13:53Z","title":"Zero-Shot Action Recognition in Surveillance Videos","summary":"  The growing demand for surveillance in public spaces presents significant\nchallenges due to the shortage of human resources. Current AI-based video\nsurveillance systems heavily rely on core computer vision models that require\nextensive finetuning, which is particularly difficult in surveillance settings\ndue to limited datasets and difficult setting (viewpoint, low quality, etc.).\nIn this work, we propose leveraging Large Vision-Language Models (LVLMs), known\nfor their strong zero and few-shot generalization, to tackle video\nunderstanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a\nstate-of-the-art LVLM, and an improved token-level sampling method,\nSelf-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset\nshow that VideoLLaMA2 represents a significant leap in zero-shot performance,\nwith 20% boost over the baseline. Self-ReS additionally increases zero-shot\naction recognition performance to 44.6%. These results highlight the potential\nof LVLMs, paired with improved sampling techniques, for advancing surveillance\nvideo analysis in diverse scenarios.\n","authors":["Joao Pereira","Vasco Lopes","David Semedo","Joao Neves"],"pdf_url":"https://arxiv.org/pdf/2410.21113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12537v5","updated":"2025-03-18T13:13:18Z","published":"2024-11-19T14:35:38Z","title":"Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues","summary":"  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers for long\nsequences. However, both Transformers and LRNNs struggle to perform\nstate-tracking, which may impair performance in tasks such as code evaluation.\nIn one forward pass, current architectures are unable to solve even parity, the\nsimplest state-tracking task, which non-linear RNNs can handle effectively.\nRecently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like\nMamba to solve parity stems from restricting the value range of their diagonal\nstate-transition matrices to $[0, 1]$ and that incorporating negative values\ncan resolve this issue. We extend this result to non-diagonal LRNNs such as\nDeltaNet. We prove that finite precision LRNNs with state-transition matrices\nhaving only positive eigenvalues cannot solve parity, while non-triangular\nmatrices are needed to count modulo $3$. Notably, we also prove that LRNNs can\nlearn any regular language when their state-transition matrices are products of\nidentity minus vector outer product matrices, each with eigenvalues in the\nrange $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of\nMamba and DeltaNet to include negative values not only enables them to solve\nparity but consistently improves their performance on state-tracking tasks. We\nalso show that state-tracking enabled LRNNs can be pretrained stably and\nefficiently at scale (1.3B parameters), achieving competitive performance on\nlanguage modeling and showing promise on code and math tasks.\n","authors":["Riccardo Grazzi","Julien Siems","Arber Zela","Jörg K. H. Franke","Frank Hutter","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2411.12537v5.pdf","comment":"V2: Correction to Theorem 1 and 2 and to point 3 of Proposition 1.\n  V3: ICLR Camera Ready, V4: ICLR Camera Ready, added figures to theory\n  section, updated modular arithmetic with brackets results because previous\n  results did not contain multiplication"},{"id":"http://arxiv.org/abs/2503.14227v1","updated":"2025-03-18T13:04:55Z","published":"2025-03-18T13:04:55Z","title":"Benchmarking Failures in Tool-Augmented Language Models","summary":"  The integration of tools has extended the capabilities of language models\n(LMs) beyond vanilla text generation to versatile scenarios. However,\ntool-augmented language models (TaLMs) often assume 'perfect' information\naccess and tool availability, which may not hold in the real world. To\nsystematically study TaLMs' imperfections, we introduce the FAIL-TALMS\nbenchmark, featuring two major failures: under-specified user queries and\nnon-available tools. FAIL-TALMS contains 1,749 examples using 906 tools across\n21 categories, including single- and multi-tool usage. We evaluate\ntop-performing proprietary and open-source models, and find all current models\nexcept for Claude struggle to recognize missing tools or information. Further,\nto study possible mitigation of the failures, we enable real-time human\ninteraction, named the Ask-and-Help (AAH) method, to provide missing\ninformation or replace non-functional tools. While AAH can help models solve\ntasks more correctly when queries are under-specified, it brings minimal\nbenefit when complex tools are broken.\n","authors":["Eduardo Treviño","Hugo Contant","James Ngai","Graham Neubig","Zora Zhiruo Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07604v2","updated":"2025-03-18T12:08:17Z","published":"2025-03-10T17:58:31Z","title":"Implicit Reasoning in Transformers is Reasoning through Shortcuts","summary":"  Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.\n","authors":["Tianhe Lin","Jian Xie","Siyu Yuan","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2503.07604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14189v1","updated":"2025-03-18T12:02:38Z","published":"2025-03-18T12:02:38Z","title":"Towards Harmless Multimodal Assistants with Blind Preference\n  Optimization","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in multimodal understanding, reasoning, and interaction. Given the\nextensive applications of MLLMs, the associated safety issues have become\nincreasingly critical. Due to the effectiveness of preference optimization in\naligning MLLMs with human preferences, there is an urgent need for\nsafety-related preference data for MLLMs. To address this, we construct the\nMMSafe-PO preference dataset towards harmless multimodal assistants, featuring\nmultimodal instructions, the conversational format, and ranked paired responses\nfrom human feedback. We also identify two insightful observations: modality\nco-defense and modality cheating, which illustrate that MLLMs possess a certain\nlevel of inherent defense while still presenting unique safety challenges.\nBased on these observations, we propose the Blind Preference Optimization (BPO)\napproach. Comprehensive experiments on three benchmarks show that BPO\neffectively enhances the safety capabilities of MLLMs. Notably, BPO\nsignificantly improves the safety rate of the base MLLM by 45.0%, outperforming\nthe DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly\nreduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on\nMM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and\nrobustness of both the dataset and the approach. We release code and data at\nhttps://lu-yang666.github.io/MMsafe-PO-Web/.\n","authors":["Yongqi Li","Lu Yang","Jian Wang","Runyang You","Wenjie Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.14189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14185v1","updated":"2025-03-18T11:59:27Z","published":"2025-03-18T11:59:27Z","title":"AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End\n  Speech-to-Text Translation","summary":"  In end-to-end speech translation, acoustic representations learned by the\nencoder are usually fixed and static, from the perspective of the decoder,\nwhich is not desirable for dealing with the cross-modal and cross-lingual\nchallenge in speech translation. In this paper, we show the benefits of varying\nacoustic states according to decoder hidden states and propose an adaptive\nspeech-to-text translation model that is able to dynamically adapt acoustic\nstates in the decoder. We concatenate the acoustic state and target word\nembedding sequence and feed the concatenated sequence into subsequent blocks in\nthe decoder. In order to model the deep interaction between acoustic states and\ntarget hidden states, a speech-text mixed attention sublayer is introduced to\nreplace the conventional cross-attention network. Experiment results on two\nwidely-used datasets show that the proposed method significantly outperforms\nstate-of-the-art neural speech translation models.\n","authors":["Wuwei Huang","Dexin Wang","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.14185v1.pdf","comment":"ACL 2021 Findings"},{"id":"http://arxiv.org/abs/2406.11139v4","updated":"2025-03-18T11:58:48Z","published":"2024-06-17T01:54:27Z","title":"Breaking Boundaries: Investigating the Effects of Model Editing on\n  Cross-linguistic Performance","summary":"  The integration of pretrained language models (PLMs) like BERT and GPT has\nrevolutionized NLP, particularly for English, but it has also created\nlinguistic imbalances. This paper strategically identifies the need for\nlinguistic equity by examining several knowledge editing techniques in\nmultilingual contexts. We evaluate the performance of models such as Mistral,\nTowerInstruct, OpenHathi, Tamil-Llama, and Kan-Llama across languages including\nEnglish, German, French, Italian, Spanish, Hindi, Tamil, and Kannada. Our\nresearch identifies significant discrepancies in normal and merged models\nconcerning cross-lingual consistency. We employ strategies like 'each language\nfor itself' (ELFI) and 'each language for others' (ELFO) to stress-test these\nmodels. Our findings demonstrate the potential for LLMs to overcome linguistic\nbarriers, laying the groundwork for future research in achieving linguistic\ninclusivity in AI technologies.\n","authors":["Somnath Banerjee","Avik Halder","Rajarshi Mandal","Sayan Layek","Ian Soboroff","Rima Hazra","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2406.11139v4.pdf","comment":"Accepted at NAACL 2025 (Industry track)"},{"id":"http://arxiv.org/abs/2501.16635v2","updated":"2025-03-18T11:50:37Z","published":"2025-01-28T02:16:18Z","title":"Why Do We Laugh? Annotation and Taxonomy Generation for Laughable\n  Contexts in Spontaneous Text Conversation","summary":"  Laughter serves as a multifaceted communicative signal in human interaction,\nyet its identification within dialogue presents a significant challenge for\nconversational AI systems. This study addresses this challenge by annotating\nlaughable contexts in Japanese spontaneous text conversation data and\ndeveloping a taxonomy to classify the underlying reasons for such contexts.\nInitially, multiple annotators manually labeled laughable contexts using a\nbinary decision (laughable or non-laughable). Subsequently, an LLM was used to\ngenerate explanations for the binary annotations of laughable contexts, which\nwere then categorized into a taxonomy comprising ten categories, including\n\"Empathy and Affinity\" and \"Humor and Surprise,\" highlighting the diverse range\nof laughter-inducing scenarios. The study also evaluated GPT-4o's performance\nin recognizing the majority labels of laughable contexts, achieving an F1 score\nof 43.14%. These findings contribute to the advancement of conversational AI by\nestablishing a foundation for more nuanced recognition and generation of\nlaughter, ultimately fostering more natural and engaging human-AI interactions.\n","authors":["Koji Inoue","Mikey Elmers","Divesh Lala","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2501.16635v2.pdf","comment":"This paper has been accepted for presentation at International\n  Workshop on Spoken Dialogue Systems Technology 2025 (IWSDS 2025) and\n  represents the author's version of the work"},{"id":"http://arxiv.org/abs/2503.14173v1","updated":"2025-03-18T11:44:19Z","published":"2025-03-18T11:44:19Z","title":"NERCat: Fine-Tuning for Enhanced Named Entity Recognition in Catalan","summary":"  Named Entity Recognition (NER) is a critical component of Natural Language\nProcessing (NLP) for extracting structured information from unstructured text.\nHowever, for low-resource languages like Catalan, the performance of NER\nsystems often suffers due to the lack of high-quality annotated datasets. This\npaper introduces NERCat, a fine-tuned version of the GLiNER[1] model, designed\nto improve NER performance specifically for Catalan text. We used a dataset of\nmanually annotated Catalan television transcriptions to train and fine-tune the\nmodel, focusing on domains such as politics, sports, and culture. The\nevaluation results show significant improvements in precision, recall, and\nF1-score, particularly for underrepresented named entity categories such as\nLaw, Product, and Facility. This study demonstrates the effectiveness of\ndomain-specific fine-tuning in low-resource languages and highlights the\npotential for enhancing Catalan NLP applications through manual annotation and\nhigh-quality datasets.\n","authors":["Guillem Cadevall Ferreres","Marc Serrano Sanz","Marc Bardeli Gámez","Pol Gerdt Basullas","Francesc Tarres Ruiz","Raul Quijada Ferrero"],"pdf_url":"https://arxiv.org/pdf/2503.14173v1.pdf","comment":"7 pages, 1 table"},{"id":"http://arxiv.org/abs/2502.05092v2","updated":"2025-03-18T11:43:52Z","published":"2025-02-07T17:11:23Z","title":"Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs","summary":"  Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.\n","authors":["Rohit Saxena","Aryo Pradipta Gema","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2502.05092v2.pdf","comment":"Accepted at the ICLR 2025 Workshop on Reasoning and Planning for\n  Large Language Models"},{"id":"http://arxiv.org/abs/2503.14167v1","updated":"2025-03-18T11:37:25Z","published":"2025-03-18T11:37:25Z","title":"Synthetic Clarification and Correction Dialogues about Data-Centric\n  Tasks -- A Teacher-Student Approach","summary":"  Real dialogues with AI assistants for solving data-centric tasks often follow\ndynamic, unpredictable paths due to imperfect information provided by the user\nor in the data, which must be caught and handled. Developing datasets which\ncapture such user-AI interactions is difficult and time-consuming. In this\nwork, we develop a novel framework for synthetically generating controlled,\nmulti-turn conversations between a user and AI assistant for the task of\ntable-based question answering, which can be generated from an existing dataset\nwith fully specified table QA examples for any target domain. Each conversation\naims to solve a table-based reasoning question through collaborative effort,\nmodeling one of two real-world scenarios: (1) an AI-initiated clarification, or\n(2) a user-initiated correction. Critically, we employ a strong teacher LLM to\nverify the correctness of our synthetic conversations, ensuring high quality.\nWe demonstrate synthetic datasets generated from TAT-QA and WikiTableQuestions\nas benchmarks of frontier LLMs. We find that even larger models struggle to\neffectively issuing clarification questions and accurately integrate user\nfeedback for corrections.\n","authors":["Christian Poelitz","Nick McKenna"],"pdf_url":"https://arxiv.org/pdf/2503.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07920v2","updated":"2025-03-18T11:34:03Z","published":"2025-03-10T23:54:52Z","title":"Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia","summary":"  Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Joel Ruben Antony Moniz","Tack Hwa Wong","Mohammad Rifqi Farhansyah","Thant Thiri Maung","Frederikus Hudi","David Anugraha","Muhammad Ravi Shulthan Habibi","Muhammad Reza Qorib","Amit Agarwal","Joseph Marvin Imperial","Hitesh Laxmichand Patel","Vicky Feliren","Bahrul Ilmi Nasution","Manuel Antonio Rufino","Genta Indra Winata","Rian Adam Rajagede","Carlos Rafael Catalan","Mohamed Fazli Imam","Priyaranjan Pattnayak","Salsabila Zahirah Pranida","Kevin Pratama","Yeshil Bangera","Adisai Na-Thalang","Patricia Nicole Monderin","Yueqi Song","Christian Simon","Lynnette Hui Xian Ng","Richardy Lobo' Sapan","Taki Hasan Rafi","Bin Wang"," Supryadi","Kanyakorn Veerakanjana","Piyalitt Ittichaiwong","Matthew Theodore Roque","Karissa Vincentio","Takdanai Kreangphet","Phakphum Artkaew","Kadek Hendrawan Palgunadi","Yanzhi Yu","Rochana Prih Hastuti","William Nixon","Mithil Bangera","Adrian Xuan Wei Lim","Aye Hninn Khine","Hanif Muhammad Zhafran","Teddy Ferdinan","Audra Aurora Izzani","Ayushman Singh"," Evan","Jauza Akbar Krito","Michael Anugraha","Fenal Ashokbhai Ilasariya","Haochen Li","John Amadeo Daniswara","Filbert Aurelian Tjiaranata","Eryawan Presma Yulianrifat","Can Udomcharoenchaikit","Fadil Risdian Ansori","Mahardika Krisna Ihsani","Giang Nguyen","Anab Maulana Barik","Dan John Velasco","Rifo Ahmad Genadi","Saptarshi Saha","Chengwei Wei","Isaiah Flores","Kenneth Ko Han Chen","Anjela Gail Santos","Wan Shen Lim","Kaung Si Phyo","Tim Santos","Meisyarah Dwiastuti","Jiayun Luo","Jan Christian Blaise Cruz","Ming Shan Hee","Ikhlasul Akmal Hanif","M. Alif Al Hakim","Muhammad Rizky Sya'ban","Kun Kerdthaisong","Lester James V. Miranda","Fajri Koto","Tirana Noor Fatyanosa","Alham Fikri Aji","Jostin Jerico Rosal","Jun Kevin","Robert Wijaya","Onno P. Kampman","Ruochen Zhang","Börje F. Karlsson","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2503.07920v2.pdf","comment":"[SEA-VL Dataset]\n  https://huggingface.co/collections/SEACrowd/sea-vl-multicultural-vl-dataset-for-southeast-asia-67cf223d0c341d4ba2b236e7\n  [Appendix J]\n  https://github.com/SEACrowd/seacrowd.github.io/blob/master/docs/SEA_VL_Appendix_J.pdf"},{"id":"http://arxiv.org/abs/2503.14153v1","updated":"2025-03-18T11:21:53Z","published":"2025-03-18T11:21:53Z","title":"Speculative Decoding for Verilog: Speed and Quality, All in One","summary":"  The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.\n","authors":["Changran Xu","Yi Liu","Yunhao Zhou","Shan Huang","Ningyi Xu","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2503.14153v1.pdf","comment":"Accepted by the 62nd Design Automation Conference (DAC 2025)"},{"id":"http://arxiv.org/abs/2503.14136v1","updated":"2025-03-18T10:58:10Z","published":"2025-03-18T10:58:10Z","title":"CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On\n  Minimal Hardware","summary":"  Large Language models have demonstrated excellent domain-specific\nquestion-answering capabilities when finetuned with a particular dataset of\nthat specific domain. However, fine-tuning the models requires a significant\namount of training time and a considerable amount of hardware. In this work, we\npropose CARE (Customer Assistance and Response Engine), a lightweight model\nmade by fine-tuning Phi3.5-mini on very minimal hardware and data, designed to\nhandle queries primarily across three domains: telecommunications support,\nmedical support, and banking support. For telecommunications and banking, the\nchatbot addresses issues and problems faced by customers regularly in the\nabove-mentioned domains. In the medical domain, CARE provides preliminary\nsupport by offering basic diagnoses and medical suggestions that a user might\ntake before consulting a healthcare professional. Since CARE is built on\nPhi3.5-mini, it can be used even on mobile devices, increasing its usability.\nOur research also shows that CARE performs relatively well on various medical\nbenchmarks, indicating that it can be used to make basic medical suggestions.\n","authors":["Ankit Dutta","Nabarup Ghosh","Ankush Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2503.14136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14125v1","updated":"2025-03-18T10:37:50Z","published":"2025-03-18T10:37:50Z","title":"Frac-Connections: Fractional Extension of Hyper-Connections","summary":"  Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.\n","authors":["Defa Zhu","Hongzhi Huang","Jundong Zhou","Zihao Huang","Yutao Zeng","Banggu Wu","Qiyang Min","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.14125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19606v3","updated":"2025-03-18T10:12:54Z","published":"2024-09-29T07:57:07Z","title":"Hyper-Connections","summary":"  We present hyper-connections, a simple yet effective method that can serve as\nan alternative to residual connections. This approach specifically addresses\ncommon drawbacks observed in residual connection variants, such as the seesaw\neffect between gradient vanishing and representation collapse. Theoretically,\nhyper-connections allow the network to adjust the strength of connections\nbetween features at different depths and dynamically rearrange layers. We\nconduct experiments focusing on the pre-training of large language models,\nincluding dense and sparse models, where hyper-connections show significant\nperformance improvements over residual connections. Additional experiments\nconducted on vision tasks also demonstrate similar improvements. We anticipate\nthat this method will be broadly applicable and beneficial across a wide range\nof AI problems.\n","authors":["Defa Zhu","Hongzhi Huang","Zihao Huang","Yutao Zeng","Yunyao Mao","Banggu Wu","Qiyang Min","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.19606v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14090v1","updated":"2025-03-18T10:09:10Z","published":"2025-03-18T10:09:10Z","title":"Wiki-Quantities and Wiki-Measurements: Datasets of Quantities and their\n  Measurement Context from Wikipedia","summary":"  To cope with the large number of publications, more and more researchers are\nautomatically extracting data of interest using natural language processing\nmethods based on supervised learning. Much data, especially in the natural and\nengineering sciences, is quantitative, but there is a lack of datasets for\nidentifying quantities and their context in text. To address this issue, we\npresent two large datasets based on Wikipedia and Wikidata: Wiki-Quantities is\na dataset consisting of over 1.2 million annotated quantities in the\nEnglish-language Wikipedia. Wiki-Measurements is a dataset of 38,738 annotated\nquantities in the English-language Wikipedia along with their respective\nmeasured entity, property, and optional qualifiers. Manual validation of 100\nsamples each of Wiki-Quantities and Wiki-Measurements found 100% and 84-94%\ncorrect, respectively. The datasets can be used in pipeline approaches to\nmeasurement extraction, where quantities are first identified and then their\nmeasurement context. To allow reproduction of this work using newer or\ndifferent versions of Wikipedia and Wikidata, we publish the code used to\ncreate the datasets along with the data.\n","authors":["Jan Göpfert","Patrick Kuckertz","Jann M. Weinand","Detlef Stolten"],"pdf_url":"https://arxiv.org/pdf/2503.14090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21630v2","updated":"2025-03-18T10:08:33Z","published":"2024-07-31T14:24:01Z","title":"TAROT: Task-Oriented Authorship Obfuscation Using Policy Optimization\n  Methods","summary":"  Authorship obfuscation aims to disguise the identity of an author within a\ntext by altering the writing style, vocabulary, syntax, and other linguistic\nfeatures associated with the text author. This alteration needs to balance\nprivacy and utility. While strong obfuscation techniques can effectively hide\nthe author's identity, they often degrade the quality and usefulness of the\ntext for its intended purpose. Conversely, maintaining high utility tends to\nprovide insufficient privacy, making it easier for an adversary to de-anonymize\nthe author. Thus, achieving an optimal trade-off between these two conflicting\nobjectives is crucial. In this paper, we propose TAROT: Task-Oriented\nAuthorship Obfuscation Using Policy Optimization, a new unsupervised authorship\nobfuscation method whose goal is to optimize the privacy-utility trade-off by\nregenerating the entire text considering its downstream utility. Our approach\nleverages policy optimization as a fine-tuning paradigm over small language\nmodels in order to rewrite texts by preserving author identity and downstream\ntask utility. We show that our approach largely reduces the accuracy of\nattackers while preserving utility. We make our code and models publicly\navailable.\n","authors":["Gabriel Loiseau","Damien Sileo","Damien Riquet","Maxime Meyer","Marc Tommasi"],"pdf_url":"https://arxiv.org/pdf/2407.21630v2.pdf","comment":"Accepted to the NAACL PrivateNLP 2025 Workshop"},{"id":"http://arxiv.org/abs/2503.14075v1","updated":"2025-03-18T09:52:45Z","published":"2025-03-18T09:52:45Z","title":"Growing a Twig to Accelerate Large Vision-Language Models","summary":"  Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods. Code will be made publicly\navailable.\n","authors":["Zhenwei Shao","Mingyang Wang","Zhou Yu","Wenwen Pan","Yan Yang","Tao Wei","Hongyuan Zhang","Ning Mao","Wei Chen","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14075v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.17178v3","updated":"2025-03-18T09:09:29Z","published":"2025-01-24T17:01:14Z","title":"Tuning LLM Judge Design Decisions for 1/1000 of the Cost","summary":"  Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.\n","authors":["David Salinas","Omar Swelam","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2501.17178v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18326v2","updated":"2025-03-18T08:57:39Z","published":"2024-06-26T13:12:40Z","title":"PaCoST: Paired Confidence Significance Testing for Benchmark\n  Contamination Detection in Large Language Models","summary":"  Large language models (LLMs) are known to be trained on vast amounts of data,\nwhich may unintentionally or intentionally include data from commonly used\nbenchmarks. This inclusion can lead to cheatingly high scores on model\nleaderboards, yet result in disappointing performance in real-world\napplications. To address this benchmark contamination problem, we first propose\na set of requirements that practical contamination detection methods should\nfollow. Following these proposed requirements, we introduce PaCoST, a Paired\nConfidence Significance Testing to effectively detect benchmark contamination\nin LLMs. Our method constructs a counterpart for each piece of data with the\nsame distribution, and performs statistical analysis of the corresponding\nconfidence to test whether the model is significantly more confident under the\noriginal benchmark. We validate the effectiveness of PaCoST and apply it on\npopular open-source models and benchmarks. We find that almost all models and\nbenchmarks we tested are suspected contaminated more or less. We finally call\nfor new LLM evaluation methods.\n","authors":["Huixuan Zhang","Yun Lin","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2406.18326v2.pdf","comment":"Accepted by EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2503.14023v1","updated":"2025-03-18T08:34:03Z","published":"2025-03-18T08:34:03Z","title":"Synthetic Data Generation Using Large Language Models: Advances in Text\n  and Code","summary":"  Large language models (LLMs) have unlocked new possibilities for generating\nsynthetic training data in both natural language and code. By producing\nartificial but task-relevant examples, these models can significantly augment\nor even replace real-world datasets, especially when labeled data is scarce or\nsensitive. This paper surveys recent advances in using LLMs to create synthetic\ntext and code, emphasizing prompt-based generation, retrieval-augmented\npipelines, and iterative self-refinement. We show how these methods enrich\nlow-resource tasks such as classification and question answering, as well as\ncode-centric applications such as instruction tuning, code translation, and bug\nrepair, by enabling automated verification of functional correctness. Alongside\npotential benefits like cost-effectiveness, broad coverage, and controllable\ndiversity, we address challenges such as factual inaccuracies in generated\ntext, lack of stylistic realism, and the risk of bias amplification. Proposed\nmitigations include filtering and weighting outputs and reinforcement learning\nwith execution feedback for code. We conclude with open research directions\nlike automated prompt engineering, cross-modal data synthesis, and robust\nevaluation frameworks, highlighting the importance of LLM-generated synthetic\ndata in advancing AI while emphasizing ethical and quality safeguards.\n","authors":["Mihai Nadas","Laura Diosan","Andreea Tomescu"],"pdf_url":"https://arxiv.org/pdf/2503.14023v1.pdf","comment":"21 pages, 3 tables, 64 references, preprint"},{"id":"http://arxiv.org/abs/2503.05592v2","updated":"2025-03-18T08:32:24Z","published":"2025-03-07T17:14:44Z","title":"R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning","summary":"  Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.\n","authors":["Huatong Song","Jinhao Jiang","Yingqian Min","Jie Chen","Zhipeng Chen","Wayne Xin Zhao","Lei Fang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2503.05592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05797v2","updated":"2025-03-18T08:03:45Z","published":"2024-06-09T14:20:55Z","title":"3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text\n  Modeling","summary":"  The integration of molecular and natural language representations has emerged\nas a focal point in molecular science, with recent advancements in Language\nModels (LMs) demonstrating significant potential for comprehensive modeling of\nboth domains. However, existing approaches face notable limitations,\nparticularly in their neglect of three-dimensional (3D) information, which is\ncrucial for understanding molecular structures and functions. While some\nefforts have been made to incorporate 3D molecular information into LMs using\nexternal structure encoding modules, significant difficulties remain, such as\ninsufficient interaction across modalities in pre-training and challenges in\nmodality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a\nunified framework designed to model molecule in both sequence and 3D structure\nspaces. The key innovation of our approach lies in mapping fine-grained 3D\nsubstructure representations into a specialized 3D token vocabulary. This\nmethodology facilitates the seamless integration of sequence and structure\nrepresentations in a tokenized format, enabling 3D-MolT5 to encode molecular\nsequences, molecular structures, and text sequences within a unified\narchitecture. Leveraging this tokenized input strategy, we build a foundation\nmodel that unifies the sequence and structure data formats. We then conduct\njoint pre-training with multi-task objectives to enhance the model's\ncomprehension of these diverse modalities within a shared representation space.\nThus, our approach significantly improves cross-modal interaction and\nalignment, addressing key challenges in previous work. Further instruction\ntuning demonstrated that our 3D-MolT5 has strong generalization ability and\nsurpasses existing methods with superior performance in multiple downstream\ntasks. Our code is available at https://github.com/QizhiPei/3D-MolT5.\n","authors":["Qizhi Pei","Rui Yan","Kaiyuan Gao","Jinhua Zhu","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.05797v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2412.02172v2","updated":"2025-03-18T08:02:22Z","published":"2024-12-03T05:04:49Z","title":"VISCO: Benchmarking Fine-Grained Critique and Correction Towards\n  Self-Improvement in Visual Reasoning","summary":"  The ability of large vision-language models (LVLMs) to critique and correct\ntheir reasoning is an essential building block towards their self-improvement.\nHowever, a systematic analysis of such capabilities in LVLMs is still lacking.\nWe propose VISCO, the first benchmark to extensively analyze the fine-grained\ncritique and correction capabilities of LVLMs. Compared to existing work that\nuses a single scalar value to critique the entire reasoning [4], VISCO features\ndense and fine-grained critique, requiring LVLMs to evaluate the correctness of\neach step in the chain-of-thought and provide natural language explanations to\nsupport their judgments. Extensive evaluation of 24 LVLMs demonstrates that\nhuman-written critiques significantly enhance the performance after correction,\nshowcasing the potential of the self-improvement strategy. However, the\nmodel-generated critiques are less helpful and sometimes detrimental to the\nperformance, suggesting that critique is the crucial bottleneck. We identified\nthree common patterns in critique failures: failure to critique visual\nperception, reluctance to \"say no\", and exaggerated assumption of error\npropagation. To address these issues, we propose an effective LookBack strategy\nthat revisits the image to verify each piece of information in the initial\nreasoning. LookBack significantly improves critique and correction performance\nby up to 13.5%.\n","authors":["Xueqing Wu","Yuheng Ding","Bingxuan Li","Pan Lu","Da Yin","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2412.02172v2.pdf","comment":"CVPR 2025. https://visco-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2503.13992v1","updated":"2025-03-18T07:52:04Z","published":"2025-03-18T07:52:04Z","title":"The KoLMogorov Test: Compression by Code Generation","summary":"  Compression is at the heart of intelligence. A theoretically optimal way to\ncompress any sequence of data is to find the shortest program that outputs that\nsequence and then halts. However, such 'Kolmogorov compression' is\nuncomputable, and code generating LLMs struggle to approximate this theoretical\nideal, as it requires reasoning, planning and search capabilities beyond those\nof current models. In this work, we introduce the KoLMogorov-Test (KT), a\ncompression-as-intelligence test for code generating LLMs. In KT a model is\npresented with a sequence of data at inference time, and asked to generate the\nshortest program that produces the sequence. We identify several benefits of KT\nfor both evaluation and training: an essentially infinite number of problem\ninstances of varying difficulty is readily available, strong baselines already\nexist, the evaluation metric (compression) cannot be gamed, and pretraining\ndata contamination is highly unlikely. To evaluate current models, we use\naudio, text, and DNA data, as well as sequences produced by random synthetic\nprograms. Current flagship models perform poorly - both GPT4-o and\nLlama-3.1-405B struggle on our natural and synthetic sequences. On our\nsynthetic distribution, we are able to train code generation models with lower\ncompression rates than previous approaches. Moreover, we show that gains on\nsynthetic data generalize poorly to real data, suggesting that new innovations\nare necessary for additional gains on KT.\n","authors":["Ori Yoran","Kunhao Zheng","Fabian Gloeckle","Jonas Gehring","Gabriel Synnaeve","Taco Cohen"],"pdf_url":"https://arxiv.org/pdf/2503.13992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13988v1","updated":"2025-03-18T07:44:49Z","published":"2025-03-18T07:44:49Z","title":"Empowering Smaller Models: Tuning LLaMA and Gemma with Chain-of-Thought\n  for Ukrainian Exam Tasks","summary":"  Leading large language models have demonstrated impressive capabilities in\nreasoning-intensive tasks, such as standardized educational testing. However,\nthey often require extensive training in low-resource settings with\ninaccessible infrastructure. Small or compact models, though more efficient,\nfrequently lack sufficient support for underrepresented languages, leaving a\nperformance gap in critical domains. This work explores the potential of\nparameter-efficient fine-tuning of compact open-weight language models to\nhandle reasoning-intensive tasks in the underrepresented Ukrainian language,\nbuilding on the findings of the ZNO-Eval benchmark. Parameter-efficient\nfine-tuning of LLaMA 3.1 (8 billion parameters), LLaMA 3.2 (3 billion\nparameters), and Gemma 2 (9 billion parameters) models on chain-of-thought\nsolutions resulted in a modest test score improvement of up to 17.4% on complex\nmatching tasks and 1.6% overall compared to tuning on answer letters alone,\noffering enhanced interpretability and robustness. In addition, the proposed\ntuning method with joint task topic and step-by-step solution generation\noutperforms standard chain-of-thought tuning in matching tasks and provides a\n5.4% gain over the best LLaMA 3.2 model due to guiding the model to recall and\napply domain-relevant information. Contrasting obtained results with zero-shot\nevaluations of leading open-weight and proprietary models such as Qwen,\nDeepSeek R1, OpenAI o1 and o3, Gemini, and Claude, highlight that fine-tuning\nLLaMA and Gemma models with 2,032 step-by-step solutions and 20 to 50 million\ntrainable parameters on a single A100 GPU lets them outperform GPT-4o mini,\nMistral Large, and larger open-weight models. This research also evaluates how\nmerging the quantized adapter with the base model influences the generation\nquality. Source code and tuned models are available at\nhttps://github.com/NLPForUA/ZNO.\n","authors":["Mykyta Syromiatnikov","Victoria Ruvinskaya","Nataliia Komleva"],"pdf_url":"https://arxiv.org/pdf/2503.13988v1.pdf","comment":"12 pages, 6 tables, 2 figures"},{"id":"http://arxiv.org/abs/2503.13975v1","updated":"2025-03-18T07:24:05Z","published":"2025-03-18T07:24:05Z","title":"Navigating Rifts in Human-LLM Grounding: Study and Benchmark","summary":"  Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, early grounding failures predicted later interaction\nbreakdowns. Building on these insights, we introduce RIFTS: a benchmark derived\nfrom publicly available LLM interaction data containing situations where LLMs\nfail to initiate grounding. We note that current frontier models perform poorly\non RIFTS, highlighting the need to reconsider how we train and prompt LLMs for\nhuman interaction. To this end, we develop a preliminary intervention that\nmitigates grounding failures.\n","authors":["Omar Shaikh","Hussein Mozannar","Gagan Bansal","Adam Fourney","Eric Horvitz"],"pdf_url":"https://arxiv.org/pdf/2503.13975v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.09174v2","updated":"2025-03-18T07:13:18Z","published":"2024-08-17T11:40:10Z","title":"TableBench: A Comprehensive and Complex Benchmark for Table Question\n  Answering","summary":"  Recent advancements in Large Language Models (LLMs) have markedly enhanced\nthe interpretation and processing of tabular data, introducing previously\nunimaginable capabilities. Despite these achievements, LLMs still encounter\nsignificant challenges when applied in industrial scenarios, particularly due\nto the increased complexity of reasoning required with real-world tabular data,\nunderscoring a notable disparity between academic benchmarks and practical\napplications. To address this discrepancy, we conduct a detailed investigation\ninto the application of tabular data in industrial scenarios and propose a\ncomprehensive and complex benchmark TableBench, including 18 fields within four\nmajor categories of table question answering (TableQA) capabilities.\nFurthermore, we introduce TableLLM, trained on our meticulously constructed\ntraining set TableInstruct, achieving comparable performance with GPT-3.5.\nMassive experiments conducted on TableBench indicate that both open-source and\nproprietary LLMs still have significant room for improvement to meet real-world\ndemands, where the most advanced model, GPT-4, achieves only a modest score\ncompared to humans.\n","authors":["Xianjie Wu","Jian Yang","Linzheng Chai","Ge Zhang","Jiaheng Liu","Xinrun Du","Di Liang","Daixin Shu","Xianfu Cheng","Tianzhen Sun","Guanglin Niu","Tongliang Li","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2408.09174v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2503.04833v2","updated":"2025-03-18T07:01:13Z","published":"2025-03-05T14:13:35Z","title":"Adversarial Training for Multimodal Large Language Models against\n  Jailbreak Attacks","summary":"  Multimodal large language models (MLLMs) have made remarkable strides in\ncross-modal comprehension and generation tasks. However, they remain vulnerable\nto jailbreak attacks, where crafted perturbations bypass security guardrails\nand elicit harmful outputs. In this paper, we present the first adversarial\ntraining (AT) paradigm tailored to defend against jailbreak attacks during the\nMLLM training phase. Extending traditional AT to this domain poses two critical\nchallenges: efficiently tuning massive parameters and ensuring robustness\nagainst attacks across multiple modalities. To address these challenges, we\nintroduce Projection Layer Against Adversarial Training (ProEAT), an end-to-end\nAT framework. ProEAT incorporates a projector-based adversarial training\narchitecture that efficiently handles large-scale parameters while maintaining\ncomputational feasibility by focusing adversarial training on a lightweight\nprojector layer instead of the entire model; additionally, we design a dynamic\nweight adjustment mechanism that optimizes the loss function's weight\nallocation based on task demands, streamlining the tuning process. To enhance\ndefense performance, we propose a joint optimization strategy across visual and\ntextual modalities, ensuring robust resistance to jailbreak attacks originating\nfrom either modality. Extensive experiments conducted on five major jailbreak\nattack methods across three mainstream MLLMs demonstrate the effectiveness of\nour approach. ProEAT achieves state-of-the-art defense performance,\noutperforming existing baselines by an average margin of +34% across text and\nimage modalities, while incurring only a 1% reduction in clean accuracy.\nFurthermore, evaluations on real-world embodied intelligent systems highlight\nthe practical applicability of our framework, paving the way for the\ndevelopment of more secure and reliable multimodal systems.\n","authors":["Liming Lu","Shuchao Pang","Siyuan Liang","Haotian Zhu","Xiyu Zeng","Aishan Liu","Yunhuai Liu","Yongbin Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.04833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16643v2","updated":"2025-03-18T06:39:36Z","published":"2025-01-28T02:27:55Z","title":"An LLM Benchmark for Addressee Recognition in Multi-modal Multi-party\n  Dialogue","summary":"  Handling multi-party dialogues represents a significant step for advancing\nspoken dialogue systems, necessitating the development of tasks specific to\nmulti-party interactions. To address this challenge, we are constructing a\nmulti-modal multi-party dialogue corpus of triadic (three-participant)\ndiscussions. This paper focuses on the task of addressee recognition,\nidentifying who is being addressed to take the next turn, a critical component\nunique to multi-party dialogue systems. A subset of the corpus was annotated\nwith addressee information, revealing that explicit addressees are indicated in\napproximately 20% of conversational turns. To evaluate the task's complexity,\nwe benchmarked the performance of a large language model (GPT-4o) on addressee\nrecognition. The results showed that GPT-4o achieved an accuracy only\nmarginally above chance, underscoring the challenges of addressee recognition\nin multi-party dialogue. These findings highlight the need for further research\nto enhance the capabilities of large language models in understanding and\nnavigating the intricacies of multi-party conversational dynamics.\n","authors":["Koji Inoue","Divesh Lala","Mikey Elmers","Keiko Ochi","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2501.16643v2.pdf","comment":"This paper has been accepted for presentation at International\n  Workshop on Spoken Dialogue Systems Technology 2025 (IWSDS 2025) and\n  represents the author's version of the work"},{"id":"http://arxiv.org/abs/2503.13923v1","updated":"2025-03-18T05:38:04Z","published":"2025-03-18T05:38:04Z","title":"ConSCompF: Consistency-focused Similarity Comparison Framework for\n  Generative Large Language Models","summary":"  Large language models (LLMs) have been one of the most important discoveries\nin machine learning in recent years. LLM-based artificial intelligence (AI)\nassistants, such as ChatGPT, have consistently attracted the attention from\nresearchers, investors, and the general public, driving the rapid growth of\nthis industry. With the frequent introduction of new LLMs to the market, it\nbecomes increasingly difficult to differentiate between them, creating a demand\nfor new LLM comparison methods.\n  In this research, the Consistency-focused Similarity Comparison Framework\n(ConSCompF) for generative large language models is proposed. It compares texts\ngenerated by two LLMs and produces a similarity score, indicating the overall\ndegree of similarity between their responses. The main advantage of this\nframework is that it can operate on a small number of unlabeled data, such as\nchatbot instruction prompts, and does not require LLM developers to disclose\nany information about their product.\n  To evaluate the efficacy of ConSCompF, two experiments aimed at identifying\nsimilarities between multiple LLMs are conducted. Additionally, these\nexperiments examine the correlation between the similarity scores generated by\nConSCompF and the differences in the outputs produced by other benchmarking\ntechniques, such as ROUGE-L. Finally, a series of few-shot LLM comparison\nexperiments is conducted to evaluate the performance of ConSCompF in a few-shot\nLLM comparison scenario.\n  The proposed framework can be used for calculating similarity matrices of\nmultiple LLMs, which can be effectively visualized using principal component\nanalysis (PCA). The ConSCompF output may provide useful insights into data that\nmight have been used during LLM training and help detect possible investment\nfraud attempts.\n","authors":["Alexey Karev","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2503.13923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12358v2","updated":"2025-03-18T05:22:24Z","published":"2025-03-16T04:53:38Z","title":"IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level\n  Generation","summary":"  Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.\n","authors":["In-Chang Baek","Sung-Hyun Kim","Seo-Young Lee","Dong-Hyeun Kim","Kyung-Joong Kim"],"pdf_url":"https://arxiv.org/pdf/2503.12358v2.pdf","comment":"9 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.12797v2","updated":"2025-03-18T05:06:22Z","published":"2025-03-17T04:06:34Z","title":"DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs\n  for Knowledge-Intensive Visual Grounding","summary":"  Human experts excel at fine-grained visual discrimination by leveraging\ndomain knowledge to refine perceptual features, a capability that remains\nunderdeveloped in current Multimodal Large Language Models (MLLMs). Despite\npossessing vast expert-level knowledge, MLLMs struggle to integrate reasoning\ninto visual perception, often generating direct responses without deeper\nanalysis. To bridge this gap, we introduce knowledge-intensive visual grounding\n(KVG), a novel visual grounding task that requires both fine-grained perception\nand domain-specific knowledge integration. To address the challenges of KVG, we\npropose DeepPerception, an MLLM enhanced with cognitive visual perception\ncapabilities. Our approach consists of (1) an automated data synthesis pipeline\nthat generates high-quality, knowledge-aligned training samples, and (2) a\ntwo-stage training framework combining supervised fine-tuning for cognitive\nreasoning scaffolding and reinforcement learning to optimize\nperception-cognition synergy. To benchmark performance, we introduce KVG-Bench\na comprehensive dataset spanning 10 domains with 1.3K manually curated test\ncases. Experimental results demonstrate that DeepPerception significantly\noutperforms direct fine-tuning, achieving +8.08\\% accuracy improvements on\nKVG-Bench and exhibiting +4.60\\% superior cross-domain generalization over\nbaseline approaches. Our findings highlight the importance of integrating\ncognitive processes into MLLMs for human-like visual perception and open new\ndirections for multimodal reasoning research. The data, codes, and models are\nreleased at https://github.com/thunlp/DeepPerception.\n","authors":["Xinyu Ma","Ziyang Ding","Zhicong Luo","Chi Chen","Zonghao Guo","Derek F. Wong","Xiaoyi Feng","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2503.12797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07521v5","updated":"2025-03-18T04:53:09Z","published":"2024-11-12T03:37:53Z","title":"Fair Summarization: Bridging Quality and Diversity in Extractive\n  Summaries","summary":"  Fairness in multi-document summarization of user-generated content remains a\ncritical challenge in natural language processing (NLP). Existing summarization\nmethods often fail to ensure equitable representation across different social\ngroups, leading to biased outputs. In this paper, we introduce two novel\nmethods for fair extractive summarization: FairExtract, a clustering-based\napproach, and FairGPT, which leverages GPT-3.5-turbo with fairness constraints.\nWe evaluate these methods using Divsumm summarization dataset of White-aligned,\nHispanic, and African-American dialect tweets and compare them against relevant\nbaselines. The results obtained using a comprehensive set of summarization\nquality metrics such as SUPERT, BLANC, SummaQA, BARTScore, and UniEval, as well\nas a fairness metric F, demonstrate that FairExtract and FairGPT achieve\nsuperior fairness while maintaining competitive summarization quality.\nAdditionally, we introduce composite metrics (e.g., SUPERT+F, BLANC+F) that\nintegrate quality and fairness into a single evaluation framework, offering a\nmore nuanced understanding of the trade-offs between these objectives. Our code\nis available online.\n","authors":["Sina Bagheri Nezhad","Sayan Bandyapadhyay","Ameeta Agrawal"],"pdf_url":"https://arxiv.org/pdf/2411.07521v5.pdf","comment":"Accepted at AFLME@NeurIPS 2024 (non-archival) & C3NLP@NAACL 2025\n  (publication)"},{"id":"http://arxiv.org/abs/2503.13413v2","updated":"2025-03-18T04:41:37Z","published":"2025-03-17T17:42:51Z","title":"DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective","summary":"  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.\n","authors":["Dengyun Peng","Yuhang Zhou","Qiguang Chen","Jinhao Liu","Jingjing Chen","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2503.13413v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.13885v1","updated":"2025-03-18T04:31:57Z","published":"2025-03-18T04:31:57Z","title":"COMM:Concentrated Margin Maximization for Robust Document-Level Relation\n  Extraction","summary":"  Document-level relation extraction (DocRE) is the process of identifying and\nextracting relations between entities that span multiple sentences within a\ndocument. Due to its realistic settings, DocRE has garnered increasing research\nattention in recent years. Previous research has mostly focused on developing\nsophisticated encoding models to better capture the intricate patterns between\nentity pairs. While these advancements are undoubtedly crucial, an even more\nfoundational challenge lies in the data itself. The complexity inherent in\nDocRE makes the labeling process prone to errors, compounded by the extreme\nsparsity of positive relation samples, which is driven by both the limited\navailability of positive instances and the broad diversity of positive relation\ntypes. These factors can lead to biased optimization processes, further\ncomplicating the task of accurate relation extraction. Recognizing these\nchallenges, we have developed a robust framework called \\textit{\\textbf{COMM}}\nto better solve DocRE. \\textit{\\textbf{COMM}} operates by initially employing\nan instance-aware reasoning method to dynamically capture pertinent information\nof entity pairs within the document and extract relational features. Following\nthis, \\textit{\\textbf{COMM}} takes into account the distribution of relations\nand the difficulty of samples to dynamically adjust the margins between\nprediction logits and the decision threshold, a process we call Concentrated\nMargin Maximization. In this way, \\textit{\\textbf{COMM}} not only enhances the\nextraction of relevant relational features but also boosts DocRE performance by\naddressing the specific challenges posed by the data. Extensive experiments and\nanalysis demonstrate the versatility and effectiveness of\n\\textit{\\textbf{COMM}}, especially its robustness when trained on low-quality\ndata (achieves \\textgreater 10\\% performance gains).\n","authors":["Zhichao Duan","Tengyu Pan","Zhenyu Li","Xiuxing Li","Jianyong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.13885v1.pdf","comment":"AAAI 2025 poster"},{"id":"http://arxiv.org/abs/2503.11911v2","updated":"2025-03-18T04:01:37Z","published":"2025-03-14T22:50:50Z","title":"LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama","summary":"  As large language models (LLMs) rapidly advance, evaluating their performance\nis critical. LLMs are trained on multilingual data, but their reasoning\nabilities are mainly evaluated using English datasets. Hence, robust evaluation\nframeworks are needed using high-quality non-English datasets, especially\nlow-resource languages (LRLs). This study evaluates eight state-of-the-art\n(SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language\nUnderstanding (MMLU) subset curated with native speakers for linguistic and\ncultural relevance. Giriama is benchmarked for the first time. Our evaluation\nshows that OpenAI's o1 model outperforms others across all languages, scoring\n92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks.\nMistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both\nLatvian and Giriama. Our results underscore the need for localized benchmarks\nand human evaluations in advancing cultural AI contextualization.\n","authors":["Naome A. Etori","Kevin Lu","Randu Karisa","Arturs Kanepajs"],"pdf_url":"https://arxiv.org/pdf/2503.11911v2.pdf","comment":"Accepted at NoDaLiDa/Baltic-HLT 2025.\n  https://hdl.handle.net/10062/107190"},{"id":"http://arxiv.org/abs/2503.13857v1","updated":"2025-03-18T03:14:23Z","published":"2025-03-18T03:14:23Z","title":"Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations","summary":"  Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AudoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources.\n","authors":["Rui Yang","Jiayi Tong","Haoyuan Wang","Hui Huang","Ziyang Hu","Peiyu Li","Nan Liu","Christopher J. Lindsell","Michael J. Pencina","Yong Chen","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2503.13857v1.pdf","comment":"28 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.13844v1","updated":"2025-03-18T02:33:38Z","published":"2025-03-18T02:33:38Z","title":"Spotting Persuasion: A Low-cost Model for Persuasion Detection in\n  Political Ads on Social Media","summary":"  In the realm of political advertising, persuasion operates as a pivotal\nelement within the broader framework of propaganda, exerting profound\ninfluences on public opinion and electoral outcomes. In this paper, we (1)\nintroduce a lightweight model for persuasive text detection that achieves\nstate-of-the-art performance in Subtask 3 of SemEval 2023 Task 3, while\nsignificantly reducing the computational resource requirements; and (2)\nleverage the proposed model to gain insights into political campaigning\nstrategies on social media platforms by applying it to a real-world dataset we\ncurated, consisting of Facebook political ads from the 2022 Australian Federal\nelection campaign. Our study shows how subtleties can be found in persuasive\npolitical advertisements and presents a pragmatic approach to detect and\nanalyze such strategies with limited resources, enhancing transparency in\nsocial media political campaigns.\n","authors":["Elyas Meguellati","Stefano Civelli","Pietro Bernardelle","Shazia Sadiq","Gianluca Demartini"],"pdf_url":"https://arxiv.org/pdf/2503.13844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13837v1","updated":"2025-03-18T02:21:07Z","published":"2025-03-18T02:21:07Z","title":"Self-Vocabularizing Training for Neural Machine Translation","summary":"  Past vocabulary learning techniques identify relevant vocabulary before\ntraining, relying on statistical and entropy-based assumptions that largely\nneglect the role of model training. Empirically, we observe that trained\ntranslation models are induced to use a byte-pair encoding (BPE) vocabulary\nsubset distinct from the original BPE vocabulary, leading to performance\nimprovements when retrained with the induced vocabulary. In this paper, we\nanalyze this discrepancy in neural machine translation by examining vocabulary\nand entropy shifts during self-training--where each iteration generates a\nlabeled dataset by pairing source sentences with the model's predictions to\ndefine a new vocabulary. Building on these insights, we propose\nself-vocabularizing training, an iterative method that self-selects a smaller,\nmore optimal vocabulary, yielding up to a 1.49 BLEU improvement. Moreover, we\nfind that deeper model architectures lead to both an increase in unique token\nusage and a 6-8% reduction in vocabulary size.\n","authors":["Pin-Jie Lin","Ernie Chang"],"pdf_url":"https://arxiv.org/pdf/2503.13837v1.pdf","comment":"Accepted to NAACL SRW 2025"},{"id":"http://arxiv.org/abs/2304.13343v4","updated":"2025-03-18T02:16:56Z","published":"2023-04-26T07:25:31Z","title":"SCM: Enhancing Large Language Model with Self-Controlled Memory\n  Framework","summary":"  Large Language Models (LLMs) are constrained by their inability to process\nlengthy inputs, resulting in the loss of critical historical information. To\naddress this limitation, in this paper, we propose the Self-Controlled Memory\n(SCM) framework to enhance the ability of LLMs to maintain long-term memory and\nrecall relevant information. Our SCM framework comprises three key components:\nan LLM-based agent serving as the backbone of the framework, a memory stream\nstoring agent memories, and a memory controller updating memories and\ndetermining when and how to utilize memories from memory stream. Additionally,\nthe proposed SCM is able to process ultra-long texts without any modification\nor fine-tuning, which can integrate with any instruction following LLMs in a\nplug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the\neffectiveness of SCM for handling lengthy inputs. The annotated dataset covers\nthree tasks: long-term dialogues, book summarization, and meeting\nsummarization. Experimental results demonstrate that our method achieves better\nretrieval recall and generates more informative responses compared to\ncompetitive baselines in long-term dialogues.\n(https://github.com/wbbeyourself/SCM4LLMs)\n","authors":["Bing Wang","Xinnian Liang","Jian Yang","Hui Huang","Shuangzhi Wu","Peihao Wu","Lu Lu","Zejun Ma","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2304.13343v4.pdf","comment":"Accepted by DASFAA 2025 main conference"},{"id":"http://arxiv.org/abs/2312.11242v6","updated":"2025-03-18T02:12:21Z","published":"2023-12-18T14:40:20Z","title":"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL","summary":"  Recent LLM-based Text-to-SQL methods usually suffer from significant\nperformance degradation on \"huge\" databases and complex user questions that\nrequire multi-step reasoning. Moreover, most existing methods neglect the\ncrucial significance of LLMs utilizing external tools and model collaboration.\nTo address these challenges, we introduce MAC-SQL, a novel LLM-based\nmulti-agent collaborative framework. Our framework comprises a core decomposer\nagent for Text-to-SQL generation with few-shot chain-of-thought reasoning,\naccompanied by two auxiliary agents that utilize external tools or models to\nacquire smaller sub-databases and refine erroneous SQL queries. The decomposer\nagent collaborates with auxiliary agents, which are activated as needed and can\nbe expanded to accommodate new features or tools for effective Text-to-SQL\nparsing. In our framework, We initially leverage GPT-4 as the strong backbone\nLLM for all agent tasks to determine the upper bound of our framework. We then\nfine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging\nCode Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that\nSQL-Llama achieves a comparable execution accuracy of 43.94, compared to the\nbaseline accuracy of 46.35 for vanilla GPT-4. At the time of writing,\nMAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the\nBIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test\nset (https://github.com/wbbeyourself/MAC-SQL).\n","authors":["Bing Wang","Changyu Ren","Jian Yang","Xinnian Liang","Jiaqi Bai","LinZheng Chai","Zhao Yan","Qian-Wen Zhang","Di Yin","Xing Sun","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2312.11242v6.pdf","comment":"Accepted by COLING 2025 (Oral)"},{"id":"http://arxiv.org/abs/2401.09002v6","updated":"2025-03-18T01:50:42Z","published":"2024-01-17T06:42:44Z","title":"AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models","summary":"  Jailbreak attacks represent one of the most sophisticated threats to the\nsecurity of large language models (LLMs). To deal with such risks, we introduce\nan innovative framework that can help evaluate the effectiveness of jailbreak\nattacks on LLMs. Unlike traditional binary evaluations focusing solely on the\nrobustness of LLMs, our method assesses the attacking prompts' effectiveness.\nWe present two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework uses a scoring range from 0 to 1,\noffering unique perspectives and allowing for the assessment of attack\neffectiveness in different scenarios. Additionally, we develop a comprehensive\nground truth dataset specifically tailored for jailbreak prompts. This dataset\nis a crucial benchmark for our current study and provides a foundational\nresource for future research. By comparing with traditional evaluation methods,\nour study shows that the current results align with baseline metrics while\noffering a more nuanced and fine-grained assessment. It also helps identify\npotentially harmful attack prompts that might appear harmless in traditional\nevaluations. Overall, our work establishes a solid foundation for assessing a\nbroader range of attack prompts in prompt injection.\n","authors":["Dong Shu","Chong Zhang","Mingyu Jin","Zihao Zhou","Lingyao Li","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.09002v6.pdf","comment":"Accepted by ACM SIGKDD Explorations 2025"},{"id":"http://arxiv.org/abs/2503.10167v2","updated":"2025-03-18T00:25:47Z","published":"2025-03-13T08:46:32Z","title":"\"Well, Keep Thinking\": Enhancing LLM Reasoning with Adaptive Injection\n  Decoding","summary":"  Large language models (LLMs) exhibit strong reasoning abilities, often\nattributed to few-shot or zero-shot chain-of-thought (CoT) prompting. While\neffective, these methods require labor-intensive prompt engineering, raising\nthe question of whether reasoning can be induced without reliance on explicit\nprompts. In this work, we unlock the reasoning capabilities of LLMs without\nexplicit prompting. Inspired by zero-shot CoT and CoT-decoding, we propose a\nnovel decoding strategy that systematically nudges LLMs to continue reasoning,\nthereby preventing immature reasoning processes. Specifically, we monitor the\nmodel's generation and inject a designated phrase whenever it is likely to\nconclude its response prematurely, before completing the reasoning process. Our\nexperimental evaluations on diverse reasoning benchmarks demonstrate that our\nproposed strategy substantially improves LLM reasoning capabilities,\nhighlighting the potential of decoding-based interventions as an alternative to\ntraditional prompting techniques.\n","authors":["Hyunbin Jin","Je Won Yeom","Seunghyun Bae","Taesup Kim"],"pdf_url":"https://arxiv.org/pdf/2503.10167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07064v2","updated":"2025-03-18T23:52:27Z","published":"2024-10-09T17:06:57Z","title":"Data Selection via Optimal Control for Language Models","summary":"  This work investigates the selection of high-quality pre-training data from\nmassive corpora to enhance LMs' capabilities for downstream usage. We formulate\ndata selection as a generalized Optimal Control problem, which can be solved\ntheoretically by Pontryagin's Maximum Principle (PMP), yielding a set of\nnecessary conditions that characterize the relationship between optimal data\nselection and LM training dynamics. Based on these theoretical results, we\nintroduce PMP-based Data Selection (PDS), a framework that approximates optimal\ndata selection by solving the PMP conditions. In our experiments, we adopt PDS\nto select data from CommmonCrawl and show that the PDS-selected corpus\naccelerates the learning of LMs and constantly boosts their performance on a\nwide range of downstream tasks across various model sizes. Moreover, the\nbenefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by\nthe extrapolation of the test loss curves according to the Scaling Laws. PDS\nalso improves data utilization when the pre-training data is limited, by\nreducing the data demand by 1.8 times, which helps mitigate the quick\nexhaustion of available web-crawled corpora. Our code, model, and data can be\nfound at https://github.com/microsoft/LMOps/tree/main/data_selection.\n","authors":["Yuxian Gu","Li Dong","Hongning Wang","Yaru Hao","Qingxiu Dong","Furu Wei","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.07064v2.pdf","comment":"ICLR 2025 Oral"},{"id":"http://arxiv.org/abs/2503.14755v1","updated":"2025-03-18T21:57:58Z","published":"2025-03-18T21:57:58Z","title":"Language Independent Named Entity Recognition via Orthogonal\n  Transformation of Word Vectors","summary":"  Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset.\n","authors":["Omar E. Rakha","Hazem M. Abbas"],"pdf_url":"https://arxiv.org/pdf/2503.14755v1.pdf","comment":"Paper was initially released in 2017 but was never published"},{"id":"http://arxiv.org/abs/2312.10048v4","updated":"2025-03-18T21:32:48Z","published":"2023-12-02T04:45:17Z","title":"Knowledge Graph Enhanced Aspect-Level Sentiment Analysis","summary":"  In this paper, we propose a novel method to enhance sentiment analysis by\naddressing the challenge of context-specific word meanings. It combines the\nadvantages of a BERT model with a knowledge graph based synonym data. This\nsynergy leverages a dynamic attention mechanism to develop a knowledge-driven\nstate vector. For classifying sentiments linked to specific aspects, the\napproach constructs a memory bank integrating positional data. The data are\nthen analyzed using a DCGRU to pinpoint sentiment characteristics related to\nspecific aspect terms. Experiments on three widely used datasets demonstrate\nthe superior performance of our method in sentiment classification.\n","authors":["Kavita Sharma","Ritu Patel","Sunita Iyer"],"pdf_url":"https://arxiv.org/pdf/2312.10048v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08247v3","updated":"2025-03-18T21:31:51Z","published":"2023-04-14T11:28:08Z","title":"MedAlpaca -- An Open-Source Collection of Medical Conversational AI\n  Models and Training Data","summary":"  As large language models (LLMs) like OpenAI's GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.\n","authors":["Tianyu Han","Lisa C. Adams","Jens-Michalis Papaioannou","Paul Grundmann","Tom Oberhauser","Alexei Figueroa","Alexander Löser","Daniel Truhn","Keno K. Bressem"],"pdf_url":"https://arxiv.org/pdf/2304.08247v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14749v1","updated":"2025-03-18T21:29:29Z","published":"2025-03-18T21:29:29Z","title":"Uncertainty Distillation: Teaching Language Models to Express Semantic\n  Confidence","summary":"  As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We demonstrate our method yields\nverbalized confidences that correlate with observed error rates with a small\nfine-tuned language model as well as with larger instruction-tuned models, and\nfind that our semantic uncertainty correlates well with lexical uncertainty on\nshort answers.\n","authors":["Sophia Hager","David Mueller","Kevin Duh","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2503.14749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11852v2","updated":"2025-03-18T21:10:24Z","published":"2024-08-14T20:48:45Z","title":"Fast Training Dataset Attribution via In-Context Learning","summary":"  We investigate the use of in-context learning and prompt engineering to\nestimate the contributions of training data in the outputs of instruction-tuned\nlarge language models (LLMs). We propose two novel approaches: (1) a\nsimilarity-based approach that measures the difference between LLM outputs with\nand without provided context, and (2) a mixture distribution model approach\nthat frames the problem of identifying contribution scores as a matrix\nfactorization task. Our empirical comparison demonstrates that the mixture\nmodel approach is more robust to retrieval noise in in-context learning,\nproviding a more reliable estimation of data contributions.\n","authors":["Milad Fotouhi","Mohammad Taha Bahadori","Oluwaseyi Feyisetan","Payman Arabshahi","David Heckerman"],"pdf_url":"https://arxiv.org/pdf/2408.11852v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14728v1","updated":"2025-03-18T20:58:43Z","published":"2025-03-18T20:58:43Z","title":"Strategic resource allocation in memory encoding: An efficiency\n  principle shaping language processing","summary":"  How is the limited capacity of working memory efficiently used to support\nhuman linguistic behaviors? In this paper, we investigate strategic resource\nallocation as an efficiency principle for memory encoding in sentence\nprocessing. The idea is that working memory resources are dynamically and\nstrategically allocated to prioritize novel and unexpected information,\nenhancing their representations to make them less susceptible to memory decay\nand interference. Theoretically, from a resource-rational perspective, we argue\nthat this efficiency principle naturally arises from two functional assumptions\nabout working memory, namely, its limited capacity and its noisy\nrepresentation. Empirically, through naturalistic corpus data, we find\nconverging evidence for strategic resource allocation in the context of\ndependency locality from both the production and the comprehension side, where\nnon-local dependencies with less predictable antecedents are associated with\nreduced locality effect. However, our results also reveal considerable\ncross-linguistic variability, highlighting the need for a closer examination of\nhow strategic resource allocation, as a universal efficiency principle,\ninteracts with language-specific phrase structures.\n","authors":["Weijie Xu","Richard Futrell"],"pdf_url":"https://arxiv.org/pdf/2503.14728v1.pdf","comment":"manuscript under review"},{"id":"http://arxiv.org/abs/2503.14718v1","updated":"2025-03-18T20:42:42Z","published":"2025-03-18T20:42:42Z","title":"Second language Korean Universal Dependency treebank v1.2: Focus on data\n  augmentation and annotation scheme refinement","summary":"  We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data.\n","authors":["Hakyung Sung","Gyu-Ho Shin"],"pdf_url":"https://arxiv.org/pdf/2503.14718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14684v2","updated":"2025-03-18T20:26:58Z","published":"2024-10-03T05:45:26Z","title":"RepoGraph: Enhancing AI Software Engineering with Repository-level Code\n  Graph","summary":"  Large Language Models (LLMs) excel in code generation yet struggle with\nmodern AI software engineering tasks. Unlike traditional function-level or\nfile-level coding tasks, AI software engineering requires not only basic coding\nproficiency but also advanced skills in managing and interacting with code\nrepositories. However, existing methods often overlook the need for\nrepository-level code understanding, which is crucial for accurately grasping\nthe broader context and developing effective solutions. On this basis, we\npresent RepoGraph, a plug-in module that manages a repository-level structure\nfor modern AI software engineering solutions. RepoGraph offers the desired\nguidance and serves as a repository-wide navigation for AI software engineers.\nWe evaluate RepoGraph on the SWE-bench by plugging it into four different\nmethods of two lines of approaches, where RepoGraph substantially boosts the\nperformance of all systems, leading to a new state-of-the-art among open-source\nframeworks. Our analyses also demonstrate the extensibility and flexibility of\nRepoGraph by testing on another repo-level coding benchmark, CrossCodeEval. Our\ncode is available at https://github.com/ozyyshr/RepoGraph.\n","authors":["Siru Ouyang","Wenhao Yu","Kaixin Ma","Zilin Xiao","Zhihan Zhang","Mengzhao Jia","Jiawei Han","Hongming Zhang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.14684v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.11630v2","updated":"2025-03-18T20:01:03Z","published":"2025-03-14T17:48:23Z","title":"The time scale of redundancy between prosody and linguistic context","summary":"  In spoken language, speakers transmit information not only using words, but\nalso via a rich array of non-verbal signals, which include prosody -- the\nauditory features of speech. However, previous studies have shown that prosodic\nfeatures exhibit significant redundancy with both past and future words. Here,\nwe examine the time scale of this relationship: How many words in the past (or\nfuture) contribute to predicting prosody? We find that this scale differs for\npast and future words. Prosody's redundancy with past words extends across\napproximately 3-8 words, whereas redundancy with future words is limited to\njust 1-2 words. These findings indicate that the prosody-future relationship\nreflects local word dependencies or short-scale processes such as next word\nprediction, while the prosody-past relationship unfolds over a longer time\nscale. The latter suggests that prosody serves to emphasize earlier information\nthat may be challenging for listeners to process given limited cognitive\nresources in real-time communication. Our results highlight the role of prosody\nin shaping efficient communication.\n","authors":["Tamar I. Regev","Chiebuka Ohams","Shaylee Xie","Lukas Wolf","Evelina Fedorenko","Alex Warstadt","Ethan G. Wilcox","Tiago Pimentel"],"pdf_url":"https://arxiv.org/pdf/2503.11630v2.pdf","comment":"12 pages, 4 figures, recently submitted to ACL. Fixed author\n  affiliations and added acknowledgements"},{"id":"http://arxiv.org/abs/2410.09907v2","updated":"2025-03-18T19:48:28Z","published":"2024-10-13T16:27:31Z","title":"Reddit is all you need: Authorship profiling for Romanian","summary":"  Authorship profiling is the process of identifying an author's\ncharacteristics based on their writings. This centuries old problem has become\nmore intriguing especially with recent developments in Natural Language\nProcessing (NLP). In this paper, we introduce a corpus of short texts in the\nRomanian language, annotated with certain author characteristic keywords; to\nour knowledge, the first of its kind. In order to do this, we exploit a social\nmedia platform called Reddit. We leverage its thematic community-based\nstructure (subreddits structure), which offers information about the author's\nbackground. We infer an user's demographic and some broad personal traits, such\nas age category, employment status, interests, and social orientation based on\nthe subreddit and other cues. We thus obtain a 23k+ samples corpus, extracted\nfrom 100+ Romanian subreddits. We analyse our dataset, and finally, we\nfine-tune and evaluate Large Language Models (LLMs) to prove baselines\ncapabilities for authorship profiling using the corpus, indicating the need for\nfurther research in the field. We publicly release all our resources.\n","authors":["Ecaterina Ştefănescu","Alexandru-Iulius Jerpelea"],"pdf_url":"https://arxiv.org/pdf/2410.09907v2.pdf","comment":"10 pages, 5 tables and 1 figure, published and presented at The 19th\n  International Conference on Linguistic Resources and Tools for Natural\n  Language Processing (ConsILR 2024)"},{"id":"http://arxiv.org/abs/2503.14671v1","updated":"2025-03-18T19:23:22Z","published":"2025-03-18T19:23:22Z","title":"Generating Medically-Informed Explanations for Depression Detection\n  using LLMs","summary":"  Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability.\n","authors":["Xiangyong Chen","Xiaochuan Lin"],"pdf_url":"https://arxiv.org/pdf/2503.14671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14662v1","updated":"2025-03-18T19:10:26Z","published":"2025-03-18T19:10:26Z","title":"ConQuer: A Framework for Concept-Based Quiz Generation","summary":"  Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer.\n","authors":["Yicheng Fu","Zikui Wang","Liuxin Yang","Meiqing Huo","Zhongdongming Dai"],"pdf_url":"https://arxiv.org/pdf/2503.14662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14649v1","updated":"2025-03-18T18:58:13Z","published":"2025-03-18T18:58:13Z","title":"RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving","summary":"  Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.\n","authors":["Wenqi Jiang","Suvinay Subramanian","Cat Graves","Gustavo Alonso","Amir Yazdanbakhsh","Vidushi Dadu"],"pdf_url":"https://arxiv.org/pdf/2503.14649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14626v1","updated":"2025-03-18T18:24:23Z","published":"2025-03-18T18:24:23Z","title":"An Explainable Framework for Misinformation Identification via Critical\n  Question Answering","summary":"  Natural language misinformation detection approaches have been, to date,\nlargely dependent on sequence classification methods, producing opaque systems\nin which the reasons behind classification as misinformation are unclear. While\nan effort has been made in the area of automated fact-checking to propose\nexplainable approaches to the problem, this is not the case for automated\nreason-checking systems. In this paper, we propose a new explainable framework\nfor both factual and rational misinformation detection based on the theory of\nArgumentation Schemes and Critical Questions. For that purpose, we create and\nrelease NLAS-CQ, the first corpus combining 3,566 textbook-like natural\nlanguage argumentation scheme instances and 4,687 corresponding answers to\ncritical questions related to these arguments. On the basis of this corpus, we\nimplement and validate our new framework which combines classification with\nquestion answering to analyse arguments in search of misinformation, and\nprovides the explanations in form of critical questions to the human user.\n","authors":["Ramon Ruiz-Dolz","John Lawrence"],"pdf_url":"https://arxiv.org/pdf/2503.14626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14620v1","updated":"2025-03-18T18:17:10Z","published":"2025-03-18T18:17:10Z","title":"Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and\n  Knowledge-Adaptive Simulations","summary":"  In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange.\n","authors":["Hikaru Shimadzu","Takehito Utsuro","Daisuke Kitayama"],"pdf_url":"https://arxiv.org/pdf/2503.14620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14615v1","updated":"2025-03-18T18:12:09Z","published":"2025-03-18T18:12:09Z","title":"Unique Hard Attention: A Tale of Two Sides","summary":"  Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality.\n","authors":["Selim Jerad","Anej Svete","Jiaoda Li","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2503.14615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14604v1","updated":"2025-03-18T18:03:56Z","published":"2025-03-18T18:03:56Z","title":"Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges\n  and Future Perspectives","summary":"  The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment.\n","authors":["Sara Sarto","Marcella Cornia","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.14604v1.pdf","comment":"Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation"},{"id":"http://arxiv.org/abs/2503.14603v1","updated":"2025-03-18T18:03:49Z","published":"2025-03-18T18:03:49Z","title":"Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and\n  Culturally Aware Arabic LLM","summary":"  Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness.\n","authors":["Yazeed Alnumay","Alexandre Barbet","Anna Bialas","William Darling","Shaan Desai","Joan Devassy","Kyle Duffy","Stephanie Howe","Olivia Lasche","Justin Lee","Anirudh Shrinivason","Jennifer Tracey"],"pdf_url":"https://arxiv.org/pdf/2503.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14232v1","updated":"2025-03-18T13:09:01Z","published":"2025-03-18T13:09:01Z","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","summary":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.\n","authors":["Yuyang Xue","Edward Moroshko","Feng Chen","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2503.14232v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.14505v1","updated":"2025-03-18T17:59:58Z","published":"2025-03-18T17:59:58Z","title":"MusicInfuser: Making Video Diffusion Listen and Dance","summary":"  We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.\n","authors":["Susung Hong","Ira Kemelmacher-Shlizerman","Brian Curless","Steven M. Seitz"],"pdf_url":"https://arxiv.org/pdf/2503.14505v1.pdf","comment":"Project page: https://susunghong.github.io/MusicInfuser"},{"id":"http://arxiv.org/abs/2503.14504v1","updated":"2025-03-18T17:59:56Z","published":"2025-03-18T17:59:56Z","title":"Aligning Multimodal LLM with Human Preference: A Survey","summary":"  Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment.\n","authors":["Tao Yu","Yi-Fan Zhang","Chaoyou Fu","Junkang Wu","Jinda Lu","Kun Wang","Xingyu Lu","Yunhang Shen","Guibin Zhang","Dingjie Song","Yibo Yan","Tianlong Xu","Qingsong Wen","Zhang Zhang","Yan Huang","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2503.14504v1.pdf","comment":"https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment"},{"id":"http://arxiv.org/abs/2503.14503v1","updated":"2025-03-18T17:59:54Z","published":"2025-03-18T17:59:54Z","title":"The Power of Context: How Multimodality Improves Image Super-Resolution","summary":"  Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.\n","authors":["Kangfu Mei","Hossein Talebi","Mojtaba Ardakani","Vishal M. Patel","Peyman Milanfar","Mauricio Delbracio"],"pdf_url":"https://arxiv.org/pdf/2503.14503v1.pdf","comment":"accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.14501v1","updated":"2025-03-18T17:59:51Z","published":"2025-03-18T17:59:51Z","title":"Advances in 4D Generation: A Survey","summary":"  Generative artificial intelligence has witnessed remarkable advancements\nacross multiple domains in recent years. Building on the successes of 2D and 3D\ncontent generation, 4D generation, which incorporates the temporal dimension\ninto generative tasks, has emerged as a burgeoning yet rapidly evolving\nresearch area. This paper presents a comprehensive survey of this emerging\nfield, systematically examining its theoretical foundations, key methodologies,\nand practical applications, with the aim of providing readers with a holistic\nunderstanding of the current state and future potential of 4D generation. We\nbegin by introducing the core concepts of 4D data representations, encompassing\nboth structured and unstructured formats, and their implications for generative\ntasks. Building upon this foundation, we delve into the enabling technologies\nthat drive 4D generation, including advancements in spatiotemporal modeling,\nneural representations, and generative frameworks. We further review recent\nstudies that employ diverse control mechanisms and representation strategies\nfor generating 4D outputs, categorizing these approaches and summarizing their\nresearch trajectories. In addition, we explore the wide-ranging applications of\n4D generation techniques, spanning dynamic object modeling, scene generation,\ndigital human synthesis, 4D content editing, and autonomous driving. Finally,\nwe analyze the key challenges inherent to 4D generation, such as data\navailability, computational efficiency, and spatiotemporal consistency, and\npropose promising directions for future research. Our code is publicly\navailable at:\n\\href{https://github.com/MiaoQiaowei/Awesome-4D}{https://github.com/MiaoQiaowei/Awesome-4D}.\n","authors":["Qiaowei Miao","Kehan Li","Jinsheng Quan","Zhiyuan Min","Shaojie Ma","Yichao Xu","Yi Yang","Yawei Luo"],"pdf_url":"https://arxiv.org/pdf/2503.14501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14500v1","updated":"2025-03-18T17:59:41Z","published":"2025-03-18T17:59:41Z","title":"Utilization of Neighbor Information for Image Classification with\n  Different Levels of Supervision","summary":"  We propose to bridge the gap between semi-supervised and unsupervised image\nrecognition with a flexible method that performs well for both generalized\ncategory discovery (GCD) and image clustering. Despite the overlap in\nmotivation between these tasks, the methods themselves are restricted to a\nsingle task -- GCD methods are reliant on the labeled portion of the data, and\ndeep image clustering methods have no built-in way to leverage the labels\nefficiently. We connect the two regimes with an innovative approach that\nUtilizes Neighbor Information for Classification (UNIC) both in the\nunsupervised (clustering) and semisupervised (GCD) setting. State-of-the-art\nclustering methods already rely heavily on nearest neighbors. We improve on\ntheir results substantially in two parts, first with a sampling and cleaning\nstrategy where we identify accurate positive and negative neighbors, and\nsecondly by finetuning the backbone with clustering losses computed by sampling\nboth types of neighbors. We then adapt this pipeline to GCD by utilizing the\nlabelled images as ground truth neighbors. Our method yields state-of-the-art\nresults for both clustering (+3% ImageNet-100, Imagenet200) and GCD (+0.8%\nImageNet-100, +5% CUB, +2% SCars, +4% Aircraft).\n","authors":["Gihan Jayatilaka","Abhinav Shrivastava","Matthew Gwilliam"],"pdf_url":"https://arxiv.org/pdf/2503.14500v1.pdf","comment":"18 pages, 16 figures, 7 tables"},{"id":"http://arxiv.org/abs/2503.14498v1","updated":"2025-03-18T17:59:12Z","published":"2025-03-18T17:59:12Z","title":"Tracking Meets Large Multimodal Models for Driving Scenario\n  Understanding","summary":"  Large Multimodal Models (LMMs) have recently gained prominence in autonomous\ndriving research, showcasing promising capabilities across various emerging\nbenchmarks. LMMs specifically designed for this domain have demonstrated\neffective perception, planning, and prediction skills. However, many of these\nmethods underutilize 3D spatial and temporal elements, relying mainly on image\ndata. As a result, their effectiveness in dynamic driving environments is\nlimited. We propose to integrate tracking information as an additional input to\nrecover 3D spatial and temporal details that are not effectively captured in\nthe images. We introduce a novel approach for embedding this tracking\ninformation into LMMs to enhance their spatiotemporal understanding of driving\nscenarios. By incorporating 3D tracking data through a track encoder, we enrich\nvisual queries with crucial spatial and temporal cues while avoiding the\ncomputational overhead associated with processing lengthy video sequences or\nextensive 3D inputs. Moreover, we employ a self-supervised approach to pretrain\nthe tracking encoder to provide LMMs with additional contextual information,\nsignificantly improving their performance in perception, planning, and\nprediction tasks for autonomous driving. Experimental results demonstrate the\neffectiveness of our approach, with a gain of 9.5% in accuracy, an increase of\n7.04 points in the ChatGPT score, and 9.4% increase in the overall score over\nbaseline models on DriveLM-nuScenes benchmark, along with a 3.7% final score\nimprovement on DriveLM-CARLA. Our code is available at\nhttps://github.com/mbzuai-oryx/TrackingMeetsLMM\n","authors":["Ayesha Ishaq","Jean Lahoud","Fahad Shahbaz Khan","Salman Khan","Hisham Cholakkal","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2503.14498v1.pdf","comment":"13 pages, 8 figures, Github:\n  https://github.com/mbzuai-oryx/TrackingMeetsLMM"},{"id":"http://arxiv.org/abs/2503.14494v1","updated":"2025-03-18T17:58:08Z","published":"2025-03-18T17:58:08Z","title":"Deeply Supervised Flow-Based Generative Models","summary":"  Flow based generative models have charted an impressive path across multiple\nvisual generation tasks by adhering to a simple principle: learning velocity\nrepresentations of a linear interpolant. However, we observe that training\nvelocity solely from the final layer output underutilizes the rich inter layer\nrepresentations, potentially impeding model convergence. To address this\nlimitation, we introduce DeepFlow, a novel framework that enhances velocity\nrepresentation through inter layer communication. DeepFlow partitions\ntransformer layers into balanced branches with deep supervision and inserts a\nlightweight Velocity Refiner with Acceleration (VeRA) block between adjacent\nbranches, which aligns the intermediate velocity features within transformer\nblocks. Powered by the improved deep supervision via the internal velocity\nalignment, DeepFlow converges 8 times faster on ImageNet with equivalent\nperformance and further reduces FID by 2.6 while halving training time compared\nto previous flow based models without a classifier free guidance. DeepFlow also\noutperforms baselines in text to image generation tasks, as evidenced by\nevaluations on MSCOCO and zero shot GenEval.\n","authors":["Inkyu Shin","Chenglin Yang","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2503.14494v1.pdf","comment":"Project website at https://deepflow-project.github.io/"},{"id":"http://arxiv.org/abs/2503.14493v1","updated":"2025-03-18T17:58:03Z","published":"2025-03-18T17:58:03Z","title":"State Space Model Meets Transformer: A New Paradigm for 3D Object\n  Detection","summary":"  DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.\n","authors":["Chuxin Wang","Wenfei Yang","Xiang Liu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14493v1.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14492v1","updated":"2025-03-18T17:57:54Z","published":"2025-03-18T17:57:54Z","title":"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control","summary":"  We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.\n","authors":[" NVIDIA"," :","Hassan Abu Alhaija","Jose Alvarez","Maciej Bala","Tiffany Cai","Tianshi Cao","Liz Cha","Joshua Chen","Mike Chen","Francesco Ferroni","Sanja Fidler","Dieter Fox","Yunhao Ge","Jinwei Gu","Ali Hassani","Michael Isaev","Pooya Jannaty","Shiyi Lan","Tobias Lasser","Huan Ling","Ming-Yu Liu","Xian Liu","Yifan Lu","Alice Luo","Qianli Ma","Hanzi Mao","Fabio Ramos","Xuanchi Ren","Tianchang Shen","Shitao Tang","Ting-Chun Wang","Jay Wu","Jiashu Xu","Stella Xu","Kevin Xie","Yuchong Ye","Xiaodong Yang","Xiaohui Zeng","Yu Zeng"],"pdf_url":"https://arxiv.org/pdf/2503.14492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14489v1","updated":"2025-03-18T17:57:22Z","published":"2025-03-18T17:57:22Z","title":"Stable Virtual Camera: Generative View Synthesis with Diffusion Models","summary":"  We present Stable Virtual Camera (Seva), a generalist diffusion model that\ncreates novel views of a scene, given any number of input views and target\ncameras. Existing works struggle to generate either large viewpoint changes or\ntemporally smooth samples, while relying on specific task configurations. Our\napproach overcomes these limitations through simple model design, optimized\ntraining recipe, and flexible sampling strategy that generalize across view\nsynthesis tasks at test time. As a result, our samples maintain high\nconsistency without requiring additional 3D representation-based distillation,\nthus streamlining view synthesis in the wild. Furthermore, we show that our\nmethod can generate high-quality videos lasting up to half a minute with\nseamless loop closure. Extensive benchmarking demonstrates that Seva\noutperforms existing methods across different datasets and settings.\n","authors":[" Jensen"," Zhou","Hang Gao","Vikram Voleti","Aaryaman Vasishta","Chun-Han Yao","Mark Boss","Philip Torr","Christian Rupprecht","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2503.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14487v1","updated":"2025-03-18T17:57:07Z","published":"2025-03-18T17:57:07Z","title":"DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers","summary":"  Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/\n","authors":["Minglei Shi","Ziyang Yuan","Haotian Yang","Xintao Wang","Mingwu Zheng","Xin Tao","Wenliang Zhao","Wenzhao Zheng","Jie Zhou","Jiwen Lu","Pengfei Wan","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2503.14487v1.pdf","comment":"Project Page: https://shiml20.github.io/DiffMoE/"},{"id":"http://arxiv.org/abs/2503.14485v1","updated":"2025-03-18T17:55:22Z","published":"2025-03-18T17:55:22Z","title":"Lux Post Facto: Learning Portrait Performance Relighting with\n  Conditional Video Diffusion and a Hybrid Dataset","summary":"  Video portrait relighting remains challenging because the results need to be\nboth photorealistic and temporally stable. This typically requires a strong\nmodel design that can capture complex facial reflections as well as intensive\ntraining on a high-quality paired video dataset, such as dynamic\none-light-at-a-time (OLAT). In this work, we introduce Lux Post Facto, a novel\nportrait video relighting method that produces both photorealistic and\ntemporally consistent lighting effects. From the model side, we design a new\nconditional video diffusion model built upon state-of-the-art pre-trained video\ndiffusion model, alongside a new lighting injection mechanism to enable precise\ncontrol. This way we leverage strong spatial and temporal generative capability\nto generate plausible solutions to the ill-posed relighting problem. Our\ntechnique uses a hybrid dataset consisting of static expression OLAT data and\nin-the-wild portrait performance videos to jointly learn relighting and\ntemporal modeling. This avoids the need to acquire paired video data in\ndifferent lighting conditions. Our extensive experiments show that our model\nproduces state-of-the-art results both in terms of photorealism and temporal\nconsistency.\n","authors":["Yiqun Mei","Mingming He","Li Ma","Julien Philip","Wenqi Xian","David M George","Xueming Yu","Gabriel Dedic","Ahmet Levent Taşel","Ning Yu","Vishal M. Patel","Paul Debevec"],"pdf_url":"https://arxiv.org/pdf/2503.14485v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14483v1","updated":"2025-03-18T17:54:06Z","published":"2025-03-18T17:54:06Z","title":"Multi-view Reconstruction via SfM-guided Monocular Depth Estimation","summary":"  In this paper, we present a new method for multi-view geometric\nreconstruction. In recent years, large vision models have rapidly developed,\nperforming excellently across various tasks and demonstrating remarkable\ngeneralization capabilities. Some works use large vision models for monocular\ndepth estimation, which have been applied to facilitate multi-view\nreconstruction tasks in an indirect manner. Due to the ambiguity of the\nmonocular depth estimation task, the estimated depth values are usually not\naccurate enough, limiting their utility in aiding multi-view reconstruction. We\npropose to incorporate SfM information, a strong multi-view prior, into the\ndepth estimation process, thus enhancing the quality of depth prediction and\nenabling their direct application in multi-view geometric reconstruction.\nExperimental results on public real-world datasets show that our method\nsignificantly improves the quality of depth estimation compared to previous\nmonocular depth estimation works. Additionally, we evaluate the reconstruction\nquality of our approach in various types of scenes including indoor,\nstreetscape, and aerial views, surpassing state-of-the-art MVS methods. The\ncode and supplementary materials are available at\nhttps://zju3dv.github.io/murre/ .\n","authors":["Haoyu Guo","He Zhu","Sida Peng","Haotong Lin","Yunzhi Yan","Tao Xie","Wenguan Wang","Xiaowei Zhou","Hujun Bao"],"pdf_url":"https://arxiv.org/pdf/2503.14483v1.pdf","comment":"CVPR 2025. Project page: https://zju3dv.github.io/murre/"},{"id":"http://arxiv.org/abs/2503.14482v1","updated":"2025-03-18T17:53:29Z","published":"2025-03-18T17:53:29Z","title":"ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and\n  Editing","summary":"  Image generation has witnessed significant advancements in the past few\nyears. However, evaluating the performance of image generation models remains a\nformidable challenge. In this paper, we propose ICE-Bench, a unified and\ncomprehensive benchmark designed to rigorously assess image generation models.\nIts comprehensiveness could be summarized in the following key features: (1)\nCoarse-to-Fine Tasks: We systematically deconstruct image generation into four\ntask categories: No-ref/Ref Image Creating/Editing, based on the presence or\nabsence of source images and reference images. And further decompose them into\n31 fine-grained tasks covering a broad spectrum of image generation\nrequirements, culminating in a comprehensive benchmark. (2) Multi-dimensional\nMetrics: The evaluation framework assesses image generation capabilities across\n6 dimensions: aesthetic quality, imaging quality, prompt following, source\nconsistency, reference consistency, and controllability. 11 metrics are\nintroduced to support the multi-dimensional evaluation. Notably, we introduce\nVLLM-QA, an innovative metric designed to assess the success of image editing\nby leveraging large models. (3) Hybrid Data: The data comes from real scenes\nand virtual generation, which effectively improves data diversity and\nalleviates the bias problem in model evaluation. Through ICE-Bench, we conduct\na thorough analysis of existing generation models, revealing both the\nchallenging nature of our benchmark and the gap between current model\ncapabilities and real-world generation requirements. To foster further\nadvancements in the field, we will open-source ICE-Bench, including its\ndataset, evaluation code, and models, thereby providing a valuable resource for\nthe research community.\n","authors":["Yulin Pan","Xiangteng He","Chaojie Mao","Zhen Han","Zeyinzi Jiang","Jingfeng Zhang","Yu Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14482v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2503.14478v1","updated":"2025-03-18T17:51:34Z","published":"2025-03-18T17:51:34Z","title":"Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM","summary":"  Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.\n","authors":["Xinyu Fang","Zhijian Chen","Kai Lan","Shengyuan Ding","Yingji Liang","Xiangyu Zhao","Farong Wen","Zicheng Zhang","Guofeng Zhang","Haodong Duan","Kai Chen","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2503.14478v1.pdf","comment":"Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench"},{"id":"http://arxiv.org/abs/2503.14475v1","updated":"2025-03-18T17:49:01Z","published":"2025-03-18T17:49:01Z","title":"Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency\n  Modulation","summary":"  The field of Novel View Synthesis has been revolutionized by 3D Gaussian\nSplatting (3DGS), which enables high-quality scene reconstruction that can be\nrendered in real-time. 3DGS-based techniques typically suffer from high GPU\nmemory and disk storage requirements which limits their practical application\non consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated\ncoarse-to-fine optimization framework that aims to minimize the number of\nGaussian primitives used to represent a scene, thus reducing memory and storage\ndemands. Opti3DGS leverages image frequency modulation, initially enforcing a\ncoarse scene representation and progressively refining it by modulating\nfrequency details in the training images. On the baseline 3DGS, we demonstrate\nan average reduction of 62% in Gaussians, a 40% reduction in the training GPU\nmemory requirements and a 20% reduction in optimization time without\nsacrificing the visual quality. Furthermore, we show that our method integrates\nseamlessly with many 3DGS-based techniques, consistently reducing the number of\nGaussian primitives while maintaining, and often improving, visual quality.\nAdditionally, Opti3DGS inherently produces a level-of-detail scene\nrepresentation at no extra cost, a natural byproduct of the optimization\npipeline. Results and code will be made publicly available.\n","authors":["Umar Farooq","Jean-Yves Guillemaut","Adrian Hilton","Marco Volino"],"pdf_url":"https://arxiv.org/pdf/2503.14475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14463v1","updated":"2025-03-18T17:42:34Z","published":"2025-03-18T17:42:34Z","title":"SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model","summary":"  The computer vision community has developed numerous techniques for digitally\nrestoring true scene information from single-view degraded photographs, an\nimportant yet extremely ill-posed task. In this work, we tackle image\nrestoration from a different perspective by jointly denoising multiple\nphotographs of the same scene. Our core hypothesis is that degraded images\ncapturing a shared scene contain complementary information that, when combined,\nbetter constrains the restoration problem. To this end, we implement a powerful\nmulti-view diffusion model that jointly generates uncorrupted views by\nextracting rich information from multi-view relationships. Our experiments show\nthat our multi-view approach outperforms existing single-view image and even\nvideo-based methods on image deblurring and super-resolution tasks. Critically,\nour model is trained to output 3D consistent images, making it a promising tool\nfor applications requiring robust multi-view integration, such as 3D\nreconstruction or pose estimation.\n","authors":["Yucheng Mao","Boyang Wang","Nilesh Kulkarni","Jeong Joon Park"],"pdf_url":"https://arxiv.org/pdf/2503.14463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19848v2","updated":"2025-03-18T17:32:47Z","published":"2025-02-27T07:47:27Z","title":"One-for-More: Continual Diffusion Model for Anomaly Detection","summary":"  With the rise of generative models, there is a growing interest in unifying\nall tasks within a generative framework. Anomaly detection methods also fall\ninto this scope and utilize diffusion models to generate or reconstruct normal\nsamples when given arbitrary anomaly images. However, our study found that the\ndiffusion model suffers from severe ``faithfulness hallucination'' and\n``catastrophic forgetting'', which can't meet the unpredictable pattern\nincrements. To mitigate the above problems, we propose a continual diffusion\nmodel that uses gradient projection to achieve stable continual learning.\nGradient projection deploys a regularization on the model updating by modifying\nthe gradient towards the direction protecting the learned knowledge. But as a\ndouble-edged sword, it also requires huge memory costs brought by the Markov\nprocess. Hence, we propose an iterative singular value decomposition method\nbased on the transitive property of linear representation, which consumes tiny\nmemory and incurs almost no performance loss. Finally, considering the risk of\n``over-fitting'' to normal images of the diffusion model, we propose an\nanomaly-masked network to enhance the condition mechanism of the diffusion\nmodel. For continual anomaly detection, ours achieves first place in 17/18\nsettings on MVTec and VisA. Code is available at\nhttps://github.com/FuNz-0/One-for-More\n","authors":["Xiaofan Li","Xin Tan","Zhuo Chen","Zhizhong Zhang","Ruixin Zhang","Rizen Guo","Guanna Jiang","Yulong Chen","Yanyun Qu","Lizhuang Ma","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2502.19848v2.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.14445v1","updated":"2025-03-18T17:24:19Z","published":"2025-03-18T17:24:19Z","title":"Bolt3D: Generating 3D Scenes in Seconds","summary":"  We present a latent diffusion model for fast feed-forward 3D scene\ngeneration. Given one or more images, our model Bolt3D directly samples a 3D\nscene representation in less than seven seconds on a single GPU. We achieve\nthis by leveraging powerful and scalable existing 2D diffusion network\narchitectures to produce consistent high-fidelity 3D scene representations. To\ntrain this model, we create a large-scale multiview-consistent dataset of 3D\ngeometry and appearance by applying state-of-the-art dense 3D reconstruction\ntechniques to existing multiview image datasets. Compared to prior multiview\ngenerative models that require per-scene optimization for 3D reconstruction,\nBolt3D reduces the inference cost by a factor of up to 300 times.\n","authors":["Stanislaw Szymanowicz","Jason Y. Zhang","Pratul Srinivasan","Ruiqi Gao","Arthur Brussee","Aleksander Holynski","Ricardo Martin-Brualla","Jonathan T. Barron","Philipp Henzler"],"pdf_url":"https://arxiv.org/pdf/2503.14445v1.pdf","comment":"Project page: https://szymanowiczs.github.io/bolt3d"},{"id":"http://arxiv.org/abs/2503.10660v2","updated":"2025-03-18T17:15:23Z","published":"2025-03-08T13:27:18Z","title":"Text-to-3D Generation using Jensen-Shannon Score Distillation","summary":"  Score distillation sampling is an effective technique to generate 3D models\nfrom text prompts, utilizing pre-trained large-scale text-to-image diffusion\nmodels as guidance. However, the produced 3D assets tend to be over-saturating,\nover-smoothing, with limited diversity. These issues are results from a reverse\nKullback-Leibler (KL) divergence objective, which makes the optimization\nunstable and results in mode-seeking behavior. In this paper, we derive a\nbounded score distillation objective based on Jensen-Shannon divergence (JSD),\nwhich stabilizes the optimization process and produces high-quality 3D\ngeneration. JSD can match well generated and target distribution, therefore\nmitigating mode seeking. We provide a practical implementation of JSD by\nutilizing the theory of generative adversarial networks to define an\napproximate objective function for the generator, assuming the discriminator is\nwell trained. By assuming the discriminator following a log-odds classifier, we\npropose a minority sampling algorithm to estimate the gradients of our proposed\nobjective, providing a practical implementation for JSD. We conduct both\ntheoretical and empirical studies to validate our method. Experimental results\non T3Bench demonstrate that our method can produce high-quality and diversified\n3D assets.\n","authors":["Khoi Do","Binh-Son Hua"],"pdf_url":"https://arxiv.org/pdf/2503.10660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20161v2","updated":"2025-03-18T17:06:31Z","published":"2025-02-27T15:00:43Z","title":"Balanced Rate-Distortion Optimization in Learned Image Compression","summary":"  Learned image compression (LIC) using deep learning architectures has seen\nsignificant advancements, yet standard rate-distortion (R-D) optimization often\nencounters imbalanced updates due to diverse gradients of the rate and\ndistortion objectives. This imbalance can lead to suboptimal optimization,\nwhere one objective dominates, thereby reducing overall compression efficiency.\nTo address this challenge, we reformulate R-D optimization as a multi-objective\noptimization (MOO) problem and introduce two balanced R-D optimization\nstrategies that adaptively adjust gradient updates to achieve more equitable\nimprovements in both rate and distortion. The first proposed strategy utilizes\na coarse-to-fine gradient descent approach along standard R-D optimization\ntrajectories, making it particularly suitable for training LIC models from\nscratch. The second proposed strategy analytically addresses the reformulated\noptimization as a quadratic programming problem with an equality constraint,\nwhich is ideal for fine-tuning existing models. Experimental results\ndemonstrate that both proposed methods enhance the R-D performance of LIC\nmodels, achieving around a 2\\% BD-Rate reduction with acceptable additional\ntraining cost, leading to a more balanced and efficient optimization process.\nCode will be available at https://gitlab.com/viper-purdue/Balanced-RD.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.20161v2.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14428v1","updated":"2025-03-18T17:02:14Z","published":"2025-03-18T17:02:14Z","title":"MagicComp: Training-free Dual-Phase Refinement for Compositional Video\n  Generation","summary":"  Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/.\n","authors":["Hongyu Zhang","Yufan Deng","Shenghai Yuan","Peng Jin","Zesen Cheng","Yian Zhao","Chang Liu","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2503.14428v1.pdf","comment":"Project webpage: https://hong-yu-zhang.github.io/MagicComp-Page/"},{"id":"http://arxiv.org/abs/2501.03575v2","updated":"2025-03-18T16:59:07Z","published":"2025-01-07T06:55:50Z","title":"Cosmos World Foundation Model Platform for Physical AI","summary":"  Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make Cosmos open-source and our models open-weight with\npermissive licenses available via\nhttps://github.com/nvidia-cosmos/cosmos-predict1.\n","authors":[" NVIDIA"," :","Niket Agarwal","Arslan Ali","Maciej Bala","Yogesh Balaji","Erik Barker","Tiffany Cai","Prithvijit Chattopadhyay","Yongxin Chen","Yin Cui","Yifan Ding","Daniel Dworakowski","Jiaojiao Fan","Michele Fenzi","Francesco Ferroni","Sanja Fidler","Dieter Fox","Songwei Ge","Yunhao Ge","Jinwei Gu","Siddharth Gururani","Ethan He","Jiahui Huang","Jacob Huffman","Pooya Jannaty","Jingyi Jin","Seung Wook Kim","Gergely Klár","Grace Lam","Shiyi Lan","Laura Leal-Taixe","Anqi Li","Zhaoshuo Li","Chen-Hsuan Lin","Tsung-Yi Lin","Huan Ling","Ming-Yu Liu","Xian Liu","Alice Luo","Qianli Ma","Hanzi Mao","Kaichun Mo","Arsalan Mousavian","Seungjun Nah","Sriharsha Niverty","David Page","Despoina Paschalidou","Zeeshan Patel","Lindsey Pavao","Morteza Ramezanali","Fitsum Reda","Xiaowei Ren","Vasanth Rao Naik Sabavat","Ed Schmerling","Stella Shi","Bartosz Stefaniak","Shitao Tang","Lyne Tchapmi","Przemek Tredak","Wei-Cheng Tseng","Jibin Varghese","Hao Wang","Haoxiang Wang","Heng Wang","Ting-Chun Wang","Fangyin Wei","Xinyue Wei","Jay Zhangjie Wu","Jiashu Xu","Wei Yang","Lin Yen-Chen","Xiaohui Zeng","Yu Zeng","Jing Zhang","Qinsheng Zhang","Yuxuan Zhang","Qingqing Zhao","Artur Zolkowski"],"pdf_url":"https://arxiv.org/pdf/2501.03575v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14421v1","updated":"2025-03-18T16:55:07Z","published":"2025-03-18T16:55:07Z","title":"ExDDV: A New Dataset for Explainable Deepfake Detection in Video","summary":"  The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.\n","authors":["Vlad Hondru","Eduard Hogea","Darian Onchis","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2503.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15686v2","updated":"2025-03-18T16:50:49Z","published":"2023-12-25T10:31:22Z","title":"PULASki: Learning inter-rater variability using statistical distances to\n  improve probabilistic segmentation","summary":"  In the domain of medical imaging, many supervised learning based methods for\nsegmentation face several challenges such as high variability in annotations\nfrom multiple experts, paucity of labelled data and class imbalanced datasets.\nThese issues may result in segmentations that lack the requisite precision for\nclinical analysis and can be misleadingly overconfident without associated\nuncertainty quantification. This work proposes the PULASki method as a\ncomputationally efficient generative tool for biomedical image segmentation\nthat accurately captures variability in expert annotations, even in small\ndatasets. This approach makes use of an improved loss function based on\nstatistical distances in a conditional variational autoencoder structure\n(Probabilistic UNet), which improves learning of the conditional decoder\ncompared to the standard cross-entropy particularly in class imbalanced\nproblems. The proposed method was analysed for two structurally different\nsegmentation tasks (intracranial vessel and multiple sclerosis (MS) lesion) and\ncompare our results to four well-established baselines in terms of quantitative\nmetrics and qualitative output. These experiments involve class-imbalanced\ndatasets characterised by challenging features, including suboptimal\nsignal-to-noise ratios and high ambiguity. Empirical results demonstrate the\nPULASKi method outperforms all baselines at the 5\\% significance level. Our\nexperiments are also of the first to present a comparative study of the\ncomputationally feasible segmentation of complex geometries using 3D patches\nand the traditional use of 2D slices. The generated segmentations are shown to\nbe much more anatomically plausible than in the 2D case, particularly for the\nvessel task.\n","authors":["Soumick Chatterjee","Franziska Gaidzik","Alessandro Sciarra","Hendrik Mattern","Gábor Janiga","Oliver Speck","Andreas Nürnberger","Sahani Pathiraja"],"pdf_url":"https://arxiv.org/pdf/2312.15686v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14405v1","updated":"2025-03-18T16:47:27Z","published":"2025-03-18T16:47:27Z","title":"DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D\n  Teachers","summary":"  Recent multi-teacher distillation methods have unified the encoders of\nmultiple foundation models into a single encoder, achieving competitive\nperformance on core vision tasks like classification, segmentation, and depth\nestimation. This led us to ask: Could similar success be achieved when the pool\nof teachers also includes vision models specialized in diverse tasks across\nboth 2D and 3D perception? In this paper, we define and investigate the problem\nof heterogeneous teacher distillation, or co-distillation, a challenging\nmulti-teacher distillation scenario where teacher models vary significantly in\nboth (a) their design objectives and (b) the data they were trained on. We\nexplore data-sharing strategies and teacher-specific encoding, and introduce\nDUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human\nperception. Our model achieves performance comparable to that of its larger\nteachers, sometimes even outperforming them, on their respective tasks.\nNotably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much\nsmaller encoder.\n","authors":["Mert Bulent Sariyildiz","Philippe Weinzaepfel","Thomas Lucas","Pau de Jorge","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2503.14405v1.pdf","comment":"Accepted to CVPR-2025. Project page:\n  https://europe.naverlabs.com/dune"},{"id":"http://arxiv.org/abs/2406.04746v2","updated":"2025-03-18T16:45:09Z","published":"2024-06-07T08:46:19Z","title":"PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance\n  Prediction","summary":"  Text-to-image generation has recently emerged as a viable alternative to\ntext-to-image retrieval, driven by the visually impressive results of\ngenerative diffusion models. Although query performance prediction is an active\nresearch topic in information retrieval, to the best of our knowledge, there is\nno prior study that analyzes the difficulty of queries (referred to as prompts)\nin text-to-image generation, based on human judgments. To this end, we\nintroduce the first dataset of prompts which are manually annotated in terms of\nimage generation performance. Additionally, we extend these evaluations to\ntext-to-image retrieval by collecting manual annotations that represent\nretrieval performance. We thus establish the first joint benchmark for prompt\nand query performance prediction (PQPP) across both tasks, comprising over 10K\nqueries. Our benchmark enables (i) the comparative assessment of prompt/query\ndifficulty in both image generation and image retrieval, and (ii) the\nevaluation of prompt/query performance predictors addressing both generation\nand retrieval. We evaluate several pre- and post-generation/retrieval\nperformance predictors, thus providing competitive baselines for future\nresearch. Our benchmark and code are publicly available at\nhttps://github.com/Eduard6421/PQPP.\n","authors":["Eduard Poesina","Adriana Valentina Costache","Adrian-Gabriel Chifu","Josiane Mothe","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2406.04746v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14402v1","updated":"2025-03-18T16:42:02Z","published":"2025-03-18T16:42:02Z","title":"Diffusion-based Facial Aesthetics Enhancement with 3D Structure Guidance","summary":"  Facial Aesthetics Enhancement (FAE) aims to improve facial attractiveness by\nadjusting the structure and appearance of a facial image while preserving its\nidentity as much as possible. Most existing methods adopted deep feature-based\nor score-based guidance for generation models to conduct FAE. Although these\nmethods achieved promising results, they potentially produced excessively\nbeautified results with lower identity consistency or insufficiently improved\nfacial attractiveness. To enhance facial aesthetics with less loss of identity,\nwe propose the Nearest Neighbor Structure Guidance based on Diffusion\n(NNSG-Diffusion), a diffusion-based FAE method that beautifies a 2D facial\nimage with 3D structure guidance. Specifically, we propose to extract FAE\nguidance from a nearest neighbor reference face. To allow for less change of\nfacial structures in the FAE process, a 3D face model is recovered by referring\nto both the matched 2D reference face and the 2D input face, so that the depth\nand contour guidance can be extracted from the 3D face model. Then the depth\nand contour clues can provide effective guidance to Stable Diffusion with\nControlNet for FAE. Extensive experiments demonstrate that our method is\nsuperior to previous relevant methods in enhancing facial aesthetics while\npreserving facial identity.\n","authors":["Lisha Li","Jingwen Hou","Weide Liu","Yuming Fang","Jiebin Yan"],"pdf_url":"https://arxiv.org/pdf/2503.14402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14395v1","updated":"2025-03-18T16:30:08Z","published":"2025-03-18T16:30:08Z","title":"Weakly Supervised Spatial Implicit Neural Representation Learning for 3D\n  MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy","summary":"  Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical\nfor real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We\npresent a weakly supervised spatial implicit neural representation (SINR)\nmethod to address modality differences and pelvic anatomy challenges.\n  Methods: The framework uses sparse surface supervision from MRI/US\nsegmentations instead of dense intensity matching. SINR models deformations as\ncontinuous spatial functions, with patient-specific surface priors guiding a\nstationary velocity field for biologically plausible deformations. Validation\nincluded 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases,\nevaluated via Dice similarity coefficient (DSC), mean surface distance (MSD),\nand 95% Hausdorff distance (HD95).\n  Results: The proposed method achieved robust registration. For the public\ndataset, prostate DSC was $0.93 \\pm 0.05$, MSD $0.87 \\pm 0.10$ mm, and HD95\n$1.58 \\pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC\n$0.88 \\pm 0.09$, MSD $1.21 \\pm 0.38$ mm, and HD95 $2.09 \\pm 1.48$ mm. Bladder\nand rectum performance was lower due to ultrasound's limited field of view.\nVisual assessments confirmed accurate alignment with minimal discrepancies.\n  Conclusion: This study introduces a novel weakly supervised SINR-based\napproach for 3D MRI-US deformable registration. By leveraging sparse surface\nsupervision and spatial priors, it achieves accurate, robust, and\ncomputationally efficient registration, enhancing real-time image guidance in\nHDR prostate brachytherapy and improving treatment precision.\n","authors":["Jing Wang","Ruirui Liu","Yu Lei","Michael J. Baine","Tian Liu","Yang Lei"],"pdf_url":"https://arxiv.org/pdf/2503.14395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10616v2","updated":"2025-03-18T16:12:19Z","published":"2025-03-13T17:56:10Z","title":"OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with\n  Transformer","summary":"  Open-vocabulary multiple object tracking aims to generalize trackers to\nunseen categories during training, enabling their application across a variety\nof real-world scenarios. However, the existing open-vocabulary tracker is\nconstrained by its framework structure, isolated frame-level perception, and\ninsufficient modal interactions, which hinder its performance in\nopen-vocabulary classification and tracking. In this paper, we propose OVTR\n(End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the\nfirst end-to-end open-vocabulary tracker that models motion, appearance, and\ncategory simultaneously. To achieve stable classification and continuous\ntracking, we design the CIP (Category Information Propagation) strategy, which\nestablishes multiple high-level category information priors for subsequent\nframes. Additionally, we introduce a dual-branch structure for generalization\ncapability and deep multimodal interaction, and incorporate protective\nstrategies in the decoder to enhance performance. Experimental results show\nthat our method surpasses previous trackers on the open-vocabulary MOT\nbenchmark while also achieving faster inference speeds and significantly\nreducing preprocessing requirements. Moreover, the experiment transferring the\nmodel to another dataset demonstrates its strong adaptability. Models and code\nare released at https://github.com/jinyanglii/OVTR.\n","authors":["Jinyang Li","En Yu","Sijia Chen","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2503.10616v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14378v1","updated":"2025-03-18T16:10:24Z","published":"2025-03-18T16:10:24Z","title":"Impossible Videos","summary":"  Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models.\n","authors":["Zechen Bai","Hai Ci","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2503.14378v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2503.14377v1","updated":"2025-03-18T16:10:11Z","published":"2025-03-18T16:10:11Z","title":"Advancing Medical Representation Learning Through High-Quality Data","summary":"  Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase.\n","authors":["Negin Baghbanzadeh","Adibvafa Fallahpour","Yasaman Parhizkar","Franklin Ogidi","Shuvendu Roy","Sajad Ashkezari","Vahid Reza Khazaie","Michael Colacci","Ali Etemad","Arash Afkanpour","Elham Dolatabadi"],"pdf_url":"https://arxiv.org/pdf/2503.14377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00513v2","updated":"2025-03-18T16:01:24Z","published":"2024-12-31T15:53:50Z","title":"CaReBench: A Fine-Grained Benchmark for Video Captioning and Retrieval","summary":"  Video understanding, including video captioning and retrieval, is still a\ngreat challenge for video-language models (VLMs). The existing video retrieval\nand caption benchmarks only include short descriptions, limits their ability of\ndetailed video understanding evaluation. To address this problem, we present\nCaReBench, a testing benchmark for fine-grained video captioning and retrieval\nwith 1,000 high-quality pairs of videos and human-annotated detailed captions.\nUniquely, it provides manually separated spatial annotations and temporal\nannotations for each video. Based on this design, we introduce two evaluation\nmetrics, ReBias and CapST, specifically tailored for video retrieval and video\ncaptioning tasks, respectively. These metrics enable a comprehensive\ninvestigation into the spatial and temporal biases inherent in VLMs. In\naddition, to handle both video retrieval and video captioning tasks in a\nunified framework, we develop a simple baseline based on a Multimodal Language\nModel (MLLM). By implementing a two-stage Supervised Fine-Tuning (SFT), we\nfully unlock the potential of MLLM, enabling it not only to generate detailed\nvideo descriptions but also to extract video features. Surprisingly,\nexperimental results demonstrate that, compared to the CLIP-based models\ndesigned for retrieval and the popular MLLMs skilled in video captioning, our\nbaseline shows competitive performance in both fine-grained video retrieval and\nvideo detailed captioning.\n","authors":["Yifan Xu","Xinhao Li","Yichun Yang","Desen Meng","Rui Huang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.00513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17273v3","updated":"2025-03-18T15:56:39Z","published":"2024-09-25T18:38:57Z","title":"Targeted Neural Architectures in Multi-Objective Frameworks for Complete\n  Glioma Characterization from Multimodal MRI","summary":"  Brain tumors result from abnormal cell growth in brain tissue. If\nundiagnosed, they cause neurological deficits, including cognitive impairment,\nmotor dysfunction, and sensory loss. As tumors grow, intracranial pressure\nincreases, potentially leading to fatal complications such as brain herniation.\nEarly diagnosis and treatment are crucial to controlling these effects and\nslowing tumor progression. Deep learning (DL) and artificial intelligence (AI)\nare increasingly used to assist doctors in early diagnosis through magnetic\nresonance imaging (MRI) scans. Our research proposes targeted neural\narchitectures within multi-objective frameworks that can localize, segment, and\nclassify the grade of these gliomas from multimodal MRI images to solve this\ncritical issue. Our localization framework utilizes a targeted architecture\nthat enhances the LinkNet framework with an encoder inspired by VGG19 for\nbetter multimodal feature extraction from the tumor along with spatial and\ngraph attention mechanisms that sharpen feature focus and inter-feature\nrelationships. For the segmentation objective, we deployed a specialized\nframework using the SeResNet101 CNN model as the encoder backbone integrated\ninto the LinkNet architecture, achieving an IoU Score of 96%. The\nclassification objective is addressed through a distinct framework implemented\nby combining the SeResNet152 feature extractor with Adaptive Boosting\nclassifier, reaching an accuracy of 98.53%. Our multi-objective approach with\ntargeted neural architectures demonstrated promising results for complete\nglioma characterization, with the potential to advance medical AI by enabling\nearly diagnosis and providing more accurate treatment options for patients.\n","authors":["Shravan Venkatraman","Pandiyaraju V","Abeshek A","Aravintakshan S A","Pavan Kumar S","Kannan A","Madhan S"],"pdf_url":"https://arxiv.org/pdf/2409.17273v3.pdf","comment":"29 pages, 25 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.09101v2","updated":"2025-03-18T15:48:38Z","published":"2025-03-12T06:37:43Z","title":"The Shape of Attraction in UMAP: Exploring the Embedding Forces in\n  Dimensionality Reduction","summary":"  Uniform manifold approximation and projection (UMAP) is among the most\npopular neighbor embedding methods. The method relies on attractive and\nrepulsive forces among high-dimensional data points to obtain a low-dimensional\nembedding. In this paper, we analyze the forces to reveal their effects on\ncluster formations and visualization. Repulsion emphasizes differences,\ncontrolling cluster boundaries and inter-cluster distance. Attraction is more\nsubtle, as attractive tension between points can manifest simultaneously as\nattraction and repulsion in the lower-dimensional mapping. This explains the\nneed for learning rate annealing and motivates the different treatments between\nattractive and repulsive terms. Moreover, by modifying attraction, we improve\nthe consistency of cluster formation under random initialization. Overall, our\nanalysis makes UMAP and similar embedding methods more interpretable, more\nrobust, and more accurate.\n","authors":["Mohammad Tariqul Islam","Jason W. Fleischer"],"pdf_url":"https://arxiv.org/pdf/2503.09101v2.pdf","comment":"9 page + appendix"},{"id":"http://arxiv.org/abs/2503.14359v1","updated":"2025-03-18T15:42:22Z","published":"2025-03-18T15:42:22Z","title":"ImViD: Immersive Volumetric Videos for Enhanced VR Engagement","summary":"  User engagement is greatly enhanced by fully immersive multi-modal\nexperiences that combine visual and auditory stimuli. Consequently, the next\nfrontier in VR/AR technologies lies in immersive volumetric videos with\ncomplete scene capture, large 6-DoF interaction space, multi-modal feedback,\nand high resolution & frame-rate contents. To stimulate the reconstruction of\nimmersive volumetric videos, we introduce ImViD, a multi-view, multi-modal\ndataset featuring complete space-oriented data capture and various\nindoor/outdoor scenarios. Our capture rig supports multi-view video-audio\ncapture while on the move, a capability absent in existing datasets,\nsignificantly enhancing the completeness, flexibility, and efficiency of data\ncapture.\n  The captured multi-view videos (with synchronized audios) are in 5K\nresolution at 60FPS, lasting from 1-5 minutes, and include rich\nforeground-background elements, and complex dynamics. We benchmark existing\nmethods using our dataset and establish a base pipeline for constructing\nimmersive volumetric videos from multi-view audiovisual inputs for 6-DoF\nmulti-modal immersive VR experiences. The benchmark and the reconstruction and\ninteraction results demonstrate the effectiveness of our dataset and baseline\nmethod, which we believe will stimulate future research on immersive volumetric\nvideo production.\n","authors":["Zhengxian Yang","Shi Pan","Shengqi Wang","Haoxiang Wang","Li Lin","Guanjun Li","Zhengqi Wen","Borong Lin","Jianhua Tao","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14359v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14358v1","updated":"2025-03-18T15:41:45Z","published":"2025-03-18T15:41:45Z","title":"RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image\n  Alignment","summary":"  Rectified Flow (RF) models trained with a Flow matching framework have\nachieved state-of-the-art performance on Text-to-Image (T2I) conditional\ngeneration. Yet, multiple benchmarks show that synthetic images can still\nsuffer from poor alignment with the prompt, i.e., images show wrong attribute\nbinding, subject positioning, numeracy, etc. While the literature offers many\nmethods to improve T2I alignment, they all consider only Diffusion Models, and\nrequire auxiliary datasets, scoring models, and linguistic analysis of the\nprompt. In this paper we aim to address these gaps. First, we introduce RFMI, a\nnovel Mutual Information (MI) estimator for RF models that uses the pre-trained\nmodel itself for the MI estimation. Then, we investigate a self-supervised\nfine-tuning approach for T2I alignment based on RFMI that does not require\nauxiliary information other than the pre-trained model itself. Specifically, a\nfine-tuning set is constructed by selecting synthetic images generated from the\npre-trained RF model and having high point-wise MI between images and prompts.\nOur experiments on MI estimation benchmarks demonstrate the validity of RFMI,\nand empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI\nfor improving T2I alignment while maintaining image quality.\n","authors":["Chao Wang","Giulio Franzese","Alessandro Finamore","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2503.14358v1.pdf","comment":"to appear at ICLR 2025 Workshop on Deep Generative Model in Machine\n  Learning: Theory, Principle and Efficacy"},{"id":"http://arxiv.org/abs/2503.14355v1","updated":"2025-03-18T15:39:44Z","published":"2025-03-18T15:39:44Z","title":"MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of\n  Pan-Tumors with Knowledge-Driven Prompts","summary":"  Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy.\n","authors":["Runqi Meng","Sifan Song","Pengfei Jin","Yujin Oh","Lin Teng","Yulin Wang","Yiqun Sun","Ling Chen","Xiang Li","Quanzheng Li","Ning Guo","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2503.14355v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.14354v1","updated":"2025-03-18T15:38:37Z","published":"2025-03-18T15:38:37Z","title":"Retrospective: A CORDIC Based Configurable Activation Function for NN\n  Applications","summary":"  A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%.\n","authors":["Omkar Kokane","Gopal Raut","Salim Ullah","Mukul Lokhande","Adam Teman","Akash Kumar","Santosh Kumar Vishvakarma"],"pdf_url":"https://arxiv.org/pdf/2503.14354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14350v1","updated":"2025-03-18T15:31:12Z","published":"2025-03-18T15:31:12Z","title":"VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded\n  Generation","summary":"  Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.\n","authors":["Shoubin Yu","Difan Liu","Ziqiao Ma","Yicong Hong","Yang Zhou","Hao Tan","Joyce Chai","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2503.14350v1.pdf","comment":"First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/"},{"id":"http://arxiv.org/abs/2501.01023v2","updated":"2025-03-18T15:30:22Z","published":"2025-01-02T02:51:16Z","title":"Hadamard Attention Recurrent Transformer: A Strong Baseline for Stereo\n  Matching Transformer","summary":"  In light of the advancements in transformer technology, extant research\nposits the construction of stereo transformers as a potential solution to the\nbinocular stereo matching challenge. However, constrained by the low-rank\nbottleneck and quadratic complexity of attention mechanisms, stereo\ntransformers still fail to demonstrate sufficient nonlinear expressiveness\nwithin a reasonable inference time. The lack of focus on key homonymous points\nrenders the representations of such methods vulnerable to challenging\nconditions, including reflections and weak textures. Furthermore, a slow\ncomputing speed is not conducive to the application. To overcome these\ndifficulties, we present the Hadamard Attention Recurrent Stereo Transformer\n(HART) that incorporates the following components: 1) For faster inference, we\npresent a Hadamard product paradigm for the attention mechanism, achieving\nlinear computational complexity. 2) We designed a Dense Attention Kernel (DAK)\nto amplify the differences between relevant and irrelevant feature responses.\nThis allows HART to focus on important details. DAK also converts zero elements\nto non-zero elements to mitigate the reduced expressiveness caused by the\nlow-rank bottleneck. 3) To compensate for the spatial and channel interaction\nmissing in the Hadamard product, we propose MKOI to capture both global and\nlocal information through the interleaving of large and small kernel\nconvolutions. Experimental results demonstrate the effectiveness of our HART.\nIn reflective area, HART ranked 1st on the KITTI 2012 benchmark among all\npublished methods at the time of submission. Code is available at\nhttps://github.com/ZYangChen/HART.\n","authors":["Ziyang Chen","Yongjun Zhang","Wenting Li","Bingshu Wang","Yabo Wu","Yong Zhao","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2501.01023v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14346v1","updated":"2025-03-18T15:25:38Z","published":"2025-03-18T15:25:38Z","title":"3D Densification for Multi-Map Monocular VSLAM in Endoscopy","summary":"  Multi-map Sparse Monocular visual Simultaneous Localization and Mapping\napplied to monocular endoscopic sequences has proven efficient to robustly\nrecover tracking after the frequent losses in endoscopy due to motion blur,\ntemporal occlusion, tools interaction or water jets. The sparse multi-maps are\nadequate for robust camera localization, however they are very poor for\nenvironment representation, they are noisy, with a high percentage of\ninaccurately reconstructed 3D points, including significant outliers, and more\nimportantly with an unacceptable low density for clinical applications.\n  We propose a method to remove outliers and densify the maps of the state of\nthe art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for\nup-to-scale depth dense predictions are aligned with the sparse CudaSIFT\nsubmaps by means of the robust to spurious LMedS. Our system mitigates the\ninherent scale ambiguity in monocular depth estimation while filtering\noutliers, leading to reliable densified 3D maps.\n  We provide experimental evidence of accurate densified maps 4.15 mm RMS\naccuracy at affordable computing time in the C3VD phantom colon dataset. We\nreport qualitative results on the real colonoscopy from the Endomapper dataset.\n","authors":["X. Anadón","Javier Rodríguez-Puigvert","J. M. M. Montiel"],"pdf_url":"https://arxiv.org/pdf/2503.14346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14343v1","updated":"2025-03-18T15:23:52Z","published":"2025-03-18T15:23:52Z","title":"Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image\n  Segmentation","summary":"  Medical image segmentation aims to identify anatomical structures at the\nvoxel-level. Segmentation accuracy relies on distinguishing voxel differences.\nCompared to advancements achieved in studies of the inter-class variance, the\nintra-class variance receives less attention. Moreover, traditional linear\nclassifiers, limited by a single learnable weight per class, struggle to\ncapture this finer distinction. To address the above challenges, we propose a\nMulti-Prototype-based Embedding Refinement method for semi-supervised medical\nimage segmentation. Specifically, we design a multi-prototype-based\nclassification strategy, rethinking the segmentation from the perspective of\nstructural relationships between voxel embeddings. The intra-class variations\nare explored by clustering voxels along the distribution of multiple prototypes\nin each class. Next, we introduce a consistency constraint to alleviate the\nlimitation of linear classifiers. This constraint integrates different\nclassification granularities from a linear classifier and the proposed\nprototype-based classifier. In the thorough evaluation on two popular\nbenchmarks, our method achieves superior performance compared with\nstate-of-the-art methods. Code is available at\nhttps://github.com/Briley-byl123/MPER.\n","authors":["Yali Bi","Enyu Che","Yinan Chen","Yuanpeng He","Jingwei Qu"],"pdf_url":"https://arxiv.org/pdf/2503.14343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16657v3","updated":"2025-03-18T15:19:15Z","published":"2024-11-25T18:41:56Z","title":"DreamRunner: Fine-Grained Compositional Story-to-Video Generation with\n  Retrieval-Augmented Motion Adaptation","summary":"  Storytelling video generation (SVG) aims to produce coherent and visually\nrich multi-scene videos that follow a structured narrative. Existing methods\nprimarily employ LLM for high-level planning to decompose a story into\nscene-level descriptions, which are then independently generated and stitched\ntogether. However, these approaches struggle with generating high-quality\nvideos aligned with the complex single-scene description, as visualizing such\ncomplex description involves coherent composition of multiple characters and\nevents, complex motion synthesis and muti-character customization. To address\nthese challenges, we propose DreamRunner, a novel story-to-video generation\nmethod: First, we structure the input script using a large language model (LLM)\nto facilitate both coarse-grained scene planning as well as fine-grained\nobject-level layout and motion planning. Next, DreamRunner presents\nretrieval-augmented test-time adaptation to capture target motion priors for\nobjects in each scene, supporting diverse motion customization based on\nretrieved videos, thus facilitating the generation of new videos with complex,\nscripted motions. Lastly, we propose a novel spatial-temporal region-based 3D\nattention and prior injection module SR3AI for fine-grained object-motion\nbinding and frame-by-frame semantic control. We compare DreamRunner with\nvarious SVG baselines, demonstrating state-of-the-art performance in character\nconsistency, text alignment, and smooth transitions. Additionally, DreamRunner\nexhibits strong fine-grained condition-following ability in compositional\ntext-to-video generation, significantly outperforming baselines on\nT2V-ComBench. Finally, we validate DreamRunner's robust ability to generate\nmulti-object interactions with qualitative examples.\n","authors":["Zun Wang","Jialu Li","Han Lin","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.16657v3.pdf","comment":"Project website: https://zunwang1.github.io/DreamRunner"},{"id":"http://arxiv.org/abs/2503.14331v1","updated":"2025-03-18T15:03:28Z","published":"2025-03-18T15:03:28Z","title":"ADAPT: An Autonomous Forklift for Construction Site Operation","summary":"  Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of the Autonomous Dynamic All-terrain Pallet\nTransporter (ADAPT), a fully autonomous off-road forklift designed for\nconstruction environments. Unlike structured warehouse settings, construction\nsites pose significant challenges, including dynamic obstacles, unstructured\nterrain, and varying weather conditions. To address these challenges, our\nsystem integrates AI-driven perception techniques with traditional approaches\nfor decision making, planning, and control, enabling reliable operation in\ncomplex environments. We validate the system through extensive real-world\ntesting, comparing its long-term performance against an experienced human\noperator across various weather conditions. We also provide a comprehensive\nanalysis of challenges and key lessons learned, contributing to the advancement\nof autonomous heavy machinery. Our findings demonstrate that autonomous outdoor\nforklifts can operate near human-level performance, offering a viable path\ntoward safer and more efficient construction logistics.\n","authors":["Johannes Huemer","Markus Murschitz","Matthias Schörghuber","Lukas Reisinger","Thomas Kadiofsky","Christoph Weidinger","Mario Niedermeyer","Benedikt Widy","Marcel Zeilinger","Csaba Beleznai","Tobias Glück","Andreas Kugi","Patrik Zips"],"pdf_url":"https://arxiv.org/pdf/2503.14331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14329v1","updated":"2025-03-18T15:01:47Z","published":"2025-03-18T15:01:47Z","title":"EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference\n  Alignment","summary":"  Dexterous robotic hands often struggle to generalize effectively in complex\nenvironments due to the limitations of models trained on low-diversity data.\nHowever, the real world presents an inherently unbounded range of scenarios,\nmaking it impractical to account for every possible variation. A natural\nsolution is to enable robots learning from experience in complex environments,\nan approach akin to evolution, where systems improve through continuous\nfeedback, learning from both failures and successes, and iterating toward\noptimal performance. Motivated by this, we propose EvolvingGrasp, an\nevolutionary grasp generation method that continuously enhances grasping\nperformance through efficient preference alignment. Specifically, we introduce\nHandpose wise Preference Optimization (HPO), which allows the model to\ncontinuously align with preferences from both positive and negative feedback\nwhile progressively refining its grasping strategies. To further enhance\nefficiency and reliability during online adjustments, we incorporate a\nPhysics-aware Consistency Model within HPO, which accelerates inference,\nreduces the number of timesteps needed for preference finetuning, and ensures\nphysical plausibility throughout the process. Extensive experiments across four\nbenchmark datasets demonstrate state of the art performance of our method in\ngrasp success rate and sampling efficiency. Our results validate that\nEvolvingGrasp enables evolutionary grasp generation, ensuring robust,\nphysically feasible, and preference-aligned grasping in both simulation and\nreal scenarios.\n","authors":["Yufei Zhu","Yiming Zhong","Zemin Yang","Peishan Cong","Jingyi Yu","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2503.14329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14325v1","updated":"2025-03-18T14:58:59Z","published":"2025-03-18T14:58:59Z","title":"LeanVAE: An Ultra-Efficient Reconstruction VAE for Video Diffusion\n  Models","summary":"  Recent advances in Latent Video Diffusion Models (LVDMs) have revolutionized\nvideo generation by leveraging Video Variational Autoencoders (Video VAEs) to\ncompress intricate video data into a compact latent space.However, as LVDM\ntraining scales, the computational overhead of Video VAEs becomes a critical\nbottleneck, particularly for encoding high-resolution videos. To address this,\nwe propose LeanVAE, a novel and ultra-efficient Video VAE framework that\nintroduces two key innovations: (1) a lightweight architecture based on a\nNeighborhood-Aware Feedforward (NAF) module and non-overlapping patch\noperations, drastically reducing computational cost, and (2) the integration of\nwavelet transforms and compressed sensing techniques to enhance reconstruction\nquality. Extensive experiments validate LeanVAE's superiority in video\nreconstruction and generation, particularly in enhancing efficiency over\nexisting Video VAEs.Our model offers up to 50x fewer FLOPs and 44x faster\ninference speed while maintaining competitive reconstruction quality, providing\ninsights for scalable, efficient video generation.Our models and code are\navailable at https://github.com/westlake-repl/LeanVAE.\n","authors":["Yu Cheng","Fajie Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.14325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18414v2","updated":"2025-03-18T14:57:30Z","published":"2024-06-26T15:09:54Z","title":"BiTrack: Bidirectional Offline 3D Multi-Object Tracking Using\n  Camera-LiDAR Data","summary":"  Compared with real-time multi-object tracking (MOT), offline multi-object\ntracking (OMOT) has the advantages to perform 2D-3D detection fusion, erroneous\nlink correction, and full track optimization but has to deal with the\nchallenges from bounding box misalignment and track evaluation, editing, and\nrefinement. This paper proposes \"BiTrack\", a 3D OMOT framework that includes\nmodules of 2D-3D detection fusion, initial trajectory generation, and\nbidirectional trajectory re-optimization to achieve optimal tracking results\nfrom camera-LiDAR data. The novelty of this paper includes threefold: (1)\ndevelopment of a point-level object registration technique that employs a\ndensity-based similarity metric to achieve accurate fusion of 2D-3D detection\nresults; (2) development of a set of data association and track management\nskills that utilizes a vertex-based similarity metric as well as false alarm\nrejection and track recovery mechanisms to generate reliable bidirectional\nobject trajectories; (3) development of a trajectory re-optimization scheme\nthat re-organizes track fragments of different fidelities in a greedy fashion,\nas well as refines each trajectory with completion and smoothing techniques.\nThe experiment results on the KITTI dataset demonstrate that BiTrack achieves\nthe state-of-the-art performance for 3D OMOT tasks in terms of accuracy and\nefficiency.\n","authors":["Kemiao Huang","Yinqi Chen","Meiying Zhang","Qi Hao"],"pdf_url":"https://arxiv.org/pdf/2406.18414v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14324v1","updated":"2025-03-18T14:56:46Z","published":"2025-03-18T14:56:46Z","title":"DualToken: Towards Unifying Visual Understanding and Generation with\n  Dual Visual Vocabularies","summary":"  The differing representation spaces required for visual understanding and\ngeneration pose a challenge in unifying them within the autoregressive paradigm\nof large language models. A vision tokenizer trained for reconstruction excels\nat capturing low-level perceptual details, making it well-suited for visual\ngeneration but lacking high-level semantic representations for understanding\ntasks. Conversely, a vision encoder trained via contrastive learning aligns\nwell with language but struggles to decode back into the pixel space for\ngeneration tasks. To bridge this gap, we propose DualToken, a method that\nunifies representations for both understanding and generation within a single\ntokenizer. However, directly integrating reconstruction and semantic objectives\nin a single tokenizer creates conflicts, leading to degraded performance in\nboth reconstruction quality and semantic performance. Instead of forcing a\nsingle codebook to handle both semantic and perceptual information, DualToken\ndisentangles them by introducing separate codebooks for high and low-level\nfeatures, effectively transforming their inherent conflict into a synergistic\nrelationship. As a result, DualToken achieves state-of-the-art performance in\nboth reconstruction and semantic tasks while demonstrating remarkable\neffectiveness in downstream MLLM understanding and generation tasks. Notably,\nwe also show that DualToken, as a unified tokenizer, surpasses the naive\ncombination of two distinct types vision encoders, providing superior\nperformance within a unified MLLM.\n","authors":["Wei Song","Yuran Wang","Zijia Song","Yadong Li","Haoze Sun","Weipeng Chen","Zenan Zhou","Jianhua Xu","Jiaqi Wang","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14304v2","updated":"2025-03-18T14:54:28Z","published":"2024-05-23T08:24:22Z","title":"Bracket Diffusion: HDR Image Generation by Consistent LDR Denoising","summary":"  We demonstrate generating HDR images using the concerted action of multiple\nblack-box, pre-trained LDR image diffusion models. Relying on a pre-trained LDR\ngenerative diffusion models is vital as, first, there is no sufficiently large\nHDR image dataset available to re-train them, and, second, even if it was,\nre-training such models is impossible for most compute budgets. Instead, we\nseek inspiration from the HDR image capture literature that traditionally fuses\nsets of LDR images, called \"exposure brackets'', to produce a single HDR image.\nWe operate multiple denoising processes to generate multiple LDR brackets that\ntogether form a valid HDR result. The key to making this work is to introduce a\nconsistency term into the diffusion process to couple the brackets such that\nthey agree across the exposure range they share while accounting for possible\ndifferences due to the quantization error. We demonstrate state-of-the-art\nunconditional and conditional or restoration-type (LDR2HDR) generative modeling\nresults, yet in HDR.\n","authors":["Mojtaba Bemana","Thomas Leimkühler","Karol Myszkowski","Hans-Peter Seidel","Tobias Ritschel"],"pdf_url":"https://arxiv.org/pdf/2405.14304v2.pdf","comment":"11 pages, 14 figures, Accepted to Eurographics 2025, see\n  https://bracketdiffusion.mpi-inf.mpg.de"},{"id":"http://arxiv.org/abs/2312.02167v2","updated":"2025-03-18T14:50:51Z","published":"2023-10-30T13:44:55Z","title":"Uncertainty Quantification in Machine Learning Based Segmentation: A\n  Post-Hoc Approach for Left Ventricle Volume Estimation in MRI","summary":"  Recent studies have confirmed cardiovascular diseases remain responsible for\nhighest death toll amongst non-communicable diseases. Accurate left ventricular\n(LV) volume estimation is critical for valid diagnosis and management of\nvarious cardiovascular conditions, but poses significant challenge due to\ninherent uncertainties associated with segmentation algorithms in magnetic\nresonance imaging (MRI). Recent machine learning advancements, particularly\nU-Net-like convolutional networks, have facilitated automated segmentation for\nmedical images, but struggles under certain pathologies and/or different\nscanner vendors and imaging protocols. This study proposes a novel methodology\nfor post-hoc uncertainty estimation in LV volume prediction using It\\^{o}\nstochastic differential equations (SDEs) to model path-wise behavior for the\nprediction error. The model describes the area of the left ventricle along the\nheart's long axis. The method is agnostic to the underlying segmentation\nalgorithm, facilitating its use with various existing and future segmentation\ntechnologies. The proposed approach provides a mechanism for quantifying\nuncertainty, enabling medical professionals to intervene for unreliable\npredictions. This is of utmost importance in critical applications such as\nmedical diagnosis, where prediction accuracy and reliability can directly\nimpact patient outcomes. The method is also robust to dataset changes, enabling\napplication for medical centers with limited access to labeled data. Our\nfindings highlight the proposed uncertainty estimation methodology's potential\nto enhance automated segmentation robustness and generalizability, paving the\nway for more reliable and accurate LV volume estimation in clinical settings as\nwell as opening new avenues for uncertainty quantification in biomedical image\nsegmentation, providing promising directions for future research.\n","authors":["F. Terhag","P. Knechtges","A. Basermann","R. Tempone"],"pdf_url":"https://arxiv.org/pdf/2312.02167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14304v1","updated":"2025-03-18T14:45:05Z","published":"2025-03-18T14:45:05Z","title":"RoMedFormer: A Rotary-Embedding Transformer Foundation Model for 3D\n  Genito-Pelvic Structure Segmentation in MRI and CT","summary":"  Deep learning-based segmentation of genito-pelvic structures in MRI and CT is\ncrucial for applications such as radiation therapy, surgical planning, and\ndisease diagnosis. However, existing segmentation models often struggle with\ngeneralizability across imaging modalities, and anatomical variations. In this\nwork, we propose RoMedFormer, a rotary-embedding transformer-based foundation\nmodel designed for 3D female genito-pelvic structure segmentation in both MRI\nand CT. RoMedFormer leverages self-supervised learning and rotary positional\nembeddings to enhance spatial feature representation and capture long-range\ndependencies in 3D medical data. We pre-train our model using a diverse dataset\nof 3D MRI and CT scans and fine-tune it for downstream segmentation tasks.\nExperimental results demonstrate that RoMedFormer achieves superior performance\nsegmenting genito-pelvic organs. Our findings highlight the potential of\ntransformer-based architectures in medical image segmentation and pave the way\nfor more transferable segmentation frameworks.\n","authors":["Yuheng Li","Mingzhe Hu","Richard L. J. Qiu","Maria Thor","Andre Williams","Deborah Marshall","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2503.14304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08344v4","updated":"2025-03-18T14:41:58Z","published":"2024-12-11T12:34:37Z","title":"CoDTS: Enhancing Sparsely Supervised Collaborative Perception with a\n  Dual Teacher-Student Framework","summary":"  Current collaborative perception methods often rely on fully annotated\ndatasets, which can be expensive to obtain in practical situations. To reduce\nannotation costs, some works adopt sparsely supervised learning techniques and\ngenerate pseudo labels for the missing instances. However, these methods fail\nto achieve an optimal confidence threshold that harmonizes the quality and\nquantity of pseudo labels. To address this issue, we propose an end-to-end\nCollaborative perception Dual Teacher-Student framework (CoDTS), which employs\nadaptive complementary learning to produce both high-quality and high-quantity\npseudo labels. Specifically, the Main Foreground Mining (MFM) module generates\nhigh-quality pseudo labels based on the prediction of the static teacher.\nSubsequently, the Supplement Foreground Mining (SFM) module ensures a balance\nbetween the quality and quantity of pseudo labels by adaptively identifying\nmissing instances based on the prediction of the dynamic teacher. Additionally,\nthe Neighbor Anchor Sampling (NAS) module is incorporated to enhance the\nrepresentation of pseudo labels. To promote the adaptive complementary\nlearning, we implement a staged training strategy that trains the student and\ndynamic teacher in a mutually beneficial manner. Extensive experiments\ndemonstrate that the CoDTS effectively ensures an optimal balance of pseudo\nlabels in both quality and quantity, establishing a new state-of-the-art in\nsparsely supervised collaborative perception. The code is available at\nhttps://github.com/CatOneTwo/CoDTS.\n","authors":["Yushan Han","Hui Zhang","Honglei Zhang","Jing Wang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2412.08344v4.pdf","comment":"AAAI 2025 (Oral)"},{"id":"http://arxiv.org/abs/2503.14295v1","updated":"2025-03-18T14:35:48Z","published":"2025-03-18T14:35:48Z","title":"PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face\n  Generation","summary":"  Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.\n","authors":["Baiqin Wang","Xiangyu Zhu","Fan Shen","Hao Xu","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2503.14295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18770v2","updated":"2025-03-18T14:32:07Z","published":"2024-05-29T05:20:02Z","title":"Multimodal Adversarial Defense for Vision-Language Models by Leveraging\n  One-To-Many Relationships","summary":"  Pre-trained vision-language (VL) models are highly vulnerable to adversarial\nattacks. However, existing defense methods primarily focus on image\nclassification, overlooking two key aspects of VL tasks: multimodal attacks,\nwhere both image and text can be perturbed, and the one-to-many relationship of\nimages and texts, where a single image can correspond to multiple textual\ndescriptions and vice versa (1:N and N:1). This work is the first to explore\ndefense strategies against multimodal attacks in VL tasks, whereas prior VL\ndefense methods focus on vision robustness. We propose multimodal adversarial\ntraining (MAT), which incorporates adversarial perturbations in both image and\ntext modalities during training, significantly outperforming existing unimodal\ndefenses. Furthermore, we discover that MAT is limited by deterministic\none-to-one (1:1) image-text pairs in VL training data. To address this, we\nconduct a comprehensive study on leveraging one-to-many relationships to\nenhance robustness, investigating diverse augmentation techniques. Our analysis\nshows that, for a more effective defense, augmented image-text pairs should be\nwell-aligned, diverse, yet avoid distribution shift -- conditions overlooked by\nprior research. Our experiments show that MAT can effectively be applied to\ndifferent VL models and tasks to improve adversarial robustness, outperforming\nprevious efforts. Our code will be made public upon acceptance.\n","authors":["Futa Waseda","Antonio Tejero-de-Pablos","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2405.18770v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.19036v2","updated":"2025-03-18T14:31:52Z","published":"2024-11-28T10:31:59Z","title":"PCDreamer: Point Cloud Completion Through Multi-view Diffusion Priors","summary":"  This paper presents PCDreamer, a novel method for point cloud completion.\nTraditional methods typically extract features from partial point clouds to\npredict missing regions, but the large solution space often leads to\nunsatisfactory results. More recent approaches have started to use images as\nextra guidance, effectively improving performance, but obtaining paired data of\nimages and partial point clouds is challenging in practice. To overcome these\nlimitations, we harness the relatively view-consistent multi-view diffusion\npriors within large models, to generate novel views of the desired shape. The\nresulting image set encodes both global and local shape cues, which are\nespecially beneficial for shape completion. To fully exploit the priors, we\nhave designed a shape fusion module for producing an initial complete shape\nfrom multi-modality input (i.e.,, images and point clouds), and a follow-up\nshape consolidation module to obtain the final complete shape by discarding\nunreliable points introduced by the inconsistency from diffusion priors.\nExtensive experimental results demonstrate our superior performance, especially\nin recovering fine details.\n","authors":["Guangshun Wei","Yuan Feng","Long Ma","Chen Wang","Yuanfeng Zhou","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2411.19036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03192v2","updated":"2025-03-18T14:28:57Z","published":"2024-12-04T10:25:53Z","title":"Biologically-inspired Semi-supervised Semantic Segmentation for\n  Biomedical Imaging","summary":"  We propose a novel bio-inspired semi-supervised learning approach for\ntraining downsampling-upsampling semantic segmentation architectures. The first\nstage does not use backpropagation. Rather, it exploits the Hebbian principle\n``fire together, wire together'' as a local learning rule for updating the\nweights of both convolutional and transpose-convolutional layers, allowing\nunsupervised discovery of data features. In the second stage, the model is\nfine-tuned with standard backpropagation on a small subset of labeled data. We\nevaluate our methodology through experiments conducted on several widely used\nbiomedical datasets, deeming that this domain is paramount in computer vision\nand is notably impacted by data scarcity. Results show that our proposed method\noutperforms SOTA approaches across different levels of label availability.\nFurthermore, we show that using our unsupervised stage to initialize the SOTA\napproaches leads to performance improvements. The code to replicate our\nexperiments can be found at\nhttps://github.com/ciampluca/hebbian-bootstraping-semi-supervised-medical-imaging\n","authors":["Luca Ciampi","Gabriele Lagani","Giuseppe Amato","Fabrizio Falchi"],"pdf_url":"https://arxiv.org/pdf/2412.03192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05839v2","updated":"2025-03-18T14:21:33Z","published":"2025-03-06T23:54:40Z","title":"Enhancing AUTOSAR-Based Firmware Over-the-Air Updates in the Automotive\n  Industry with a Practical Implementation on a Steering System","summary":"  The automotive industry is increasingly reliant on software to manage complex\nvehicle functionalities, making efficient and secure firmware updates\nessential. Traditional firmware update methods, requiring physical connections\nthrough On-Board Diagnostics (OBD) ports, are inconvenient, costly, and\ntime-consuming. Firmware Over-the-Air (FOTA) technology offers a revolutionary\nsolution by enabling wireless updates, reducing operational costs, and\nenhancing the user experience. This project aims to design and implement an\nadvanced FOTA system tailored for modern vehicles, incorporating the AUTOSAR\narchitecture for scalability and standardization, and utilizing delta updating\nto minimize firmware update sizes, thereby improving bandwidth efficiency and\nreducing flashing times. To ensure security, the system integrates the UDS 0x27\nprotocol for authentication and data integrity during the update process.\nCommunication between Electronic Control Units (ECUs) is achieved using the CAN\nprotocol, while the ESP8266 module and the master ECU communicate via SPI for\ndata transfer. The system's architecture includes key components such as a\nbootloader, boot manager, and bootloader updater to facilitate seamless\nfirmware updates. The functionality of the system is demonstrated through two\napplications: a blinking LED and a Lane Keeping Assist (LKA) system, showcasing\nits versatility in handling critical automotive features. This project\nrepresents a significant step forward in automotive technology, offering a\nuser-centric, efficient, and secure solution for automotive firmware\nmanagement.\n","authors":["Mostafa A. Mostafa","Mohamed K. Mohamed","Radwa W. Ezzat"],"pdf_url":"https://arxiv.org/pdf/2503.05839v2.pdf","comment":"Bachelor's thesis"},{"id":"http://arxiv.org/abs/2503.14277v1","updated":"2025-03-18T14:16:21Z","published":"2025-03-18T14:16:21Z","title":"Towards synthetic generation of realistic wooden logs","summary":"  In this work, we propose a novel method to synthetically generate realistic\n3D representations of wooden logs. Efficient sawmilling heavily relies on\naccurate measurement of logs and the distribution of knots inside them.\nComputed Tomography (CT) can be used to obtain accurate information about the\nknots but is often not feasible in a sawmill environment. A promising\nalternative is to utilize surface measurements and machine learning techniques\nto predict the inner structure of the logs. However, obtaining enough training\ndata remains a challenge. We focus mainly on two aspects of log generation: the\nmodeling of knot growth inside the tree, and the realistic synthesis of the\nsurface including the regions, where the knots reach the surface. This results\nin the first log synthesis approach capable of generating both the internal\nknot and external surface structures of wood. We demonstrate that the proposed\nmathematical log model accurately fits to real data obtained from CT scans and\nenables the generation of realistic logs.\n","authors":["Fedor Zolotarev","Borek Reich","Tuomas Eerola","Tomi Kauppi","Pavel Zemcik"],"pdf_url":"https://arxiv.org/pdf/2503.14277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02009v2","updated":"2025-03-18T14:11:26Z","published":"2025-03-03T19:33:22Z","title":"Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization","summary":"  Exploring real-world spaces using novel-view synthesis is fun, and\nreimagining those worlds in a different style adds another layer of excitement.\nStylized worlds can also be used for downstream tasks where there is limited\ntraining data and a need to expand a model's training distribution. Most\ncurrent novel-view synthesis stylization techniques lack the ability to\nconvincingly change geometry. This is because any geometry change requires\nincreased style strength which is often capped for stylization stability and\nconsistency. In this work, we propose a new autoregressive 3D Gaussian\nSplatting stylization method. As part of this method, we contribute a new RGBD\ndiffusion model that allows for strength control over appearance and shape\nstylization. To ensure consistency across stylized frames, we use a combination\nof novel depth-guided cross attention, feature injection, and a Warp ControlNet\nconditioned on composite frames for guiding the stylization of new frames. We\nvalidate our method via extensive qualitative results, quantitative\nexperiments, and a user study. Code online.\n","authors":["Jamie Wynn","Zawar Qureshi","Jakub Powierza","Jamie Watson","Mohamed Sayed"],"pdf_url":"https://arxiv.org/pdf/2503.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14275v1","updated":"2025-03-18T14:10:43Z","published":"2025-03-18T14:10:43Z","title":"Free-Lunch Color-Texture Disentanglement for Stylized Image Generation","summary":"  Recent advances in Text-to-Image (T2I) diffusion models have transformed\nimage generation, enabling significant progress in stylized generation using\nonly a few style reference images. However, current diffusion-based methods\nstruggle with fine-grained style customization due to challenges in controlling\nmultiple style attributes, such as color and texture. This paper introduces the\nfirst tuning-free approach to achieve free-lunch color-texture disentanglement\nin stylized T2I generation, addressing the need for independently controlled\nstyle elements for the Disentangled Stylized Image Generation (DisIG) problem.\nOur approach leverages the Image-Prompt Additivity property in the CLIP image\nembedding space to develop techniques for separating and extracting\nColor-Texture Embeddings (CTE) from individual color and texture reference\nimages. To ensure that the color palette of the generated image aligns closely\nwith the color reference, we apply a whitening and coloring transformation to\nenhance color consistency. Additionally, to prevent texture loss due to the\nsignal-leak bias inherent in diffusion training, we introduce a noise term that\npreserves textural fidelity during the Regularized Whitening and Coloring\nTransformation (RegWCT). Through these methods, our Style Attributes\nDisentanglement approach (SADis) delivers a more precise and customizable\nsolution for stylized image generation. Experiments on images from the WikiArt\nand StyleDrop datasets demonstrate that, both qualitatively and quantitatively,\nSADis surpasses state-of-the-art stylization methods in the DisIG task.\n","authors":["Jiang Qin","Senmao Li","Alexandra Gomez-Villa","Shiqi Yang","Yaxing Wang","Kai Wang","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2503.14275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14274v1","updated":"2025-03-18T14:09:10Z","published":"2025-03-18T14:09:10Z","title":"Improving Adaptive Density Control for 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) has become one of the most influential works in\nthe past year. Due to its efficient and high-quality novel view synthesis\ncapabilities, it has been widely adopted in many research fields and\napplications. Nevertheless, 3DGS still faces challenges to properly manage the\nnumber of Gaussian primitives that are used during scene reconstruction.\nFollowing the adaptive density control (ADC) mechanism of 3D Gaussian\nSplatting, new Gaussians in under-reconstructed regions are created, while\nGaussians that do not contribute to the rendering quality are pruned. We\nobserve that those criteria for densifying and pruning Gaussians can sometimes\nlead to worse rendering by introducing artifacts. We especially observe\nunder-reconstructed background or overfitted foreground regions. To encounter\nboth problems, we propose three new improvements to the adaptive density\ncontrol mechanism. Those include a correction for the scene extent calculation\nthat does not only rely on camera positions, an exponentially ascending\ngradient threshold to improve training convergence, and significance-aware\npruning strategy to avoid background artifacts. With these adaptions, we show\nthat the rendering quality improves while using the same number of Gaussians\nprimitives. Furthermore, with our improvements, the training converges\nconsiderably faster, allowing for more than twice as fast training times while\nyielding better quality than 3DGS. Finally, our contributions are easily\ncompatible with most existing derivative works of 3DGS making them relevant for\nfuture works.\n","authors":["Glenn Grubert","Florian Barthel","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2503.14274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14273v1","updated":"2025-03-18T14:09:00Z","published":"2025-03-18T14:09:00Z","title":"Manual Labelling Artificially Inflates Deep Learning-Based Segmentation\n  Performance on Closed Canopy: Validation Using TLS","summary":"  Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.\n","authors":["Matthew J. Allen","Harry J. F. Owen","Stuart W. D. Grieve","Emily R. Lines"],"pdf_url":"https://arxiv.org/pdf/2503.14273v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.14272v1","updated":"2025-03-18T14:06:39Z","published":"2025-03-18T14:06:39Z","title":"CTSR: Controllable Fidelity-Realness Trade-off Distillation for\n  Real-World Image Super Resolution","summary":"  Real-world image super-resolution is a critical image processing task, where\ntwo key evaluation criteria are the fidelity to the original image and the\nvisual realness of the generated results. Although existing methods based on\ndiffusion models excel in visual realness by leveraging strong priors, they\noften struggle to achieve an effective balance between fidelity and realness.\nIn our preliminary experiments, we observe that a linear combination of\nmultiple models outperforms individual models, motivating us to harness the\nstrengths of different models for a more effective trade-off. Based on this\ninsight, we propose a distillation-based approach that leverages the geometric\ndecomposition of both fidelity and realness, alongside the performance\nadvantages of multiple teacher models, to strike a more balanced trade-off.\nFurthermore, we explore the controllability of this trade-off, enabling a\nflexible and adjustable super-resolution process, which we call CTSR\n(Controllable Trade-off Super-Resolution). Experiments conducted on several\nreal-world image super-resolution benchmarks demonstrate that our method\nsurpasses existing state-of-the-art approaches, achieving superior performance\nacross both fidelity and realness metrics.\n","authors":["Runyi Li","Bin Chen","Jian Zhang","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2503.14272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13501v2","updated":"2025-03-18T13:55:22Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v2.pdf","comment":"Accepted at ICLR 2025. Code: https://github.com/boschresearch/VSTAR\n  and project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2412.07612v2","updated":"2025-03-18T13:47:36Z","published":"2024-12-10T15:51:17Z","title":"ViewDelta: Text-Prompted Change Detection in Unaligned Images","summary":"  Detecting changes between images is fundamental in applications such as\ninfrastructure assessment, environmental monitoring, and industrial automation.\nExisting supervised models demonstrate strong performance but are inherently\nlimited by the scope of their training data, requiring retraining to recognize\nnovel changes. To overcome this limitation, we introduce a novel change\ndetection task utilizing textual prompts alongside two potentially unaligned\nimages to produce binary segmentations highlighting user-relevant changes. This\ntext-conditioned framework significantly broadens the scope of change\ndetection, enabling unparalleled flexibility and straightforward scalability by\nincorporating diverse future datasets without restriction to specific change\ntypes. As a first approach to address this challenge, we propose ViewDelta, a\nmultimodal architecture extending the vision transformer into the domain of\ntext-conditioned change detection. ViewDelta establishes a robust baseline,\ndemonstrating flexibility across various scenarios and achieving competitive\nresults compared to specialized, fine-tuned models trained on aligned images.\nMoreover, we create and release the first text-prompt-conditioned change\ndetection dataset, comprising 501,153 image pairs with corresponding textual\nprompts and annotated labels. Extensive experiments confirm the robustness and\nversatility of our model across diverse environments, including indoor,\noutdoor, street-level, synthetic, and satellite imagery.\nhttps://joshuakgao.github.io/viewdelta/\n","authors":["Subin Varghese","Joshua Gao","Vedhus Hoskere"],"pdf_url":"https://arxiv.org/pdf/2412.07612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01658v2","updated":"2025-03-18T13:42:34Z","published":"2024-06-03T17:36:36Z","title":"Proxy Denoising for Source-Free Domain Adaptation","summary":"  Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model\nto an unlabeled target domain with no access to the source data. Inspired by\nthe success of large Vision-Language (ViL) models in many applications, the\nlatest research has validated ViL's benefit for SFDA by using their predictions\nas pseudo supervision. However, we observe that ViL's supervision could be\nnoisy and inaccurate at an unknown rate, introducing additional negative\neffects during adaption. To address this thus-far ignored challenge, we\nintroduce a novel Proxy Denoising (ProDe) approach. The key idea is to leverage\nthe ViL model as a proxy to facilitate the adaptation process towards the\nlatent domain-invariant space. We design a proxy denoising mechanism to correct\nViL's predictions, grounded on a proxy confidence theory that models the\ndynamic effect of proxy's divergence against the domain-invariant space during\nadaptation. To capitalize on the corrected proxy, we derive a mutual knowledge\ndistilling regularization. Extensive experiments show that ProDe significantly\noutperforms current state-of-the-art alternatives under the conventional closed\nset setting and more challenging open set, partial set, generalized SFDA,\nmulti-target, multi-source, and test-time settings. Our code and data are\navailable at https://github.com/tntek/source-free-domain-adaptation.\n","authors":["Song Tang","Wenxin Su","Mao Ye","Jianwei Zhang","Xiatian Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.01658v2.pdf","comment":"This paper is accepted by ICLR 2025 (Oral, Top 1.8%)"},{"id":"http://arxiv.org/abs/2503.12303v2","updated":"2025-03-18T13:42:31Z","published":"2025-03-16T00:25:13Z","title":"Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs","summary":"  Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent multimodal pre-training approaches focus on enhancing perception by\ntraining on high-quality image captions due to the extremely high cost of\ncollecting chain-of-thought (CoT) reasoning data for improving reasoning. While\nleveraging advanced MLLMs for caption generation enhances scalability, the\noutputs often lack comprehensiveness and accuracy. In this paper, we introduce\nSelf-Improving cognition (SIcog), a self-learning framework designed to\nconstruct next-generation foundation MLLMs by enhancing their systematic\ncognitive capabilities through multimodal pre-training with self-generated\ndata. Specifically, we propose Chain-of-Description, an approach that improves\nan MLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used for\nmultimodal pre-training to develop next-generation foundation models. Extensive\nexperiments on both low- and high-resolution MLLMs across diverse benchmarks\ndemonstrate that, with merely 213K self-generated pre-training samples, SIcog\nproduces next-generation foundation MLLMs with significantly improved\ncognition, achieving benchmark-leading performance compared to prevalent\npre-training approaches.\n","authors":["Xiaoying Zhang","Da Peng","Yipeng Zhang","Zonghao Guo","Chengyue Wu","Chi Chen","Wei Ke","Helen Meng","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2503.12303v2.pdf","comment":"38 pages. Preprint, work in progress"},{"id":"http://arxiv.org/abs/2503.13074v2","updated":"2025-03-18T13:39:06Z","published":"2025-03-17T11:25:48Z","title":"Rethinking Image Evaluation in Super-Resolution","summary":"  While recent advancing image super-resolution (SR) techniques are continually\nimproving the perceptual quality of their outputs, they can usually fail in\nquantitative evaluations. This inconsistency leads to a growing distrust in\nexisting image metrics for SR evaluations. Though image evaluation depends on\nboth the metric and the reference ground truth (GT), researchers typically do\nnot inspect the role of GTs, as they are generally accepted as `perfect'\nreferences. However, due to the data being collected in the early years and the\nignorance of controlling other types of distortions, we point out that GTs in\nexisting SR datasets can exhibit relatively poor quality, which leads to biased\nevaluations. Following this observation, in this paper, we are interested in\nthe following questions: Are GT images in existing SR datasets 100% trustworthy\nfor model evaluations? How does GT quality affect this evaluation? And how to\nmake fair evaluations if there exist imperfect GTs? To answer these questions,\nthis paper presents two main contributions. First, by systematically analyzing\nseven state-of-the-art SR models across three real-world SR datasets, we show\nthat SR performances can be consistently affected across models by low-quality\nGTs, and models can perform quite differently when GT quality is controlled.\nSecond, we propose a novel perceptual quality metric, Relative Quality Index\n(RQI), that measures the relative quality discrepancy of image pairs, thus\nissuing the biased evaluations caused by unreliable GTs. Our proposed model\nachieves significantly better consistency with human opinions. We expect our\nwork to provide insights for the SR community on how future datasets, models,\nand metrics should be developed.\n","authors":["Shaolin Su","Josep M. Rocafort","Danna Xue","David Serrano-Lozano","Lei Sun","Javier Vazquez-Corral"],"pdf_url":"https://arxiv.org/pdf/2503.13074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03261v2","updated":"2025-03-18T13:34:40Z","published":"2024-12-04T12:07:20Z","title":"Is JPEG AI going to change image forensics?","summary":"  In this paper, we investigate the counter-forensic effects of the new JPEG AI\nstandard based on neural image compression, focusing on two critical areas:\ndeepfake image detection and image splicing localization. Neural image\ncompression leverages advanced neural network algorithms to achieve higher\ncompression rates while maintaining image quality. However, it introduces\nartifacts that closely resemble those generated by image synthesis techniques\nand image splicing pipelines, complicating the work of researchers when\ndiscriminating pristine from manipulated content. We comprehensively analyze\nJPEG AI's counter-forensic effects through extensive experiments on several\nstate-of-the-art detectors and datasets. Our results demonstrate a reduction in\nthe performance of leading forensic detectors when analyzing content processed\nthrough JPEG AI. By exposing the vulnerabilities of the available forensic\ntools, we aim to raise the urgent need for multimedia forensics researchers to\ninclude JPEG AI images in their experimental setups and develop robust forensic\ntechniques to distinguish between neural compression artifacts and actual\nmanipulations.\n","authors":["Edoardo Daniele Cannas","Sara Mandelli","Nataša Popović","Ayman Alkhateeb","Alessandro Gnutti","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2412.03261v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21113v2","updated":"2025-03-18T13:30:27Z","published":"2024-10-28T15:13:53Z","title":"Zero-Shot Action Recognition in Surveillance Videos","summary":"  The growing demand for surveillance in public spaces presents significant\nchallenges due to the shortage of human resources. Current AI-based video\nsurveillance systems heavily rely on core computer vision models that require\nextensive finetuning, which is particularly difficult in surveillance settings\ndue to limited datasets and difficult setting (viewpoint, low quality, etc.).\nIn this work, we propose leveraging Large Vision-Language Models (LVLMs), known\nfor their strong zero and few-shot generalization, to tackle video\nunderstanding tasks in surveillance. Specifically, we explore VideoLLaMA2, a\nstate-of-the-art LVLM, and an improved token-level sampling method,\nSelf-Reflective Sampling (Self-ReS). Our experiments on the UCF-Crime dataset\nshow that VideoLLaMA2 represents a significant leap in zero-shot performance,\nwith 20% boost over the baseline. Self-ReS additionally increases zero-shot\naction recognition performance to 44.6%. These results highlight the potential\nof LVLMs, paired with improved sampling techniques, for advancing surveillance\nvideo analysis in diverse scenarios.\n","authors":["Joao Pereira","Vasco Lopes","David Semedo","Joao Neves"],"pdf_url":"https://arxiv.org/pdf/2410.21113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14244v1","updated":"2025-03-18T13:28:10Z","published":"2025-03-18T13:28:10Z","title":"Deep Unsupervised Segmentation of Log Point Clouds","summary":"  In sawmills, it is essential to accurately measure the raw material, i.e.\nwooden logs, to optimise the sawing process. Earlier studies have shown that\naccurate predictions of the inner structure of the logs can be obtained using\njust surface point clouds produced by a laser scanner. This provides a\ncost-efficient and fast alternative to the X-ray CT-based measurement devices.\nThe essential steps in analysing log point clouds is segmentation, as it forms\nthe basis for finding the fine surface details that provide the cues about the\ninner structure of the log. We propose a novel Point Transformer-based point\ncloud segmentation technique that learns to find the points belonging to the\nlog surface in unsupervised manner. This is obtained using a loss function that\nutilises the geometrical properties of a cylinder while taking into account the\nshape variation common in timber logs. We demonstrate the accuracy of the\nmethod on wooden logs, but the approach could be utilised also on other\ncylindrical objects.\n","authors":["Fedor Zolotarev","Tuomas Eerola","Tomi Kauppi"],"pdf_url":"https://arxiv.org/pdf/2503.14244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14237v1","updated":"2025-03-18T13:15:58Z","published":"2025-03-18T13:15:58Z","title":"Make Your Training Flexible: Towards Deployment-Efficient Video Models","summary":"  Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT.\n","authors":["Chenting Wang","Kunchang Li","Tianxiang Jiang","Xiangyu Zeng","Yi Wang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14232v1","updated":"2025-03-18T13:09:01Z","published":"2025-03-18T13:09:01Z","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","summary":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.\n","authors":["Yuyang Xue","Edward Moroshko","Feng Chen","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2503.14232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14231v1","updated":"2025-03-18T13:09:00Z","published":"2025-03-18T13:09:00Z","title":"Multi-task Learning for Identification of Porcelain in Song and Yuan\n  Dynasties","summary":"  Chinese porcelain holds immense historical and cultural value, making its\naccurate classification essential for archaeological research and cultural\nheritage preservation. Traditional classification methods rely heavily on\nexpert analysis, which is time-consuming, subjective, and difficult to scale.\nThis paper explores the application of DL and transfer learning techniques to\nautomate the classification of porcelain artifacts across four key attributes:\ndynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks\n(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their\nperformance with and without pre-trained weights. Our results demonstrate that\ntransfer learning significantly enhances classification accuracy, particularly\nfor complex tasks like type classification, where models trained from scratch\nexhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high\naccuracy and robustness across all tasks, while VGG16 struggles with more\ndiverse classifications. We further discuss the impact of dataset limitations\nand propose future directions, including domain-specific pre-training,\nintegration of attention mechanisms, explainable AI methods, and generalization\nto other cultural artifacts.\n","authors":["Ziyao Ling","Giovanni Delnevo","Paola Salomoni","Silvia Mirri"],"pdf_url":"https://arxiv.org/pdf/2503.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14229v1","updated":"2025-03-18T13:05:55Z","published":"2025-03-18T13:05:55Z","title":"HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous\n  Environments with Dynamic Multi-Human Interactions, Real-World Validation,\n  and an Open Leaderboard","summary":"  Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research.\n","authors":["Yifei Dong","Fengyi Wu","Qi He","Heng Li","Minghan Li","Zebang Cheng","Yuxuan Zhou","Jingdong Sun","Qi Dai","Zhi-Qi Cheng","Alexander G Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2503.14229v1.pdf","comment":"27 pages, website: https://ha-vln-project.vercel.app/"},{"id":"http://arxiv.org/abs/2503.14228v1","updated":"2025-03-18T13:05:41Z","published":"2025-03-18T13:05:41Z","title":"Panoramic Distortion-Aware Tokenization for Person Detection and\n  Localization Using Transformers in Overhead Fisheye Images","summary":"  Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets.\n","authors":["Nobuhiko Wakai","Satoshi Sato","Yasunori Ishii","Takayoshi Yamashita"],"pdf_url":"https://arxiv.org/pdf/2503.14228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13637v4","updated":"2025-03-18T13:03:48Z","published":"2024-05-22T13:36:48Z","title":"Curriculum Direct Preference Optimization for Diffusion and Consistency\n  Models","summary":"  Direct Preference Optimization (DPO) has been proposed as an effective and\nefficient alternative to reinforcement learning from human feedback (RLHF). In\nthis paper, we propose a novel and enhanced version of DPO based on curriculum\nlearning for text-to-image generation. Our method is divided into two training\nstages. First, a ranking of the examples generated for each prompt is obtained\nby employing a reward model. Then, increasingly difficult pairs of examples are\nsampled and provided to a text-to-image generative (diffusion or consistency)\nmodel. Generated samples that are far apart in the ranking are considered to\nform easy pairs, while those that are close in the ranking form hard pairs. In\nother words, we use the rank difference between samples as a measure of\ndifficulty. The sampled pairs are split into batches according to their\ndifficulty levels, which are gradually used to train the generative model. Our\napproach, Curriculum DPO, is compared against state-of-the-art fine-tuning\napproaches on nine benchmarks, outperforming the competing methods in terms of\ntext alignment, aesthetics and human preference. Our code is available at\nhttps://github.com/CroitoruAlin/Curriculum-DPO.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Nicu Sebe","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2405.13637v4.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2412.14295v2","updated":"2025-03-18T13:01:07Z","published":"2024-12-18T19:46:04Z","title":"Temporally Consistent Object-Centric Learning by Contrasting Slots","summary":"  Unsupervised object-centric learning from videos is a promising approach to\nextract structured representations from large, unlabeled collections of videos.\nTo support downstream tasks like autonomous control, these representations must\nbe both compositional and temporally consistent. Existing approaches based on\nrecurrent processing often lack long-term stability across frames because their\ntraining objective does not enforce temporal consistency. In this work, we\nintroduce a novel object-level temporal contrastive loss for video\nobject-centric models that explicitly promotes temporal consistency. Our method\nsignificantly improves the temporal consistency of the learned object-centric\nrepresentations, yielding more reliable video decompositions that facilitate\nchallenging downstream tasks such as unsupervised object dynamics prediction.\nFurthermore, the inductive bias added by our loss strongly improves object\ndiscovery, leading to state-of-the-art results on both synthetic and real-world\ndatasets, outperforming even weakly-supervised methods that leverage motion\nmasks as additional cues.\n","authors":["Anna Manasyan","Maximilian Seitzer","Filip Radovic","Georg Martius","Andrii Zadaianchuk"],"pdf_url":"https://arxiv.org/pdf/2412.14295v2.pdf","comment":"Published at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.08516v2","updated":"2025-03-18T12:58:46Z","published":"2025-03-11T15:08:37Z","title":"High-Quality 3D Head Reconstruction from Any Single Portrait Image","summary":"  In this work, we introduce a novel high-fidelity 3D head reconstruction\nmethod from a single portrait image, regardless of perspective, expression, or\naccessories. Despite significant efforts in adapting 2D generative models for\nnovel view synthesis and 3D optimization, most methods struggle to produce\nhigh-quality 3D portraits. The lack of crucial information, such as identity,\nexpression, hair, and accessories, limits these approaches in generating\nrealistic 3D head models. To address these challenges, we construct a new\nhigh-quality dataset containing 227 sequences of digital human portraits\ncaptured from 96 different perspectives, totalling 21,792 frames, featuring\ndiverse expressions and accessories. To further improve performance, we\nintegrate identity and expression information into the multi-view diffusion\nprocess to enhance facial consistency across views. Specifically, we apply\nidentity- and expression-aware guidance and supervision to extract accurate\nfacial representations, which guide the model and enforce objective functions\nto ensure high identity and expression consistency during generation. Finally,\nwe generate an orbital video around the portrait consisting of 96 multi-view\nframes, which can be used for 3D portrait model reconstruction. Our method\ndemonstrates robust performance across challenging scenarios, including\nside-face angles and complex accessories\n","authors":["Jianfu Zhang","Yujie Gao","Jiahui Zhan","Wentao Wang","Yiyi Zhang","Haohua Zhao","Liqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.08516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12620v3","updated":"2025-03-18T12:56:55Z","published":"2024-09-19T09:50:03Z","title":"Accurate Automatic 3D Annotation of Traffic Lights and Signs for\n  Autonomous Driving","summary":"  3D detection of traffic management objects, such as traffic lights and road\nsigns, is vital for self-driving cars, particularly for address-to-address\nnavigation where vehicles encounter numerous intersections with these static\nobjects. This paper introduces a novel method for automatically generating\naccurate and temporally consistent 3D bounding box annotations for traffic\nlights and signs, effective up to a range of 200 meters. These annotations are\nsuitable for training real-time models used in self-driving cars, which need a\nlarge amount of training data. The proposed method relies only on RGB images\nwith 2D bounding boxes of traffic management objects, which can be\nautomatically obtained using an off-the-shelf image-space detector neural\nnetwork, along with GNSS/INS data, eliminating the need for LiDAR point cloud\ndata.\n","authors":["Sándor Kunsági-Máté","Levente Pető","Lehel Seres","Tamás Matuszka"],"pdf_url":"https://arxiv.org/pdf/2409.12620v3.pdf","comment":"Accepted at the 2nd Workshop on Vision-Centric Autonomous Driving\n  (VCAD) as part of ECCV 2024"},{"id":"http://arxiv.org/abs/2503.14219v1","updated":"2025-03-18T12:54:36Z","published":"2025-03-18T12:54:36Z","title":"Segmentation-Guided Neural Radiance Fields for Novel Street View\n  Synthesis","summary":"  Recent advances in Neural Radiance Fields (NeRF) have shown great potential\nin 3D reconstruction and novel view synthesis, particularly for indoor and\nsmall-scale scenes. However, extending NeRF to large-scale outdoor environments\npresents challenges such as transient objects, sparse cameras and textures, and\nvarying lighting conditions. In this paper, we propose a segmentation-guided\nenhancement to NeRF for outdoor street scenes, focusing on complex urban\nenvironments. Our approach extends ZipNeRF and utilizes Grounded SAM for\nsegmentation mask generation, enabling effective handling of transient objects,\nmodeling of the sky, and regularization of the ground. We also introduce\nappearance embeddings to adapt to inconsistent lighting across view sequences.\nExperimental results demonstrate that our method outperforms the baseline\nZipNeRF, improving novel view synthesis quality with fewer artifacts and\nsharper details.\n","authors":["Yizhou Li","Yusuke Monno","Masatoshi Okutomi","Yuuichi Tanaka","Seiichi Kataoka","Teruaki Kosiba"],"pdf_url":"https://arxiv.org/pdf/2503.14219v1.pdf","comment":"Presented at VISAPP2025. Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/NVS/index.html"},{"id":"http://arxiv.org/abs/2410.01273v2","updated":"2025-03-18T12:44:59Z","published":"2024-10-02T06:34:45Z","title":"CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot\n  Interaction","summary":"  Real-life robot navigation involves more than just reaching a destination; it\nrequires optimizing movements while addressing scenario-specific goals. An\nintuitive way for humans to express these goals is through abstract cues like\nverbal commands or rough sketches. Such human guidance may lack details or be\nnoisy. Nonetheless, we expect robots to navigate as intended. For robots to\ninterpret and execute these abstract instructions in line with human\nexpectations, they must share a common understanding of basic navigation\nconcepts with humans. To this end, we introduce CANVAS, a novel framework that\ncombines visual and linguistic instructions for commonsense-aware navigation.\nIts success is driven by imitation learning, enabling the robot to learn from\nhuman navigation behavior. We present COMMAND, a comprehensive dataset with\nhuman-annotated navigation results, spanning over 48 hours and 219 km, designed\nto train commonsense-aware navigation systems in simulated environments. Our\nexperiments show that CANVAS outperforms the strong rule-based system ROS\nNavStack across all environments, demonstrating superior performance with noisy\ninstructions. Notably, in the orchard environment, where ROS NavStack records a\n0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also\nclosely aligns with human demonstrations and commonsense constraints, even in\nunseen environments. Furthermore, real-world deployment of CANVAS showcases\nimpressive Sim2Real transfer with a total success rate of 69%, highlighting the\npotential of learning from human demonstrations in simulated environments for\nreal-world applications.\n","authors":["Suhwan Choi","Yongjun Cho","Minchan Kim","Jaeyoon Jung","Myunchul Joe","Yubeen Park","Minseo Kim","Sungwoong Kim","Sungjae Lee","Hwiseong Park","Jiwan Chung","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01273v2.pdf","comment":"Accepted to ICRA 2025, project page https://worv-ai.github.io/canvas"},{"id":"http://arxiv.org/abs/2503.14209v1","updated":"2025-03-18T12:35:56Z","published":"2025-03-18T12:35:56Z","title":"AI-Driven Diabetic Retinopathy Diagnosis Enhancement through Image\n  Processing and Salp Swarm Algorithm-Optimized Ensemble Network","summary":"  Diabetic retinopathy is a leading cause of blindness in diabetic patients and\nearly detection plays a crucial role in preventing vision loss. Traditional\ndiagnostic methods are often time-consuming and prone to errors. The emergence\nof deep learning techniques has provided innovative solutions to improve\ndiagnostic efficiency. However, single deep learning models frequently face\nissues related to extracting key features from complex retinal images. To\nhandle this problem, we present an effective ensemble method for DR diagnosis\ncomprising four main phases: image pre-processing, selection of backbone\npre-trained models, feature enhancement, and optimization. Our methodology\ninitiates with the pre-processing phase, where we apply CLAHE to enhance image\ncontrast and Gamma correction is then used to adjust the brightness for better\nfeature recognition. We then apply Discrete Wavelet Transform (DWT) for image\nfusion by combining multi-resolution details to create a richer dataset. Then,\nwe selected three pre-trained models with the best performance named\nDenseNet169, MobileNetV1, and Xception for diverse feature extraction. To\nfurther improve feature extraction, an improved residual block is integrated\ninto each model. Finally, the predictions from these base models are then\naggregated using weighted ensemble approach, with the weights optimized by\nusing Salp Swarm Algorithm (SSA).SSA intelligently explores the weight space\nand finds the optimal configuration of base architectures to maximize the\nperformance of the ensemble model. The proposed model is evaluated on the\nmulticlass Kaggle APTOS 2019 dataset and obtained 88.52% accuracy.\n","authors":["Saif Ur Rehman Khan","Muhammad Nabeel Asim","Sebastian Vollmer","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2503.14209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14198v1","updated":"2025-03-18T12:18:34Z","published":"2025-03-18T12:18:34Z","title":"RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from\n  Sparse Multi-View Images","summary":"  This paper presents RoGSplat, a novel approach for synthesizing high-fidelity\nnovel views of unseen human from sparse multi-view images, while requiring no\ncumbersome per-subject optimization. Unlike previous methods that typically\nstruggle with sparse views with few overlappings and are less effective in\nreconstructing complex human geometry, the proposed method enables robust\nreconstruction in such challenging conditions. Our key idea is to lift SMPL\nvertices to dense and reliable 3D prior points representing accurate human body\ngeometry, and then regress human Gaussian parameters based on the points. To\naccount for possible misalignment between SMPL model and images, we propose to\npredict image-aligned 3D prior points by leveraging both pixel-level features\nand voxel-level features, from which we regress the coarse Gaussians. To\nenhance the ability to capture high-frequency details, we further render depth\nmaps from the coarse 3D Gaussians to help regress fine-grained pixel-wise\nGaussians. Experiments on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art methods in novel view synthesis and\ncross-dataset generalization. Our code is available at\nhttps://github.com/iSEE-Laboratory/RoGSplat.\n","authors":["Junjin Xiao","Qing Zhang","Yonewei Nie","Lei Zhu","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.14198v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.14195v1","updated":"2025-03-18T12:13:55Z","published":"2025-03-18T12:13:55Z","title":"Mapping Urban Villages in China: Progress and Challenges","summary":"  The shift toward high-quality urbanization has brought increased attention to\nthe issue of \"urban villages\", which has become a prominent social problem in\nChina. However, there is a lack of available geospatial data on urban villages,\nmaking it crucial to prioritize urban village mapping. In order to assess the\ncurrent progress in urban village mapping and identify challenges and future\ndirections, we have conducted a comprehensive review, which to the best of our\nknowledge is the first of its kind in this field. Our review begins by\nproviding a clear context for urban villages and elaborating the method for\nliterature review, then summarizes the study areas, data sources, and\napproaches used for urban village mapping in China. We also address the\nchallenges and future directions for further research. Through thorough\ninvestigation, we find that current studies only cover very limited study areas\nand periods and lack sufficient investigation into the scalability,\ntransferability, and interpretability of identification approaches due to the\nchallenges in concept fuzziness and variances, spatial heterogeneity and\nvariances of urban villages, and data availability. Future research can\ncomplement and further the current research in the following potential\ndirections in order to achieve large-area mapping across the whole nation...\n","authors":["Rui Cao","Wei Tu","Dongsheng Chen","Wenyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14195v1.pdf","comment":"Updated review at\n  https://github.com/rui-research/urban-village-review"},{"id":"http://arxiv.org/abs/2503.14189v1","updated":"2025-03-18T12:02:38Z","published":"2025-03-18T12:02:38Z","title":"Towards Harmless Multimodal Assistants with Blind Preference\n  Optimization","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in multimodal understanding, reasoning, and interaction. Given the\nextensive applications of MLLMs, the associated safety issues have become\nincreasingly critical. Due to the effectiveness of preference optimization in\naligning MLLMs with human preferences, there is an urgent need for\nsafety-related preference data for MLLMs. To address this, we construct the\nMMSafe-PO preference dataset towards harmless multimodal assistants, featuring\nmultimodal instructions, the conversational format, and ranked paired responses\nfrom human feedback. We also identify two insightful observations: modality\nco-defense and modality cheating, which illustrate that MLLMs possess a certain\nlevel of inherent defense while still presenting unique safety challenges.\nBased on these observations, we propose the Blind Preference Optimization (BPO)\napproach. Comprehensive experiments on three benchmarks show that BPO\neffectively enhances the safety capabilities of MLLMs. Notably, BPO\nsignificantly improves the safety rate of the base MLLM by 45.0%, outperforming\nthe DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly\nreduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on\nMM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and\nrobustness of both the dataset and the approach. We release code and data at\nhttps://lu-yang666.github.io/MMsafe-PO-Web/.\n","authors":["Yongqi Li","Lu Yang","Jian Wang","Runyang You","Wenjie Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.14189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14182v1","updated":"2025-03-18T11:57:31Z","published":"2025-03-18T11:57:31Z","title":"Bridging Past and Future: End-to-End Autonomous Driving with Historical\n  Prediction and Planning","summary":"  End-to-end autonomous driving unifies tasks in a differentiable framework,\nenabling planning-oriented optimization and attracting growing attention.\nCurrent methods aggregate historical information either through dense\nhistorical bird's-eye-view (BEV) features or by querying a sparse memory bank,\nfollowing paradigms inherited from detection. However, we argue that these\nparadigms either omit historical information in motion planning or fail to\nalign with its multi-step nature, which requires predicting or planning\nmultiple future time steps. In line with the philosophy of future is a\ncontinuation of past, we propose BridgeAD, which reformulates motion and\nplanning queries as multi-step queries to differentiate the queries for each\nfuture time step. This design enables the effective use of historical\nprediction and planning by applying them to the appropriate parts of the\nend-to-end system based on the time steps, which improves both perception and\nmotion planning. Specifically, historical queries for the current frame are\ncombined with perception, while queries for future frames are integrated with\nmotion planning. In this way, we bridge the gap between past and future by\naggregating historical insights at every time step, enhancing the overall\ncoherence and accuracy of the end-to-end autonomous driving pipeline. Extensive\nexperiments on the nuScenes dataset in both open-loop and closed-loop settings\ndemonstrate that BridgeAD achieves state-of-the-art performance.\n","authors":["Bozhou Zhang","Nan Song","Xin Jin","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14182v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2502.05092v2","updated":"2025-03-18T11:43:52Z","published":"2025-02-07T17:11:23Z","title":"Lost in Time: Clock and Calendar Understanding Challenges in Multimodal\n  LLMs","summary":"  Understanding time from visual representations is a fundamental cognitive\nskill, yet it remains a challenge for multimodal large language models (MLLMs).\nIn this work, we investigate the capabilities of MLLMs in interpreting time and\ndate through analogue clocks and yearly calendars. To facilitate this, we\ncurated a structured dataset comprising two subsets: 1) $\\textit{ClockQA}$,\nwhich comprises various types of clock styles$-$standard, black-dial,\nno-second-hand, Roman numeral, and arrow-hand clocks$-$paired with time related\nquestions; and 2) $\\textit{CalendarQA}$, which consists of yearly calendar\nimages with questions ranging from commonly known dates (e.g., Christmas, New\nYear's Day) to computationally derived ones (e.g., the 100th or 153rd day of\nthe year). We aim to analyse how MLLMs can perform visual recognition,\nnumerical reasoning, and temporal inference when presented with time-related\nvisual data. Our evaluations show that despite recent advancements, reliably\nunderstanding time remains a significant challenge for MLLMs.\n","authors":["Rohit Saxena","Aryo Pradipta Gema","Pasquale Minervini"],"pdf_url":"https://arxiv.org/pdf/2502.05092v2.pdf","comment":"Accepted at the ICLR 2025 Workshop on Reasoning and Planning for\n  Large Language Models"},{"id":"http://arxiv.org/abs/2503.14171v1","updated":"2025-03-18T11:42:52Z","published":"2025-03-18T11:42:52Z","title":"Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images","summary":"  We introduce an image upscaling technique tailored for 3D Gaussian Splatting\n(3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher\nrendering speeds and reduces artifacts commonly observed in 3DGS\nreconstructions. Our technique upscales low-resolution 3DGS renderings with a\nmarginal increase in cost by directly leveraging the analytical image gradients\nof Gaussians for gradient-based bicubic spline interpolation. The technique is\nagnostic to the specific 3DGS implementation, achieving novel view synthesis at\nrates 3x-4x higher than the baseline implementation. Through extensive\nexperiments on multiple datasets, we showcase the performance improvements and\nhigh reconstruction fidelity attainable with gradient-aware upscaling of 3DGS\nimages. We further demonstrate the integration of gradient-aware upscaling into\nthe gradient-based optimization of a 3DGS model and analyze its effects on\nreconstruction quality and performance.\n","authors":["Simon Niedermayr","Christoph Neuhauser Rüdiger Westermann"],"pdf_url":"https://arxiv.org/pdf/2503.14171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08993v2","updated":"2025-03-18T11:40:36Z","published":"2024-07-12T05:18:26Z","title":"Task-driven single-image super-resolution reconstruction of document\n  scans","summary":"  Super-resolution reconstruction is aimed at generating images of high spatial\nresolution from low-resolution observations. State-of-the-art super-resolution\ntechniques underpinned with deep learning allow for obtaining results of\noutstanding visual quality, but it is seldom verified whether they constitute a\nvaluable source for specific computer vision applications. In this paper, we\ninvestigate the possibility of employing super-resolution as a preprocessing\nstep to improve optical character recognition from document scans. To achieve\nthat, we propose to train deep networks for single-image super-resolution in a\ntask-driven way to make them better adapted for the purpose of text detection.\nAs problems limited to a specific task are heavily ill-posed, we introduce a\nmulti-task loss function that embraces components related with text detection\ncoupled with those guided by image similarity. The obtained results reported in\nthis paper are encouraging and they constitute an important step towards\nreal-world super-resolution of document images.\n","authors":["Maciej Zyrek","Michal Kawulok"],"pdf_url":"https://arxiv.org/pdf/2407.08993v2.pdf","comment":"Accepted for FedCSIS 2024"},{"id":"http://arxiv.org/abs/2503.07920v2","updated":"2025-03-18T11:34:03Z","published":"2025-03-10T23:54:52Z","title":"Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural\n  Vision-Language Dataset for Southeast Asia","summary":"  Southeast Asia (SEA) is a region of extraordinary linguistic and cultural\ndiversity, yet it remains significantly underrepresented in vision-language\n(VL) research. This often results in artificial intelligence (AI) models that\nfail to capture SEA cultural nuances. To fill this gap, we present SEA-VL, an\nopen-source initiative dedicated to developing high-quality, culturally\nrelevant data for SEA languages. By involving contributors from SEA countries,\nSEA-VL aims to ensure better cultural relevance and diversity, fostering\ngreater inclusivity of underrepresented languages in VL research. Beyond\ncrowdsourcing, our initiative goes one step further in the exploration of the\nautomatic collection of culturally relevant images through crawling and image\ngeneration. First, we find that image crawling achieves approximately ~85%\ncultural relevance while being more cost- and time-efficient than\ncrowdsourcing. Second, despite the substantial progress in generative vision\nmodels, synthetic images remain unreliable in accurately reflecting SEA\ncultures. The generated images often fail to reflect the nuanced traditions and\ncultural contexts of the region. Collectively, we gather 1.28M SEA\nculturally-relevant images, more than 50 times larger than other existing\ndatasets. Through SEA-VL, we aim to bridge the representation gap in SEA,\nfostering the development of more inclusive AI systems that authentically\nrepresent diverse cultures across SEA.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Joel Ruben Antony Moniz","Tack Hwa Wong","Mohammad Rifqi Farhansyah","Thant Thiri Maung","Frederikus Hudi","David Anugraha","Muhammad Ravi Shulthan Habibi","Muhammad Reza Qorib","Amit Agarwal","Joseph Marvin Imperial","Hitesh Laxmichand Patel","Vicky Feliren","Bahrul Ilmi Nasution","Manuel Antonio Rufino","Genta Indra Winata","Rian Adam Rajagede","Carlos Rafael Catalan","Mohamed Fazli Imam","Priyaranjan Pattnayak","Salsabila Zahirah Pranida","Kevin Pratama","Yeshil Bangera","Adisai Na-Thalang","Patricia Nicole Monderin","Yueqi Song","Christian Simon","Lynnette Hui Xian Ng","Richardy Lobo' Sapan","Taki Hasan Rafi","Bin Wang"," Supryadi","Kanyakorn Veerakanjana","Piyalitt Ittichaiwong","Matthew Theodore Roque","Karissa Vincentio","Takdanai Kreangphet","Phakphum Artkaew","Kadek Hendrawan Palgunadi","Yanzhi Yu","Rochana Prih Hastuti","William Nixon","Mithil Bangera","Adrian Xuan Wei Lim","Aye Hninn Khine","Hanif Muhammad Zhafran","Teddy Ferdinan","Audra Aurora Izzani","Ayushman Singh"," Evan","Jauza Akbar Krito","Michael Anugraha","Fenal Ashokbhai Ilasariya","Haochen Li","John Amadeo Daniswara","Filbert Aurelian Tjiaranata","Eryawan Presma Yulianrifat","Can Udomcharoenchaikit","Fadil Risdian Ansori","Mahardika Krisna Ihsani","Giang Nguyen","Anab Maulana Barik","Dan John Velasco","Rifo Ahmad Genadi","Saptarshi Saha","Chengwei Wei","Isaiah Flores","Kenneth Ko Han Chen","Anjela Gail Santos","Wan Shen Lim","Kaung Si Phyo","Tim Santos","Meisyarah Dwiastuti","Jiayun Luo","Jan Christian Blaise Cruz","Ming Shan Hee","Ikhlasul Akmal Hanif","M. Alif Al Hakim","Muhammad Rizky Sya'ban","Kun Kerdthaisong","Lester James V. Miranda","Fajri Koto","Tirana Noor Fatyanosa","Alham Fikri Aji","Jostin Jerico Rosal","Jun Kevin","Robert Wijaya","Onno P. Kampman","Ruochen Zhang","Börje F. Karlsson","Peerat Limkonchotiwat"],"pdf_url":"https://arxiv.org/pdf/2503.07920v2.pdf","comment":"[SEA-VL Dataset]\n  https://huggingface.co/collections/SEACrowd/sea-vl-multicultural-vl-dataset-for-southeast-asia-67cf223d0c341d4ba2b236e7\n  [Appendix J]\n  https://github.com/SEACrowd/seacrowd.github.io/blob/master/docs/SEA_VL_Appendix_J.pdf"},{"id":"http://arxiv.org/abs/2503.14161v1","updated":"2025-03-18T11:31:58Z","published":"2025-03-18T11:31:58Z","title":"CoSpace: Benchmarking Continuous Space Perception Ability for\n  Vision-Language Models","summary":"  Vision-Language Models (VLMs) have recently witnessed significant progress in\nvisual comprehension. As the permitting length of image context grows, VLMs can\nnow comprehend a broader range of views and spaces. Current benchmarks provide\ninsightful analysis of VLMs in tasks involving complex visual instructions\nfollowing, multi-image understanding and spatial reasoning. However, they\nusually focus on spatially irrelevant images or discrete images captured from\nvaried viewpoints. The compositional characteristic of images captured from a\nstatic viewpoint remains underestimated. We term this characteristic as\nContinuous Space Perception. When observing a scene from a static viewpoint\nwhile shifting orientations, it produces a series of spatially continuous\nimages, enabling the reconstruction of the entire space. In this paper, we\npresent CoSpace, a multi-image visual understanding benchmark designed to\nassess the Continuous Space perception ability for VLMs. CoSpace contains 2,918\nimages and 1,626 question-answer pairs, covering seven types of tasks. We\nconduct evaluation across 19 proprietary and open-source VLMs. Results reveal\nthat there exist pitfalls on the continuous space perception ability for most\nof the evaluated models, including proprietary ones. Interestingly, we find\nthat the main discrepancy between open-source and proprietary models lies not\nin accuracy but in the consistency of responses. We believe that enhancing the\nability of continuous space perception is essential for VLMs to perform\neffectively in real-world tasks and encourage further research to advance this\ncapability.\n","authors":["Yiqi Zhu","Ziyue Wang","Can Zhang","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00741v3","updated":"2025-03-18T11:31:57Z","published":"2025-03-02T05:36:04Z","title":"LesionDiffusion: Towards Text-controlled General Lesion Synthesis","summary":"  Fully-supervised lesion recognition methods in medical imaging face\nchallenges due to the reliance on large annotated datasets, which are expensive\nand difficult to collect. To address this, synthetic lesion generation has\nbecome a promising approach. However, existing models struggle with\nscalability, fine-grained control over lesion attributes, and the generation of\ncomplex structures. We propose LesionDiffusion, a text-controllable lesion\nsynthesis framework for 3D CT imaging that generates both lesions and\ncorresponding masks. By utilizing a structured lesion report template, our\nmodel provides greater control over lesion attributes and supports a wider\nvariety of lesion types. We introduce a dataset of 1,505 annotated CT scans\nwith paired lesion masks and structured reports, covering 14 lesion types\nacross 8 organs. LesionDiffusion consists of two components: a lesion mask\nsynthesis network (LMNet) and a lesion inpainting network (LINet), both guided\nby lesion attributes and image features. Extensive experiments demonstrate that\nLesionDiffusion significantly improves segmentation performance, with strong\ngeneralization to unseen lesion types and organs, outperforming current\nstate-of-the-art models. Code will be available at\nhttps://github.com/HengruiTianSJTU/LesionDiffusion.\n","authors":["Henrui Tian","Wenhui Lei","Linrui Dai","Hanyu Chen","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.00741v3.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.11040v3","updated":"2025-03-18T11:26:12Z","published":"2023-10-17T07:13:28Z","title":"Co-Learning Semantic-aware Unsupervised Segmentation for Pathological\n  Image Registration","summary":"  The registration of pathological images plays an important role in medical\napplications. Despite its significance, most researchers in this field\nprimarily focus on the registration of normal tissue into normal tissue. The\nnegative impact of focal tissue, such as the loss of spatial correspondence\ninformation and the abnormal distortion of tissue, are rarely considered. In\nthis paper, we propose GIRNet, a novel unsupervised approach for pathological\nimage registration by incorporating segmentation and inpainting through the\nprinciples of Generation, Inpainting, and Registration (GIR). The registration,\nsegmentation, and inpainting modules are trained simultaneously in a\nco-learning manner so that the segmentation of the focal area and the\nregistration of inpainted pairs can improve collaboratively. Overall, the\nregistration of pathological images is achieved in a completely unsupervised\nlearning framework. Experimental results on multiple datasets, including\nMagnetic Resonance Imaging (MRI) of T1 sequences, demonstrate the efficacy of\nour proposed method. Our results show that our method can accurately achieve\nthe registration of pathological images and identify lesions even in\nchallenging imaging modalities. Our unsupervised approach offers a promising\nsolution for the efficient and cost-effective registration of pathological\nimages. Our code is available at\nhttps://github.com/brain-intelligence-lab/GIRNet.\n","authors":["Yang Liu","Shi Gu"],"pdf_url":"https://arxiv.org/pdf/2310.11040v3.pdf","comment":"13 pages, 7 figures, published in Medical Image Computing and\n  Computer Assisted Intervention (MICCAI) 2023"},{"id":"http://arxiv.org/abs/2503.14154v1","updated":"2025-03-18T11:25:55Z","published":"2025-03-18T11:25:55Z","title":"RBFIM: Perceptual Quality Assessment for Compressed Point Clouds Using\n  Radial Basis Function Interpolation","summary":"  One of the main challenges in point cloud compression (PCC) is how to\nevaluate the perceived distortion so that the codec can be optimized for\nperceptual quality. Current standard practices in PCC highlight a primary\nissue: while single-feature metrics are widely used to assess compression\ndistortion, the classic method of searching point-to-point nearest neighbors\nfrequently fails to adequately build precise correspondences between point\nclouds, resulting in an ineffective capture of human perceptual features. To\novercome the related limitations, we propose a novel assessment method called\nRBFIM, utilizing radial basis function (RBF) interpolation to convert discrete\npoint features into a continuous feature function for the distorted point\ncloud. By substituting the geometry coordinates of the original point cloud\ninto the feature function, we obtain the bijective sets of point features. This\nenables an establishment of precise corresponding features between distorted\nand original point clouds and significantly improves the accuracy of quality\nassessments. Moreover, this method avoids the complexity caused by\nbidirectional searches. Extensive experiments on multiple subjective quality\ndatasets of compressed point clouds demonstrate that our RBFIM excels in\naddressing human perception tasks, thereby providing robust support for PCC\noptimization efforts.\n","authors":["Zhang Chen","Shuai Wan","Siyu Ren","Fuzheng Yang","Mengting Yu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2503.14154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07696v2","updated":"2025-03-18T11:19:45Z","published":"2024-04-11T12:42:18Z","title":"Flatness Improves Backbone Generalisation in Few-shot Classification","summary":"  Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. However, approaches for multi-domain FSC\ntypically result in complex pipelines aimed at information fusion and\ntask-specific adaptation without consideration of the importance of backbone\ntraining. In this work, we introduce an effective strategy for backbone\ntraining and selection in multi-domain FSC by utilizing flatness-aware training\nand fine-tuning. Our work is theoretically grounded and empirically performs on\npar or better than state-of-the-art methods despite being simpler. Further, our\nresults indicate that backbone training is crucial for good generalisation in\nFSC across different adaptation methods.\n","authors":["Rui Li","Martin Trapp","Marcus Klasson","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2404.07696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14151v1","updated":"2025-03-18T11:17:32Z","published":"2025-03-18T11:17:32Z","title":"Concat-ID: Towards Universal Identity-Preserving Video Synthesis","summary":"  We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications.\n","authors":["Yong Zhong","Zhuoyi Yang","Jiayan Teng","Xiaotao Gu","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.14151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14150v1","updated":"2025-03-18T11:16:48Z","published":"2025-03-18T11:16:48Z","title":"Comparative and Interpretative Analysis of CNN and Transformer Models in\n  Predicting Wildfire Spread Using Remote Sensing Data","summary":"  Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios.\n","authors":["Yihang Zhou","Ruige Kong","Zhengsen Xu","Linlin Xu","Sibo Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.14150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14140v1","updated":"2025-03-18T11:07:14Z","published":"2025-03-18T11:07:14Z","title":"Marten: Visual Question Answering with Mask Generation for Multi-modal\n  Document Understanding","summary":"  Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps://github.com/PriNing/Marten.\n","authors":["Zining Wang","Tongkun Guan","Pei Fu","Chen Duan","Qianyi Jiang","Zhentao Guo","Shan Guo","Junfeng Luo","Wei Shen","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2503.14140v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.14138v1","updated":"2025-03-18T11:04:57Z","published":"2025-03-18T11:04:57Z","title":"Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The\n  Role of Datasets, Architectures, and Loss Functions","summary":"  Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment.\n","authors":["Siddharth D Jaiswal","Sagnik Basu","Sandipan Sikdar","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2503.14138v1.pdf","comment":"This work has been accepted for publication at AAAI ICWSM 2025"},{"id":"http://arxiv.org/abs/2411.17002v2","updated":"2025-03-18T11:02:05Z","published":"2024-11-26T00:15:37Z","title":"Words Matter: Leveraging Individual Text Embeddings for Code Generation\n  in CLIP Test-Time Adaptation","summary":"  Vision-language foundation models, such as CLIP, have shown unprecedented\nzero-shot performance across a wide range of tasks. Nevertheless, these models\nmay be unreliable under distributional shifts, as their performance is\nsignificantly degraded. In this work, we explore how to efficiently leverage\nclass text information to mitigate these distribution drifts encountered by\nlarge pre-trained vision-language models (VLMs) during test-time inference. In\nparticular, we propose to generate pseudo-labels for the test-time samples by\nexploiting generic class text embeddings as fixed centroids of a label\nassignment problem, which is efficiently solved with Optimal Transport.\nFurthermore, the proposed adaptation method (CLIP-OT) integrates a multiple\ntemplate knowledge distillation approach, which replicates multi-view\ncontrastive learning strategies in unsupervised representation learning but\nwithout incurring additional computational complexity. Extensive experiments on\nmultiple popular test-time adaptation benchmarks presenting diverse complexity\nempirically show the superiority of CLIP-OT, achieving performance gains of up\nto 7% over recent state-of-the-art methods, yet being computationally and\nmemory efficient.\n","authors":["Shambhavi Mishra","Julio Silva-Rodrıguez","Ismail Ben Ayed","Marco Pedersoli","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2411.17002v2.pdf","comment":"Added additional figures to communicate the algorithm"},{"id":"http://arxiv.org/abs/2407.19310v2","updated":"2025-03-18T10:58:47Z","published":"2024-07-27T17:55:28Z","title":"Ensembling convolutional neural networks for human skin segmentation","summary":"  Detecting and segmenting human skin regions in digital images is an\nintensively explored topic of computer vision with a variety of approaches\nproposed over the years that have been found useful in numerous practical\napplications. The first methods were based on pixel-wise skin color modeling\nand they were later enhanced with context-based analysis to include the\ntextural and geometrical features, recently extracted using deep convolutional\nneural networks. It has been also demonstrated that skin regions can be\nsegmented from grayscale images without using color information at all.\nHowever, the possibility to combine these two sources of information has not\nbeen explored so far and we address this research gap with the contribution\nreported in this paper. We propose to train a convolutional network using the\ndatasets focused on different features to create an ensemble whose individual\noutcomes are effectively combined using yet another convolutional network\ntrained to produce the final segmentation map. The experimental results clearly\nindicate that the proposed approach outperforms the basic classifiers, as well\nas an ensemble based on the voting scheme. We expect that this study will help\nin developing new ensemble-based techniques that will improve the performance\nof semantic segmentation systems, reaching beyond the problem of detecting\nhuman skin.\n","authors":["Patryk Kuban","Michal Kawulok"],"pdf_url":"https://arxiv.org/pdf/2407.19310v2.pdf","comment":"Paper accepted for IBERAMIA 2024"},{"id":"http://arxiv.org/abs/2412.02114v2","updated":"2025-03-18T10:51:59Z","published":"2024-12-03T03:10:19Z","title":"Beyond Generation: Unlocking Universal Editing via Self-Supervised\n  Fine-Tuning","summary":"  Recent advances in video generation have outpaced progress in video editing,\nwhich remains constrained by several limiting factors, namely: (a) the task's\ndependency on supervision severely limits generality, (b) an unnecessary\nartificial separation between the generation and editing task, and (c) the high\ncomputational costs of training a video model. In this work, we propose UES\n(Unlocking Universal Editing via Self-Supervision), a lightweight\nself-supervised fine-tuning strategy that transforms generation models into\nunified generation-editing systems through self-supervised semantic alignment.\nOur approach establishes a dual-conditioning mechanism where original\nvideo-text pairs jointly provide visual and textual semantics, enabling\nstructured learning of intrinsic spatiotemporal correspondences. Key advantages\ninclude: (i) Universality through supervision-free adaptation to diverse\nediting tasks, (ii) Unification of generation and editing applicable to most\ntext(+image)-to-video model, and (iii) Efficiency via lightweight fine-tune\nthat reduces tunable parameters by 92.67%. To enable systematic evaluation, we\nintroduce OmniBench-99, a comprehensive benchmark spanning 99 videos across\nhumans/animals, environments, and objects, comprising 4 editing types and 8\nscenarios. Extensive experiments show UES enables models without inherent\nediting capability to perform powerful and universal editing while preserving\nor even enhancing their original generation performance.\n","authors":["Harold Haodong Chen","Harry Yang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2412.02114v2.pdf","comment":"Project: https://haroldchen19.github.io/UES-Page/"},{"id":"http://arxiv.org/abs/2503.14129v1","updated":"2025-03-18T10:47:46Z","published":"2025-03-18T10:47:46Z","title":"SketchFusion: Learning Universal Sketch Features through Fusing\n  Foundation Models","summary":"  While foundation models have revolutionised computer vision, their\neffectiveness for sketch understanding remains limited by the unique challenges\nof abstract, sparse visual inputs. Through systematic analysis, we uncover two\nfundamental limitations: Stable Diffusion (SD) struggles to extract meaningful\nfeatures from abstract sketches (unlike its success with photos), and exhibits\na pronounced frequency-domain bias that suppresses essential low-frequency\ncomponents needed for sketch understanding. Rather than costly retraining, we\naddress these limitations by strategically combining SD with CLIP, whose strong\nsemantic understanding naturally compensates for SD's spatial-frequency biases.\nBy dynamically injecting CLIP features into SD's denoising process and\nadaptively aggregating features across semantic levels, our method achieves\nstate-of-the-art performance in sketch retrieval (+3.35%), recognition\n(+1.06%), segmentation (+29.42%), and correspondence learning (+21.22%),\ndemonstrating the first truly universal sketch feature representation in the\nera of foundation models.\n","authors":["Subhadeep Koley","Tapas Kumar Dutta","Aneeshan Sain","Pinaki Nath Chowdhury","Ayan Kumar Bhunia","Yi-Zhe Song"],"pdf_url":"https://arxiv.org/pdf/2503.14129v1.pdf","comment":"Accepted in CVPR 2025. Project page available at\n  https://subhadeepkoley.github.io/SketchFusion/"},{"id":"http://arxiv.org/abs/2501.01790v2","updated":"2025-03-18T10:47:27Z","published":"2025-01-03T12:45:22Z","title":"Ingredients: Blending Custom Photos with Video Diffusion Transformers","summary":"  This paper presents a powerful framework to customize video creations by\nincorporating multiple specific identity (ID) photos, with video diffusion\nTransformers, referred to as Ingredients. Generally, our method consists of\nthree primary modules: (i) a facial extractor that captures versatile and\nprecise facial features for each human ID from both global and local\nperspectives; (ii) a multi-scale projector that maps face embeddings into the\ncontextual space of image query in video diffusion transformers; (iii) an ID\nrouter that dynamically combines and allocates multiple ID embedding to the\ncorresponding space-time regions. Leveraging a meticulously curated text-video\ndataset and a multi-stage training protocol, Ingredients demonstrates superior\nperformance in turning custom photos into dynamic and personalized video\ncontent. Qualitative evaluations highlight the advantages of proposed method,\npositioning it as a significant advancement toward more effective generative\nvideo control tools in Transformer-based architecture, compared to existing\nmethods. The data, code, and model weights are publicly available at:\nhttps://github.com/feizc/Ingredients.\n","authors":["Zhengcong Fei","Debang Li","Di Qiu","Changqian Yu","Mingyuan Fan"],"pdf_url":"https://arxiv.org/pdf/2501.01790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19149v2","updated":"2025-03-18T10:46:27Z","published":"2024-11-28T13:51:16Z","title":"Counting Stacked Objects","summary":"  Visual object counting is a fundamental computer vision task underpinning\nnumerous real-world applications, from cell counting in biomedicine to traffic\nand wildlife monitoring. However, existing methods struggle to handle the\nchallenge of stacked 3D objects in which most objects are hidden by those above\nthem. To address this important yet underexplored problem, we propose a novel\n3D counting approach that decomposes the task into two complementary\nsubproblems - estimating the 3D geometry of the object stack and the occupancy\nratio from multi-view images. By combining geometric reconstruction and deep\nlearning-based depth analysis, our method can accurately count identical\nobjects within containers, even when they are irregularly stacked. We validate\nour 3D Counting pipeline on diverse real-world and large-scale synthetic\ndatasets, which we will release publicly to facilitate further research.\n","authors":["Corentin Dumery","Noa Etté","Aoxiang Fan","Ren Li","Jingyi Xu","Hieu Le","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2411.19149v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2503.14112v1","updated":"2025-03-18T10:29:47Z","published":"2025-03-18T10:29:47Z","title":"Condensing Action Segmentation Datasets via Generative Network Inversion","summary":"  This work presents the first condensation approach for procedural video\ndatasets used in temporal action segmentation. We propose a condensation\nframework that leverages generative prior learned from the dataset and network\ninversion to condense data into compact latent codes with significant storage\nreduced across temporal and channel aspects. Orthogonally, we propose sampling\ndiverse and representative action sequences to minimize video-wise redundancy.\nOur evaluation on standard benchmarks demonstrates consistent effectiveness in\ncondensing TAS datasets and achieving competitive performances. Specifically,\non the Breakfast dataset, our approach reduces storage by over 500$\\times$\nwhile retaining 83% of the performance compared to training with the full\ndataset. Furthermore, when applied to a downstream incremental learning task,\nit yields superior performance compared to the state-of-the-art.\n","authors":["Guodong Ding","Rongyu Chen","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2503.14112v1.pdf","comment":"10 pages, 3 figures, 5 tables, Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.14111v1","updated":"2025-03-18T10:28:14Z","published":"2025-03-18T10:28:14Z","title":"Towards properties of adversarial image perturbations","summary":"  Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization.\n","authors":["Egor Kuznetsov","Kirill Aistov","Maxim Koroteev"],"pdf_url":"https://arxiv.org/pdf/2503.14111v1.pdf","comment":"13 pages, 40 figures"},{"id":"http://arxiv.org/abs/2503.14109v1","updated":"2025-03-18T10:25:28Z","published":"2025-03-18T10:25:28Z","title":"Operational Change Detection for Geographical Information: Overview and\n  Challenges","summary":"  Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies.\n","authors":["Nicolas Gonthier"],"pdf_url":"https://arxiv.org/pdf/2503.14109v1.pdf","comment":"Preprint under review"},{"id":"http://arxiv.org/abs/2503.07204v3","updated":"2025-03-18T10:21:53Z","published":"2025-03-10T11:42:37Z","title":"Endo-FASt3r: Endoscopic Foundation model Adaptation for Structure from\n  motion","summary":"  Accurate depth and camera pose estimation is essential for achieving\nhigh-quality 3D visualisations in robotic-assisted surgery. Despite recent\nadvancements in foundation model adaptation to monocular depth estimation of\nendoscopic scenes via self-supervised learning (SSL), no prior work has\nexplored their use for pose estimation. These methods rely on low rank-based\nadaptation approaches, which constrain model updates to a low-rank space. We\npropose Endo-FASt3r, the first monocular SSL depth and pose estimation\nframework that uses foundation models for both tasks. We extend the Reloc3r\nrelative pose estimation foundation model by designing Reloc3rX, introducing\nmodifications necessary for convergence in SSL. We also present DoMoRA, a novel\nadaptation technique that enables higher-rank updates and faster convergence.\nExperiments on the SCARED dataset show that Endo-FASt3r achieves a substantial\n$10\\%$ improvement in pose estimation and a $2\\%$ improvement in depth\nestimation over prior work. Similar performance gains on the Hamlyn and\nStereoMIS datasets reinforce the generalisability of Endo-FASt3r across\ndifferent datasets.\n","authors":["Mona Sheikh Zeinoddin","Mobarakol Islam","Zafer Tandogdu","Greg Shaw","Mathew J. Clarkson","Evangelos Mazomenos","Danail Stoyanov"],"pdf_url":"https://arxiv.org/pdf/2503.07204v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14106v1","updated":"2025-03-18T10:21:32Z","published":"2025-03-18T10:21:32Z","title":"Reliable uncertainty quantification for 2D/3D anatomical landmark\n  localization using multi-output conformal prediction","summary":"  Automatic anatomical landmark localization in medical imaging requires not\njust accurate predictions but reliable uncertainty quantification for effective\nclinical decision support. Current uncertainty quantification approaches often\nfall short, particularly when combined with normality assumptions,\nsystematically underestimating total predictive uncertainty. This paper\nintroduces conformal prediction as a framework for reliable uncertainty\nquantification in anatomical landmark localization, addressing a critical gap\nin automatic landmark localization. We present two novel approaches\nguaranteeing finite-sample validity for multi-output prediction: Multi-output\nRegression-as-Classification Conformal Prediction (M-R2CCP) and its variant\nMulti-output Regression to Classification Conformal Prediction set to Region\n(M-R2C2R). Unlike conventional methods that produce axis-aligned\nhyperrectangular or ellipsoidal regions, our approaches generate flexible,\nnon-convex prediction regions that better capture the underlying uncertainty\nstructure of landmark predictions. Through extensive empirical evaluation\nacross multiple 2D and 3D datasets, we demonstrate that our methods\nconsistently outperform existing multi-output conformal prediction approaches\nin both validity and efficiency. This work represents a significant advancement\nin reliable uncertainty estimation for anatomical landmark localization,\nproviding clinicians with trustworthy confidence measures for their diagnoses.\nWhile developed for medical imaging, these methods show promise for broader\napplications in multi-output regression problems.\n","authors":["Jef Jonkers","Frank Coopman","Luc Duchateau","Glenn Van Wallendael","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2503.14106v1.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2411.10077v2","updated":"2025-03-18T10:17:16Z","published":"2024-11-15T09:45:32Z","title":"Hierarchical Mutual Distillation for Multi-View Fusion: Learning from\n  All Possible View Combinations","summary":"  Multi-view learning often faces challenges in effectively leveraging images\ncaptured from different angles and locations. This challenge is particularly\npronounced when addressing inconsistencies and uncertainties between views. In\nthis paper, we propose a novel Multi-View Uncertainty-Weighted Mutual\nDistillation (MV-UWMD) method. Our method enhances prediction consistency by\nperforming hierarchical mutual distillation across all possible view\ncombinations, including single-view, partial multi-view, and full multi-view\npredictions. This introduces an uncertainty-based weighting mechanism through\nmutual distillation, allowing effective exploitation of unique information from\neach view while mitigating the impact of uncertain predictions. We extend a\nCNN-Transformer hybrid architecture to facilitate robust feature learning and\nintegration across multiple view combinations. We conducted extensive\nexperiments using a large, unstructured dataset captured from diverse,\nnon-fixed viewpoints. The results demonstrate that MV-UWMD improves prediction\naccuracy and consistency compared to existing multi-view learning approaches.\n","authors":["Jiwoong Yang","Haejun Chung","Ikbeom Jang"],"pdf_url":"https://arxiv.org/pdf/2411.10077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14097v1","updated":"2025-03-18T10:14:49Z","published":"2025-03-18T10:14:49Z","title":"SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human\n  Pose Estimation","summary":"  Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but\nsuffer from computational overhead and slow inference, while knowledge\ndistillation methods fail to address spatial relationships between joints and\ntemporal correlations in multi-frame inputs. In this paper, we propose Sparse\nCorrelation and Joint Distillation (SCJD), a novel framework that balances\nefficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input\nSequence Downsampling to reduce redundancy in student network inputs while\npreserving inter-frame correlations. For effective knowledge transfer, we\npropose Dynamic Joint Spatial Attention Distillation, which includes Dynamic\nJoint Embedding Distillation to enhance the student's feature representation\nusing the teacher's multi-frame context feature, and Adjacent Joint Attention\nDistillation to improve the student network's focus on adjacent joint\nrelationships for better spatial understanding. Additionally, Temporal\nConsistency Distillation aligns the temporal correlations between teacher and\nstudent networks through upsampling and global supervision. Extensive\nexperiments demonstrate that SCJD achieves state-of-the-art performance. Code\nis available at https://github.com/wileychan/SCJD.\n","authors":["Weihong Chen","Xuemiao Xu","Haoxin Yang","Yi Xie","Peng Xiao","Cheng Xu","Huaidong Zhang","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2503.14097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19606v3","updated":"2025-03-18T10:12:54Z","published":"2024-09-29T07:57:07Z","title":"Hyper-Connections","summary":"  We present hyper-connections, a simple yet effective method that can serve as\nan alternative to residual connections. This approach specifically addresses\ncommon drawbacks observed in residual connection variants, such as the seesaw\neffect between gradient vanishing and representation collapse. Theoretically,\nhyper-connections allow the network to adjust the strength of connections\nbetween features at different depths and dynamically rearrange layers. We\nconduct experiments focusing on the pre-training of large language models,\nincluding dense and sparse models, where hyper-connections show significant\nperformance improvements over residual connections. Additional experiments\nconducted on vision tasks also demonstrate similar improvements. We anticipate\nthat this method will be broadly applicable and beneficial across a wide range\nof AI problems.\n","authors":["Defa Zhu","Hongzhi Huang","Zihao Huang","Yutao Zeng","Yunyao Mao","Banggu Wu","Qiyang Min","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.19606v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14094v1","updated":"2025-03-18T10:11:49Z","published":"2025-03-18T10:11:49Z","title":"Image-Based Metrics in Ultrasound for Estimation of Global\n  Speed-of-Sound","summary":"  Accurate speed-of-sound (SoS) estimation is crucial for ultrasound image\nformation, yet conventional systems often rely on an assumed value for imaging.\nWhile several methods exist for SoS estimation, they typically depend on\ncomplex physical models of acoustic propagation. We propose to leverage\nconventional image analysis techniques and metrics, as a novel and simple\napproach to estimate tissue SoS. We study eleven metrics in three categories\nfor assessing image quality, image similarity and multi-frame variation, by\ntesting them in numerical simulations and phantom experiments. Among\nsingle-frame image quality metrics, conventional Focus and our proposed\nSmoothed Threshold Tenengrad metrics achieved satisfactory accuracy, however\nonly when applied to compounded images. Image quality metrics were largely\nsurpassed by various image comparison metrics, which exhibited errors\nconsistently under 8 m/s even applied to a single pair of images. Particularly,\nMean Square Error is a computationally efficient alternative for global\nestimation. Mutual Information and Correlation are found to be robust to\nprocessing small image segments, making them suitable, e.g., for multi-layer\nSoS estimation. The above metrics do not require access to raw channel data as\nthey can operate on post-beamformed data, and in the case of image quality\nmetrics they can operate on B-mode images, given that the beamforming SoS can\nbe controlled for beamforming using a multitude of values. These image analysis\nbased SoS estimation methods offer a computationally efficient and\ndata-accessible alternative to conventional physics-based methods, with\npotential extensions to layered or local SoS imaging.\n","authors":["Roman Denkin","Orcun Goksel"],"pdf_url":"https://arxiv.org/pdf/2503.14094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10630v3","updated":"2025-03-18T10:07:07Z","published":"2025-03-13T17:59:48Z","title":"UniGoal: Towards Universal Zero-shot Goal-oriented Navigation","summary":"  In this paper, we propose a general framework for universal zero-shot\ngoal-oriented navigation. Existing zero-shot methods build inference framework\nupon large language models (LLM) for specific tasks, which differs a lot in\noverall pipeline and fails to generalize across different types of goal.\nTowards the aim of universal zero-shot navigation, we propose a uniform graph\nrepresentation to unify different goals, including object category, instance\nimage and text description. We also convert the observation of agent into an\nonline maintained scene graph. With this consistent scene and goal\nrepresentation, we preserve most structural information compared with pure text\nand are able to leverage LLM for explicit graph-based reasoning. Specifically,\nwe conduct graph matching between the scene graph and goal graph at each time\ninstant and propose different strategies to generate long-term goal of\nexploration according to different matching states. The agent first iteratively\nsearches subgraph of goal when zero-matched. With partial matching, the agent\nthen utilizes coordinate projection and anchor pair alignment to infer the goal\nlocation. Finally scene graph correction and goal verification are applied for\nperfect matching. We also present a blacklist mechanism to enable robust switch\nbetween stages. Extensive experiments on several benchmarks show that our\nUniGoal achieves state-of-the-art zero-shot performance on three studied\nnavigation tasks with a single model, even outperforming task-specific\nzero-shot methods and supervised universal methods.\n","authors":["Hang Yin","Xiuwei Xu","Lingqing Zhao","Ziwei Wang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2503.10630v3.pdf","comment":"Accepted to CVPR 2025. Project page:\n  https://bagh2178.github.io/UniGoal/"},{"id":"http://arxiv.org/abs/2503.14075v1","updated":"2025-03-18T09:52:45Z","published":"2025-03-18T09:52:45Z","title":"Growing a Twig to Accelerate Large Vision-Language Models","summary":"  Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods. Code will be made publicly\navailable.\n","authors":["Zhenwei Shao","Mingyang Wang","Zhou Yu","Wenwen Pan","Yan Yang","Tao Wei","Hongyuan Zhang","Ning Mao","Wei Chen","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14075v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.14074v1","updated":"2025-03-18T09:52:41Z","published":"2025-03-18T09:52:41Z","title":"Limb-Aware Virtual Try-On Network with Progressive Clothing Warping","summary":"  Image-based virtual try-on aims to transfer an in-shop clothing image to a\nperson image. Most existing methods adopt a single global deformation to\nperform clothing warping directly, which lacks fine-grained modeling of in-shop\nclothing and leads to distorted clothing appearance. In addition, existing\nmethods usually fail to generate limb details well because they are limited by\nthe used clothing-agnostic person representation without referring to the limb\ntextures of the person image. To address these problems, we propose Limb-aware\nVirtual Try-on Network named PL-VTON, which performs fine-grained clothing\nwarping progressively and generates high-quality try-on results with realistic\nlimb details. Specifically, we present Progressive Clothing Warping (PCW) that\nexplicitly models the location and size of in-shop clothing and utilizes a\ntwo-stage alignment strategy to progressively align the in-shop clothing with\nthe human body. Moreover, a novel gravity-aware loss that considers the fit of\nthe person wearing clothing is adopted to better handle the clothing edges.\nThen, we design Person Parsing Estimator (PPE) with a non-limb target parsing\nmap to semantically divide the person into various regions, which provides\nstructural constraints on the human body and therefore alleviates texture\nbleeding between clothing and body regions. Finally, we introduce Limb-aware\nTexture Fusion (LTF) that focuses on generating realistic details in limb\nregions, where a coarse try-on result is first generated by fusing the warped\nclothing image with the person image, then limb textures are further fused with\nthe coarse result under limb-aware guidance to refine limb details. Extensive\nexperiments demonstrate that our PL-VTON outperforms the state-of-the-art\nmethods both qualitatively and quantitatively.\n","authors":["Shengping Zhang","Xiaoyu Han","Weigang Zhang","Xiangyuan Lan","Hongxun Yao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2503.14074v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia (TMM). The code is\n  available at https://github.com/aipixel/PL-VTONv2"},{"id":"http://arxiv.org/abs/2411.02210v2","updated":"2025-03-18T09:50:15Z","published":"2024-11-04T16:04:59Z","title":"One VLM to Keep it Learning: Generation and Balancing for Data-free\n  Continual Visual Question Answering","summary":"  Vision-Language Models (VLMs) have shown significant promise in Visual\nQuestion Answering (VQA) tasks by leveraging web-scale multimodal datasets.\nHowever, these models often struggle with continual learning due to\ncatastrophic forgetting when adapting to new tasks. As an effective remedy to\nmitigate catastrophic forgetting, rehearsal strategy uses the data of past\ntasks upon learning new task. However, such strategy incurs the need of storing\npast data, which might not be feasible due to hardware constraints or privacy\nconcerns. In this work, we propose the first data-free method that leverages\nthe language generation capability of a VLM, instead of relying on external\nmodels, to produce pseudo-rehearsal data for addressing continual VQA. Our\nproposal, named as GaB, generates pseudo-rehearsal data by posing previous task\nquestions on new task data. Yet, despite being effective, the distribution of\ngenerated questions skews towards the most frequently posed questions due to\nthe limited and task-specific training data. To mitigate this issue, we\nintroduce a pseudo-rehearsal balancing module that aligns the generated data\ntowards the ground-truth data distribution using either the question\nmeta-statistics or an unsupervised clustering method. We evaluate our proposed\nmethod on two recent benchmarks, \\ie VQACL-VQAv2 and CLOVE-function benchmarks.\nGaB outperforms all the data-free baselines with substantial improvement in\nmaintaining VQA performance across evolving tasks, while being on-par with\nmethods with access to the past data.\n","authors":["Deepayan Das","Davide Talon","Massimiliano Mancini","Yiming Wang","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2411.02210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14070v1","updated":"2025-03-18T09:42:55Z","published":"2025-03-18T09:42:55Z","title":"Fast Autoregressive Video Generation with Diagonal Decoding","summary":"  Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity.\n","authors":["Yang Ye","Junliang Guo","Haoyu Wu","Tianyu He","Tim Pearce","Tabish Rashid","Katja Hofmann","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2503.14070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14064v1","updated":"2025-03-18T09:36:33Z","published":"2025-03-18T09:36:33Z","title":"AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted\n  Benchmark","summary":"  The rapid advancement in AI-generated video synthesis has led to a growth\ndemand for standardized and effective evaluation metrics. Existing metrics lack\na unified framework for systematically categorizing methodologies, limiting a\nholistic understanding of the evaluation landscape. Additionally, fragmented\nimplementations and the absence of standardized interfaces lead to redundant\nprocessing overhead. Furthermore, many prior approaches are constrained by\ndataset-specific dependencies, limiting their applicability across diverse\nvideo domains. To address these challenges, we introduce AIGVE-Tool\n(AI-Generated Video Evaluation Toolkit), a unified framework that provides a\nstructured and extensible evaluation pipeline for a comprehensive AI-generated\nvideo evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool\nintegrates multiple evaluation methodologies while allowing flexible\ncustomization through a modular configuration system. Additionally, we propose\nAIGVE-Bench, a large-scale benchmark dataset created with five SOTA video\ngeneration models based on hand-crafted instructions and prompts. This dataset\nsystematically evaluates various video generation models across nine critical\nquality dimensions. Extensive experiments demonstrate the effectiveness of\nAIGVE-Tool in providing standardized and reliable evaluation results,\nhighlighting specific strengths and limitations of current models and\nfacilitating the advancements of next-generation AI-generated video techniques.\n","authors":["Xinhao Xiang","Xiao Liu","Zizhong Li","Zhuosheng Liu","Jiawei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10266v2","updated":"2025-03-18T09:28:00Z","published":"2025-01-17T15:48:37Z","title":"MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object\n  Detection","summary":"  Radar and LiDAR have been widely used in autonomous driving as LiDAR provides\nrich structure information, and radar demonstrates high robustness under\nadverse weather. Recent studies highlight the effectiveness of fusing radar and\nLiDAR point clouds. However, challenges remain due to the modality misalignment\nand information loss during feature extractions. To address these issues, we\npropose a 4D radar-LiDAR framework to mutually enhance their representations.\nInitially, the indicative features from radar are utilized to guide both radar\nand LiDAR geometric feature learning. Subsequently, to mitigate their sparsity\ngap, the shape information from LiDAR is used to enrich radar BEV features.\nExtensive experiments on the View-of-Delft (VoD) dataset demonstrate our\napproach's superiority over existing methods, achieving the highest mAP of\n71.76% across the entire area and 86.36\\% within the driving corridor.\nEspecially for cars, we improve the AP by 4.17% and 4.20% due to the strong\nindicative features and symmetric shapes.\n","authors":["Xiangyuan Peng","Huawei Sun","Kay Bierzynski","Anton Fischbacher","Lorenzo Servadei","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2501.10266v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2503.12009v2","updated":"2025-03-18T09:27:50Z","published":"2025-03-15T06:22:31Z","title":"UniMamba: Unified Spatial-Channel Representation Learning with\n  Group-Efficient Mamba for LiDAR-based 3D Object Detection","summary":"  Recent advances in LiDAR 3D detection have demonstrated the effectiveness of\nTransformer-based frameworks in capturing the global dependencies from point\ncloud spaces, which serialize the 3D voxels into the flattened 1D sequence for\niterative self-attention. However, the spatial structure of 3D voxels will be\ninevitably destroyed during the serialization process. Besides, due to the\nconsiderable number of 3D voxels and quadratic complexity of Transformers,\nmultiple sequences are grouped before feeding to Transformers, leading to a\nlimited receptive field. Inspired by the impressive performance of State Space\nModels (SSM) achieved in the field of 2D vision tasks, in this paper, we\npropose a novel Unified Mamba (UniMamba), which seamlessly integrates the\nmerits of 3D convolution and SSM in a concise multi-head manner, aiming to\nperform \"local and global\" spatial context aggregation efficiently and\nsimultaneously. Specifically, a UniMamba block is designed which mainly\nconsists of spatial locality modeling, complementary Z-order serialization and\nlocal-global sequential aggregator. The spatial locality modeling module\nintegrates 3D submanifold convolution to capture the dynamic spatial position\nembedding before serialization. Then the efficient Z-order curve is adopted for\nserialization both horizontally and vertically. Furthermore, the local-global\nsequential aggregator adopts the channel grouping strategy to efficiently\nencode both \"local and global\" spatial inter-dependencies using multi-head SSM.\nAdditionally, an encoder-decoder architecture with stacked UniMamba blocks is\nformed to facilitate multi-scale spatial learning hierarchically. Extensive\nexperiments are conducted on three popular datasets: nuScenes, Waymo and\nArgoverse 2. Particularly, our UniMamba achieves 70.2 mAP on the nuScenes\ndataset.\n","authors":["Xin Jin","Haisheng Su","Kai Liu","Cong Ma","Wei Wu","Fei Hui","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2503.12009v2.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.09446v2","updated":"2025-03-18T09:12:52Z","published":"2025-03-12T14:46:40Z","title":"Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have achieved remarkable progress in\ngenerating high-quality images but also raise people's concerns about\ngenerating harmful or misleading content. While extensive approaches have been\nproposed to erase unwanted concepts without requiring retraining from scratch,\nthey inadvertently degrade performance on normal generation tasks. In this\nwork, we propose Interpret then Deactivate (ItD), a novel framework to enable\nprecise concept removal in T2I diffusion models while preserving overall\nperformance. ItD first employs a sparse autoencoder (SAE) to interpret each\nconcept as a combination of multiple features. By permanently deactivating the\nspecific features associated with target concepts, we repurpose SAE as a\nzero-shot classifier that identifies whether the input prompt includes target\nconcepts, allowing selective concept erasure in diffusion models. Moreover, we\ndemonstrate that ItD can be easily extended to erase multiple concepts without\nrequiring further training. Comprehensive experiments across celebrity\nidentities, artistic styles, and explicit content demonstrate ItD's\neffectiveness in eliminating targeted concepts without interfering with normal\nconcept generation. Additionally, ItD is also robust against adversarial\nprompts designed to circumvent content filters. Code is available at:\nhttps://github.com/NANSirun/Interpret-then-deactivate.\n","authors":["Zhihua Tian","Sirun Nan","Ming Xu","Shengfang Zhai","Wenjie Qu","Jian Liu","Kui Ren","Ruoxi Jia","Jiaheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.09446v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2503.14051v1","updated":"2025-03-18T09:12:49Z","published":"2025-03-18T09:12:49Z","title":"Foundation Feature-Driven Online End-Effector Pose Estimation: A\n  Marker-Free and Learning-Free Approach","summary":"  Accurate transformation estimation between camera space and robot space is\nessential. Traditional methods using markers for hand-eye calibration require\noffline image collection, limiting their suitability for online\nself-calibration. Recent learning-based robot pose estimation methods, while\nadvancing online calibration, struggle with cross-robot generalization and\nrequire the robot to be fully visible. This work proposes a Foundation\nfeature-driven online End-Effector Pose Estimation (FEEPE) algorithm,\ncharacterized by its training-free and cross end-effector generalization\ncapabilities. Inspired by the zero-shot generalization capabilities of\nfoundation models, FEEPE leverages pre-trained visual features to estimate\n2D-3D correspondences derived from the CAD model and target image, enabling 6D\npose estimation via the PnP algorithm. To resolve ambiguities from partial\nobservations and symmetry, a multi-historical key frame enhanced pose\noptimization algorithm is introduced, utilizing temporal information for\nimproved accuracy. Compared to traditional hand-eye calibration, FEEPE enables\nmarker-free online calibration. Unlike robot pose estimation, it generalizes\nacross robots and end-effectors in a training-free manner. Extensive\nexperiments demonstrate its superior flexibility, generalization, and\nperformance.\n","authors":["Tianshu Wu","Jiyao Zhang","Shiqian Liang","Zhengxiao Han","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2503.14051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12891v2","updated":"2025-03-18T09:08:21Z","published":"2023-11-21T06:26:28Z","title":"Text-Guided Texturing by Synchronized Multi-View Diffusion","summary":"  This paper introduces a novel approach to synthesize texture to dress up a\ngiven 3D object, given a text prompt. Based on the pretrained text-to-image\n(T2I) diffusion model, existing methods usually employ a project-and-inpaint\napproach, in which a view of the given object is first generated and warped to\nanother view for inpainting. But it tends to generate inconsistent texture due\nto the asynchronous diffusion of multiple views. We believe such asynchronous\ndiffusion and insufficient information sharing among views are the root causes\nof the inconsistent artifact. In this paper, we propose a synchronized\nmulti-view diffusion approach that allows the diffusion processes from\ndifferent views to reach a consensus of the generated content early in the\nprocess, and hence ensures the texture consistency. To synchronize the\ndiffusion, we share the denoised content among different views in each\ndenoising step, specifically blending the latent content in the texture domain\nfrom views with overlap. Our method demonstrates superior performance in\ngenerating consistent, seamless, highly detailed textures, comparing to\nstate-of-the-art methods.\n","authors":["Yuxin Liu","Minshan Xie","Hanyuan Liu","Tien-Tsin Wong"],"pdf_url":"https://arxiv.org/pdf/2311.12891v2.pdf","comment":"11 pages, 11 figures, technical papers, \"Text, Texturing, and\n  Stylization\"@SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2503.14040v1","updated":"2025-03-18T09:02:02Z","published":"2025-03-18T09:02:02Z","title":"MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation\n  without Vector Quantization","summary":"  This work focuses on full-body co-speech gesture generation. Existing methods\ntypically employ an autoregressive model accompanied by vector-quantized tokens\nfor gesture generation, which results in information loss and compromises the\nrealism of the generated gestures. To address this, inspired by the natural\ncontinuity of real-world human motion, we propose MAG, a novel multi-modal\naligned framework for high-quality and diverse co-speech gesture synthesis\nwithout relying on discrete tokenization. Specifically, (1) we introduce a\nmotion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages\npre-trained WavCaps' text and audio embeddings to enhance both semantic and\nrhythmic alignment with motion, ultimately producing more realistic gestures.\n(2) Building on this, we propose a multimodal masked autoregressive model\n(MMAG) that enables autoregressive modeling in continuous motion embeddings\nthrough diffusion without vector quantization. To further ensure multi-modal\nconsistency, MMAG incorporates a hybrid granularity audio-text fusion block,\nwhich serves as conditioning for diffusion process. Extensive experiments on\ntwo benchmark datasets demonstrate that MAG achieves stateof-the-art\nperformance both quantitatively and qualitatively, producing highly realistic\nand diverse co-speech gestures.The code will be released to facilitate future\nresearch.\n","authors":["Binjie Liu","Lina Liu","Sanyi Zhang","Songen Gu","Yihao Zhi","Tianyi Zhu","Lei Yang","Long Ye"],"pdf_url":"https://arxiv.org/pdf/2503.14040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21301v2","updated":"2025-03-18T09:00:53Z","published":"2024-10-21T11:39:03Z","title":"Evaluating the Posterior Sampling Ability of Plug&Play Diffusion Methods\n  in Sparse-View CT","summary":"  Plug&Play (PnP) diffusion models are state-of-the-art methods in computed\ntomography (CT) reconstruction. Such methods usually consider applications\nwhere the sinogram contains a sufficient amount of information for the\nposterior distribution to be concentrated around a single mode, and\nconsequently are evaluated using image-to-image metrics such as PSNR/SSIM.\nInstead, we are interested in reconstructing compressible flow images from\nsinograms having a small number of projections, which results in a posterior\ndistribution no longer concentrated or even multimodal. Thus, in this paper, we\naim at evaluating the approximate posterior of PnP diffusion models and\nintroduce two posterior evaluation properties. We quantitatively evaluate three\nPnP diffusion methods on three different datasets for several numbers of\nprojections. We surprisingly find that, for each method, the approximate\nposterior deviates from the true posterior when the number of projections\ndecreases.\n","authors":["Liam Moroy","Guillaume Bourmaud","Frédéric Champagnat","Jean-François Giovannelli"],"pdf_url":"https://arxiv.org/pdf/2410.21301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10344v6","updated":"2025-03-18T09:00:34Z","published":"2024-03-15T14:31:17Z","title":"ViiNeuS: Volumetric Initialization for Implicit Neural Surface\n  reconstruction of urban scenes with limited image overlap","summary":"  Neural implicit surface representation methods have recently shown impressive\n3D reconstruction results. However, existing solutions struggle to reconstruct\ndriving scenes due to their large size, highly complex nature and their limited\nvisual observation overlap. Hence, to achieve accurate reconstructions,\nadditional supervision data such as LiDAR, strong geometric priors, and long\ntraining times are required. To tackle such limitations, we present ViiNeuS, a\nnew hybrid implicit surface learning method that efficiently initializes the\nsigned distance field to reconstruct large driving scenes from 2D street view\nimages. ViiNeuS's hybrid architecture models two separate implicit fields: one\nrepresenting the volumetric density of the scene, and another one representing\nthe signed distance to the surface. To accurately reconstruct urban outdoor\ndriving scenarios, we introduce a novel volume-rendering strategy that relies\non self-supervised probabilistic density estimation to sample points near the\nsurface and transition progressively from volumetric to surface representation.\nOur solution permits a proper and fast initialization of the signed distance\nfield without relying on any geometric prior on the scene, compared to\nconcurrent methods. By conducting extensive experiments on four outdoor driving\ndatasets, we show that ViiNeuS can learn an accurate and detailed 3D surface\nrepresentation of various urban scene while being two times faster to train\ncompared to previous state-of-the-art solutions.\n","authors":["Hala Djeghim","Nathan Piasco","Moussab Bennehar","Luis Roldão","Dzmitry Tsishkou","Désiré Sidibé"],"pdf_url":"https://arxiv.org/pdf/2403.10344v6.pdf","comment":"CVPR2025. Project page: https://hala-djeghim.github.io/ViiNeuS/"},{"id":"http://arxiv.org/abs/2503.14037v1","updated":"2025-03-18T08:56:02Z","published":"2025-03-18T08:56:02Z","title":"Intra and Inter Parser-Prompted Transformers for Effective Image\n  Restoration","summary":"  We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that\nexplore useful features from visual foundation models for image restoration.\nSpecifically, PPTformer contains two parts: an Image Restoration Network\n(IRNet) for restoring images from degraded observations and a Parser-Prompted\nFeature Generation Network (PPFGNet) for providing IRNet with reliable parser\ninformation to boost restoration. To enhance the integration of the parser\nwithin IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter\nParser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful\nparser features to facilitate restoration. The IntraPPA re-considers cross\nattention between parser and restoration features, enabling implicit perception\nof the parser from a long-range and intra-layer perspective. Conversely, the\nInterPPA initially fuses restoration features with those of the parser,\nfollowed by formulating these fused features within an attention mechanism to\nexplicitly perceive parser information. Further, we propose a parser-prompted\nfeed-forward network to guide restoration within pixel-wise gating modulation.\nExperimental results show that PPTformer achieves state-of-the-art performance\non image deraining, defocus deblurring, desnowing, and low-light enhancement.\n","authors":["Cong Wang","Jinshan Pan","Liyan Wang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14037v1.pdf","comment":"This version is accepted by the Association for the Advancement of\n  Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2409.04013v2","updated":"2025-03-18T08:54:41Z","published":"2024-09-06T03:53:59Z","title":"3D-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian\n  Geometric Priors","summary":"  Existing multi-view image compression methods often rely on 2D\nprojection-based similarities between views to estimate disparities. While\neffective for small disparities, such as those in stereo images, these methods\nstruggle with the more complex disparities encountered in wide-baseline\nmulti-camera systems, commonly found in virtual reality and autonomous driving\napplications. To address this limitation, we propose 3D-LMVIC, a novel\nlearning-based multi-view image compression framework that leverages 3D\nGaussian Splatting to derive geometric priors for accurate disparity\nestimation. Furthermore, we introduce a depth map compression model to minimize\ngeometric redundancy across views, along with a multi-view sequence ordering\nstrategy based on a defined distance measure between views to enhance\ncorrelations between adjacent views. Experimental results demonstrate that\n3D-LMVIC achieves superior performance compared to both traditional and\nlearning-based methods. Additionally, it significantly improves disparity\nestimation accuracy over existing two-view approaches.\n","authors":["Yujun Huang","Bin Chen","Niu Lian","Baoyi An","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2409.04013v2.pdf","comment":"17 pages, 10 figures, conference"},{"id":"http://arxiv.org/abs/2503.10615v2","updated":"2025-03-18T08:52:34Z","published":"2025-03-13T17:56:05Z","title":"R1-Onevision: Advancing Generalized Multimodal Reasoning through\n  Cross-Modal Formalization","summary":"  Large Language Models have demonstrated remarkable reasoning capability in\ncomplex textual tasks. However, multimodal reasoning, which requires\nintegrating visual and textual information, remains a significant challenge.\nExisting visual-language models often struggle to effectively analyze and\nreason visual content, resulting in suboptimal performance on complex reasoning\ntasks. Moreover, the absence of comprehensive benchmarks hinders the accurate\nassessment of multimodal reasoning capabilities. In this paper, we introduce\nR1-Onevision, a multimodal reasoning model designed to bridge the gap between\nvisual perception and deep reasoning. To achieve this, we propose a cross-modal\nreasoning pipeline that transforms images into formal textural representations,\nenabling precise language-based reasoning. Leveraging this pipeline, we\nconstruct the R1-Onevision dataset which provides detailed, step-by-step\nmultimodal reasoning annotations across diverse domains. We further develop the\nR1-Onevision model through supervised fine-tuning and reinforcement learning to\ncultivate advanced reasoning and robust generalization abilities. To\ncomprehensively evaluate multimodal reasoning performance across different\ngrades, we introduce R1-Onevision-Bench, a benchmark aligned with human\neducational stages, covering exams from junior high school to university and\nbeyond. Experimental results show that R1-Onevision achieves state-of-the-art\nperformance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple\nchallenging multimodal reasoning benchmarks.\n","authors":["Yi Yang","Xiaoxuan He","Hongkun Pan","Xiyan Jiang","Yan Deng","Xingtao Yang","Haoyu Lu","Dacheng Yin","Fengyun Rao","Minfeng Zhu","Bo Zhang","Wei Chen"],"pdf_url":"https://arxiv.org/pdf/2503.10615v2.pdf","comment":"Code and Model: https://github.com/Fancy-MLLM/R1-onevision"},{"id":"http://arxiv.org/abs/2503.14035v1","updated":"2025-03-18T08:51:50Z","published":"2025-03-18T08:51:50Z","title":"A Revisit to the Decoder for Camouflaged Object Detection","summary":"  Camouflaged object detection (COD) aims to generate a fine-grained\nsegmentation map of camouflaged objects hidden in their background. Due to the\nhidden nature of camouflaged objects, it is essential for the decoder to be\ntailored to effectively extract proper features of camouflaged objects and\nextra-carefully generate their complex boundaries. In this paper, we propose a\nnovel architecture that augments the prevalent decoding strategy in COD with\nEnrich Decoder and Retouch Decoder, which help to generate a fine-grained\nsegmentation map. Specifically, the Enrich Decoder amplifies the channels of\nfeatures that are important for COD using channel-wise attention. Retouch\nDecoder further refines the segmentation maps by spatially attending to\nimportant pixels, such as the boundary regions. With extensive experiments, we\ndemonstrate that ENTO shows superior performance using various encoders, with\nthe two novel components playing their unique roles that are mutually\ncomplementary.\n","authors":["Seung Woo Ko","Joopyo Hong","Suyoung Kim","Seungjai Bang","Sungzoon Cho","Nojun Kwak","Hyung-Sin Kim","Joonseok Lee"],"pdf_url":"https://arxiv.org/pdf/2503.14035v1.pdf","comment":"Published in BMVC 2024, 13 pages, 7 figures (Appendix: 5 pages, 2\n  figures)"},{"id":"http://arxiv.org/abs/2503.14034v1","updated":"2025-03-18T08:51:20Z","published":"2025-03-18T08:51:20Z","title":"Shift, Scale and Rotation Invariant Multiple Object Detection using\n  Balanced Joint Transform Correlator","summary":"  The Polar Mellin Transform (PMT) is a well-known technique that converts\nimages into shift, scale and rotation invariant signatures for object detection\nusing opto-electronic correlators. However, this technique cannot be properly\napplied when there are multiple targets in a single input. Here, we propose a\nSegmented PMT (SPMT) that extends this methodology for cases where multiple\nobjects are present within the same frame. Simulations show that this SPMT can\nbe integrated into an opto-electronic joint transform correlator to create a\ncorrelation system capable of detecting multiple objects simultaneously,\npresenting robust detection capabilities across various transformation\nconditions, with remarkable discrimination between matching and non-matching\ntargets.\n","authors":["Xi Shen","Julian Gamboa","Tabassom Hamidfar","Shamima Mitu","Selim M. Shahriar"],"pdf_url":"https://arxiv.org/pdf/2503.14034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14029v1","updated":"2025-03-18T08:42:23Z","published":"2025-03-18T08:42:23Z","title":"Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting","summary":"  Lifting multi-view 2D instance segmentation to a radiance field has proven to\nbe effective to enhance 3D understanding. Existing methods rely on direct\nmatching for end-to-end lifting, yielding inferior results; or employ a\ntwo-stage solution constrained by complex pre- or post-processing. In this\nwork, we design a new end-to-end object-aware lifting approach, named\nUnified-Lift that provides accurate 3D segmentation based on the 3D Gaussian\nrepresentation. To start, we augment each Gaussian point with an additional\nGaussian-level feature learned using a contrastive loss to encode instance\ninformation. Importantly, we introduce a learnable object-level codebook to\naccount for individual objects in the scene for an explicit object-level\nunderstanding and associate the encoded object-level features with the\nGaussian-level point features for segmentation predictions. While promising,\nachieving effective codebook learning is non-trivial and a naive solution leads\nto degraded performance. Therefore, we formulate the association learning\nmodule and the noisy label filtering module for effective and robust codebook\nlearning. We conduct experiments on three benchmarks: LERF-Masked, Replica, and\nMessy Rooms datasets. Both qualitative and quantitative results manifest that\nour Unified-Lift clearly outperforms existing methods in terms of segmentation\nquality and time efficiency. The code is publicly available at\n\\href{https://github.com/Runsong123/Unified-Lift}{https://github.com/Runsong123/Unified-Lift}.\n","authors":["Runsong Zhu","Shi Qiu","Zhengzhe Liu","Ka-Hei Hui","Qianyi Wu","Pheng-Ann Heng","Chi-Wing Fu"],"pdf_url":"https://arxiv.org/pdf/2503.14029v1.pdf","comment":"CVPR 2025. The code is publicly available at this https URL\n  (https://github.com/Runsong123/Unified-Lift)"},{"id":"http://arxiv.org/abs/2503.14024v1","updated":"2025-03-18T08:35:39Z","published":"2025-03-18T08:35:39Z","title":"Uncertainty-Aware Global-View Reconstruction for Multi-View Multi-Label\n  Feature Selection","summary":"  In recent years, multi-view multi-label learning (MVML) has gained popularity\ndue to its close resemblance to real-world scenarios. However, the challenge of\nselecting informative features to ensure both performance and efficiency\nremains a significant question in MVML. Existing methods often extract\ninformation separately from the consistency part and the complementary part,\nwhich may result in noise due to unclear segmentation. In this paper, we\npropose a unified model constructed from the perspective of global-view\nreconstruction. Additionally, while feature selection methods can discern the\nimportance of features, they typically overlook the uncertainty of samples,\nwhich is prevalent in realistic scenarios. To address this, we incorporate the\nperception of sample uncertainty during the reconstruction process to enhance\ntrustworthiness. Thus, the global-view is reconstructed through the graph\nstructure between samples, sample confidence, and the view relationship. The\naccurate mapping is established between the reconstructed view and the label\nmatrix. Experimental results demonstrate the superior performance of our method\non multi-view datasets.\n","authors":["Pingting Hao","Kunpeng Liu","Wanfu Gao"],"pdf_url":"https://arxiv.org/pdf/2503.14024v1.pdf","comment":"9 pages,5 figures, accept in AAAI 25"},{"id":"http://arxiv.org/abs/2405.18839v4","updated":"2025-03-18T08:34:42Z","published":"2024-05-29T07:40:31Z","title":"MEGA: Masked Generative Autoencoder for Human Mesh Recovery","summary":"  Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous\nproblem, as an infinite set of 3D interpretations can explain the 2D\nobservation equally well. Nevertheless, most HMR methods overlook this issue\nand make a single prediction without accounting for this ambiguity. A few\napproaches generate a distribution of human meshes, enabling the sampling of\nmultiple predictions; however, none of them is competitive with the latest\nsingle-output model when making a single prediction. This work proposes a new\napproach based on masked generative modeling. By tokenizing the human pose and\nshape, we formulate the HMR task as generating a sequence of discrete tokens\nconditioned on an input image. We introduce MEGA, a MaskEd Generative\nAutoencoder trained to recover human meshes from images and partial human mesh\ntoken sequences. Given an image, our flexible generation scheme allows us to\npredict a single human mesh in deterministic mode or to generate multiple human\nmeshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA\nachieves state-of-the-art performance in deterministic and stochastic modes,\noutperforming single-output and multi-output approaches.\n","authors":["Guénolé Fiche","Simon Leglaive","Xavier Alameda-Pineda","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2405.18839v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16064v3","updated":"2025-03-18T08:34:36Z","published":"2024-11-25T03:28:09Z","title":"Multi-Granularity Class Prototype Topology Distillation for\n  Class-Incremental Source-Free Unsupervised Domain Adaptation","summary":"  This paper explores the Class-Incremental Source-Free Unsupervised Domain\nAdaptation (CI-SFUDA) problem, where the unlabeled target data come\nincrementally without access to labeled source instances. This problem poses\ntwo challenges, the interference of similar source-class knowledge in\ntarget-class representation learning and the shocks of new target knowledge to\nold ones. To address them, we propose the Multi-Granularity Class Prototype\nTopology Distillation (GROTO) algorithm, which effectively transfers the source\nknowledge to the class-incremental target domain. Concretely, we design the\nmulti-granularity class prototype self-organization module and the prototype\ntopology distillation module. First, we mine the positive classes by modeling\naccumulation distributions. Next, we introduce multi-granularity class\nprototypes to generate reliable pseudo-labels, and exploit them to promote the\npositive-class target feature self-organization. Second, the positive-class\nprototypes are leveraged to construct the topological structures of source and\ntarget feature spaces. Then, we perform the topology distillation to\ncontinually mitigate the shocks of new target knowledge to old ones. Extensive\nexperiments demonstrate that our proposed method achieves state-of-the-art\nperformance on three public datasets.\n","authors":["Peihua Deng","Jiehua Zhang","Xichun Sheng","Chenggang Yan","Yaoqi Sun","Ying Fu","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2411.16064v3.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14021v1","updated":"2025-03-18T08:32:22Z","published":"2025-03-18T08:32:22Z","title":"MP-GUI: Modality Perception with MLLMs for GUI Understanding","summary":"  Graphical user interface (GUI) has become integral to modern society, making\nit crucial to be understood for human-centric systems. However, unlike natural\nimages or documents, GUIs comprise artificially designed graphical elements\narranged to convey specific semantic meanings. Current multi-modal large\nlanguage models (MLLMs) already proficient in processing graphical and textual\ncomponents suffer from hurdles in GUI understanding due to the lack of explicit\nspatial structure modeling. Moreover, obtaining high-quality spatial structure\ndata is challenging due to privacy issues and noisy environments. To address\nthese challenges, we present MP-GUI, a specially designed MLLM for GUI\nunderstanding. MP-GUI features three precisely specialized perceivers to\nextract graphical, textual, and spatial modalities from the screen as\nGUI-tailored visual clues, with spatial structure refinement strategy and\nadaptively combined via a fusion gate to meet the specific preferences of\ndifferent GUI understanding tasks. To cope with the scarcity of training data,\nwe also introduce a pipeline for automatically data collecting. Extensive\nexperiments demonstrate that MP-GUI achieves impressive results on various GUI\nunderstanding tasks with limited data.\n","authors":["Ziwei Wang","Weizhi Chen","Leyang Yang","Sheng Zhou","Shengchu Zhao","Hanbei Zhan","Jiongchao Jin","Liangcheng Li","Zirui Shao","Jiajun Bu"],"pdf_url":"https://arxiv.org/pdf/2503.14021v1.pdf","comment":"Paper accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2412.08976v2","updated":"2025-03-18T08:30:23Z","published":"2024-12-12T06:13:32Z","title":"Identity-Preserving Pose-Guided Character Animation via Facial Landmarks\n  Transformation","summary":"  Creating realistic pose-guided image-to-video character animations while\npreserving facial identity remains challenging, especially in complex and\ndynamic scenarios such as dancing, where precise identity consistency is\ncrucial. Existing methods frequently encounter difficulties maintaining facial\ncoherence due to misalignments between facial landmarks extracted from driving\nvideos that provide head pose and expression cues and the facial geometry of\nthe reference images. To address this limitation, we introduce the Facial\nLandmarks Transformation (FLT) method, which leverages a 3D Morphable Model to\naddress this limitation. FLT converts 2D landmarks into a 3D face model,\nadjusts the 3D face model to align with the reference identity, and then\ntransforms them back into 2D landmarks to guide the image-to-video generation\nprocess. This approach ensures accurate alignment with the reference facial\ngeometry, enhancing the consistency between generated videos and reference\nimages. Experimental results demonstrate that FLT effectively preserves facial\nidentity, significantly improving pose-guided character animation models.\n","authors":["Lianrui Mu","Xingze Zhou","Wenjie Zheng","Jiangnan Ye","Haoji Hu"],"pdf_url":"https://arxiv.org/pdf/2412.08976v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.14013v1","updated":"2025-03-18T08:20:35Z","published":"2025-03-18T08:20:35Z","title":"Boosting Semi-Supervised Medical Image Segmentation via Masked Image\n  Consistency and Discrepancy Learning","summary":"  Semi-supervised learning is of great significance in medical image\nsegmentation by exploiting unlabeled data. Among its strategies, the\nco-training framework is prominent. However, previous co-training studies\npredominantly concentrate on network initialization variances and pseudo-label\ngeneration, while overlooking the equilibrium between information interchange\nand model diversity preservation. In this paper, we propose the Masked Image\nConsistency and Discrepancy Learning (MICD) framework with three key modules.\nThe Masked Cross Pseudo Consistency (MCPC) module enriches context perception\nand small sample learning via pseudo-labeling across masked-input branches. The\nCross Feature Consistency (CFC) module fortifies information exchange and model\nrobustness by ensuring decoder feature consistency. The Cross Model Discrepancy\n(CMD) module utilizes EMA teacher networks to oversee outputs and preserve\nbranch diversity. Together, these modules address existing limitations by\nfocusing on fine-grained local information and maintaining diversity in a\nheterogeneous framework. Experiments on two public medical image datasets, AMOS\nand Synapse, demonstrate that our approach outperforms state-of-the-art\nmethods.\n","authors":["Pengcheng Zhou","Lantian Zhang","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2503.14013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14012v1","updated":"2025-03-18T08:20:24Z","published":"2025-03-18T08:20:24Z","title":"LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote\n  Sensing Image Object Detection","summary":"  Remote sensing object detection (RSOD) faces formidable challenges in complex\nvisual environments. Aerial and satellite images inherently suffer from\nlimitations such as low spatial resolution, sensor noise, blurred objects,\nlow-light degradation, and partial occlusions. These degradation factors\ncollectively compromise the feature discriminability in detection models,\nresulting in three key issues: (1) reduced contrast that hampers\nforeground-background separation, (2) structural discontinuities in edge\nrepresentations, and (3) ambiguous feature responses caused by variations in\nillumination. These collectively weaken model robustness and deployment\nfeasibility. To address these challenges, we propose LEGNet, a lightweight\nnetwork that incorporates a novel edge-Gaussian aggregation (EGA) module\nspecifically designed for low-quality remote sensing images. Our key innovation\nlies in the synergistic integration of Scharr operator-based edge priors with\nuncertainty-aware Gaussian modeling: (a) The orientation-aware Scharr filters\npreserve high-frequency edge details with rotational invariance; (b) The\nuncertainty-aware Gaussian layers probabilistically refine low-confidence\nfeatures through variance estimation. This design enables precision enhancement\nwhile maintaining architectural simplicity. Comprehensive evaluations across\nfour RSOD benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0) and a UAV-view\ndataset (VisDrone2019) demonstrate significant improvements. LEGNet achieves\nstate-of-the-art performance across five benchmark datasets while ensuring\ncomputational efficiency, making it well-suited for deployment on\nresource-constrained edge devices in real-world remote sensing applications.\nThe code is available at https://github.com/lwCVer/LEGNet.\n","authors":["Wei Lu","Si-Bao Chen","Hui-Dong Li","Qing-Ling Shu","Chris H. Q. Ding","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2503.14012v1.pdf","comment":"12 pages, 5 figures. Remote Sensing Image Object Detection"},{"id":"http://arxiv.org/abs/2411.16156v2","updated":"2025-03-18T08:15:28Z","published":"2024-11-25T07:32:02Z","title":"VideoOrion: Tokenizing Object Dynamics in Videos","summary":"  We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos - the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.\n","authors":["Yicheng Feng","Yijiang Li","Wanpeng Zhang","Hao Luo","Zihao Yue","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16156v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14002v1","updated":"2025-03-18T08:09:24Z","published":"2025-03-18T08:09:24Z","title":"MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling","summary":"  Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.\n","authors":["Damian Boborzi","Phillip Mueller","Jonas Emrich","Dominik Schmid","Sebastian Mueller","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2503.14002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14001v1","updated":"2025-03-18T08:09:19Z","published":"2025-03-18T08:09:19Z","title":"Multimodal Feature-Driven Deep Learning for the Prediction of Duck Body\n  Dimensions and Weight","summary":"  Accurate body dimension and weight measurements are critical for optimizing\npoultry management, health assessment, and economic efficiency. This study\nintroduces an innovative deep learning-based model leveraging multimodal\ndata-2D RGB images from different views, depth images, and 3D point clouds-for\nthe non-invasive estimation of duck body dimensions and weight. A dataset of\n1,023 Linwu ducks, comprising over 5,000 samples with diverse postures and\nconditions, was collected to support model training. The proposed method\ninnovatively employs PointNet++ to extract key feature points from point\nclouds, extracts and computes corresponding 3D geometric features, and fuses\nthem with multi-view convolutional 2D features. A Transformer encoder is then\nutilized to capture long-range dependencies and refine feature interactions,\nthereby enhancing prediction robustness. The model achieved a mean absolute\npercentage error (MAPE) of 6.33% and an R2 of 0.953 across eight morphometric\nparameters, demonstrating strong predictive capability. Unlike conventional\nmanual measurements, the proposed model enables high-precision estimation while\neliminating the necessity for physical handling, thereby reducing animal stress\nand broadening its application scope. This study marks the first application of\ndeep learning techniques to poultry body dimension and weight estimation,\nproviding a valuable reference for the intelligent and precise management of\nthe livestock industry with far-reaching practical significance.\n","authors":["Yi Xiao","Qiannan Han","Guiping Liang","Hongyan Zhang","Song Wang","Zhihao Xu","Weican Wan","Chuang Li","Guitao Jiang","Wenbo Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.14001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02784v3","updated":"2025-03-18T08:08:21Z","published":"2024-03-05T08:57:28Z","title":"DDF: A Novel Dual-Domain Image Fusion Strategy for Remote Sensing Image\n  Semantic Segmentation with Unsupervised Domain Adaptation","summary":"  Semantic segmentation of remote sensing images is a challenging and hot issue\ndue to the large amount of unlabeled data. Unsupervised domain adaptation (UDA)\nhas proven to be advantageous in incorporating unclassified information from\nthe target domain. However, independently fine-tuning UDA models on the source\nand target domains has a limited effect on the outcome. This paper proposes a\nhybrid training strategy as well as a novel dual-domain image fusion strategy\nthat effectively utilizes the original image, transformation image, and\nintermediate domain information. Moreover, to enhance the precision of\npseudo-labels, we present a pseudo-label region-specific weight strategy. The\nefficacy of our approach is substantiated by extensive benchmark experiments\nand ablation studies conducted on the ISPRS Vaihingen and Potsdam datasets.\n","authors":["Lingyan Ran","Lushuang Wang","Tao Zhuo","Yinghui Xing"],"pdf_url":"https://arxiv.org/pdf/2403.02784v3.pdf","comment":"Accepted to IEEE Transactions on Geoscience and Remote Sensing"},{"id":"http://arxiv.org/abs/2503.13999v1","updated":"2025-03-18T08:06:05Z","published":"2025-03-18T08:06:05Z","title":"BI-RADS prediction of mammographic masses using uncertainty information\n  extracted from a Bayesian Deep Learning model","summary":"  The BI_RADS score is a probabilistic reporting tool used by radiologists to\nexpress the level of uncertainty in predicting breast cancer based on some\nmorphological features in mammography images. There is a significant\nvariability in describing masses which sometimes leads to BI_RADS\nmisclassification. Using a BI_RADS prediction system is required to support the\nfinal radiologist decisions. In this study, the uncertainty information\nextracted by a Bayesian deep learning model is utilized to predict the BI_RADS\nscore. The investigation results based on the pathology information demonstrate\nthat the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and\n48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%\nand 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the\nmodel can distinguish malignant from benign samples in the BI_RADS 0 category\nof the used dataset with an accuracy of 75.86% and correctly identify all\nmalignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays\nattention to the morphological features of the lesions. Therefore, this study\nshows the uncertainty-aware Bayesian Deep Learning model can report his\nuncertainty about the malignancy of a lesion based on morphological features,\nlike a radiologist.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2503.13999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01754v2","updated":"2025-03-18T08:05:25Z","published":"2025-03-03T17:24:42Z","title":"SDRT: Enhance Vision-Language Models by Self-Distillation with Diverse\n  Reasoning Traces","summary":"  Reasoning is increasingly crucial for various tasks. While chain-of-thought\nprompting enables large language models to leverage reasoning effectively,\nharnessing the reasoning capabilities of Vision-Language Models (VLMs) remains\nchallenging. To solve this problem, we propose a novel self-distillation\nframework that enhances the reasoning capabilities of the model. The proposed\nframework introduces several key innovations. We start by employing a prompt\nlibrary tailored to visual reasoning tasks to generate diverse in-context\nquestions and utilize a two-step reasoning procedure to derive reasoning-guided\nresponses. These responses are then used for self-distillation, enabling the\nmodel to internalize the reasoning process. Additionally, we improve the model\narchitecture with several innovative components, including an intervention\nadapter for efficient parameter updates, a cross-modal skip connection to\nfacilitate information exchange between modalities, and an ensemble learning\nalgorithm to integrate diverse reasoning from multiple in-context questions.\nExtensive experiments show that our method significantly improves the baseline\nperformance across five VQA datasets.\n","authors":["Guande Wu","Huan Song","Yawei Wang","Qiaojing Yan","Yijun Tian","Lin Lee Cheong","Panpan Xu"],"pdf_url":"https://arxiv.org/pdf/2503.01754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17212v2","updated":"2025-03-18T08:05:10Z","published":"2025-02-24T14:44:40Z","title":"A Two-step Linear Mixing Model for Unmixing under Hyperspectral\n  Variability","summary":"  Spectral unmixing is an important task in the research field of hyperspectral\nimage processing. It can be thought of as a regression problem, where the\nobserved variable (i.e., an image pixel) is to be found as a function of the\nresponse variables (i.e., the pure materials in a scene, called endmembers).\nThe Linear Mixing Model (LMM) has received a great deal of attention, due to\nits simplicity and ease of use in, e.g., optimization problems. Its biggest\nflaw is that it assumes that any pure material can be characterized by one\nunique spectrum throughout the entire scene. In many cases this is incorrect:\nthe endmembers face a significant amount of spectral variability caused by,\ne.g., illumination conditions, atmospheric effects, or intrinsic variability.\nResearchers have suggested several generalizations of the LMM to mitigate this\neffect. However, most models lead to ill-posed and highly non-convex\noptimization problems, which are hard to solve and have hyperparameters that\nare difficult to tune. In this paper, we propose a two-step LMM that bridges\nthe gap between model complexity and computational tractability. We show that\nthis model leads to only a mildly non-convex optimization problem, which we\nsolve with an interior-point solver. This method requires virtually no\nhyperparameter tuning, and can therefore be used easily and quickly in a wide\nrange of unmixing tasks. We show that the model is competitive and in some\ncases superior to existing and well-established unmixing methods and\nalgorithms. We do this through several experiments on synthetic data, real-life\nsatellite data, and hybrid synthetic-real data.\n","authors":["Xander Haijen","Bikram Koirala","Xuanwen Tao","Paul Scheunders"],"pdf_url":"https://arxiv.org/pdf/2502.17212v2.pdf","comment":"13 pages, 10 figures, 5 tables. This work has been submitted to the\n  IEEE for possible publication"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2501.09292v3","updated":"2025-03-18T16:42:17Z","published":"2025-01-16T04:56:33Z","title":"To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic\n  Retrieval Augmented Generation","summary":"  Retrieval-Augmented Generation equips large language models with the\ncapability to retrieve external knowledge, thereby mitigating hallucinations by\nincorporating information beyond the model's intrinsic abilities. However, most\nprior works have focused on invoking retrieval deterministically, which makes\nit unsuitable for tasks such as long-form question answering. Instead,\ndynamically performing retrieval by invoking it only when the underlying LLM\nlacks the required knowledge can be more efficient. In this context, we delve\ndeeper into the question, \"To Retrieve or Not to Retrieve?\" by exploring\nmultiple uncertainty detection methods. We evaluate these methods for the task\nof long-form question answering, employing dynamic retrieval, and present our\ncomparisons. Our findings suggest that uncertainty detection metrics, such as\nDegree Matrix Jaccard and Eccentricity, can reduce the number of retrieval\ncalls by almost half, with only a slight reduction in question-answering\naccuracy.\n","authors":["Kaustubh D. Dhole"],"pdf_url":"https://arxiv.org/pdf/2501.09292v3.pdf","comment":"1st workshop of \"Quantify Uncertainty and Hallucination in Foundation\n  Models: The Next Frontier in Reliable AI\" at ICLR 2025"},{"id":"http://arxiv.org/abs/2501.00513v2","updated":"2025-03-18T16:01:24Z","published":"2024-12-31T15:53:50Z","title":"CaReBench: A Fine-Grained Benchmark for Video Captioning and Retrieval","summary":"  Video understanding, including video captioning and retrieval, is still a\ngreat challenge for video-language models (VLMs). The existing video retrieval\nand caption benchmarks only include short descriptions, limits their ability of\ndetailed video understanding evaluation. To address this problem, we present\nCaReBench, a testing benchmark for fine-grained video captioning and retrieval\nwith 1,000 high-quality pairs of videos and human-annotated detailed captions.\nUniquely, it provides manually separated spatial annotations and temporal\nannotations for each video. Based on this design, we introduce two evaluation\nmetrics, ReBias and CapST, specifically tailored for video retrieval and video\ncaptioning tasks, respectively. These metrics enable a comprehensive\ninvestigation into the spatial and temporal biases inherent in VLMs. In\naddition, to handle both video retrieval and video captioning tasks in a\nunified framework, we develop a simple baseline based on a Multimodal Language\nModel (MLLM). By implementing a two-stage Supervised Fine-Tuning (SFT), we\nfully unlock the potential of MLLM, enabling it not only to generate detailed\nvideo descriptions but also to extract video features. Surprisingly,\nexperimental results demonstrate that, compared to the CLIP-based models\ndesigned for retrieval and the popular MLLMs skilled in video captioning, our\nbaseline shows competitive performance in both fine-grained video retrieval and\nvideo detailed captioning.\n","authors":["Yifan Xu","Xinhao Li","Yichun Yang","Desen Meng","Rui Huang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.00513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18770v2","updated":"2025-03-18T14:32:07Z","published":"2024-05-29T05:20:02Z","title":"Multimodal Adversarial Defense for Vision-Language Models by Leveraging\n  One-To-Many Relationships","summary":"  Pre-trained vision-language (VL) models are highly vulnerable to adversarial\nattacks. However, existing defense methods primarily focus on image\nclassification, overlooking two key aspects of VL tasks: multimodal attacks,\nwhere both image and text can be perturbed, and the one-to-many relationship of\nimages and texts, where a single image can correspond to multiple textual\ndescriptions and vice versa (1:N and N:1). This work is the first to explore\ndefense strategies against multimodal attacks in VL tasks, whereas prior VL\ndefense methods focus on vision robustness. We propose multimodal adversarial\ntraining (MAT), which incorporates adversarial perturbations in both image and\ntext modalities during training, significantly outperforming existing unimodal\ndefenses. Furthermore, we discover that MAT is limited by deterministic\none-to-one (1:1) image-text pairs in VL training data. To address this, we\nconduct a comprehensive study on leveraging one-to-many relationships to\nenhance robustness, investigating diverse augmentation techniques. Our analysis\nshows that, for a more effective defense, augmented image-text pairs should be\nwell-aligned, diverse, yet avoid distribution shift -- conditions overlooked by\nprior research. Our experiments show that MAT can effectively be applied to\ndifferent VL models and tasks to improve adversarial robustness, outperforming\nprevious efforts. Our code will be made public upon acceptance.\n","authors":["Futa Waseda","Antonio Tejero-de-Pablos","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2405.18770v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2301.02457v2","updated":"2025-03-18T14:02:20Z","published":"2023-01-06T10:42:29Z","title":"Better Differentially Private Approximate Histograms and Heavy Hitters\n  using the Misra-Gries Sketch","summary":"  We consider the problem of computing differentially private approximate\nhistograms and heavy hitters in a stream of elements. In the non-private\nsetting, this is often done using the sketch of Misra and Gries [Science of\nComputer Programming, 1982]. Chan, Li, Shi, and Xu [PETS 2012] describe a\ndifferentially private version of the Misra-Gries sketch, but the amount of\nnoise it adds can be large and scales linearly with the size of the sketch; the\nmore accurate the sketch is, the more noise this approach has to add. We\npresent a better mechanism for releasing a Misra-Gries sketch under\n$(\\varepsilon,\\delta)$-differential privacy. It adds noise with magnitude\nindependent of the size of the sketch; in fact, the maximum error coming from\nthe noise is the same as the best known in the private non-streaming setting,\nup to a constant factor. Our mechanism is simple and likely to be practical. We\nalso give a simple post-processing step of the Misra-Gries sketch that does not\nincrease the worst-case error guarantee. It is sufficient to add noise to this\nnew sketch with less than twice the magnitude of the non-streaming setting.\nThis improves on the previous result for $\\varepsilon$-differential privacy\nwhere the noise scales linearly to the size of the sketch. Finally, we consider\na general setting where users can contribute multiple distinct elements. We\npresent a new sketch with maximum error matching the Misra-Gries sketch. For\nmany parameters in this setting our sketch can be released with less noise\nunder $(\\varepsilon, \\delta)$-differential privacy.\n","authors":["Christian Janos Lebeda","Jakub Tětek"],"pdf_url":"https://arxiv.org/pdf/2301.02457v2.pdf","comment":"Added content for full version"},{"id":"http://arxiv.org/abs/2312.11356v2","updated":"2025-03-18T14:01:54Z","published":"2023-12-18T17:12:35Z","title":"The Problem of Coherence in Natural Language Explanations of\n  Recommendations","summary":"  Providing natural language explanations for recommendations is particularly\nuseful from the perspective of a non-expert user. Although several methods for\nproviding such explanations have recently been proposed, we argue that an\nimportant aspect of explanation quality has been overlooked in their\nexperimental evaluation. Specifically, the coherence between generated text and\npredicted rating, which is a necessary condition for an explanation to be\nuseful, is not properly captured by currently used evaluation measures. In this\npaper, we highlight the issue of explanation and prediction coherence by 1)\npresenting results from a manual verification of explanations generated by one\nof the state-of-the-art approaches 2) proposing a method of automatic coherence\nevaluation 3) introducing a new transformer-based method that aims to produce\nmore coherent explanations than the state-of-the-art approaches 4) performing\nan experimental evaluation which demonstrates that this method significantly\nimproves the explanation coherence without affecting the other aspects of\nrecommendation performance.\n","authors":["Jakub Raczyński","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2312.11356v2.pdf","comment":"ECAI 2023"},{"id":"http://arxiv.org/abs/2503.14258v1","updated":"2025-03-18T13:48:18Z","published":"2025-03-18T13:48:18Z","title":"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System","summary":"  This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.\n","authors":["Weihang Su","Baoqing Yue","Qingyao Ai","Yiran Hu","Jiaqi Li","Changyue Wang","Kaiyuan Zhang","Yueyue Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14251v1","updated":"2025-03-18T13:39:46Z","published":"2025-03-18T13:39:46Z","title":"Towards a Barrier-free GeoQA Portal: Natural Language Interaction with\n  Geospatial Data Using Multi-Agent LLMs and Semantic Search","summary":"  A Barrier-Free GeoQA Portal: Enhancing Geospatial Data Accessibility with a\nMulti-Agent LLM Framework\n  Geoportals are vital for accessing and analyzing geospatial data, promoting\nopen spatial data sharing and online geo-information management. Designed with\nGIS-like interaction and layered visualization, they often challenge non-expert\nusers with complex functionalities and overlapping layers that obscure spatial\nrelationships. We propose a GeoQA Portal using a multi-agent Large Language\nModel framework for seamless natural language interaction with geospatial data.\nComplex queries are broken into subtasks handled by specialized agents,\nretrieving relevant geographic data efficiently. Task plans are shown to users,\nboosting transparency. The portal supports default and custom data inputs for\nflexibility. Semantic search via word vector similarity aids data retrieval\ndespite imperfect terms. Case studies, evaluations, and user tests confirm its\neffectiveness for non-experts, bridging GIS complexity and public access, and\noffering an intuitive solution for future geoportals.\n","authors":["Yu Feng","Puzhen Zhang","Guohui Xiao","Linfang Ding","Liqiu Meng"],"pdf_url":"https://arxiv.org/pdf/2503.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14213v1","updated":"2025-03-18T12:47:01Z","published":"2025-03-18T12:47:01Z","title":"Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for\n  Credit Bond Recommendation","summary":"  Graph Neural Networks have significantly advanced research in recommender\nsystems over the past few years. These methods typically capture global\ninterests using aggregated past interactions and rely on static embeddings of\nusers and items over extended periods of time. While effective in some domains,\nthese methods fall short in many real-world scenarios, especially in finance,\nwhere user interests and item popularity evolve rapidly over time. To address\nthese challenges, we introduce a novel extension to Light Graph Convolutional\nNetwork (LightGCN) designed to learn temporal node embeddings that capture\ndynamic interests. Our approach employs causal convolution to maintain a\nforward-looking model architecture. By preserving the chronological order of\nuser-item interactions and introducing a dynamic update mechanism for\nembeddings through a sliding window, the proposed model generates well-timed\nand contextually relevant recommendations. Extensive experiments on a\nreal-world dataset from BNP Paribas demonstrate that our approach significantly\nenhances the performance of LightGCN while maintaining the simplicity and\nefficiency of its architecture. Our findings provide new insights into\ndesigning graph-based recommender systems in time-sensitive applications,\nparticularly for financial product recommendations.\n","authors":["Ashraf Ghiye","Baptiste Barreau","Laurent Carlier","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2503.14213v1.pdf","comment":"8 pages, published in the international conference for AI in Finance\n  (ACM ICAIF'24)"},{"id":"http://arxiv.org/abs/2503.14110v1","updated":"2025-03-18T10:27:14Z","published":"2025-03-18T10:27:14Z","title":"A Comprehensive Survey on Cross-Domain Recommendation: Taxonomy,\n  Progress, and Prospects","summary":"  Recommender systems (RS) have become crucial tools for information filtering\nin various real world scenarios. And cross domain recommendation (CDR) has been\nwidely explored in recent years in order to provide better recommendation\nresults in the target domain with the help of other domains. The CDR technology\nhas developed rapidly, yet there is a lack of a comprehensive survey\nsummarizing recent works. Therefore, in this paper, we will summarize the\nprogress and prospects based on the main procedure of CDR, including Cross\nDomain Relevance, Cross Domain Interaction, Cross Domain Representation\nEnhancement and Model Optimization. To help researchers better understand and\nengage in this field, we also organize the applications and resources, and\nhighlight several current important challenges and future directions of CDR.\nMore details of the survey articles are available at\nhttps://github.com/USTCAGI/Awesome-Cross-Domain\nRecommendation-Papers-and-Resources.\n","authors":["Hao Zhang","Mingyue Cheng","Qi Liu","Junzhe Jiang","Xianquan Wang","Rujiao Zhang","Chenyi Lei","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.14110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.05592v2","updated":"2025-03-18T08:32:24Z","published":"2025-03-07T17:14:44Z","title":"R1-Searcher: Incentivizing the Search Capability in LLMs via\n  Reinforcement Learning","summary":"  Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.\n","authors":["Huatong Song","Jinhao Jiang","Yingqian Min","Jie Chen","Zhipeng Chen","Wayne Xin Zhao","Lei Fang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2503.05592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21967v2","updated":"2025-03-18T04:42:54Z","published":"2024-10-29T11:51:06Z","title":"Dual Conditional Diffusion Models for Sequential Recommendation","summary":"  Recent advancements in diffusion models have shown promising results in\nsequential recommendation (SR). Existing approaches predominantly rely on\nimplicit conditional diffusion models, which compress user behaviors into a\nsingle representation during the forward diffusion process. While effective to\nsome extent, this oversimplification often leads to the loss of sequential and\ncontextual information, which is critical for understanding user behavior.\nMoreover, explicit information, such as user-item interactions or sequential\npatterns, remains underutilized, despite its potential to directly guide the\nrecommendation process and improve precision. However, combining implicit and\nexplicit information is non-trivial, as it requires dynamically integrating\nthese complementary signals while avoiding noise and irrelevant patterns within\nuser behaviors. To address these challenges, we propose Dual Conditional\nDiffusion Models for Sequential Recommendation (DCRec), which effectively\nintegrates implicit and explicit information by embedding dual conditions into\nboth the forward and reverse diffusion processes. This allows the model to\nretain valuable sequential and contextual information while leveraging explicit\nuser-item interactions to guide the recommendation process. Specifically, we\nintroduce the Dual Conditional Diffusion Transformer (DCDT), which employs a\ncross-attention mechanism to dynamically integrate explicit signals throughout\nthe diffusion stages, ensuring contextual understanding and minimizing the\ninfluence of irrelevant patterns. This design enables precise and contextually\nrelevant recommendations. Extensive experiments on public benchmark datasets\ndemonstrate that DCRec significantly outperforms state-of-the-art methods in\nboth accuracy and computational efficiency.\n","authors":["Hongtao Huang","Chengkai Huang","Tong Yu","Xiaojun Chang","Wen Hu","Julian McAuley","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2410.21967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09188v2","updated":"2025-03-18T04:06:55Z","published":"2024-06-13T14:49:28Z","title":"An Efficient Post-hoc Framework for Reducing Task Discrepancy of Text\n  Encoders for Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable image searches.\nThe mainstream Zero-Shot (ZS) CIR methods bypass the need for expensive\ntraining CIR triplets by projecting image embeddings into the text token\nembedding space, forming a composed query for retrieval. However, we highlight\nan inherent limitation in these projection-based CIR: a task discrepancy of\ntext encoders between the original pre-training task of the encoders (text\n$\\leftrightarrow$ image) and the target CIR task (image + text\n$\\leftrightarrow$ image), which potentially negatively impacts CIR performance.\nTo reduce such a discrepancy, a naive solution would be to train both image and\ntext encoders with CIR triplets in a supervised manner. Instead, we introduce\nReducing Task Discrepancy of Text Encoders (RTD), an efficient text-only\npost-hoc framework that complements projection-based CIR methods. We devise a\nnovel target-anchored text contrastive learning designed to enhance the\ncapability of the text encoder for CIR. We also propose two key enhancements:\n(1) a hard negative-based refined batch sampling strategy and (2) a refined\nconcatenation scheme to further mitigate training-inference discrepancy.\nIntegrating RTD into state-of-the-art projection-based methods achieves\nperformance comparable to, or even surpassing, resource-intensive\nstate-of-the-art synthetic CIR triplet-based approaches only with 23 minutes of\nadditional training on 4 A100 GPUs (up to $100\\times$ faster in training). Our\ncode will be available upon acceptance.\n","authors":["Jaeseok Byun","Seokhyeon Jeong","Wonjae Kim","Sanghyuk Chun","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2406.09188v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2503.14649v1","updated":"2025-03-18T18:58:13Z","published":"2025-03-18T18:58:13Z","title":"RAGO: Systematic Performance Optimization for Retrieval-Augmented\n  Generation Serving","summary":"  Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.\n","authors":["Wenqi Jiang","Suvinay Subramanian","Cat Graves","Gustavo Alonso","Amir Yazdanbakhsh","Vidushi Dadu"],"pdf_url":"https://arxiv.org/pdf/2503.14649v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.14505v1","updated":"2025-03-18T17:59:58Z","published":"2025-03-18T17:59:58Z","title":"MusicInfuser: Making Video Diffusion Listen and Dance","summary":"  We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser.\n","authors":["Susung Hong","Ira Kemelmacher-Shlizerman","Brian Curless","Steven M. Seitz"],"pdf_url":"https://arxiv.org/pdf/2503.14505v1.pdf","comment":"Project page: https://susunghong.github.io/MusicInfuser"},{"id":"http://arxiv.org/abs/2503.14503v1","updated":"2025-03-18T17:59:54Z","published":"2025-03-18T17:59:54Z","title":"The Power of Context: How Multimodality Improves Image Super-Resolution","summary":"  Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/.\n","authors":["Kangfu Mei","Hossein Talebi","Mojtaba Ardakani","Vishal M. Patel","Peyman Milanfar","Mauricio Delbracio"],"pdf_url":"https://arxiv.org/pdf/2503.14503v1.pdf","comment":"accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.14500v1","updated":"2025-03-18T17:59:41Z","published":"2025-03-18T17:59:41Z","title":"Utilization of Neighbor Information for Image Classification with\n  Different Levels of Supervision","summary":"  We propose to bridge the gap between semi-supervised and unsupervised image\nrecognition with a flexible method that performs well for both generalized\ncategory discovery (GCD) and image clustering. Despite the overlap in\nmotivation between these tasks, the methods themselves are restricted to a\nsingle task -- GCD methods are reliant on the labeled portion of the data, and\ndeep image clustering methods have no built-in way to leverage the labels\nefficiently. We connect the two regimes with an innovative approach that\nUtilizes Neighbor Information for Classification (UNIC) both in the\nunsupervised (clustering) and semisupervised (GCD) setting. State-of-the-art\nclustering methods already rely heavily on nearest neighbors. We improve on\ntheir results substantially in two parts, first with a sampling and cleaning\nstrategy where we identify accurate positive and negative neighbors, and\nsecondly by finetuning the backbone with clustering losses computed by sampling\nboth types of neighbors. We then adapt this pipeline to GCD by utilizing the\nlabelled images as ground truth neighbors. Our method yields state-of-the-art\nresults for both clustering (+3% ImageNet-100, Imagenet200) and GCD (+0.8%\nImageNet-100, +5% CUB, +2% SCars, +4% Aircraft).\n","authors":["Gihan Jayatilaka","Abhinav Shrivastava","Matthew Gwilliam"],"pdf_url":"https://arxiv.org/pdf/2503.14500v1.pdf","comment":"18 pages, 16 figures, 7 tables"},{"id":"http://arxiv.org/abs/2503.14499v1","updated":"2025-03-18T17:59:31Z","published":"2025-03-18T17:59:31Z","title":"Measuring AI Ability to Complete Long Tasks","summary":"  Despite rapid progress on AI benchmarks, the real-world meaning of benchmark\nperformance remains unclear. To quantify the capabilities of AI systems in\nterms of human capabilities, we propose a new metric: 50%-task-completion time\nhorizon. This is the time humans typically take to complete tasks that AI\nmodels can complete with 50% success rate. We first timed humans with relevant\ndomain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter\ntasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet\nhave a 50% time horizon of around 50 minutes. Furthermore, frontier AI time\nhorizon has been doubling approximately every seven months since 2019, though\nthe trend may have accelerated in 2024. The increase in AI models' time\nhorizons seems to be primarily driven by greater reliability and ability to\nadapt to mistakes, combined with better logical reasoning and tool use\ncapabilities. We discuss the limitations of our results -- including their\ndegree of external validity -- and the implications of increased autonomy for\ndangerous capabilities. If these results generalize to real-world software\ntasks, extrapolation of this trend predicts that within 5 years, AI systems\nwill be capable of automating many software tasks that currently take humans a\nmonth.\n","authors":["Thomas Kwa","Ben West","Joel Becker","Amy Deng","Katharyn Garcia","Max Hasin","Sami Jawhar","Megan Kinniment","Nate Rush","Sydney Von Arx","Ryan Bloom","Thomas Broadley","Haoxing Du","Brian Goodrich","Nikola Jurkovic","Luke Harold Miles","Seraphina Nix","Tao Lin","Neev Parikh","David Rein","Lucas Jun Koba Sato","Hjalmar Wijk","Daniel M. Ziegler","Elizabeth Barnes","Lawrence Chan"],"pdf_url":"https://arxiv.org/pdf/2503.14499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14495v1","updated":"2025-03-18T17:58:28Z","published":"2025-03-18T17:58:28Z","title":"Temporal Consistency for LLM Reasoning Process Error Identification","summary":"  Verification is crucial for effective mathematical reasoning. We present a\nnew temporal consistency method where verifiers iteratively refine their\njudgments based on the previous assessment. Unlike one-round verification or\nmulti-model debate approaches, our method leverages consistency in a sequence\nof self-reflection actions to improve verification accuracy. Empirical\nevaluations across diverse mathematical process error identification benchmarks\n(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements\nover baseline methods. When applied to the recent DeepSeek R1 distilled models,\nour method demonstrates strong performance, enabling 7B/8B distilled models to\noutperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the\ndistilled 14B model with our method achieves performance comparable to\nDeepseek-R1. Our codes are available at\nhttps://github.com/jcguo123/Temporal-Consistency\n","authors":["Jiacheng Guo","Yue Wu","Jiahao Qiu","Kaixuan Huang","Xinzhe Juan","Ling Yang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14492v1","updated":"2025-03-18T17:57:54Z","published":"2025-03-18T17:57:54Z","title":"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal\n  Control","summary":"  We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1.\n","authors":[" NVIDIA"," :","Hassan Abu Alhaija","Jose Alvarez","Maciej Bala","Tiffany Cai","Tianshi Cao","Liz Cha","Joshua Chen","Mike Chen","Francesco Ferroni","Sanja Fidler","Dieter Fox","Yunhao Ge","Jinwei Gu","Ali Hassani","Michael Isaev","Pooya Jannaty","Shiyi Lan","Tobias Lasser","Huan Ling","Ming-Yu Liu","Xian Liu","Yifan Lu","Alice Luo","Qianli Ma","Hanzi Mao","Fabio Ramos","Xuanchi Ren","Tianchang Shen","Shitao Tang","Ting-Chun Wang","Jay Wu","Jiashu Xu","Stella Xu","Kevin Xie","Yuchong Ye","Xiaodong Yang","Xiaohui Zeng","Yu Zeng"],"pdf_url":"https://arxiv.org/pdf/2503.14492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17263v4","updated":"2025-03-18T17:56:58Z","published":"2024-10-07T08:43:22Z","title":"An Effective Theory of Bias Amplification","summary":"  Machine learning models can capture and amplify biases present in data,\nleading to disparate test performance across social groups. To better\nunderstand, evaluate, and mitigate these biases, a deeper theoretical\nunderstanding of how model design choices and data distribution properties\ncontribute to bias is needed. In this work, we contribute a precise analytical\ntheory in the context of ridge regression, both with and without random\nprojections, where the former models feedforward neural networks in a\nsimplified regime. Our theory offers a unified and rigorous explanation of\nmachine learning bias, providing insights into phenomena such as bias\namplification and minority-group bias in various feature and parameter regimes.\nFor example, we observe that there may be an optimal regularization penalty or\ntraining time to avoid bias amplification, and there can be differences in test\nerror between groups that are not alleviated with increased parameterization.\nImportantly, our theoretical predictions align with empirical observations\nreported in the literature on machine learning bias. We extensively empirically\nvalidate our theory on synthetic and semi-synthetic datasets.\n","authors":["Arjun Subramonian","Samuel J. Bell","Levent Sagun","Elvis Dohmatob"],"pdf_url":"https://arxiv.org/pdf/2410.17263v4.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14481v1","updated":"2025-03-18T17:53:20Z","published":"2025-03-18T17:53:20Z","title":"Don't lie to your friends: Learning what you know from collaborative\n  self-play","summary":"  To be helpful assistants, AI agents must be aware of their own capabilities\nand limitations. This includes knowing when to answer from parametric knowledge\nversus using tools, when to trust tool outputs, and when to abstain or hedge.\nSuch capabilities are hard to teach through supervised fine-tuning because they\nrequire constructing examples that reflect the agent's specific capabilities.\nWe therefore propose a radically new approach to teaching agents what they\nknow: \\emph{collaborative self-play}. We construct multi-agent collaborations\nin which the group is rewarded for collectively arriving at correct answers.\nThe desired meta-knowledge emerges from the incentives built into the structure\nof the interaction. We focus on small societies of agents that have access to\nheterogeneous tools (corpus-specific retrieval), and therefore must collaborate\nto maximize their success while minimizing their effort. Experiments show that\ngroup-level rewards for multi-agent communities can induce policies that\n\\emph{transfer} to improve tool use and selective prediction in settings where\nindividual agents are deployed in isolation.\n","authors":["Jacob Eisenstein","Reza Aghajani","Adam Fisch","Dheeru Dua","Fantine Huot","Mirella Lapata","Vicky Zayats","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2503.14481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14476v1","updated":"2025-03-18T17:49:06Z","published":"2025-03-18T17:49:06Z","title":"DAPO: An Open-Source LLM Reinforcement Learning System at Scale","summary":"  Inference scaling empowers LLMs with unprecedented reasoning ability, with\nreinforcement learning as the core technique to elicit complex reasoning.\nHowever, key technical details of state-of-the-art reasoning LLMs are concealed\n(such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the\ncommunity still struggles to reproduce their RL training results. We propose\nthe $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling\n$\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and\nfully open-source a state-of-the-art large-scale RL system that achieves 50\npoints on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that\nwithhold training details, we introduce four key techniques of our algorithm\nthat make large-scale LLM RL a success. In addition, we open-source our\ntraining code, which is built on the verl framework, along with a carefully\ncurated and processed dataset. These components of our open-source system\nenhance reproducibility and support future research in large-scale LLM RL.\n","authors":["Qiying Yu","Zheng Zhang","Ruofei Zhu","Yufeng Yuan","Xiaochen Zuo","Yu Yue","Tiantian Fan","Gaohong Liu","Lingjun Liu","Xin Liu","Haibin Lin","Zhiqi Lin","Bole Ma","Guangming Sheng","Yuxuan Tong","Chi Zhang","Mofan Zhang","Wang Zhang","Hang Zhu","Jinhua Zhu","Jiaze Chen","Jiangjie Chen","Chengyi Wang","Hongli Yu","Weinan Dai","Yuxuan Song","Xiangpeng Wei","Hao Zhou","Jingjing Liu","Wei-Ying Ma","Ya-Qin Zhang","Lin Yan","Mu Qiao","Yonghui Wu","Mingxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14476v1.pdf","comment":"Project Page: https://dapo-sia.github.io/"},{"id":"http://arxiv.org/abs/2410.12730v3","updated":"2025-03-18T17:48:13Z","published":"2024-10-16T16:44:12Z","title":"Counterfactual Generative Modeling with Variational Causal Inference","summary":"  Estimating an individual's counterfactual outcomes under interventions is a\nchallenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in the observed outcome in addition to the covariates.\nPrior works using variational inference in counterfactual generative modeling\nhave been focusing on neural adaptations and model variants within the\nconditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage disentangled exogenous noise abduction that aids the correct\nidentification of causal effect in counterfactual generations. In experiments,\nwe demonstrate the advantage of our framework compared to state-of-the-art\nmodels in counterfactual generative modeling on multiple benchmarks.\n","authors":["Yulun Wu","Louie McConnell","Claudia Iriondo"],"pdf_url":"https://arxiv.org/pdf/2410.12730v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2403.03362v2","updated":"2025-03-18T17:48:12Z","published":"2024-03-05T23:16:13Z","title":"Level Set Teleportation: An Optimization Perspective","summary":"  We study level set teleportation, an optimization routine which tries to\naccelerate gradient descent (GD) by maximizing the gradient norm over a level\nset of the objective. While teleportation intuitively speeds-up GD via bigger\nsteps, current work lacks convergence theory for convex functions, guarantees\nfor solving the teleportation operator, and even clear empirical evidence\nshowing this acceleration. We resolve these open questions. For convex\nfunctions satisfying Hessian stability, we prove that GD with teleportation\nobtains a combined sub-linear/linear convergence rate which is strictly faster\nthan GD when the optimality gap is small. This is in sharp contrast to the\nstandard (strongly) convex setting, where teleportation neither improves nor\nworsens convergence. To evaluate teleportation in practice, we develop a\nprojected-gradient method requiring only Hessian-vector products. We use this\nto show that gradient methods with access to a teleportation oracle out-perform\ntheir standard versions on a variety of problems. We also find that GD with\nteleportation is faster than truncated Newton methods, particularly for\nnon-convex optimization.\n","authors":["Aaron Mishkin","Alberto Bietti","Robert M. Gower"],"pdf_url":"https://arxiv.org/pdf/2403.03362v2.pdf","comment":"Published at AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.14473v1","updated":"2025-03-18T17:48:03Z","published":"2025-03-18T17:48:03Z","title":"EnQode: Fast Amplitude Embedding for Quantum Machine Learning Using\n  Classical Data","summary":"  Amplitude embedding (AE) is essential in quantum machine learning (QML) for\nencoding classical data onto quantum circuits. However, conventional AE methods\nsuffer from deep, variable-length circuits that introduce high output error due\nto extensive gate usage and variable error rates across samples, resulting in\nnoise-driven inconsistencies that degrade model accuracy. We introduce EnQode,\na fast AE technique based on symbolic representation that addresses these\nlimitations by clustering dataset samples and solving for cluster mean states\nthrough a low-depth, machine-specific ansatz. Optimized to reduce physical\ngates and SWAP operations, EnQode ensures all samples face consistent, low\nnoise levels by standardizing circuit depth and composition. With over 90%\nfidelity in data mapping, EnQode enables robust, high-performance QML on noisy\nintermediate-scale quantum (NISQ) devices. Our open-source solution provides a\nscalable and efficient alternative for integrating classical data with quantum\nmodels.\n","authors":["Jason Han","Nicholas S. DiBrita","Younghyun Cho","Hengrui Luo","Tirthak Patel"],"pdf_url":"https://arxiv.org/pdf/2503.14473v1.pdf","comment":"EnQode will appear in the Proceedings of the Design Automation\n  Conference (DAC), 2025"},{"id":"http://arxiv.org/abs/2409.07510v5","updated":"2025-03-18T17:46:41Z","published":"2024-09-11T17:58:39Z","title":"Still More Shades of Null: An Evaluation Suite for Responsible Missing\n  Value Imputation","summary":"  Data missingness is a practical challenge of sustained interest to the\nscientific community. In this paper, we present Shades-of-Null, an evaluation\nsuite for responsible missing value imputation. Our work is novel in two ways\n(i) we model realistic and socially-salient missingness scenarios that go\nbeyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random\n(MAR) and Missing Not At Random (MNAR) settings, to include multi-mechanism\nmissingness (when different missingness patterns co-exist in the data) and\nmissingness shift (when the missingness mechanism changes between training and\ntest) (ii) we evaluate imputers holistically, based on imputation quality and\nimputation fairness, as well as on the predictive performance, fairness and\nstability of the models that are trained and tested on the data\npost-imputation.\n  We use Shades-of-Null to conduct a large-scale empirical study involving\n29,736 experimental pipelines, and find that while there is no single\nbest-performing imputation approach for all missingness types, interesting\ntrade-offs arise between predictive performance, fairness and stability, based\non the combination of missingness scenario, imputer choice, and the\narchitecture of the predictive model. We make Shades-of-Null publicly\navailable, to enable researchers to rigorously evaluate missing value\nimputation methods on a wide range of metrics in plausible and socially\nmeaningful scenarios.\n","authors":["Falaah Arif Khan","Denys Herasymuk","Nazar Protsiv","Julia Stoyanovich"],"pdf_url":"https://arxiv.org/pdf/2409.07510v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14459v1","updated":"2025-03-18T17:33:10Z","published":"2025-03-18T17:33:10Z","title":"Doubly robust identification of treatment effects from multiple\n  environments","summary":"  Practical and ethical constraints often require the use of observational data\nfor causal inference, particularly in medicine and social sciences. Yet,\nobservational datasets are prone to confounding, potentially compromising the\nvalidity of causal conclusions. While it is possible to correct for biases if\nthe underlying causal graph is known, this is rarely a feasible ask in\npractical scenarios. A common strategy is to adjust for all available\ncovariates, yet this approach can yield biased treatment effect estimates,\nespecially when post-treatment or unobserved variables are present. We propose\nRAMEN, an algorithm that produces unbiased treatment effect estimates by\nleveraging the heterogeneity of multiple data sources without the need to know\nor learn the underlying causal graph. Notably, RAMEN achieves doubly robust\nidentification: it can identify the treatment effect whenever the causal\nparents of the treatment or those of the outcome are observed, and the node\nwhose parents are observed satisfies an invariance assumption. Empirical\nevaluations on synthetic and real-world datasets show that our approach\noutperforms existing methods.\n","authors":["Piersilvio De Bartolomeis","Julia Kostin","Javier Abad","Yixin Wang","Fanny Yang"],"pdf_url":"https://arxiv.org/pdf/2503.14459v1.pdf","comment":"Accepted for presentation at the International Conference on Learning\n  Representations (ICLR) 2025"},{"id":"http://arxiv.org/abs/2405.17465v2","updated":"2025-03-18T17:32:09Z","published":"2024-05-23T17:53:31Z","title":"Information Fusion in Smart Agriculture: Machine Learning Applications\n  and Future Research Directions","summary":"  Machine learning (ML) is a rapidly evolving technology with expanding\napplications across various fields. This paper presents a comprehensive survey\nof recent ML applications in agriculture for sustainability and efficiency.\nExisting reviews mainly focus on narrow subdomains or lack a fusion-driven\nperspectives. This study provides a combined analysis of ML applications in\nagriculture, structured around five key objectives: (i) Analyzing ML techniques\nacross pre-harvesting, harvesting, and post-harvesting phases. (ii)\nDemonstrating how ML can be used with agricultural data and data fusion. (iii)\nConducting a bibliometric and statistical analysis to reveal research trends\nand activity. (iv) Investigating real-world case studies of leading artificial\nintelligence (AI)-driven agricultural companies that use different types of\nmultisensors and multisource data. (v) Compiling publicly available datasets to\nsupport ML model training. Going beyond existing previous reviews, this review\nfocuses on how machine learning (ML) techniques, combined with multi-source\ndata fusion (integrating remote sensing, IoT, and climate analytics), enhance\nprecision agriculture by improving predictive accuracy and decision-making.\nCase studies and statistical insights illustrate the evolving landscape of AI\ndriven smart farming, while future research directions also discusses\nchallenges associated with data fusion for heterogeneous datasets. This review\nbridges the gap between AI research and agricultural applications, offering a\nroadmap for researchers, industry professionals, and policymakers to harness\ninformation fusion and ML for advancing precision agriculture.\n","authors":["Aashu Katharria","Kanchan Rajwar","Millie Pant","Juan D. Velásquez","Václav Snášel","Kusum Deep"],"pdf_url":"https://arxiv.org/pdf/2405.17465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14456v1","updated":"2025-03-18T17:31:05Z","published":"2025-03-18T17:31:05Z","title":"RWKV-7 \"Goose\" with Expressive Dynamic State Evolution","summary":"  We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with\npre-trained language models that establish a new state-of-the-art in downstream\nperformance at the 3 billion parameter scale on multilingual tasks, and match\ncurrent SoTA English language performance despite being trained on dramatically\nfewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only\nconstant memory usage and constant inference time per token. RWKV-7 introduces\na newly generalized formulation of the delta rule with vector-valued gating and\nin-context learning rates, as well as a relaxed value replacement rule. We show\nthat RWKV-7 can perform state tracking and recognize all regular languages,\nwhile retaining parallelizability of training. This exceeds the capabilities of\nTransformers under standard complexity conjectures, which are limited to\n$\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also\npresent an extended open source 3.1 trillion token multilingual corpus, and\ntrain four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on\nthis dataset.\n  To foster openness, reproduction, and adoption, we release our models and\ndataset component listing at https://huggingface.co/RWKV, and our training and\ninference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0\nLicense.\n","authors":["Bo Peng","Ruichong Zhang","Daniel Goldstein","Eric Alcaide","Haowen Hou","Janna Lu","William Merrill","Guangyu Song","Kaifeng Tan","Saiteja Utpala","Nathan Wilce","Johan S. Wind","Tianyi Wu","Daniel Wuttke","Christian Zhou-Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.14456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14453v1","updated":"2025-03-18T17:30:26Z","published":"2025-03-18T17:30:26Z","title":"Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud\n  Offloading","summary":"  Consider an edge computing setting in which a user submits queries for the\nsolution of a linear system to an edge processor, which is subject to\ntime-varying computing availability. The edge processor applies a probabilistic\nlinear solver (PLS) so as to be able to respond to the user's query within the\nallotted time and computing budget. Feedback to the user is in the form of an\nuncertainty set. Due to model misspecification, the uncertainty set obtained\nvia a direct application of PLS does not come with coverage guarantees with\nrespect to the true solution of the linear system. This work introduces a new\nmethod to calibrate the uncertainty sets produced by PLS with the aim of\nguaranteeing long-term coverage requirements. The proposed method, referred to\nas online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from\ncloud to edge. This enables the online calibration of uncertainty thresholds\nvia online conformal prediction (OCP), an online optimization method previously\nstudied in the context of prediction models. The validity of OCP-PLS is\nverified via experiments that bring insights into trade-offs between coverage,\nprediction set size, and cloud usage.\n","authors":["Qiushuo Hou","Sangwoo Park","Matteo Zecchin","Yunlong Cai","Guanding Yu","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2503.14453v1.pdf","comment":"This paper has been submitted to a conference"},{"id":"http://arxiv.org/abs/2212.06492v2","updated":"2025-03-18T17:28:14Z","published":"2022-12-13T11:17:32Z","title":"FNDaaS: Content-agnostic Detection of Fake News sites","summary":"  Automatic fake news detection is a challenging problem in misinformation\nspreading, and it has tremendous real-world political and social impacts. Past\nstudies have proposed machine learning-based methods for detecting such fake\nnews, focusing on different properties of the published news articles, such as\nlinguistic characteristics of the actual content, which however have\nlimitations due to the apparent language barriers. Departing from such efforts,\nwe propose Fake News Detection-as-a Service (FNDaaS), the first automatic,\ncontent-agnostic fake news detection method, that considers new and unstudied\nfeatures such as network and structural characteristics per news website. This\nmethod can be enforced as-a-Service, either at the ISP-side for easier\nscalability and maintenance, or user-side for better end-user privacy. We\ndemonstrate the efficacy of our method using more than 340K datapoints crawled\nfrom existing lists of 637 fake and 1183 real news websites, and by building\nand testing a proof of concept system that materializes our proposal. Our\nanalysis of data collected from these websites shows that the vast majority of\nfake news domains are very young and appear to have lower time periods of an IP\nassociated with their domain than real news ones. By conducting various\nexperiments with machine learning classifiers, we demonstrate that FNDaaS can\nachieve an AUC score of up to 0.967 on past sites, and up to 77-92% accuracy on\nnewly-flagged ones.\n","authors":["Panagiotis Papadopoulos","Dimitris Spithouris","Evangelos P. Markatos","Nicolas Kourtellis"],"pdf_url":"https://arxiv.org/pdf/2212.06492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17371v3","updated":"2025-03-18T17:20:37Z","published":"2025-02-24T17:52:01Z","title":"Sustainable Greenhouse Microclimate Modeling: A Comparative Analysis of\n  Recurrent and Graph Neural Networks","summary":"  The integration of photovoltaic (PV) systems into greenhouses not only\noptimizes land use but also enhances sustainable agricultural practices by\nenabling dual benefits of food production and renewable energy generation.\nHowever, accurate prediction of internal environmental conditions is crucial to\nensure optimal crop growth while maximizing energy production. This study\nintroduces a novel application of Spatio-Temporal Graph Neural Networks\n(STGNNs) to greenhouse microclimate modeling, comparing their performance with\ntraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal\npattern recognition, they cannot explicitly model the directional relationships\nbetween environmental variables. Our STGNN approach addresses this limitation\nby representing these relationships as directed graphs, enabling the model to\ncapture both environmental dependencies and their directionality. Using\nhigh-frequency data collected at 15-minute intervals from a greenhouse in\nVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter\nconditions ($R^2 = 0.985$) but show limitations during summer cooling system\noperation. Though STGNNs currently show lower performance (winter $R^2 =\n0.947$), their architecture offers greater potential for integrating additional\nvariables such as PV generation and crop growth indicators.\n","authors":["Emiliano Seri","Marcello Petitta","Cristina Cornaro"],"pdf_url":"https://arxiv.org/pdf/2502.17371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14443v1","updated":"2025-03-18T17:19:12Z","published":"2025-03-18T17:19:12Z","title":"EnvBench: A Benchmark for Automated Environment Setup","summary":"  Recent advances in Large Language Models (LLMs) have enabled researchers to\nfocus on practical repository-level tasks in software engineering domain. In\nthis work, we consider a cornerstone task for automating work with software\nrepositories-environment setup, i.e., a task of configuring a\nrepository-specific development environment on a system. Existing studies on\nenvironment setup introduce innovative agentic strategies, but their evaluation\nis often based on small datasets that may not capture the full range of\nconfiguration challenges encountered in practice. To address this gap, we\nintroduce a comprehensive environment setup benchmark EnvBench. It encompasses\n329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on\nrepositories that present genuine configuration challenges, excluding projects\nthat can be fully configured by simple deterministic scripts. To enable further\nbenchmark extension and usage for model tuning, we implement two automatic\nmetrics: a static analysis check for missing imports in Python and a\ncompilation check for JVM languages. We demonstrate the applicability of our\nbenchmark by evaluating three environment setup approaches, including a simple\nzero-shot baseline and two agentic workflows, that we test with two powerful\nLLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to\nsuccessfully configure 6.69% repositories for Python and 29.47% repositories\nfor JVM, suggesting that EnvBench remains challenging for current approaches.\nOur benchmark suite is publicly available at\nhttps://github.com/JetBrains-Research/EnvBench. The dataset and experiment\ntrajectories are available at https://jb.gg/envbench.\n","authors":["Aleksandra Eliseeva","Alexander Kovrigin","Ilia Kholkin","Egor Bogomolov","Yaroslav Zharov"],"pdf_url":"https://arxiv.org/pdf/2503.14443v1.pdf","comment":"Accepted at the DL4Code workshop at ICLR'25"},{"id":"http://arxiv.org/abs/2503.14442v1","updated":"2025-03-18T17:18:42Z","published":"2025-03-18T17:18:42Z","title":"Inducing Causal Structure for Interpretable Neural Networks Applied to\n  Glucose Prediction for T1DM Patients","summary":"  Causal abstraction techniques such as Interchange Intervention Training (IIT)\nhave been proposed to infuse neural network with expert knowledge encoded in\ncausal models, but their application to real-world problems remains limited.\nThis article explores the application of IIT in predicting blood glucose levels\nin Type 1 Diabetes Mellitus (T1DM) patients. The study utilizes an acyclic\nversion of the simglucose simulator approved by the FDA to train a Multi-Layer\nPerceptron (MLP) model, employing IIT to impose causal relationships. Results\nshow that the model trained with IIT effectively abstracted the causal\nstructure and outperformed the standard one in terms of predictive performance\nacross different prediction horizons (PHs) post-meal. Furthermore, the\nbreakdown of the counterfactual loss can be leveraged to explain which part of\nthe causal mechanism are more or less effectively captured by the model. These\npreliminary results suggest the potential of IIT in enhancing predictive models\nin healthcare by effectively complying with expert knowledge.\n","authors":["Ana Esponera","Giovanni Cinnà"],"pdf_url":"https://arxiv.org/pdf/2503.14442v1.pdf","comment":"27 pages, 10 pages, to be published in the Proceedings of Machine\n  Learning Research (PMLR), to be presented at the conference CLeaR 2025"},{"id":"http://arxiv.org/abs/2503.14439v1","updated":"2025-03-18T17:16:40Z","published":"2025-03-18T17:16:40Z","title":"Graph-CNNs for RF Imaging: Learning the Electric Field Integral\n  Equations","summary":"  Radio-Frequency (RF) imaging concerns the digital recreation of the surfaces\nof scene objects based on the scattered field at distributed receivers. To\nsolve this difficult inverse scattering problems, data-driven methods are often\nemployed that extract patterns from similar training examples, while offering\nminimal latency. In this paper, we first provide an approximate yet fast\nelectromagnetic model, which is based on the electric field integral equations,\nfor data generation, and subsequently propose a Deep Neural Network (DNN)\narchitecture to learn the corresponding inverse model. A graph-attention\nbackbone allows for the system geometry to be passed to the DNN, where residual\nconvolutional layers extract features about the objects, while a UNet head\nperforms the final image reconstruction. Our quantitative and qualitative\nevaluations on two synthetic data sets of different characteristics showcase\nthe performance gains of thee proposed advanced architecture and its relative\nresilience to signal noise levels and various reception configurations.\n","authors":["Kyriakos Stylianopoulos","Panagiotis Gavriilidis","Gabriele Gradoni","George C. Alexandropoulos"],"pdf_url":"https://arxiv.org/pdf/2503.14439v1.pdf","comment":"Submitted to EUSIPCO 2025"},{"id":"http://arxiv.org/abs/2503.14434v1","updated":"2025-03-18T17:11:24Z","published":"2025-03-18T17:11:24Z","title":"LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as\n  Evolutionary Optimizers","summary":"  Automated feature engineering plays a critical role in improving predictive\nmodel performance for tabular learning tasks. Traditional automated feature\nengineering methods are limited by their reliance on pre-defined\ntransformations within fixed, manually designed search spaces, often neglecting\ndomain knowledge. Recent advances using Large Language Models (LLMs) have\nenabled the integration of domain knowledge into the feature engineering\nprocess. However, existing LLM-based approaches use direct prompting or rely\nsolely on validation scores for feature selection, failing to leverage insights\nfrom prior feature discovery experiments or establish meaningful reasoning\nbetween feature generation and data-driven performance. To address these\nchallenges, we propose LLM-FE, a novel framework that combines evolutionary\nsearch with the domain knowledge and reasoning capabilities of LLMs to\nautomatically discover effective features for tabular learning tasks. LLM-FE\nformulates feature engineering as a program search problem, where LLMs propose\nnew feature transformation programs iteratively, and data-driven feedback\nguides the search process. Our results demonstrate that LLM-FE consistently\noutperforms state-of-the-art baselines, significantly enhancing the performance\nof tabular prediction models across diverse classification and regression\nbenchmarks.\n","authors":["Nikhil Abhyankar","Parshin Shojaee","Chandan K. Reddy"],"pdf_url":"https://arxiv.org/pdf/2503.14434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14432v1","updated":"2025-03-18T17:09:57Z","published":"2025-03-18T17:09:57Z","title":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via\n  Tool Play","summary":"  Large language models (LLMs) are increasingly integrated with specialized\nexternal tools, yet many tasks demand zero-shot tool usage with minimal or\nnoisy documentation. Existing solutions rely on manual rewriting or labeled\ndata for validation, making them inapplicable in true zero-shot settings. To\naddress these challenges, we propose PLAY2PROMPT, an automated framework that\nsystematically \"plays\" with each tool to explore its input-output behaviors.\nThrough this iterative trial-and-error process, PLAY2PROMPT refines tool\ndocumentation and generates usage examples without any labeled data. These\nexamples not only guide LLM inference but also serve as validation to further\nenhance tool utilization. Extensive experiments on real-world tasks demonstrate\nthat PLAY2PROMPT significantly improves zero-shot tool performance across both\nopen and closed models, offering a scalable and effective solution for\ndomain-specific tool integration.\n","authors":["Wei Fang","Yang Zhang","Kaizhi Qian","James Glass","Yada Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.14432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10460v2","updated":"2025-03-18T17:07:21Z","published":"2025-03-13T15:29:22Z","title":"Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and\n  Beyond","summary":"  This paper introduces Light-R1, an open-source suite for training long\nreasoning models using reproducible and cost-effective methodology. Given the\nproprietary nature of data used in the DeepSeek-R1 series, we develop an\nalternative approach leveraging exclusively public data and models. Our\ncurriculum training progressively increases data difficulty, combined with\nmulti-staged post-training. Our Light-R1-32B model, trained from\nQwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math\nreasoning.\n  Experimental results show that this curriculum approach becomes more\neffective when distinct, diverse datasets are available for different training\nstages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on\nproprietary data) with 3,000 challenging examples from our curriculum dataset\nyielded state-of-the-art 7B and 14B models, while the 32B model,\nLight-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.\n  Furthermore, we extend our work by applying GRPO on long reasoning models.\nOur final Light-R1-14B-DS achieves SOTA performance among 14B models in math,\nwith AIME24 \\& 25 scores of 74.0 and 60.2 respectively, surpassing many 32B\nmodels and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training,\nLight-R1-14B-DS demonstrates strong cross-domain generalization.\n  Light-R1 represents a significant advancement in making sophisticated\nreasoning models more accessible and implementable in real-world applications.\nOur models, training data and code have been made available at\nhttps://github.com/Qihoo360/Light-R1.\n","authors":["Liang Wen","Yunke Cai","Fenrui Xiao","Xin He","Qi An","Zhenyu Duan","Yimin Du","Junchen Liu","Lifu Tang","Xiaowei Lv","Haosheng Zou","Yongchao Deng","Shousheng Jia","Xiangzheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.10460v2.pdf","comment":"v2: better writing & format for later submission; all release at\n  https://github.com/Qihoo360/Light-R1"},{"id":"http://arxiv.org/abs/2501.03575v2","updated":"2025-03-18T16:59:07Z","published":"2025-01-07T06:55:50Z","title":"Cosmos World Foundation Model Platform for Physical AI","summary":"  Physical AI needs to be trained digitally first. It needs a digital twin of\nitself, the policy model, and a digital twin of the world, the world model. In\nthis paper, we present the Cosmos World Foundation Model Platform to help\ndevelopers build customized world models for their Physical AI setups. We\nposition a world foundation model as a general-purpose world model that can be\nfine-tuned into customized world models for downstream applications. Our\nplatform covers a video curation pipeline, pre-trained world foundation models,\nexamples of post-training of pre-trained world foundation models, and video\ntokenizers. To help Physical AI builders solve the most critical problems of\nour society, we make Cosmos open-source and our models open-weight with\npermissive licenses available via\nhttps://github.com/nvidia-cosmos/cosmos-predict1.\n","authors":[" NVIDIA"," :","Niket Agarwal","Arslan Ali","Maciej Bala","Yogesh Balaji","Erik Barker","Tiffany Cai","Prithvijit Chattopadhyay","Yongxin Chen","Yin Cui","Yifan Ding","Daniel Dworakowski","Jiaojiao Fan","Michele Fenzi","Francesco Ferroni","Sanja Fidler","Dieter Fox","Songwei Ge","Yunhao Ge","Jinwei Gu","Siddharth Gururani","Ethan He","Jiahui Huang","Jacob Huffman","Pooya Jannaty","Jingyi Jin","Seung Wook Kim","Gergely Klár","Grace Lam","Shiyi Lan","Laura Leal-Taixe","Anqi Li","Zhaoshuo Li","Chen-Hsuan Lin","Tsung-Yi Lin","Huan Ling","Ming-Yu Liu","Xian Liu","Alice Luo","Qianli Ma","Hanzi Mao","Kaichun Mo","Arsalan Mousavian","Seungjun Nah","Sriharsha Niverty","David Page","Despoina Paschalidou","Zeeshan Patel","Lindsey Pavao","Morteza Ramezanali","Fitsum Reda","Xiaowei Ren","Vasanth Rao Naik Sabavat","Ed Schmerling","Stella Shi","Bartosz Stefaniak","Shitao Tang","Lyne Tchapmi","Przemek Tredak","Wei-Cheng Tseng","Jibin Varghese","Hao Wang","Haoxiang Wang","Heng Wang","Ting-Chun Wang","Fangyin Wei","Xinyue Wei","Jay Zhangjie Wu","Jiashu Xu","Wei Yang","Lin Yen-Chen","Xiaohui Zeng","Yu Zeng","Jing Zhang","Qinsheng Zhang","Yuxuan Zhang","Qingqing Zhao","Artur Zolkowski"],"pdf_url":"https://arxiv.org/pdf/2501.03575v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14421v1","updated":"2025-03-18T16:55:07Z","published":"2025-03-18T16:55:07Z","title":"ExDDV: A New Dataset for Explainable Deepfake Detection in Video","summary":"  The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.\n","authors":["Vlad Hondru","Eduard Hogea","Darian Onchis","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2503.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15686v2","updated":"2025-03-18T16:50:49Z","published":"2023-12-25T10:31:22Z","title":"PULASki: Learning inter-rater variability using statistical distances to\n  improve probabilistic segmentation","summary":"  In the domain of medical imaging, many supervised learning based methods for\nsegmentation face several challenges such as high variability in annotations\nfrom multiple experts, paucity of labelled data and class imbalanced datasets.\nThese issues may result in segmentations that lack the requisite precision for\nclinical analysis and can be misleadingly overconfident without associated\nuncertainty quantification. This work proposes the PULASki method as a\ncomputationally efficient generative tool for biomedical image segmentation\nthat accurately captures variability in expert annotations, even in small\ndatasets. This approach makes use of an improved loss function based on\nstatistical distances in a conditional variational autoencoder structure\n(Probabilistic UNet), which improves learning of the conditional decoder\ncompared to the standard cross-entropy particularly in class imbalanced\nproblems. The proposed method was analysed for two structurally different\nsegmentation tasks (intracranial vessel and multiple sclerosis (MS) lesion) and\ncompare our results to four well-established baselines in terms of quantitative\nmetrics and qualitative output. These experiments involve class-imbalanced\ndatasets characterised by challenging features, including suboptimal\nsignal-to-noise ratios and high ambiguity. Empirical results demonstrate the\nPULASKi method outperforms all baselines at the 5\\% significance level. Our\nexperiments are also of the first to present a comparative study of the\ncomputationally feasible segmentation of complex geometries using 3D patches\nand the traditional use of 2D slices. The generated segmentations are shown to\nbe much more anatomically plausible than in the 2D case, particularly for the\nvessel task.\n","authors":["Soumick Chatterjee","Franziska Gaidzik","Alessandro Sciarra","Hendrik Mattern","Gábor Janiga","Oliver Speck","Andreas Nürnberger","Sahani Pathiraja"],"pdf_url":"https://arxiv.org/pdf/2312.15686v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04264v5","updated":"2025-03-18T16:48:07Z","published":"2023-10-06T14:11:21Z","title":"Deep learning modelling of manufacturing and build variations on\n  multi-stage axial compressors aerodynamics","summary":"  Applications of deep learning to physical simulations such as Computational\nFluid Dynamics have recently experienced a surge in interest, and their\nviability has been demonstrated in different domains. However, due to the\nhighly complex, turbulent, and three-dimensional flows, they have not yet been\nproven usable for turbomachinery applications. Multistage axial compressors for\ngas turbine applications represent a remarkably challenging case, due to the\nhigh-dimensionality of the regression of the flow field from geometrical and\noperational variables. This paper demonstrates the development and application\nof a deep learning framework for predictions of the flow field and aerodynamic\nperformance of multistage axial compressors. A physics-based dimensionality\nreduction approach unlocks the potential for flow-field predictions, as it\nre-formulates the regression problem from an unstructured to a structured one,\nas well as reducing the number of degrees of freedom. Compared to traditional\n\"black-box\" surrogate models, it provides explainability to the predictions of\nthe overall performance by identifying the corresponding aerodynamic drivers.\nThe model is applied to manufacturing and build variations, as the associated\nperformance scatter is known to have a significant impact on $CO_2$ emissions,\nwhich poses a challenge of great industrial and environmental relevance. The\nproposed architecture is proven to achieve an accuracy comparable to that of\nthe CFD benchmark, in real-time, for an industrially relevant application. The\ndeployed model is readily integrated within the manufacturing and build process\nof gas turbines, thus providing the opportunity to analytically assess the\nimpact on performance with actionable and explainable data.\n","authors":["Giuseppe Bruni","Sepehr Maleki","Senthil K. Krishnababu"],"pdf_url":"https://arxiv.org/pdf/2310.04264v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14405v1","updated":"2025-03-18T16:47:27Z","published":"2025-03-18T16:47:27Z","title":"DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D\n  Teachers","summary":"  Recent multi-teacher distillation methods have unified the encoders of\nmultiple foundation models into a single encoder, achieving competitive\nperformance on core vision tasks like classification, segmentation, and depth\nestimation. This led us to ask: Could similar success be achieved when the pool\nof teachers also includes vision models specialized in diverse tasks across\nboth 2D and 3D perception? In this paper, we define and investigate the problem\nof heterogeneous teacher distillation, or co-distillation, a challenging\nmulti-teacher distillation scenario where teacher models vary significantly in\nboth (a) their design objectives and (b) the data they were trained on. We\nexplore data-sharing strategies and teacher-specific encoding, and introduce\nDUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human\nperception. Our model achieves performance comparable to that of its larger\nteachers, sometimes even outperforming them, on their respective tasks.\nNotably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much\nsmaller encoder.\n","authors":["Mert Bulent Sariyildiz","Philippe Weinzaepfel","Thomas Lucas","Pau de Jorge","Diane Larlus","Yannis Kalantidis"],"pdf_url":"https://arxiv.org/pdf/2503.14405v1.pdf","comment":"Accepted to CVPR-2025. Project page:\n  https://europe.naverlabs.com/dune"},{"id":"http://arxiv.org/abs/2406.04746v2","updated":"2025-03-18T16:45:09Z","published":"2024-06-07T08:46:19Z","title":"PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance\n  Prediction","summary":"  Text-to-image generation has recently emerged as a viable alternative to\ntext-to-image retrieval, driven by the visually impressive results of\ngenerative diffusion models. Although query performance prediction is an active\nresearch topic in information retrieval, to the best of our knowledge, there is\nno prior study that analyzes the difficulty of queries (referred to as prompts)\nin text-to-image generation, based on human judgments. To this end, we\nintroduce the first dataset of prompts which are manually annotated in terms of\nimage generation performance. Additionally, we extend these evaluations to\ntext-to-image retrieval by collecting manual annotations that represent\nretrieval performance. We thus establish the first joint benchmark for prompt\nand query performance prediction (PQPP) across both tasks, comprising over 10K\nqueries. Our benchmark enables (i) the comparative assessment of prompt/query\ndifficulty in both image generation and image retrieval, and (ii) the\nevaluation of prompt/query performance predictors addressing both generation\nand retrieval. We evaluate several pre- and post-generation/retrieval\nperformance predictors, thus providing competitive baselines for future\nresearch. Our benchmark and code are publicly available at\nhttps://github.com/Eduard6421/PQPP.\n","authors":["Eduard Poesina","Adriana Valentina Costache","Adrian-Gabriel Chifu","Josiane Mothe","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2406.04746v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14403v1","updated":"2025-03-18T16:44:33Z","published":"2025-03-18T16:44:33Z","title":"Landscape Complexity for the Empirical Risk of Generalized Linear\n  Models: Discrimination between Structured Data","summary":"  We use the Kac-Rice formula and results from random matrix theory to obtain\nthe average number of critical points of a family of high-dimensional empirical\nloss functions, where the data are correlated $d$-dimensional Gaussian vectors,\nwhose number has a fixed ratio with their dimension. The correlations are\nintroduced to model the existence of structure in the data, as is common in\ncurrent Machine-Learning systems. Under a technical hypothesis, our results are\nexact in the large-$d$ limit, and characterize the annealed landscape\ncomplexity, namely the logarithm of the expected number of critical points at a\ngiven value of the loss.\n  We first address in detail the landscape of the loss function of a single\nperceptron and then generalize it to the case where two competing data sets\nwith different covariance matrices are present, with the perceptron seeking to\ndiscriminate between them. The latter model can be applied to understand the\ninterplay between adversity and non-trivial data structure. For completeness,\nwe also treat the case of a loss function used in training Generalized Linear\nModels in the presence of correlated input data.\n","authors":["Theodoros G. Tsironis","Aris L. Moustakas"],"pdf_url":"https://arxiv.org/pdf/2503.14403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14396v1","updated":"2025-03-18T16:36:59Z","published":"2025-03-18T16:36:59Z","title":"Technical Report: Aggregation on Learnable Manifolds for Asynchronous\n  Federated Optimization","summary":"  In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity, in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry, at each of\nthe local training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated thorough\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.\n","authors":["Archie Licudi"],"pdf_url":"https://arxiv.org/pdf/2503.14396v1.pdf","comment":"22 pages, 3 figures. Preliminary technical project report"},{"id":"http://arxiv.org/abs/2407.16970v3","updated":"2025-03-18T16:34:14Z","published":"2024-07-24T03:32:05Z","title":"Towards Aligning Language Models with Textual Feedback","summary":"  We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.\n","authors":["Saüc Abadal Lloret","Shehzaad Dhuliawala","Keerthiram Murugesan","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2407.16970v3.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2501.15785v2","updated":"2025-03-18T16:31:23Z","published":"2025-01-27T05:17:06Z","title":"Memorization and Regularization in Generative Diffusion Models","summary":"  Diffusion models have emerged as a powerful framework for generative\nmodeling. At the heart of the methodology is score matching: learning gradients\nof families of log-densities for noisy versions of the data distribution at\ndifferent scales. When the loss function adopted in score matching is evaluated\nusing empirical data, rather than the population loss, the minimizer\ncorresponds to the score of a time-dependent Gaussian mixture. However, use of\nthis analytically tractable minimizer leads to data memorization: in both\nunconditioned and conditioned settings, the generative model returns the\ntraining samples. This paper contains an analysis of the dynamical mechanism\nunderlying memorization. The analysis highlights the need for regularization to\navoid reproducing the analytically tractable minimizer; and, in so doing, lays\nthe foundations for a principled understanding of how to regularize. Numerical\nexperiments investigate the properties of: (i) Tikhonov regularization; (ii)\nregularization designed to promote asymptotic consistency; and (iii)\nregularizations induced by under-parameterization of a neural network or by\nearly stopping when training a neural network. These experiments are evaluated\nin the context of memorization, and directions for future development of\nregularization are highlighted.\n","authors":["Ricardo Baptista","Agnimitra Dasgupta","Nikola B. Kovachki","Assad Oberai","Andrew M. Stuart"],"pdf_url":"https://arxiv.org/pdf/2501.15785v2.pdf","comment":"59 pages, 20 figures"},{"id":"http://arxiv.org/abs/2308.03239v2","updated":"2025-03-18T16:30:39Z","published":"2023-08-07T01:32:09Z","title":"Unsynchronized Decentralized Q-Learning: Two Timescale Analysis By\n  Persistence","summary":"  Non-stationarity is a fundamental challenge in multi-agent reinforcement\nlearning (MARL), where agents update their behaviour as they learn. Many\ntheoretical advances in MARL avoid the challenge of non-stationarity by\ncoordinating the policy updates of agents in various ways, including\nsynchronizing times at which agents are allowed to revise their policies.\nSynchronization enables analysis of many MARL algorithms via multi-timescale\nmethods, but such synchronization is infeasible in many decentralized\napplications. In this paper, we study an unsynchronized variant of the\ndecentralized Q-learning algorithm, a recent MARL algorithm for stochastic\ngames. We provide sufficient conditions under which the unsynchronized\nalgorithm drives play to equilibrium with high probability. Our solution\nutilizes constant learning rates in the Q-factor update, which we show to be\ncritical for relaxing the synchronization assumptions of earlier work. Our\nanalysis also applies to unsynchronized generalizations of a number of other\nalgorithms from the regret testing tradition, whose performance is analyzed by\nmulti-timescale methods that study Markov chains obtained via policy update\ndynamics. This work extends the applicability of the decentralized Q-learning\nalgorithm and its relatives to settings in which parameters are selected in an\nindependent manner, and tames non-stationarity without imposing the\ncoordination assumptions of prior work.\n","authors":["Bora Yongacoglu","Gürdal Arslan","Serdar Yüksel"],"pdf_url":"https://arxiv.org/pdf/2308.03239v2.pdf","comment":"Accepted to SIAM Journal on Control and Optimization"},{"id":"http://arxiv.org/abs/2503.14393v1","updated":"2025-03-18T16:28:14Z","published":"2025-03-18T16:28:14Z","title":"On the clustering behavior of sliding windows","summary":"  Things can go spectacularly wrong when clustering timeseries data that has\nbeen preprocessed with a sliding window. We highlight three surprising failures\nthat emerge depending on how the window size compares with the timeseries\nlength. In addition to computational examples, we present theoretical\nexplanations for each of these failure modes.\n","authors":["Boris Alexeev","Wenyan Luo","Dustin G. Mixon","Yan X Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11709v2","updated":"2025-03-18T16:16:17Z","published":"2025-03-12T18:18:09Z","title":"Conformal Prediction and Human Decision Making","summary":"  Methods to quantify uncertainty in predictions from arbitrary models are in\ndemand in high-stakes domains like medicine and finance. Conformal prediction\nhas emerged as a popular method for producing a set of predictions with\nspecified average coverage, in place of a single prediction and confidence\nvalue. However, the value of conformal prediction sets to assist human\ndecisions remains elusive due to the murky relationship between coverage\nguarantees and decision makers' goals and strategies. How should we think about\nconformal prediction sets as a form of decision support? We outline a decision\ntheoretic framework for evaluating predictive uncertainty as informative\nsignals, then contrast what can be said within this framework about idealized\nuse of calibrated probabilities versus conformal prediction sets. Informed by\nprior empirical results and theories of human decisions under uncertainty, we\nformalize a set of possible strategies by which a decision maker might use a\nprediction set. We identify ways in which conformal prediction sets and posthoc\npredictive uncertainty quantification more broadly are in tension with common\ngoals and needs in human-AI decision making. We give recommendations for future\nresearch in predictive uncertainty quantification to support human decision\nmakers.\n","authors":["Jessica Hullman","Yifan Wu","Dawei Xie","Ziyang Guo","Andrew Gelman"],"pdf_url":"https://arxiv.org/pdf/2503.11709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14381v1","updated":"2025-03-18T16:14:38Z","published":"2025-03-18T16:14:38Z","title":"Optimizing High-Dimensional Oblique Splits","summary":"  Orthogonal-split trees perform well, but evidence suggests oblique splits can\nenhance their performance. This paper explores optimizing high-dimensional\n$s$-sparse oblique splits from $\\{(\\vec{w}, \\vec{w}^{\\top}\\boldsymbol{X}_{i}) :\ni\\in \\{1,\\dots, n\\}, \\vec{w} \\in \\mathbb{R}^p, \\| \\vec{w} \\|_{2} = 1, \\|\n\\vec{w} \\|_{0} \\leq s \\}$ for growing oblique trees, where $ s $ is a\nuser-defined sparsity parameter. We establish a connection between SID\nconvergence and $s_0$-sparse oblique splits with $s_0\\ge 1$, showing that the\nSID function class expands as $s_0$ increases, enabling the capture of more\ncomplex data-generating functions such as the $s_0$-dimensional XOR function.\nThus, $s_0$ represents the unknown potential complexity of the underlying\ndata-generating function. Learning these complex functions requires an\n$s$-sparse oblique tree with $s \\geq s_0$ and greater computational resources.\nThis highlights a trade-off between statistical accuracy, governed by the SID\nfunction class size depending on $s_0$, and computational cost. In contrast,\nprevious studies have explored the problem of SID convergence using orthogonal\nsplits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we\nintroduce a practical framework for oblique trees that integrates optimized\noblique splits alongside orthogonal splits into random forests. The proposed\napproach is assessed through simulations and real-data experiments, comparing\nits performance against various oblique tree models.\n","authors":["Chien-Ming Chi"],"pdf_url":"https://arxiv.org/pdf/2503.14381v1.pdf","comment":"79 pages, 9 tables"},{"id":"http://arxiv.org/abs/2503.14377v1","updated":"2025-03-18T16:10:11Z","published":"2025-03-18T16:10:11Z","title":"Advancing Medical Representation Learning Through High-Quality Data","summary":"  Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase.\n","authors":["Negin Baghbanzadeh","Adibvafa Fallahpour","Yasaman Parhizkar","Franklin Ogidi","Shuvendu Roy","Sajad Ashkezari","Vahid Reza Khazaie","Michael Colacci","Ali Etemad","Arash Afkanpour","Elham Dolatabadi"],"pdf_url":"https://arxiv.org/pdf/2503.14377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14376v1","updated":"2025-03-18T16:09:47Z","published":"2025-03-18T16:09:47Z","title":"Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM\n  Kernels","summary":"  Linear RNNs with gating recently demonstrated competitive performance\ncompared to Transformers in language modeling. Although their linear compute\nscaling in sequence length offers theoretical runtime advantages over\nTransformers, realizing these benefits in practice requires optimized custom\nkernels, as Transformers rely on the highly efficient Flash Attention kernels.\nLeveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear\nAttention (FLA) shows that linear RNN kernels are faster than Flash Attention,\nby parallelizing over chunks of the input sequence. However, since the chunk\nsize of FLA is limited, many intermediate states must be materialized in GPU\nmemory. This leads to low arithmetic intensity and causes high memory\nconsumption and IO cost, especially for long-context pre-training. In this\nwork, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm\nfor linear RNNs, that enables arbitrary large chunk sizes by introducing an\nadditional level of sequence parallelization within each chunk. First, we apply\nTFLA to the xLSTM with matrix memory, the mLSTM. Second, we propose an mLSTM\nvariant with sigmoid input gate and reduced computation for even faster kernel\nruntimes at equal language modeling performance. In our speed benchmarks, we\nshow that our new mLSTM kernels based on TFLA outperform highly optimized Flash\nAttention, Linear Attention and Mamba kernels, setting a new state of the art\nfor efficient long-context sequence modeling primitives.\n","authors":["Maximilian Beck","Korbinian Pöppel","Phillip Lippe","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2503.14376v1.pdf","comment":"Code available at: https://github.com/NX-AI/mlstm_kernels"},{"id":"http://arxiv.org/abs/2502.12617v2","updated":"2025-03-18T16:08:31Z","published":"2025-02-18T08:02:17Z","title":"A Graph-Enhanced Deep-Reinforcement Learning Framework for the Aircraft\n  Landing Problem","summary":"  The Aircraft Landing Problem (ALP) is one of the challenging problems in\naircraft transportation and management. The challenge is to schedule the\narriving aircraft in a sequence so that the cost and delays are optimized.\nThere are various solution approaches to solving this problem, most of which\nare based on operations research algorithms and meta-heuristics. Although\ntraditional methods perform better on one or the other factors, there remains a\nproblem of solving real-time rescheduling and computational scalability\naltogether. This paper presents a novel deep reinforcement learning (DRL)\nframework that combines graph neural networks with actor-critic architectures\nto address the ALP. This paper introduces three key contributions: A\ngraph-based state representation that efficiently captures temporal and spatial\nrelationships between aircraft, a specialized actor-critic architecture\ndesigned to handle multiple competing objectives in landing scheduling, and a\nrunway balance strategy that ensures efficient resource utilization while\nmaintaining safety constraints. The results show that the trained algorithm can\nbe tested on different problem sets and the results are competitive to\noperation research algorithms. The experimental results on standard benchmark\ndata sets demonstrate a 99.95% reduction in computational time compared to\nMixed Integer Programming (MIP) and 38% higher runway throughput over First\nCome First Serve (FCFS) approaches. Therefore, the proposed solution is\ncompetitive to traditional approaches and achieves substantial advancements.\nNotably, it does not require retraining, making it particularly suitable for\nindustrial deployment. The frameworks capability to generate solutions within 1\nsecond enables real-time rescheduling, addressing critical requirements of air\ntraffic management.\n","authors":["Vatsal Maru"],"pdf_url":"https://arxiv.org/pdf/2502.12617v2.pdf","comment":"27 pages, submitted to ESWA, comments are welcome"},{"id":"http://arxiv.org/abs/2503.14375v1","updated":"2025-03-18T16:07:29Z","published":"2025-03-18T16:07:29Z","title":"Evaluating Machine Learning Approaches for ASCII Art Generation","summary":"  Generating structured ASCII art using computational techniques demands a\ncareful interplay between aesthetic representation and computational precision,\nrequiring models that can effectively translate visual information into\nsymbolic text characters. Although Convolutional Neural Networks (CNNs) have\nshown promise in this domain, the comparative performance of deep learning\narchitectures and classical machine learning methods remains unexplored. This\npaper explores the application of contemporary ML and DL methods to generate\nstructured ASCII art, focusing on three key criteria: fidelity, character\nclassification accuracy, and output quality. We investigate deep learning\narchitectures, including Multilayer Perceptrons (MLPs), ResNet, and\nMobileNetV2, alongside classical approaches such as Random Forests, Support\nVector Machines (SVMs) and k-Nearest Neighbors (k-NN), trained on an augmented\nsynthetic dataset of ASCII characters. Our results show that complex neural\nnetwork architectures often fall short in producing high-quality ASCII art,\nwhereas classical machine learning classifiers, despite their simplicity,\nachieve performance similar to CNNs. Our findings highlight the strength of\nclassical methods in bridging model simplicity with output quality, offering\nnew insights into ASCII art synthesis and machine learning on image data with\nlow dimensionality.\n","authors":["Sai Coumar","Zachary Kingston"],"pdf_url":"https://arxiv.org/pdf/2503.14375v1.pdf","comment":"9 pages, 7 figures, 3 tables. Code available at\n  https://github.com/saiccoumar/deep_ascii_converter"},{"id":"http://arxiv.org/abs/2501.00513v2","updated":"2025-03-18T16:01:24Z","published":"2024-12-31T15:53:50Z","title":"CaReBench: A Fine-Grained Benchmark for Video Captioning and Retrieval","summary":"  Video understanding, including video captioning and retrieval, is still a\ngreat challenge for video-language models (VLMs). The existing video retrieval\nand caption benchmarks only include short descriptions, limits their ability of\ndetailed video understanding evaluation. To address this problem, we present\nCaReBench, a testing benchmark for fine-grained video captioning and retrieval\nwith 1,000 high-quality pairs of videos and human-annotated detailed captions.\nUniquely, it provides manually separated spatial annotations and temporal\nannotations for each video. Based on this design, we introduce two evaluation\nmetrics, ReBias and CapST, specifically tailored for video retrieval and video\ncaptioning tasks, respectively. These metrics enable a comprehensive\ninvestigation into the spatial and temporal biases inherent in VLMs. In\naddition, to handle both video retrieval and video captioning tasks in a\nunified framework, we develop a simple baseline based on a Multimodal Language\nModel (MLLM). By implementing a two-stage Supervised Fine-Tuning (SFT), we\nfully unlock the potential of MLLM, enabling it not only to generate detailed\nvideo descriptions but also to extract video features. Surprisingly,\nexperimental results demonstrate that, compared to the CLIP-based models\ndesigned for retrieval and the popular MLLMs skilled in video captioning, our\nbaseline shows competitive performance in both fine-grained video retrieval and\nvideo detailed captioning.\n","authors":["Yifan Xu","Xinhao Li","Yichun Yang","Desen Meng","Rui Huang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2501.00513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06728v2","updated":"2025-03-18T16:00:57Z","published":"2025-02-10T17:55:59Z","title":"FlexDeMo: Decoupled Momentum Optimization for Hybrid Sharded Data\n  Parallel Training","summary":"  Training large neural network models requires extensive computational\nresources, often distributed across several nodes and accelerators. Recent\nfindings suggest that it may be sufficient to only exchange the fast moving\ncomponents of the gradients, while accumulating momentum locally (Decoupled\nMomentum, or DeMo). However, when considering larger models that do not fit on\na single accelerator, the exchange of gradient information and the integration\nof DeMo needs to be reconsidered. Here, we propose employing a hybrid sharded\ndata parallel training strategy, FlexDeMo, whereby nodes fully shard model\nparameters locally between different accelerators, while inter-node\ncommunication bandwidth requirements are reduced by synchronizing only\nfast-moving components instead of the full gradients. This effectively combines\nprevious hybrid sharded strategies with the advantages of decoupled momentum.\nOur experimental results show that FlexDeMo is on par with hybrid sharded data\nparallel training employing AdamW and full gradient synchronization in terms of\nvalidation loss, demonstrating its viability. Furthermore, FlexDeMo achieves\nimproved training speed compared to full gradient synchronization across nodes.\nIn a bandwidth-constrained 2-node setup, FlexDeMo allows reaching desired\nlevels of validation loss faster than hybrid sharded data parallel training\nwith full gradient synchronization.\n","authors":["Mogens Henrik From","Jacob Nielsen","Lukas Galke","Peter Schneider-Kamp"],"pdf_url":"https://arxiv.org/pdf/2502.06728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14369v1","updated":"2025-03-18T15:58:58Z","published":"2025-03-18T15:58:58Z","title":"C(NN)FD -- Deep Learning Modelling of Multi-Stage Axial Compressors\n  Aerodynamics","summary":"  The field of scientific machine learning and its applications to numerical\nanalyses such as CFD has recently experienced a surge in interest. While its\nviability has been demonstrated in different domains, it has not yet reached a\nlevel of robustness and scalability to make it practical for industrial\napplications in the turbomachinery field. The highly complex, turbulent, and\nthree-dimensional flows of multi-stage axial compressors for gas turbine\napplications represent a remarkably challenging case. This is due to the\nhigh-dimensionality of the regression of the flow-field from geometrical and\noperational variables, and the high computational cost associated with the\nlarge scale of the CFD domains. This paper demonstrates the development and\napplication of a generalized deep learning framework for predictions of the\nflow field and aerodynamic performance of multi-stage axial compressors, also\npotentially applicable to any type of turbomachinery. A physics-based\ndimensionality reduction unlocks the potential for flow-field predictions for\nlarge-scale domains, re-formulating the regression problem from an unstructured\nto a structured one. The relevant physical equations are used to define a\nmulti-dimensional physical loss function. Compared to \"black-box\" approaches,\nthe proposed framework has the advantage of physically explainable predictions\nof overall performance, as the corresponding aerodynamic drivers can be\nidentified on a 0D/1D/2D/3D level. An iterative architecture is employed,\nimproving the accuracy of the predictions, as well as estimating the associated\nuncertainty. The model is trained on a series of dataset including\nmanufacturing and build variations, different geometries, compressor designs\nand operating conditions. This demonstrates the capability to predict the\nflow-field and the overall performance in a generalizable manner, with accuracy\ncomparable to the benchmark.\n","authors":["Giuseppe Bruni","Sepehr Maleki","Senthil K Krishnababu"],"pdf_url":"https://arxiv.org/pdf/2503.14369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09573v2","updated":"2025-03-18T15:58:18Z","published":"2025-03-12T17:43:40Z","title":"Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models","summary":"  Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/\n","authors":["Marianne Arriola","Aaron Gokaslan","Justin T Chiu","Zhihan Yang","Zhixuan Qi","Jiaqi Han","Subham Sekhar Sahoo","Volodymyr Kuleshov"],"pdf_url":"https://arxiv.org/pdf/2503.09573v2.pdf","comment":"ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms"},{"id":"http://arxiv.org/abs/2409.17273v3","updated":"2025-03-18T15:56:39Z","published":"2024-09-25T18:38:57Z","title":"Targeted Neural Architectures in Multi-Objective Frameworks for Complete\n  Glioma Characterization from Multimodal MRI","summary":"  Brain tumors result from abnormal cell growth in brain tissue. If\nundiagnosed, they cause neurological deficits, including cognitive impairment,\nmotor dysfunction, and sensory loss. As tumors grow, intracranial pressure\nincreases, potentially leading to fatal complications such as brain herniation.\nEarly diagnosis and treatment are crucial to controlling these effects and\nslowing tumor progression. Deep learning (DL) and artificial intelligence (AI)\nare increasingly used to assist doctors in early diagnosis through magnetic\nresonance imaging (MRI) scans. Our research proposes targeted neural\narchitectures within multi-objective frameworks that can localize, segment, and\nclassify the grade of these gliomas from multimodal MRI images to solve this\ncritical issue. Our localization framework utilizes a targeted architecture\nthat enhances the LinkNet framework with an encoder inspired by VGG19 for\nbetter multimodal feature extraction from the tumor along with spatial and\ngraph attention mechanisms that sharpen feature focus and inter-feature\nrelationships. For the segmentation objective, we deployed a specialized\nframework using the SeResNet101 CNN model as the encoder backbone integrated\ninto the LinkNet architecture, achieving an IoU Score of 96%. The\nclassification objective is addressed through a distinct framework implemented\nby combining the SeResNet152 feature extractor with Adaptive Boosting\nclassifier, reaching an accuracy of 98.53%. Our multi-objective approach with\ntargeted neural architectures demonstrated promising results for complete\nglioma characterization, with the potential to advance medical AI by enabling\nearly diagnosis and providing more accurate treatment options for patients.\n","authors":["Shravan Venkatraman","Pandiyaraju V","Abeshek A","Aravintakshan S A","Pavan Kumar S","Kannan A","Madhan S"],"pdf_url":"https://arxiv.org/pdf/2409.17273v3.pdf","comment":"29 pages, 25 figures, 6 tables"},{"id":"http://arxiv.org/abs/2404.17644v6","updated":"2025-03-18T15:55:12Z","published":"2024-04-26T18:08:15Z","title":"A Conditional Independence Test in the Presence of Discretization","summary":"  Testing conditional independence has many applications, such as in Bayesian\nnetwork learning and causal discovery. Different test methods have been\nproposed. However, existing methods generally can not work when only\ndiscretized observations are available. Specifically, consider $X_1$,\n$\\tilde{X}_2$ and $X_3$ are observed variables, where $\\tilde{X}_2$ is a\ndiscretization of latent variables $X_2$. Applying existing test methods to the\nobservations of $X_1$, $\\tilde{X}_2$ and $X_3$ can lead to a false conclusion\nabout the underlying conditional independence of variables $X_1$, $X_2$ and\n$X_3$. Motivated by this, we propose a conditional independence test\nspecifically designed to accommodate the presence of such discretization. To\nachieve this, we design the bridge equations to recover the parameter\nreflecting the statistical information of the underlying latent continuous\nvariables. An appropriate test statistic and its asymptotic distribution under\nthe null hypothesis of conditional independence have also been derived. Both\ntheoretical results and empirical validation have been provided, demonstrating\nthe effectiveness of our test methods.\n","authors":["Boyang Sun","Yu Yao","Guang-Yuan Hao","Yumou Qiu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.17644v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09101v2","updated":"2025-03-18T15:48:38Z","published":"2025-03-12T06:37:43Z","title":"The Shape of Attraction in UMAP: Exploring the Embedding Forces in\n  Dimensionality Reduction","summary":"  Uniform manifold approximation and projection (UMAP) is among the most\npopular neighbor embedding methods. The method relies on attractive and\nrepulsive forces among high-dimensional data points to obtain a low-dimensional\nembedding. In this paper, we analyze the forces to reveal their effects on\ncluster formations and visualization. Repulsion emphasizes differences,\ncontrolling cluster boundaries and inter-cluster distance. Attraction is more\nsubtle, as attractive tension between points can manifest simultaneously as\nattraction and repulsion in the lower-dimensional mapping. This explains the\nneed for learning rate annealing and motivates the different treatments between\nattractive and repulsive terms. Moreover, by modifying attraction, we improve\nthe consistency of cluster formation under random initialization. Overall, our\nanalysis makes UMAP and similar embedding methods more interpretable, more\nrobust, and more accurate.\n","authors":["Mohammad Tariqul Islam","Jason W. Fleischer"],"pdf_url":"https://arxiv.org/pdf/2503.09101v2.pdf","comment":"9 page + appendix"},{"id":"http://arxiv.org/abs/2503.14358v1","updated":"2025-03-18T15:41:45Z","published":"2025-03-18T15:41:45Z","title":"RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image\n  Alignment","summary":"  Rectified Flow (RF) models trained with a Flow matching framework have\nachieved state-of-the-art performance on Text-to-Image (T2I) conditional\ngeneration. Yet, multiple benchmarks show that synthetic images can still\nsuffer from poor alignment with the prompt, i.e., images show wrong attribute\nbinding, subject positioning, numeracy, etc. While the literature offers many\nmethods to improve T2I alignment, they all consider only Diffusion Models, and\nrequire auxiliary datasets, scoring models, and linguistic analysis of the\nprompt. In this paper we aim to address these gaps. First, we introduce RFMI, a\nnovel Mutual Information (MI) estimator for RF models that uses the pre-trained\nmodel itself for the MI estimation. Then, we investigate a self-supervised\nfine-tuning approach for T2I alignment based on RFMI that does not require\nauxiliary information other than the pre-trained model itself. Specifically, a\nfine-tuning set is constructed by selecting synthetic images generated from the\npre-trained RF model and having high point-wise MI between images and prompts.\nOur experiments on MI estimation benchmarks demonstrate the validity of RFMI,\nand empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI\nfor improving T2I alignment while maintaining image quality.\n","authors":["Chao Wang","Giulio Franzese","Alessandro Finamore","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2503.14358v1.pdf","comment":"to appear at ICLR 2025 Workshop on Deep Generative Model in Machine\n  Learning: Theory, Principle and Efficacy"},{"id":"http://arxiv.org/abs/2503.14357v1","updated":"2025-03-18T15:40:55Z","published":"2025-03-18T15:40:55Z","title":"Wasserstein-based Kernels for Clustering: Application to Power\n  Distribution Graphs","summary":"  Many data clustering applications must handle objects that cannot be\nrepresented as vector data. In this context, the bag-of-vectors representation\ncan be leveraged to describe complex objects through discrete distributions,\nand the Wasserstein distance can effectively measure the dissimilarity between\nthem. Additionally, kernel methods can be used to embed data into feature\nspaces that are easier to analyze. Despite significant progress in data\nclustering, a method that simultaneously accounts for distributional and\nvectorial dissimilarity measures is still lacking. To tackle this gap, this\nwork explores kernel methods and Wasserstein distance metrics to develop a\ncomputationally tractable clustering framework. The compositional properties of\nkernels allow the simultaneous handling of different metrics, enabling the\nintegration of both vectors and discrete distributions for object\nrepresentation. This approach is flexible enough to be applied in various\ndomains, such as graph analysis and image processing. The framework consists of\nthree main components. First, we efficiently approximate pairwise Wasserstein\ndistances using multiple reference distributions. Second, we employ kernel\nfunctions based on Wasserstein distances and present ways of composing kernels\nto express different types of information. Finally, we use the kernels to\ncluster data and evaluate the quality of the results using scalable and\ndistance-agnostic validity indices. A case study involving two datasets of 879\nand 34,920 power distribution graphs demonstrates the framework's effectiveness\nand efficiency.\n","authors":["Alfredo Oneto","Blazhe Gjorgiev","Giovanni Sansavini"],"pdf_url":"https://arxiv.org/pdf/2503.14357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14356v1","updated":"2025-03-18T15:40:18Z","published":"2025-03-18T15:40:18Z","title":"Benchmarking community drug response prediction models: datasets,\n  models, tools, and metrics for cross-dataset generalization analysis","summary":"  Deep learning (DL) and machine learning (ML) models have shown promise in\ndrug response prediction (DRP), yet their ability to generalize across datasets\nremains an open question, raising concerns about their real-world\napplicability. Due to the lack of standardized benchmarking approaches, model\nevaluations and comparisons often rely on inconsistent datasets and evaluation\ncriteria, making it difficult to assess true predictive capabilities. In this\nwork, we introduce a benchmarking framework for evaluating cross-dataset\nprediction generalization in DRP models. Our framework incorporates five\npublicly available drug screening datasets, six standardized DRP models, and a\nscalable workflow for systematic evaluation. To assess model generalization, we\nintroduce a set of evaluation metrics that quantify both absolute performance\n(e.g., predictive accuracy across datasets) and relative performance (e.g.,\nperformance drop compared to within-dataset results), enabling a more\ncomprehensive assessment of model transferability. Our results reveal\nsubstantial performance drops when models are tested on unseen datasets,\nunderscoring the importance of rigorous generalization assessments. While\nseveral models demonstrate relatively strong cross-dataset generalization, no\nsingle model consistently outperforms across all datasets. Furthermore, we\nidentify CTRPv2 as the most effective source dataset for training, yielding\nhigher generalization scores across target datasets. By sharing this\nstandardized evaluation framework with the community, our study aims to\nestablish a rigorous foundation for model comparison, and accelerate the\ndevelopment of robust DRP models for real-world applications.\n","authors":["Alexander Partin","Priyanka Vasanthakumari","Oleksandr Narykov","Andreas Wilke","Natasha Koussa","Sara E. Jones","Yitan Zhu","Jamie C. Overbeek","Rajeev Jain","Gayara Demini Fernando","Cesar Sanchez-Villalobos","Cristina Garcia-Cardona","Jamaludin Mohd-Yusof","Nicholas Chia","Justin M. Wozniak","Souparno Ghosh","Ranadip Pal","Thomas S. Brettin","M. Ryan Weil","Rick L. Stevens"],"pdf_url":"https://arxiv.org/pdf/2503.14356v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.14353v1","updated":"2025-03-18T15:36:36Z","published":"2025-03-18T15:36:36Z","title":"Unified Analysis of Decentralized Gradient Descent: a Contraction\n  Mapping Framework","summary":"  The decentralized gradient descent (DGD) algorithm, and its sibling,\ndiffusion, are workhorses in decentralized machine learning, distributed\ninference and estimation, and multi-agent coordination. We propose a novel,\nprincipled framework for the analysis of DGD and diffusion for strongly convex,\nsmooth objectives, and arbitrary undirected topologies, using contraction\nmappings coupled with a result called the mean Hessian theorem (MHT). The use\nof these tools yields tight convergence bounds, both in the noise-free and\nnoisy regimes. While these bounds are qualitatively similar to results found in\nthe literature, our approach using contractions together with the MHT decouples\nthe algorithm dynamics (how quickly the algorithm converges to its fixed point)\nfrom its asymptotic convergence properties (how far the fixed point is from the\nglobal optimum). This yields a simple, intuitive analysis that is accessible to\na broader audience. Extensions are provided to multiple local gradient updates,\ntime-varying step sizes, noisy gradients (stochastic DGD and diffusion),\ncommunication noise, and random topologies.\n","authors":["Erik G. Larsson","Nicolo Michelusi"],"pdf_url":"https://arxiv.org/pdf/2503.14353v1.pdf","comment":"submitted to the IEEE Open Journal of Signal Processing"},{"id":"http://arxiv.org/abs/2410.07933v2","updated":"2025-03-18T15:30:08Z","published":"2024-10-10T14:00:21Z","title":"Offline Hierarchical Reinforcement Learning via Inverse Optimization","summary":"  Hierarchical policies enable strong performance in many sequential\ndecision-making problems, such as those with high-dimensional action spaces,\nthose requiring long-horizon planning, and settings with sparse rewards.\nHowever, learning hierarchical policies from static offline datasets presents a\nsignificant challenge. Crucially, actions taken by higher-level policies may\nnot be directly observable within hierarchical controllers, and the offline\ndataset might have been generated using a different policy structure, hindering\nthe use of standard offline learning algorithms. In this work, we propose OHIO:\na framework for offline reinforcement learning (RL) of hierarchical policies.\nOur framework leverages knowledge of the policy structure to solve the\n\\textit{inverse problem}, recovering the unobservable high-level actions that\nlikely generated the observed data under our hierarchical policy. This approach\nconstructs a dataset suitable for off-the-shelf offline training. We\ndemonstrate our framework on robotic and network optimization problems and show\nthat it substantially outperforms end-to-end RL methods and improves\nrobustness. We investigate a variety of instantiations of our framework, both\nin direct deployment of policies trained offline and when online fine-tuning is\nperformed. Code and data are available at\nhttps://ohio-offline-hierarchical-rl.github.io\n","authors":["Carolin Schmidt","Daniele Gammelli","James Harrison","Marco Pavone","Filipe Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2410.07933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14345v1","updated":"2025-03-18T15:25:08Z","published":"2025-03-18T15:25:08Z","title":"MoonCast: High-Quality Zero-Shot Podcast Generation","summary":"  Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.\n","authors":["Zeqian Ju","Dongchao Yang","Jianwei Yu","Kai Shen","Yichong Leng","Zhengtao Wang","Xu Tan","Xinyu Zhou","Tao Qin","Xiangyang Li"],"pdf_url":"https://arxiv.org/pdf/2503.14345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14342v1","updated":"2025-03-18T15:23:03Z","published":"2025-03-18T15:23:03Z","title":"End-to-End Optimal Detector Design with Mutual Information Surrogates","summary":"  We introduce a novel approach for end-to-end black-box optimization of high\nenergy physics (HEP) detectors using local deep learning (DL) surrogates. These\nsurrogates approximate a scalar objective function that encapsulates the\ncomplex interplay of particle-matter interactions and physics analysis goals.\nIn addition to a standard reconstruction-based metric commonly used in the\nfield, we investigate the information-theoretic metric of mutual information.\nUnlike traditional methods, mutual information is inherently task-agnostic,\noffering a broader optimization paradigm that is less constrained by predefined\ntargets. We demonstrate the effectiveness of our method in a realistic physics\nanalysis scenario: optimizing the thicknesses of calorimeter detector layers\nbased on simulated particle interactions. The surrogate model learns to\napproximate objective gradients, enabling efficient optimization with respect\nto energy resolution. Our findings reveal three key insights: (1) end-to-end\nblack-box optimization using local surrogates is a practical and compelling\napproach for detector design, providing direct optimization of detector\nparameters in alignment with physics analysis goals; (2) mutual\ninformation-based optimization yields design choices that closely match those\nfrom state-of-the-art physics-informed methods, indicating that these\napproaches operate near optimality and reinforcing their reliability in HEP\ndetector design; and (3) information-theoretic methods provide a powerful,\ngeneralizable framework for optimizing scientific instruments. By reframing the\noptimization process through an information-theoretic lens rather than\ndomain-specific heuristics, mutual information enables the exploration of new\navenues for discovery beyond conventional approaches.\n","authors":["Kinga Anna Wozniak","Stephen Mulligan","Jan Kieseler","Markus Klute","Francois Fleuret","Tobias Golling"],"pdf_url":"https://arxiv.org/pdf/2503.14342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15292v2","updated":"2025-03-18T15:17:32Z","published":"2024-11-22T18:14:26Z","title":"Influence functions and regularity tangents for efficient active\n  learning","summary":"  In this paper we describe an efficient method for providing a regression\nmodel with a sense of curiosity about its data. In the field of machine\nlearning, our framework for representing curiosity is called Active Learning,\nwhich concerns the problem of automatically choosing data points for which to\nquery labels in the semi-supervised setting. The methods we propose are based\non computing a \"regularity tangent\" vector that can be calculated (with only a\nconstant slow-down) together with the model's parameter vector during training.\nWe then take the inner product of this tangent vector with the gradient vector\nof the model's loss at a given data point to obtain a measure of the influence\nof that point on the complexity of the model. In the simplest instantiation,\nthere is only a single regularity tangent vector, of the same dimension as the\nparameter vector. Thus, in the proposed technique, once training is complete,\nevaluating our \"curiosity\" about a potential query data point can be done as\nquickly as calculating the model's loss gradient at that point. The new vector\nonly doubles the amount of storage required by the model. We show that the\nquantity computed by our technique is an example of an \"influence function\",\nand that it measures the expected squared change in model complexity incurred\nby up-weighting a given data point. We propose a number of ways for using this\nand other related quantities to choose new training data points for a\nregression model.\n","authors":["Frederik Eaton"],"pdf_url":"https://arxiv.org/pdf/2411.15292v2.pdf","comment":"37 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.14338v1","updated":"2025-03-18T15:14:49Z","published":"2025-03-18T15:14:49Z","title":"Higher-Order Graphon Neural Networks: Approximation and Cut Distance","summary":"  Graph limit models, like graphons for limits of dense graphs, have recently\nbeen used to study size transferability of graph neural networks (GNNs). While\nmost literature focuses on message passing GNNs (MPNNs), in this work we attend\nto the more powerful higher-order GNNs. First, we extend the $k$-WL test for\ngraphons (B\\\"oker, 2023) to the graphon-signal space and introduce\nsignal-weighted homomorphism densities as a key tool. As an exemplary focus, we\ngeneralize Invariant Graph Networks (IGNs) to graphons, proposing Invariant\nGraphon Networks (IWNs) defined via a subset of the IGN basis corresponding to\nbounded linear operators. Even with this restricted basis, we show that IWNs of\norder $k$ are at least as powerful as the $k$-WL test, and we establish\nuniversal approximation results for graphon-signals in $L^p$ distances. This\nsignificantly extends the prior work of Cai & Wang (2022), showing that IWNs--a\nsubset of their IGN-small--retain effectively the same expressivity as the full\nIGN basis in the limit. In contrast to their approach, our blueprint of IWNs\nalso aligns better with the geometry of graphon space, for example facilitating\ncomparability to MPNNs. We highlight that, while typical higher-order GNNs are\ndiscontinuous w.r.t. cut distance--which causes their lack of convergence and\nis inherently tied to the definition of $k$-WL--their transferability remains\ncomparable to MPNNs.\n","authors":["Daniel Herbst","Stefanie Jegelka"],"pdf_url":"https://arxiv.org/pdf/2503.14338v1.pdf","comment":"51 pages, 6 figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14337v1","updated":"2025-03-18T15:14:14Z","published":"2025-03-18T15:14:14Z","title":"PENCIL: Long Thoughts with Short Memory","summary":"  While recent works (e.g. o1, DeepSeek R1) have demonstrated great promise of\nusing long Chain-of-Thought (CoT) to improve reasoning capabilities of language\nmodels, scaling it up during test-time is challenging due to inefficient memory\nusage -- intermediate computations accumulate indefinitely in context even no\nlonger needed for future thoughts. We propose PENCIL, which incorporates a\nreduction mechanism into the autoregressive generation process, allowing the\nmodel to recursively clean up intermediate thoughts based on patterns learned\nfrom training. With this reduction mechanism, PENCIL significantly reduces the\nmaximal context length required during generation, and thus can generate longer\nthoughts with limited memory, solving larger-scale problems given more thinking\ntime. For example, we demonstrate PENCIL achieves 97\\% accuracy on the\nchallenging Einstein's puzzle -- a task even large models like GPT-4 struggle\nwith -- using only a small 25M-parameter transformer with 2048 context length.\nTheoretically, we prove PENCIL can perform universal space-efficient\ncomputation by simulating Turing machines with optimal time and space\ncomplexity, and thus can solve arbitrary computational tasks that would\notherwise be intractable given context window constraints.\n","authors":["Chenxiao Yang","Nathan Srebro","David McAllester","Zhiyuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.14337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14333v1","updated":"2025-03-18T15:08:19Z","published":"2025-03-18T15:08:19Z","title":"Revealing higher-order neural representations with generative artificial\n  intelligence","summary":"  Studies often aim to reveal how neural representations encode aspects of an\nobserver's environment, such as its contents or structure. These are\n``first-order\" representations (FORs), because they're ``about\" the external\nworld. A less-common target is ``higher-order\" representations (HORs), which\nare ``about\" FORs -- their contents, stability, or uncertainty. HORs of\nuncertainty appear critically involved in adaptive behaviors including learning\nunder uncertainty, influencing learning rates and internal model updating based\non environmental feedback. However, HORs about uncertainty are unlikely to be\ndirect ``read-outs\" of FOR characteristics, instead reflecting estimation\nprocesses which may be lossy, bias-prone, or distortive and which may also\nincorporate estimates of distributions of uncertainty the observer is likely to\nexperience. While some research has targeted neural representations of\n``instantaneously\" estimated uncertainty, how the brain represents\n\\textit{distributions} of expected uncertainty remains largely unexplored.\nHere, we propose a novel reinforcement learning (RL) based generative\nartificial intelligence (genAI) approach to explore neural representations of\nuncertainty distributions. We use existing functional magnetic resonance\nimaging data, where humans learned to `de-noise' their brain states to achieve\ntarget neural patterns, to train denoising diffusion genAI models with RL\nalgorithms to learn noise distributions similar to how humans might learn to do\nthe same. We then explore these models' learned noise-distribution HORs\ncompared to control models trained with traditional backpropagation. Results\nreveal model-dependent differences in noise distribution representations --\nwith the RL-based model offering much higher explanatory power for human\nbehavior -- offering an exciting path towards using genAI to explore neural\nnoise-distribution HORs.\n","authors":["Hojjat Azimi Asrari","Megan A. K. Peters"],"pdf_url":"https://arxiv.org/pdf/2503.14333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16793v2","updated":"2025-03-18T15:07:23Z","published":"2025-02-24T03:04:48Z","title":"VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on\n  Contrastive Learning","summary":"  Graph Neural Networks (GNNs) have gained attention for their ability to learn\nrepresentations from graph data. Due to privacy concerns and conflicts of\ninterest that prevent clients from directly sharing graph data with one\nanother, Vertical Graph Federated Learning (VGFL) frameworks have been\ndeveloped. Recent studies have shown that VGFL is vulnerable to adversarial\nattacks that degrade performance. However, it is a common problem that client\nnodes are often unlabeled in the realm of VGFL. Consequently, the existing\nattacks, which rely on the availability of labeling information to obtain\ngradients, are inherently constrained in their applicability. This limitation\nprecludes their deployment in practical, real-world environments. To address\nthe above problems, we propose a novel graph adversarial attack against VGFL,\nreferred to as VGFL-SA, to degrade the performance of VGFL by modifying the\nlocal clients structure without using labels. Specifically, VGFL-SA uses a\ncontrastive learning method to complete the attack before the local clients are\ntrained. VGFL-SA first accesses the graph structure and node feature\ninformation of the poisoned clients, and generates the contrastive views by\nnode-degree-based edge augmentation and feature shuffling augmentation. Then,\nVGFL-SA uses the shared graph encoder to get the embedding of each view, and\nthe gradients of the adjacency matrices are obtained by the contrastive\nfunction. Finally, perturbed edges are generated using gradient modification\nrules. We validated the performance of VGFL-SA by performing a node\nclassification task on real-world datasets, and the results show that VGFL-SA\nachieves good attack effectiveness and transferability.\n","authors":["Yang Chen","Bin Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.16793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13281v2","updated":"2025-03-18T14:56:41Z","published":"2025-03-17T15:31:55Z","title":"LLM-Match: An Open-Sourced Patient Matching Model Based on Large\n  Language Models and Retrieval-Augmented Generation","summary":"  Patient matching is the process of linking patients to appropriate clinical\ntrials by accurately identifying and matching their medical records with trial\neligibility criteria. We propose LLM-Match, a novel framework for patient\nmatching leveraging fine-tuned open-source large language models. Our approach\nconsists of four key components. First, a retrieval-augmented generation (RAG)\nmodule extracts relevant patient context from a vast pool of electronic health\nrecords (EHRs). Second, a prompt generation module constructs input prompts by\nintegrating trial eligibility criteria (both inclusion and exclusion criteria),\npatient context, and system instructions. Third, a fine-tuning module with a\nclassification head optimizes the model parameters using structured prompts and\nground-truth labels. Fourth, an evaluation module assesses the fine-tuned\nmodel's performance on the testing datasets. We evaluated LLM-Match on four\nopen datasets - n2c2, SIGIR, TREC 2021, and TREC 2022 - using open-source\nmodels, comparing it against TrialGPT, Zero-Shot, and GPT-4-based closed\nmodels. LLM-Match outperformed all baselines.\n","authors":["Xiaodi Li","Shaika Chowdhury","Chung Il Wi","Maria Vassilaki","Ken Liu","Terence T Sio","Owen Garrick","Young J Juhn","James R Cerhan","Cui Tao","Nansu Zong"],"pdf_url":"https://arxiv.org/pdf/2503.13281v2.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2407.17226v3","updated":"2025-03-18T14:55:51Z","published":"2024-07-24T12:26:21Z","title":"Sublinear Regret for a Class of Continuous-Time Linear-Quadratic\n  Reinforcement Learning Problems","summary":"  We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions, where states are\nscalar-valued and running control rewards are absent but volatilities of the\nstate processes depend on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an RL algorithm to learn the optimal policy\nparameter directly. Our main contributions include the introduction of an\nexploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor, where $N$ is the number of learning episodes. We conduct\na simulation study to validate the theoretical results and demonstrate the\neffectiveness and reliability of the proposed algorithm. We also perform\nnumerical comparisons between our method and those of the recent model-based\nstochastic LQ RL studies adapted to the state- and control-dependent volatility\nsetting, demonstrating a better performance of the former in terms of regret\nbounds.\n","authors":["Yilie Huang","Yanwei Jia","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.17226v3.pdf","comment":"49 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.14322v1","updated":"2025-03-18T14:53:20Z","published":"2025-03-18T14:53:20Z","title":"Consumer-grade EEG-based Eye Tracking","summary":"  Electroencephalography-based eye tracking (EEG-ET) leverages eye movement\nartifacts in EEG signals as an alternative to camera-based tracking. While\nEEG-ET offers advantages such as robustness in low-light conditions and better\nintegration with brain-computer interfaces, its development lags behind\ntraditional methods, particularly in consumer-grade settings. To support\nresearch in this area, we present a dataset comprising simultaneous EEG and\neye-tracking recordings from 113 participants across 116 sessions, amounting to\n11 hours and 45 minutes of recordings. Data was collected using a\nconsumer-grade EEG headset and webcam-based eye tracking, capturing eye\nmovements under four experimental paradigms with varying complexity. The\ndataset enables the evaluation of EEG-ET methods across different gaze\nconditions and serves as a benchmark for assessing feasibility with affordable\nhardware. Data preprocessing includes handling of missing values and filtering\nto enhance usability. In addition to the dataset, code for data preprocessing\nand analysis is available to support reproducibility and further research.\n","authors":["Tiago Vasconcelos Afonso","Florian Heinrichs"],"pdf_url":"https://arxiv.org/pdf/2503.14322v1.pdf","comment":"Data descriptor, 13 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.14321v1","updated":"2025-03-18T14:51:42Z","published":"2025-03-18T14:51:42Z","title":"COPA: Comparing the Incomparable to Explore the Pareto Front","summary":"  In machine learning (ML), it is common to account for multiple objectives\nwhen, e.g., selecting a model to deploy. However, it is often unclear how one\nshould compare, aggregate and, ultimately, trade-off these objectives, as they\nmight be measured in different units or scales. For example, when deploying\nlarge language models (LLMs), we might not only care about their performance,\nbut also their CO2 consumption. In this work, we investigate how objectives can\nbe sensibly compared and aggregated to navigate their Pareto front. To do so,\nwe propose to make incomparable objectives comparable via their CDFs,\napproximated by their relative rankings. This allows us to aggregate them while\nmatching user-specific preferences, allowing practitioners to meaningfully\nnavigate and search for models in the Pareto front. We demonstrate the\npotential impact of our methodology in diverse areas such as LLM selection,\ndomain generalization, and AutoML benchmarking, where classical ways to\naggregate and normalize objectives fail.\n","authors":["Adrián Javaloy","Antonio Vergari","Isabel Valera"],"pdf_url":"https://arxiv.org/pdf/2503.14321v1.pdf","comment":"19 pages, 14 figures. Under submission"},{"id":"http://arxiv.org/abs/2503.14301v1","updated":"2025-03-18T14:42:38Z","published":"2025-03-18T14:42:38Z","title":"FeNeC: Enhancing Continual Learning via Feature Clustering with\n  Neighbor- or Logit-Based Classification","summary":"  The ability of deep learning models to learn continuously is essential for\nadapting to new data categories and evolving data distributions. In recent\nyears, approaches leveraging frozen feature extractors after an initial\nlearning phase have been extensively studied. Many of these methods estimate\nper-class covariance matrices and prototypes based on backbone-derived feature\nrepresentations. Within this paradigm, we introduce FeNeC (Feature Neighborhood\nClassifier) and FeNeC-Log, its variant based on the log-likelihood function.\nOur approach generalizes the existing concept by incorporating data clustering\nto capture greater intra-class variability. Utilizing the Mahalanobis distance,\nour models classify samples either through a nearest neighbor approach or\ntrainable logit values assigned to consecutive classes. Our proposition may be\nreduced to the existing approaches in a special case while extending them with\nthe ability of more flexible adaptation to data. We demonstrate that two FeNeC\nvariants achieve competitive performance in scenarios where task identities are\nunknown and establish state-of-the-art results on several benchmarks.\n","authors":["Kamil Książek","Hubert Jastrzębski","Bartosz Trojan","Krzysztof Pniaczek","Michał Karp","Jacek Tabor"],"pdf_url":"https://arxiv.org/pdf/2503.14301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14299v1","updated":"2025-03-18T14:41:33Z","published":"2025-03-18T14:41:33Z","title":"Unveiling the Role of Randomization in Multiclass Adversarial\n  Classification: Insights from Graph Theory","summary":"  Randomization as a mean to improve the adversarial robustness of machine\nlearning models has recently attracted significant attention. Unfortunately,\nmuch of the theoretical analysis so far has focused on binary classification,\nproviding only limited insights into the more complex multiclass setting. In\nthis paper, we take a step toward closing this gap by drawing inspiration from\nthe field of graph theory. Our analysis focuses on discrete data distributions,\nallowing us to cast the adversarial risk minimization problems within the\nwell-established framework of set packing problems. By doing so, we are able to\nidentify three structural conditions on the support of the data distribution\nthat are necessary for randomization to improve robustness. Furthermore, we are\nable to construct several data distributions where (contrarily to binary\nclassification) switching from a deterministic to a randomized solution\nsignificantly reduces the optimal adversarial risk. These findings highlight\nthe crucial role randomization can play in enhancing robustness to adversarial\nattacks in multiclass classification.\n","authors":["Lucas Gnecco-Heredia","Matteo Sammut","Muni Sreenivas Pydi","Rafael Pinot","Benjamin Negrevergne","Yann Chevaleyre"],"pdf_url":"https://arxiv.org/pdf/2503.14299v1.pdf","comment":"9 pages (main), 30 in total. Camera-ready version, accepted at\n  AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.14297v1","updated":"2025-03-18T14:39:47Z","published":"2025-03-18T14:39:47Z","title":"Improved Scalable Lipschitz Bounds for Deep Neural Networks","summary":"  Computing tight Lipschitz bounds for deep neural networks is crucial for\nanalyzing their robustness and stability, but existing approaches either\nproduce relatively conservative estimates or rely on semidefinite programming\n(SDP) formulations (namely the LipSDP condition) that face scalability issues.\nBuilding upon ECLipsE-Fast, the state-of-the-art Lipschitz bound method that\navoids SDP formulations, we derive a new family of improved scalable Lipschitz\nbounds that can be combined to outperform ECLipsE-Fast. Specifically, we\nleverage more general parameterizations of feasible points of LipSDP to derive\nvarious closed-form Lipschitz bounds, avoiding the use of SDP solvers. In\naddition, we show that our technique encompasses ECLipsE-Fast as a special case\nand leads to a much larger class of scalable Lipschitz bounds for deep neural\nnetworks. Our empirical study shows that our bounds improve ECLipsE-Fast,\nfurther advancing the scalability and precision of Lipschitz estimation for\nlarge neural networks.\n","authors":["Usman Syed","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2503.14297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11737v2","updated":"2025-03-18T14:34:49Z","published":"2025-03-14T14:44:54Z","title":"Multi-View Node Pruning for Accurate Graph Representation","summary":"  Graph pooling, which compresses a whole graph into a smaller coarsened graph,\nis an essential component of graph representation learning. To efficiently\ncompress a given graph, graph pooling methods often drop their nodes with\nattention-based scoring with the task loss. However, this often results in\nsimply removing nodes with lower degrees without consideration of their\nfeature-level relevance to the given task. To fix this problem, we propose a\nMulti-View Pruning(MVP), a graph pruning method based on a multi-view framework\nand reconstruction loss. Given a graph, MVP first constructs multiple graphs\nfor different views either by utilizing the predefined modalities or by\nrandomly partitioning the input features, to consider the importance of each\nnode in diverse perspectives. Then, it learns the score for each node by\nconsidering both the reconstruction and the task loss. MVP can be incorporated\nwith any hierarchical pooling framework to score the nodes. We validate MVP on\nmultiple benchmark datasets by coupling it with two graph pooling methods, and\nshow that it significantly improves the performance of the base graph pooling\nmethod, outperforming all baselines. Further analysis shows that both the\nencoding of multiple views and the consideration of reconstruction loss are the\nkey to the success of MVP, and that it indeed identifies nodes that are less\nimportant according to domain knowledge.\n","authors":["Jiseong Park","Hanjin Kim","Seojin Kim","Jueun Choi"],"pdf_url":"https://arxiv.org/pdf/2503.11737v2.pdf","comment":"Jiseong Park and Hanjin Kim are co-first author for this work"},{"id":"http://arxiv.org/abs/2503.14286v1","updated":"2025-03-18T14:23:37Z","published":"2025-03-18T14:23:37Z","title":"Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs","summary":"  We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance.\n","authors":["Nicolas Le Roux","Marc G. Bellemare","Jonathan Lebensold","Arnaud Bergeron","Joshua Greaves","Alex Fréchette","Carolyne Pelletier","Eric Thibodeau-Laufer Sándor Toth","Samantha Work"],"pdf_url":"https://arxiv.org/pdf/2503.14286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14281v1","updated":"2025-03-18T14:20:54Z","published":"2025-03-18T14:20:54Z","title":"XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding\n  Assistants","summary":"  AI coding assistants are widely used for tasks like code generation, bug\ndetection, and comprehension. These tools now require large and complex\ncontexts, automatically sourced from various origins$\\unicode{x2014}$across\nfiles, projects, and contributors$\\unicode{x2014}$forming part of the prompt\nfed to underlying LLMs. This automatic context-gathering introduces new\nvulnerabilities, allowing attackers to subtly poison input to compromise the\nassistant's outputs, potentially generating vulnerable code, overlooking flaws,\nor introducing critical errors. We propose a novel attack, Cross-Origin Context\nPoisoning (XOXO), that is particularly challenging to detect as it relies on\nadversarial code modifications that are semantically equivalent. Traditional\nprogram analysis techniques struggle to identify these correlations since the\nsemantics of the code remain correct, making it appear legitimate. This allows\nattackers to manipulate code assistants into producing incorrect outputs,\nincluding vulnerabilities or backdoors, while shifting the blame to the victim\ndeveloper or tester. We introduce a novel, task-agnostic black-box attack\nalgorithm GCGS that systematically searches the transformation space using a\nCayley Graph, achieving an 83.09% attack success rate on average across five\ntasks and eleven models, including GPT-4o and Claude 3.5 Sonnet v2 used by many\npopular AI coding assistants. Furthermore, existing defenses, including\nadversarial fine-tuning, are ineffective against our attack, underscoring the\nneed for new security measures in LLM-powered coding tools.\n","authors":["Adam Štorek","Mukur Gupta","Noopur Bhatt","Aditya Gupta","Janie Kim","Prashast Srivastava","Suman Jana"],"pdf_url":"https://arxiv.org/pdf/2503.14281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05994v3","updated":"2025-03-18T14:17:32Z","published":"2024-12-08T16:58:29Z","title":"PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh\n  Representations","summary":"  The numerical approximation of partial differential equations (PDEs) using\nneural networks has seen significant advancements through Physics-Informed\nNeural Networks (PINNs). Despite their straightforward optimization framework\nand flexibility in implementing various PDEs, PINNs often suffer from limited\naccuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which\nstruggle to effectively learn high-frequency and nonlinear components.\nRecently, parametric mesh representations in combination with neural networks\nhave been investigated as a promising approach to eliminate the inductive bias\nof MLPs. However, they usually require high-resolution grids and a large number\nof collocation points to achieve high accuracy while avoiding overfitting. In\naddition, the fixed positions of the mesh parameters restrict their\nflexibility, making accurate approximation of complex PDEs challenging. To\novercome these limitations, we propose Physics-Informed Gaussians (PIGs), which\ncombine feature embeddings using Gaussian functions with a lightweight neural\nnetwork. Our approach uses trainable parameters for the mean and variance of\neach Gaussian, allowing for dynamic adjustment of their positions and shapes\nduring training. This adaptability enables our model to optimally approximate\nPDE solutions, unlike models with fixed parameter positions. Furthermore, the\nproposed approach maintains the same optimization framework used in PINNs,\nallowing us to benefit from their excellent properties. Experimental results\nshow the competitive performance of our model across various PDEs,\ndemonstrating its potential as a robust tool for solving complex PDEs. Our\nproject page is available at\nhttps://namgyukang.github.io/Physics-Informed-Gaussians/\n","authors":["Namgyu Kang","Jaemin Oh","Youngjoon Hong","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2412.05994v3.pdf","comment":"Accepted by ICLR 2025. Project page:\n  https://namgyukang.github.io/Physics-Informed-Gaussians/"},{"id":"http://arxiv.org/abs/2305.07612v2","updated":"2025-03-18T14:16:09Z","published":"2023-05-12T17:02:43Z","title":"Lower Bounds and Accelerated Algorithms in Distributed Stochastic\n  Optimization with Communication Compression","summary":"  Communication compression is an essential strategy for alleviating\ncommunication overhead by reducing the volume of information exchanged between\ncomputing nodes in large-scale distributed stochastic optimization. Although\nnumerous algorithms with convergence guarantees have been obtained, the optimal\nperformance limit under communication compression remains unclear.\n  In this paper, we investigate the performance limit of distributed stochastic\noptimization algorithms employing communication compression. We focus on two\nmain types of compressors, unbiased and contractive, and address the\nbest-possible convergence rates one can obtain with these compressors. We\nestablish the lower bounds for the convergence rates of distributed stochastic\noptimization in six different settings, combining strongly-convex,\ngenerally-convex, or non-convex functions with unbiased or contractive\ncompressor types. To bridge the gap between lower bounds and existing\nalgorithms' rates, we propose NEOLITHIC, a nearly optimal algorithm with\ncompression that achieves the established lower bounds up to logarithmic\nfactors under mild conditions. Extensive experimental results support our\ntheoretical findings. This work provides insights into the theoretical\nlimitations of existing compressors and motivates further research into\nfundamentally new compressor properties.\n","authors":["Yutong He","Xinmeng Huang","Yiming Chen","Wotao Yin","Kun Yuan"],"pdf_url":"https://arxiv.org/pdf/2305.07612v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11356v2","updated":"2025-03-18T14:01:54Z","published":"2023-12-18T17:12:35Z","title":"The Problem of Coherence in Natural Language Explanations of\n  Recommendations","summary":"  Providing natural language explanations for recommendations is particularly\nuseful from the perspective of a non-expert user. Although several methods for\nproviding such explanations have recently been proposed, we argue that an\nimportant aspect of explanation quality has been overlooked in their\nexperimental evaluation. Specifically, the coherence between generated text and\npredicted rating, which is a necessary condition for an explanation to be\nuseful, is not properly captured by currently used evaluation measures. In this\npaper, we highlight the issue of explanation and prediction coherence by 1)\npresenting results from a manual verification of explanations generated by one\nof the state-of-the-art approaches 2) proposing a method of automatic coherence\nevaluation 3) introducing a new transformer-based method that aims to produce\nmore coherent explanations than the state-of-the-art approaches 4) performing\nan experimental evaluation which demonstrates that this method significantly\nimproves the explanation coherence without affecting the other aspects of\nrecommendation performance.\n","authors":["Jakub Raczyński","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2312.11356v2.pdf","comment":"ECAI 2023"},{"id":"http://arxiv.org/abs/2403.13501v2","updated":"2025-03-18T13:55:22Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v2.pdf","comment":"Accepted at ICLR 2025. Code: https://github.com/boschresearch/VSTAR\n  and project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2503.14260v1","updated":"2025-03-18T13:50:44Z","published":"2025-03-18T13:50:44Z","title":"Automating Experimental Optics with Sample Efficient Machine Learning\n  Methods","summary":"  As free-space optical systems grow in scale and complexity, troubleshooting\nbecomes increasingly time-consuming and, in the case of remote installations,\nperhaps impractical. An example of a task that is often laborious is the\nalignment of a high-finesse optical resonator, which is highly sensitive to the\nmode of the input beam. In this work, we demonstrate how machine learning can\nbe used to achieve autonomous mode-matching of a free-space optical resonator\nwith minimal supervision. Our approach leverages sample-efficient algorithms to\nreduce data requirements while maintaining a simple architecture for easy\ndeployment. The reinforcement learning scheme that we have developed shows that\nautomation is feasible even in systems prone to drift in experimental\nparameters, as may well be the case in real-world applications.\n","authors":["Arindam Saha","Baramee Charoensombutamon","Thibault Michel","V. Vijendran","Lachlan Walker","Akira Furusawa","Syed M. Assad","Ben C. Buchler","Ping Koy Lam","Aaron D. Tranter"],"pdf_url":"https://arxiv.org/pdf/2503.14260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14259v1","updated":"2025-03-18T13:50:35Z","published":"2025-03-18T13:50:35Z","title":"Quantization-Free Autoregressive Action Transformer","summary":"  Current transformer-based imitation learning approaches introduce discrete\naction representations and train an autoregressive transformer decoder on the\nresulting latent code. However, the initial quantization breaks the continuous\nstructure of the action space thereby limiting the capabilities of the\ngenerative model. We propose a quantization-free method instead that leverages\nGenerative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous\npolicy parametrization for autoregressive transformers. This simplifies the\nimitation learning pipeline while achieving state-of-the-art performance on a\nvariety of popular simulated robotics tasks. We enhance our policy roll-outs by\ncarefully studying sampling algorithms, further improving the results.\n","authors":["Ziyad Sheebaelhamd","Michael Tschannen","Michael Muehlebach","Claire Vernade"],"pdf_url":"https://arxiv.org/pdf/2503.14259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14253v1","updated":"2025-03-18T13:43:57Z","published":"2025-03-18T13:43:57Z","title":"CINNAMON: A hybrid approach to change point detection and parameter\n  estimation in single-particle tracking data","summary":"  Change point detection has become an important part of the analysis of the\nsingle-particle tracking data, as it allows one to identify moments, in which\nthe motion patterns of observed particles undergo significant changes. The\nsegmentation of diffusive trajectories based on those moments may provide\ninsight into various phenomena in soft condensed matter and biological physics.\nIn this paper, we propose CINNAMON, a hybrid approach to classifying\nsingle-particle tracking trajectories, detecting change points within them, and\nestimating diffusion parameters in the segments between the change points. Our\nmethod is based on a combination of neural networks, feature-based machine\nlearning, and statistical techniques. It has been benchmarked in the second\nAnomalous Diffusion Challenge. The method offers a high level of\ninterpretability due to its analytical and feature-based components. A\npotential use of features from topological data analysis is also discussed.\n","authors":["Jakub Malinowski","Marcin Kostrzewa","Michał Balcerek","Weronika Tomczuk","Janusz Szwabiński"],"pdf_url":"https://arxiv.org/pdf/2503.14253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04653v2","updated":"2025-03-18T13:39:31Z","published":"2024-11-07T12:28:52Z","title":"IGDrivSim: A Benchmark for the Imitation Gap in Autonomous Driving","summary":"  Developing autonomous vehicles that can navigate complex environments with\nhuman-level safety and efficiency is a central goal in self-driving research. A\ncommon approach to achieving this is imitation learning, where agents are\ntrained to mimic human expert demonstrations collected from real-world driving\nscenarios. However, discrepancies between human perception and the self-driving\ncar's sensors can introduce an $\\textit{imitation}$ gap, leading to imitation\nlearning failures. In this work, we introduce $\\textbf{IGDrivSim}$, a benchmark\nbuilt on top of the Waymax simulator, designed to investigate the effects of\nthe imitation gap in learning autonomous driving policy from human expert\ndemonstrations. Our experiments show that this perception gap between human\nexperts and self-driving agents can hinder the learning of safe and effective\ndriving behaviors. We further show that combining imitation with reinforcement\nlearning, using a simple penalty reward for prohibited behaviors, effectively\nmitigates these failures. Our code is open-sourced at:\nhttps://github.com/clemgris/IGDrivSim.git.\n","authors":["Clémence Grislain","Risto Vuorio","Cong Lu","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2411.04653v2.pdf","comment":"8 pages, 4 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.14246v1","updated":"2025-03-18T13:35:24Z","published":"2025-03-18T13:35:24Z","title":"Trading-off Accuracy and Communication Cost in Federated Learning","summary":"  Leveraging the training-by-pruning paradigm introduced by Zhou et al. and\nIsik et al. introduced a federated learning protocol that achieves a 34-fold\nreduction in communication cost. We achieve a compression improvements of\norders of orders of magnitude over the state-of-the-art. The central idea of\nour framework is to encode the network weights $\\vec w$ by a the vector of\ntrainable parameters $\\vec p$, such that $\\vec w = Q\\cdot \\vec p$ where $Q$ is\na carefully-generate sparse random matrix (that remains fixed throughout\ntraining). In such framework, the previous work of Zhou et al. [NeurIPS'19] is\nretrieved when $Q$ is diagonal and $\\vec p$ has the same dimension of $\\vec w$.\nWe instead show that $\\vec p$ can effectively be chosen much smaller than $\\vec\nw$, while retaining the same accuracy at the price of a decrease of the\nsparsity of $Q$. Since server and clients only need to share $\\vec p$, such a\ntrade-off leads to a substantial improvement in communication cost. Moreover,\nwe provide theoretical insight into our framework and establish a novel link\nbetween training-by-sampling and random convex geometry.\n","authors":["Mattia Jacopo Villani","Emanuele Natale","Frederik Mallmann-Trenn"],"pdf_url":"https://arxiv.org/pdf/2503.14246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14240v1","updated":"2025-03-18T13:22:52Z","published":"2025-03-18T13:22:52Z","title":"Persistent Homology-induced Graph Ensembles for Time Series Regressions","summary":"  The effectiveness of Spatio-temporal Graph Neural Networks (STGNNs) in\ntime-series applications is often limited by their dependence on fixed,\nhand-crafted input graph structures. Motivated by insights from the Topological\nData Analysis (TDA) paradigm, of which real-world data exhibits multi-scale\npatterns, we construct several graphs using \\textit{Persistent Homology\nFiltration} -- a mathematical framework describing the multiscale structural\nproperties of data points. Then, we use the constructed graphs as an input to\ncreate an ensemble of Graph Neural Networks. The ensemble aggregates the\nsignals from the individual learners via an attention-based routing mechanism,\nthus systematically encoding the inherent multiscale structures of data. Four\ndifferent real-world experiments on seismic activity prediction and traffic\nforecasting (PEMS-BAY, METR-LA) demonstrate that our approach consistently\noutperforms single-graph baselines while providing interpretable insights.\n","authors":["Viet The Nguyen","Duy Anh Pham","An Thai Le","Jans Peter","Gunther Gust"],"pdf_url":"https://arxiv.org/pdf/2503.14240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08786v2","updated":"2025-03-18T13:22:41Z","published":"2024-09-13T12:45:30Z","title":"Modular Neural Wiretap Codes for Fading Channels","summary":"  The wiretap channel is a well-studied problem in the physical layer security\nliterature. Although it is proven that the decoding error probability and\ninformation leakage can be made arbitrarily small in the asymptotic regime,\nfurther research on finite-blocklength codes is required on the path towards\npractical, secure communication systems. This work provides the first\nexperimental characterization of a deep learning-based, finite-blocklength code\nconstruction for multi-tap fading wiretap channels without channel state\ninformation. In addition to the evaluation of the average probability of error\nand information leakage, we examine the designed codes in the presence of\nfading in terms of the equivocation rate and illustrate the influence of (i)\nthe number of fading taps, (ii) differing variances of the fading coefficients,\nand (iii) the seed selection for the hash function-based security layer.\n","authors":["Daniel Seifert","Onur Günlü","Rafael F. Schaefer"],"pdf_url":"https://arxiv.org/pdf/2409.08786v2.pdf","comment":"Limit performance assessment to constant rate scenarios, add\n  examination of equivocation rate"},{"id":"http://arxiv.org/abs/2501.05661v2","updated":"2025-03-18T13:21:08Z","published":"2025-01-10T02:25:39Z","title":"TAMER: A Test-Time Adaptive MoE-Driven Framework for EHR Representation\n  Learning","summary":"  We propose TAMER, a Test-time Adaptive MoE-driven framework for Electronic\nHealth Record (EHR) Representation learning. TAMER introduces a framework where\na Mixture-of-Experts (MoE) architecture is co-designed with Test-Time\nAdaptation (TTA) to jointly mitigate the intertwined challenges of patient\nheterogeneity and distribution shifts in EHR modeling. The MoE focuses on\nlatent patient subgroups through domain-aware expert specialization, while TTA\nenables real-time adaptation to evolving health status distributions when new\npatient samples are introduced. Extensive experiments across four real-world\nEHR datasets demonstrate that TAMER consistently improves predictive\nperformance for both mortality and readmission risk tasks when combined with\ndiverse EHR modeling backbones. TAMER offers a promising approach for dynamic\nand personalized EHR-based predictions in practical clinical settings.\n","authors":["Yinghao Zhu","Xiaochen Zheng","Ahmed Allam","Michael Krauthammer"],"pdf_url":"https://arxiv.org/pdf/2501.05661v2.pdf","comment":"8 pages, 3 figures, 7 tables. Code is available at:\n  https://github.com/yhzhu99/TAMER"},{"id":"http://arxiv.org/abs/2503.14239v1","updated":"2025-03-18T13:20:22Z","published":"2025-03-18T13:20:22Z","title":"Predicting Cardiopulmonary Exercise Testing Outcomes in Congenital Heart\n  Disease Through Multi-modal Data Integration and Geometric Learning","summary":"  Cardiopulmonary exercise testing (CPET) provides a comprehensive assessment\nof functional capacity by measuring key physiological variables including\noxygen consumption ($VO_2$), carbon dioxide production ($VCO_2$), and pulmonary\nventilation ($VE$) during exercise. Previous research has established that\nparameters such as peak $VO_2$ and $VE/VCO_2$ ratio serve as robust predictors\nof mortality risk in chronic heart failure patients. In this study, we leverage\nCPET variables as surrogate mortality endpoints for patients with Congenital\nHeart Disease (CHD). To our knowledge, this represents the first successful\nimplementation of an advanced machine learning approach that predicts CPET\noutcomes by integrating electrocardiograms (ECGs) with information derived from\nclinical letters. Our methodology began with extracting unstructured patient\ninformation-including intervention history, diagnoses, and medication\nregimens-from clinical letters using natural language processing techniques,\norganizing this data into a structured database. We then digitized ECGs to\nobtain quantifiable waveforms and established comprehensive data linkages. The\ncore innovation of our approach lies in exploiting the Riemannian geometric\nproperties of covariance matrices derived from both 12-lead ECGs and clinical\ntext data to develop robust regression and classification models. Through\nextensive ablation studies, we demonstrated that the integration of ECG signals\nwith clinical documentation, enhanced by covariance augmentation techniques in\nRiemannian space, consistently produced superior predictive performance\ncompared to conventional approaches.\n","authors":["Muhammet Alkan","Gruschen Veldtman","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2503.14239v1.pdf","comment":"preprint for Scientific Reports"},{"id":"http://arxiv.org/abs/2411.12537v5","updated":"2025-03-18T13:13:18Z","published":"2024-11-19T14:35:38Z","title":"Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues","summary":"  Linear Recurrent Neural Networks (LRNNs) such as Mamba, RWKV, GLA, mLSTM, and\nDeltaNet have emerged as efficient alternatives to Transformers for long\nsequences. However, both Transformers and LRNNs struggle to perform\nstate-tracking, which may impair performance in tasks such as code evaluation.\nIn one forward pass, current architectures are unable to solve even parity, the\nsimplest state-tracking task, which non-linear RNNs can handle effectively.\nRecently, Sarrof et al. (2024) demonstrated that the failure of LRNNs like\nMamba to solve parity stems from restricting the value range of their diagonal\nstate-transition matrices to $[0, 1]$ and that incorporating negative values\ncan resolve this issue. We extend this result to non-diagonal LRNNs such as\nDeltaNet. We prove that finite precision LRNNs with state-transition matrices\nhaving only positive eigenvalues cannot solve parity, while non-triangular\nmatrices are needed to count modulo $3$. Notably, we also prove that LRNNs can\nlearn any regular language when their state-transition matrices are products of\nidentity minus vector outer product matrices, each with eigenvalues in the\nrange $[-1, 1]$. Our experiments confirm that extending the eigenvalue range of\nMamba and DeltaNet to include negative values not only enables them to solve\nparity but consistently improves their performance on state-tracking tasks. We\nalso show that state-tracking enabled LRNNs can be pretrained stably and\nefficiently at scale (1.3B parameters), achieving competitive performance on\nlanguage modeling and showing promise on code and math tasks.\n","authors":["Riccardo Grazzi","Julien Siems","Arber Zela","Jörg K. H. Franke","Frank Hutter","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2411.12537v5.pdf","comment":"V2: Correction to Theorem 1 and 2 and to point 3 of Proposition 1.\n  V3: ICLR Camera Ready, V4: ICLR Camera Ready, added figures to theory\n  section, updated modular arithmetic with brackets results because previous\n  results did not contain multiplication"},{"id":"http://arxiv.org/abs/2309.11647v4","updated":"2025-03-18T13:11:57Z","published":"2023-09-20T21:23:52Z","title":"Potential and limitations of random Fourier features for dequantizing\n  quantum machine learning","summary":"  Quantum machine learning is arguably one of the most explored applications of\nnear-term quantum devices. Much focus has been put on notions of variational\nquantum machine learning where parameterized quantum circuits (PQCs) are used\nas learning models. These PQC models have a rich structure which suggests that\nthey might be amenable to efficient dequantization via random Fourier features\n(RFF). In this work, we establish necessary and sufficient conditions under\nwhich RFF does indeed provide an efficient dequantization of variational\nquantum machine learning for regression. We build on these insights to make\nconcrete suggestions for PQC architecture design, and to identify structures\nwhich are necessary for a regression problem to admit a potential quantum\nadvantage via PQC based optimization.\n","authors":["Ryan Sweke","Erik Recio-Armengol","Sofiene Jerbi","Elies Gil-Fuster","Bryce Fuller","Jens Eisert","Johannes Jakob Meyer"],"pdf_url":"https://arxiv.org/pdf/2309.11647v4.pdf","comment":"44 pages (33+11). 6 Figures, with many clarifying figures added to\n  this version from original version. Comments and feedback welcome. Now\n  accepted in Quantum - this is the final version"},{"id":"http://arxiv.org/abs/2503.14232v1","updated":"2025-03-18T13:09:01Z","published":"2025-03-18T13:09:01Z","title":"CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion\n  Models","summary":"  Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks.\n","authors":["Yuyang Xue","Edward Moroshko","Feng Chen","Steven McDonagh","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2503.14232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14231v1","updated":"2025-03-18T13:09:00Z","published":"2025-03-18T13:09:00Z","title":"Multi-task Learning for Identification of Porcelain in Song and Yuan\n  Dynasties","summary":"  Chinese porcelain holds immense historical and cultural value, making its\naccurate classification essential for archaeological research and cultural\nheritage preservation. Traditional classification methods rely heavily on\nexpert analysis, which is time-consuming, subjective, and difficult to scale.\nThis paper explores the application of DL and transfer learning techniques to\nautomate the classification of porcelain artifacts across four key attributes:\ndynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks\n(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their\nperformance with and without pre-trained weights. Our results demonstrate that\ntransfer learning significantly enhances classification accuracy, particularly\nfor complex tasks like type classification, where models trained from scratch\nexhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high\naccuracy and robustness across all tasks, while VGG16 struggles with more\ndiverse classifications. We further discuss the impact of dataset limitations\nand propose future directions, including domain-specific pre-training,\nintegration of attention mechanisms, explainable AI methods, and generalization\nto other cultural artifacts.\n","authors":["Ziyao Ling","Giovanni Delnevo","Paola Salomoni","Silvia Mirri"],"pdf_url":"https://arxiv.org/pdf/2503.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13637v4","updated":"2025-03-18T13:03:48Z","published":"2024-05-22T13:36:48Z","title":"Curriculum Direct Preference Optimization for Diffusion and Consistency\n  Models","summary":"  Direct Preference Optimization (DPO) has been proposed as an effective and\nefficient alternative to reinforcement learning from human feedback (RLHF). In\nthis paper, we propose a novel and enhanced version of DPO based on curriculum\nlearning for text-to-image generation. Our method is divided into two training\nstages. First, a ranking of the examples generated for each prompt is obtained\nby employing a reward model. Then, increasingly difficult pairs of examples are\nsampled and provided to a text-to-image generative (diffusion or consistency)\nmodel. Generated samples that are far apart in the ranking are considered to\nform easy pairs, while those that are close in the ranking form hard pairs. In\nother words, we use the rank difference between samples as a measure of\ndifficulty. The sampled pairs are split into batches according to their\ndifficulty levels, which are gradually used to train the generative model. Our\napproach, Curriculum DPO, is compared against state-of-the-art fine-tuning\napproaches on nine benchmarks, outperforming the competing methods in terms of\ntext alignment, aesthetics and human preference. Our code is available at\nhttps://github.com/CroitoruAlin/Curriculum-DPO.\n","authors":["Florinel-Alin Croitoru","Vlad Hondru","Radu Tudor Ionescu","Nicu Sebe","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2405.13637v4.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2412.14295v2","updated":"2025-03-18T13:01:07Z","published":"2024-12-18T19:46:04Z","title":"Temporally Consistent Object-Centric Learning by Contrasting Slots","summary":"  Unsupervised object-centric learning from videos is a promising approach to\nextract structured representations from large, unlabeled collections of videos.\nTo support downstream tasks like autonomous control, these representations must\nbe both compositional and temporally consistent. Existing approaches based on\nrecurrent processing often lack long-term stability across frames because their\ntraining objective does not enforce temporal consistency. In this work, we\nintroduce a novel object-level temporal contrastive loss for video\nobject-centric models that explicitly promotes temporal consistency. Our method\nsignificantly improves the temporal consistency of the learned object-centric\nrepresentations, yielding more reliable video decompositions that facilitate\nchallenging downstream tasks such as unsupervised object dynamics prediction.\nFurthermore, the inductive bias added by our loss strongly improves object\ndiscovery, leading to state-of-the-art results on both synthetic and real-world\ndatasets, outperforming even weakly-supervised methods that leverage motion\nmasks as additional cues.\n","authors":["Anna Manasyan","Maximilian Seitzer","Filip Radovic","Georg Martius","Andrii Zadaianchuk"],"pdf_url":"https://arxiv.org/pdf/2412.14295v2.pdf","comment":"Published at CVPR 2025"},{"id":"http://arxiv.org/abs/2501.08669v2","updated":"2025-03-18T12:54:07Z","published":"2025-01-15T09:04:19Z","title":"SPEQ: Offline Stabilization Phases for Efficient Q-Learning in High\n  Update-To-Data Ratio Reinforcement Learning","summary":"  High update-to-data (UTD) ratio algorithms in reinforcement learning (RL)\nimprove sample efficiency but incur high computational costs, limiting\nreal-world scalability. We propose Offline Stabilization Phases for Efficient\nQ-Learning (SPEQ), an RL algorithm that combines low-UTD online training with\nperiodic offline stabilization phases. During these phases, Q-functions are\nfine-tuned with high UTD ratios on a fixed replay buffer, reducing redundant\nupdates on suboptimal data. This structured training schedule optimally\nbalances computational and sample efficiency, addressing the limitations of\nboth high and low UTD ratio approaches. We empirically demonstrate that SPEQ\nrequires from 40% to 99% fewer gradient updates and 27% to 78% less training\ntime compared to state-of-the-art high UTD ratio methods while maintaining or\nsurpassing their performance on the MuJoCo continuous control benchmark. Our\nfindings highlight the potential of periodic stabilization phases as an\neffective alternative to conventional training schedules, paving the way for\nmore scalable reinforcement learning solutions in real-world applications where\ncomputational resources are constrained.\n","authors":["Carlo Romeo","Girolamo Macaluso","Alessandro Sestini","Andrew D. Bagdanov"],"pdf_url":"https://arxiv.org/pdf/2501.08669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14217v1","updated":"2025-03-18T12:52:03Z","published":"2025-03-18T12:52:03Z","title":"Decision Tree Induction Through LLMs via Semantically-Aware Evolution","summary":"  Decision trees are a crucial class of models offering robust predictive\nperformance and inherent interpretability across various domains, including\nhealthcare, finance, and logistics. However, current tree induction methods\noften face limitations such as suboptimal solutions from greedy methods or\nprohibitive computational costs and limited applicability of exact optimization\napproaches. To address these challenges, we propose an evolutionary\noptimization method for decision tree induction based on genetic programming\n(GP). Our key innovation is the integration of semantic priors and\ndomain-specific knowledge about the search space into the optimization\nalgorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that\nincorporates semantic priors into genetic search operators through the use of\nLarge Language Models (LLMs), thereby enhancing search efficiency and targeting\nregions of the search space that yield decision trees with superior\ngeneralization performance. This is operationalized through novel genetic\noperators that work with structured natural language prompts, effectively\nutilizing LLMs as conditional generative models and sources of semantic\nknowledge. Specifically, we introduce $\\textit{fitness-guided}$ crossover to\nexploit high-performing regions, and $\\textit{diversity-guided}$ mutation for\nefficient global exploration of the search space. These operators are\ncontrolled by corresponding hyperparameters that enable a more nuanced balance\nbetween exploration and exploitation across the search space. Empirically, we\ndemonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves\nsuperior-performing trees compared to existing tree induction methods, and\nexhibits significantly more efficient search performance compared to\nconventional GP approaches.\n","authors":["Tennison Liu","Nicolas Huynh","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2503.14217v1.pdf","comment":"*Liu and Huynh contributed equally. Published as a conference paper\n  at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14213v1","updated":"2025-03-18T12:47:01Z","published":"2025-03-18T12:47:01Z","title":"Rolling Forward: Enhancing LightGCN with Causal Graph Convolution for\n  Credit Bond Recommendation","summary":"  Graph Neural Networks have significantly advanced research in recommender\nsystems over the past few years. These methods typically capture global\ninterests using aggregated past interactions and rely on static embeddings of\nusers and items over extended periods of time. While effective in some domains,\nthese methods fall short in many real-world scenarios, especially in finance,\nwhere user interests and item popularity evolve rapidly over time. To address\nthese challenges, we introduce a novel extension to Light Graph Convolutional\nNetwork (LightGCN) designed to learn temporal node embeddings that capture\ndynamic interests. Our approach employs causal convolution to maintain a\nforward-looking model architecture. By preserving the chronological order of\nuser-item interactions and introducing a dynamic update mechanism for\nembeddings through a sliding window, the proposed model generates well-timed\nand contextually relevant recommendations. Extensive experiments on a\nreal-world dataset from BNP Paribas demonstrate that our approach significantly\nenhances the performance of LightGCN while maintaining the simplicity and\nefficiency of its architecture. Our findings provide new insights into\ndesigning graph-based recommender systems in time-sensitive applications,\nparticularly for financial product recommendations.\n","authors":["Ashraf Ghiye","Baptiste Barreau","Laurent Carlier","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2503.14213v1.pdf","comment":"8 pages, published in the international conference for AI in Finance\n  (ACM ICAIF'24)"},{"id":"http://arxiv.org/abs/2410.01273v2","updated":"2025-03-18T12:44:59Z","published":"2024-10-02T06:34:45Z","title":"CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot\n  Interaction","summary":"  Real-life robot navigation involves more than just reaching a destination; it\nrequires optimizing movements while addressing scenario-specific goals. An\nintuitive way for humans to express these goals is through abstract cues like\nverbal commands or rough sketches. Such human guidance may lack details or be\nnoisy. Nonetheless, we expect robots to navigate as intended. For robots to\ninterpret and execute these abstract instructions in line with human\nexpectations, they must share a common understanding of basic navigation\nconcepts with humans. To this end, we introduce CANVAS, a novel framework that\ncombines visual and linguistic instructions for commonsense-aware navigation.\nIts success is driven by imitation learning, enabling the robot to learn from\nhuman navigation behavior. We present COMMAND, a comprehensive dataset with\nhuman-annotated navigation results, spanning over 48 hours and 219 km, designed\nto train commonsense-aware navigation systems in simulated environments. Our\nexperiments show that CANVAS outperforms the strong rule-based system ROS\nNavStack across all environments, demonstrating superior performance with noisy\ninstructions. Notably, in the orchard environment, where ROS NavStack records a\n0% total success rate, CANVAS achieves a total success rate of 67%. CANVAS also\nclosely aligns with human demonstrations and commonsense constraints, even in\nunseen environments. Furthermore, real-world deployment of CANVAS showcases\nimpressive Sim2Real transfer with a total success rate of 69%, highlighting the\npotential of learning from human demonstrations in simulated environments for\nreal-world applications.\n","authors":["Suhwan Choi","Yongjun Cho","Minchan Kim","Jaeyoon Jung","Myunchul Joe","Yubeen Park","Minseo Kim","Sungwoong Kim","Sungjae Lee","Hwiseong Park","Jiwan Chung","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2410.01273v2.pdf","comment":"Accepted to ICRA 2025, project page https://worv-ai.github.io/canvas"},{"id":"http://arxiv.org/abs/2301.12351v4","updated":"2025-03-18T12:39:18Z","published":"2023-01-29T04:10:12Z","title":"Emerging Synergies in Causality and Deep Generative Models: A Survey","summary":"  In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area.\n","authors":["Guanglin Zhou","Shaoan Xie","Guang-Yuan Hao","Shiming Chen","Biwei Huang","Xiwei Xu","Chen Wang","Liming Zhu","Lina Yao","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.12351v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14205v1","updated":"2025-03-18T12:30:57Z","published":"2025-03-18T12:30:57Z","title":"Layer-wise Adaptive Gradient Norm Penalizing Method for Efficient and\n  Accurate Deep Learning","summary":"  Sharpness-aware minimization (SAM) is known to improve the generalization\nperformance of neural networks. However, it is not widely used in real-world\napplications yet due to its expensive model perturbation cost. A few variants\nof SAM have been proposed to tackle such an issue, but they commonly do not\nalleviate the cost noticeably. In this paper, we propose a lightweight\nlayer-wise gradient norm penalizing method that tackles the expensive\ncomputational cost of SAM while maintaining its superior generalization\nperformance. Our study empirically proves that the gradient norm of the whole\nmodel can be effectively suppressed by penalizing the gradient norm of only a\nfew critical layers. We also theoretically show that such a partial model\nperturbation does not harm the convergence rate of SAM, allowing them to be\nsafely adapted in real-world applications. To demonstrate the efficacy of the\nproposed method, we perform extensive experiments comparing the proposed method\nto mini-batch SGD and the conventional SAM using representative computer vision\nand language modeling benchmarks.\n","authors":["Sunwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2503.14205v1.pdf","comment":"Published in KDD 2024"},{"id":"http://arxiv.org/abs/2411.18425v2","updated":"2025-03-18T12:18:04Z","published":"2024-11-27T15:07:44Z","title":"Streamlining Prediction in Bayesian Deep Learning","summary":"  The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.\n","authors":["Rui Li","Marcus Klasson","Arno Solin","Martin Trapp"],"pdf_url":"https://arxiv.org/pdf/2411.18425v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04610v7","updated":"2025-03-18T12:13:54Z","published":"2023-12-07T16:16:09Z","title":"Data-Driven Semi-Supervised Machine Learning with Safety Indicators for\n  Abnormal Driving Behavior Detection","summary":"  Detecting abnormal driving behavior is critical for road traffic safety and\nthe evaluation of drivers' behavior. With the advancement of machine learning\n(ML) algorithms and the accumulation of naturalistic driving data, many ML\nmodels have been adopted for abnormal driving behavior detection (also referred\nto in this paper as \"anomalies\"). Most existing ML-based detectors rely on\n(fully) supervised ML methods, which require substantial labeled data. However,\nground truth labels are not always available in the real world, and labeling\nlarge amounts of data is tedious. Thus, there is a need to explore unsupervised\nor semi-supervised methods to make the anomaly detection process more feasible\nand efficient. To fill this research gap, this study analyzes large-scale\nreal-world data revealing several abnormal driving behaviors (e.g., sudden\nacceleration, rapid lane-changing) and develops a hierarchical extreme learning\nmachine (HELM)-based semi-supervised ML method using partly labeled data to\ndetect the identified abnormal driving behaviors. Moreover, previous ML-based\napproaches predominantly utilized basic vehicle motion features (such as\nvelocity and acceleration) to label and detect abnormal driving behaviors,\nwhile this study seeks to introduce event-level safety indicators as input\nfeatures for ML models to improve detection performance. Results from extensive\nexperiments demonstrate the effectiveness of the proposed semi-supervised ML\nmodel with the introduced safety indicators serving as important features. The\nproposed semi-supervised ML method outperforms other baseline semi-supervised\nor unsupervised methods: for example, it delivers the best accuracy at 99.58%\nand the best F1-score at 0.9913. The ablation study further highlights the\nsignificance of safety indicators for advancing the detection performance of\nabnormal driving behaviors.\n","authors":["Yongqi Dong","Lanxin Zhang","Haneen Farah","Arkady Zgonnikov","Bart van Arem"],"pdf_url":"https://arxiv.org/pdf/2312.04610v7.pdf","comment":"16 pages, 10 figures, accepted by the 103rd Transportation Research\n  Board (TRB) Annual Meeting, accepted and published by Transportation Research\n  Record: Journal of the Transportation Research Board"},{"id":"http://arxiv.org/abs/2503.14192v1","updated":"2025-03-18T12:11:11Z","published":"2025-03-18T12:11:11Z","title":"Strategic White Paper on AI Infrastructure for Particle, Nuclear, and\n  Astroparticle Physics: Insights from JENA and EuCAIF","summary":"  Artificial intelligence (AI) is transforming scientific research, with deep\nlearning methods playing a central role in data analysis, simulations, and\nsignal detection across particle, nuclear, and astroparticle physics. Within\nthe JENA communities-ECFA, NuPECC, and APPEC-and as part of the EuCAIF\ninitiative, AI integration is advancing steadily. However, broader adoption\nremains constrained by challenges such as limited computational resources, a\nlack of expertise, and difficulties in transitioning from research and\ndevelopment (R&D) to production. This white paper provides a strategic roadmap,\ninformed by a community survey, to address these barriers. It outlines critical\ninfrastructure requirements, prioritizes training initiatives, and proposes\nfunding strategies to scale AI capabilities across fundamental physics over the\nnext five years.\n","authors":["Sascha Caron","Andreas Ipp","Gert Aarts","Gábor Bíró","Daniele Bonacorsi","Elena Cuoco","Caterina Doglioni","Tommaso Dorigo","Julián García Pardiñas","Stefano Giagu","Tobias Golling","Lukas Heinrich","Ik Siong Heng","Paula Gina Isar","Karolos Potamianos","Liliana Teodorescu","John Veitch","Pietro Vischia","Christoph Weniger"],"pdf_url":"https://arxiv.org/pdf/2503.14192v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.20963v2","updated":"2025-03-18T12:00:26Z","published":"2025-02-28T11:25:11Z","title":"Retrieval Augmented Generation for Topic Modeling in Organizational\n  Research: An Introduction with Empirical Demonstration","summary":"  Analyzing textual data is the cornerstone of qualitative research. While\ntraditional methods such as grounded theory and content analysis are widely\nused, they are labor-intensive and time-consuming. Topic modeling offers an\nautomated complement. Yet, existing approaches, including LLM-based topic\nmodeling, still struggle with issues such as high data preprocessing\nrequirements, interpretability, and reliability. This paper introduces Agentic\nRetrieval-Augmented Generation (Agentic RAG) as a method for topic modeling\nwith LLMs. It integrates three key components: (1) retrieval, enabling\nautomatized access to external data beyond an LLM's pre-trained knowledge; (2)\ngeneration, leveraging LLM capabilities for text synthesis; and (3)\nagent-driven learning, iteratively refining retrieval and query formulation\nprocesses. To empirically validate Agentic RAG for topic modeling, we reanalyze\na Twitter/X dataset, previously examined by Mu et al. (2024a). Our findings\ndemonstrate that the approach is more efficient, interpretable and at the same\ntime achieves higher reliability and validity in comparison to the standard\nmachine learning approach but also in comparison to LLM prompting for topic\nmodeling. These results highlight Agentic RAG's ability to generate\nsemantically relevant and reproducible topics, positioning it as a robust,\nscalable, and transparent alternative for AI-driven qualitative research in\nleadership, managerial, and organizational research.\n","authors":["Gerion Spielberger","Florian M. Artinger","Jochen Reb","Rudolf Kerschreiter"],"pdf_url":"https://arxiv.org/pdf/2502.20963v2.pdf","comment":"30 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.04585v4","updated":"2025-03-18T11:55:29Z","published":"2023-08-08T21:11:06Z","title":"Kernel Single Proxy Control for Deterministic Confounding","summary":"  We consider the problem of causal effect estimation with an unobserved\nconfounder, where we observe a single proxy variable that is associated with\nthe confounder. Although it has been shown that the recovery of an average\ncausal effect is impossible in general from a single proxy variable, we show\nthat causal recovery is possible if the outcome is generated deterministically.\nThis generalizes existing work on causal methods with a single proxy variable\nto the continuous treatment setting. We propose two kernel-based methods for\nthis setting: the first based on the two-stage regression approach, and the\nsecond based on a maximum moment restriction approach. We prove that both\napproaches can consistently estimate the causal effect, and we empirically\ndemonstrate that we can successfully recover the causal effect on challenging\nsynthetic benchmarks.\n","authors":["Liyuan Xu","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2308.04585v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10489v2","updated":"2025-03-18T11:38:08Z","published":"2025-03-13T15:55:01Z","title":"Beyond Atoms: Enhancing Molecular Pretrained Representations with 3D\n  Space Modeling","summary":"  Molecular pretrained representations (MPR) has emerged as a powerful approach\nfor addressing the challenge of limited supervised data in applications such as\ndrug discovery and material design. While early MPR methods relied on 1D\nsequences and 2D graphs, recent advancements have incorporated 3D\nconformational information to capture rich atomic interactions. However, these\nprior models treat molecules merely as discrete atom sets, overlooking the\nspace surrounding them. We argue from a physical perspective that only modeling\nthese discrete points is insufficient. We first present a simple yet insightful\nobservation: naively adding randomly sampled virtual points beyond atoms can\nsurprisingly enhance MPR performance. In light of this, we propose a principled\nframework that incorporates the entire 3D space spanned by molecules. We\nimplement the framework via a novel Transformer-based architecture, dubbed\nSpaceFormer, with three key components: (1) grid-based space discretization;\n(2) grid sampling/merging; and (3) efficient 3D positional encoding. Extensive\nexperiments show that SpaceFormer significantly outperforms previous 3D MPR\nmodels across various downstream tasks with limited data, validating the\nbenefit of leveraging the additional 3D space beyond atoms in MPR models.\n","authors":["Shuqi Lu","Xiaohong Ji","Bohang Zhang","Lin Yao","Siyuan Liu","Zhifeng Gao","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2503.10489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14153v1","updated":"2025-03-18T11:21:53Z","published":"2025-03-18T11:21:53Z","title":"Speculative Decoding for Verilog: Speed and Quality, All in One","summary":"  The rapid advancement of large language models (LLMs) has revolutionized code\ngeneration tasks across various programming languages. However, the unique\ncharacteristics of programming languages, particularly those like Verilog with\nspecific syntax and lower representation in training datasets, pose significant\nchallenges for conventional tokenization and decoding approaches. In this\npaper, we introduce a novel application of speculative decoding for Verilog\ncode generation, showing that it can improve both inference speed and output\nquality, effectively achieving speed and quality all in one. Unlike standard\nLLM tokenization schemes, which often fragment meaningful code structures, our\napproach aligns decoding stops with syntactically significant tokens, making it\neasier for models to learn the token distribution. This refinement addresses\ninherent tokenization issues and enhances the model's ability to capture\nVerilog's logical constructs more effectively. Our experimental results show\nthat our method achieves up to a 5.05x speedup in Verilog code generation and\nincreases pass@10 functional accuracy on RTLLM by up to 17.19% compared to\nconventional training strategies. These findings highlight speculative decoding\nas a promising approach to bridge the quality gap in code generation for\nspecialized programming languages.\n","authors":["Changran Xu","Yi Liu","Yunhao Zhou","Shan Huang","Ningyi Xu","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2503.14153v1.pdf","comment":"Accepted by the 62nd Design Automation Conference (DAC 2025)"},{"id":"http://arxiv.org/abs/2404.07696v2","updated":"2025-03-18T11:19:45Z","published":"2024-04-11T12:42:18Z","title":"Flatness Improves Backbone Generalisation in Few-shot Classification","summary":"  Deployment of deep neural networks in real-world settings typically requires\nadaptation to new tasks with few examples. Few-shot classification (FSC)\nprovides a solution to this problem by leveraging pre-trained backbones for\nfast adaptation to new classes. However, approaches for multi-domain FSC\ntypically result in complex pipelines aimed at information fusion and\ntask-specific adaptation without consideration of the importance of backbone\ntraining. In this work, we introduce an effective strategy for backbone\ntraining and selection in multi-domain FSC by utilizing flatness-aware training\nand fine-tuning. Our work is theoretically grounded and empirically performs on\npar or better than state-of-the-art methods despite being simpler. Further, our\nresults indicate that backbone training is crucial for good generalisation in\nFSC across different adaptation methods.\n","authors":["Rui Li","Martin Trapp","Marcus Klasson","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2404.07696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12687v3","updated":"2025-03-18T10:50:58Z","published":"2024-12-17T09:08:18Z","title":"Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large\n  Language Models","summary":"  This paper studies a hybrid language model (HLM) architecture that integrates\na small language model (SLM) operating on a mobile device with a large language\nmodel (LLM) hosted at the base station (BS) of a wireless network. The HLM\ntoken generation process follows the speculative inference principle: the SLM's\nvocabulary distribution is uploaded to the LLM, which either accepts or rejects\nit, with rejected tokens being resampled by the LLM. While this approach\nensures alignment between the vocabulary distributions of the SLM and LLM, it\nsuffers from low token throughput due to uplink transmission and the\ncomputation costs of running both language models. To address this, we propose\na novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM),\nwherein the SLM locally measures its output uncertainty and skips both uplink\ntransmissions and LLM operations for tokens that are likely to be accepted.\nThis opportunistic skipping is enabled by our empirical finding of a linear\ncorrelation between the SLM's uncertainty and the LLM's rejection probability.\nWe analytically derive the uncertainty threshold and evaluate its expected risk\nof rejection. Simulations show that U-HLM reduces uplink transmissions and LLM\ncomputations by 45.93%, while achieving up to 97.54% of the LLM's inference\naccuracy and 2.54$\\times$ faster token throughput than HLM without skipping.\n","authors":["Seungeun Oh","Jinhyuk Kim","Jihong Park","Seung-Woo Ko","Tony Q. S. Quek","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2412.12687v3.pdf","comment":"7 pages, 6 figures; to be presented at IEEE International Conference\n  on Machine Learning for Communication and Networking (ICMLCN) 2025"},{"id":"http://arxiv.org/abs/2503.14125v1","updated":"2025-03-18T10:37:50Z","published":"2025-03-18T10:37:50Z","title":"Frac-Connections: Fractional Extension of Hyper-Connections","summary":"  Residual connections are central to modern deep learning architectures,\nenabling the training of very deep networks by mitigating gradient vanishing.\nHyper-Connections recently generalized residual connections by introducing\nmultiple connection strengths at different depths, thereby addressing the\nseesaw effect between gradient vanishing and representation collapse. However,\nHyper-Connections increase memory access costs by expanding the width of hidden\nstates. In this paper, we propose Frac-Connections, a novel approach that\ndivides hidden states into multiple parts rather than expanding their width.\nFrac-Connections retain partial benefits of Hyper-Connections while reducing\nmemory consumption. To validate their effectiveness, we conduct large-scale\nexperiments on language tasks, with the largest being a 7B MoE model trained on\nup to 3T tokens, demonstrating that Frac-Connections significantly outperform\nresidual connections.\n","authors":["Defa Zhu","Hongzhi Huang","Jundong Zhou","Zihao Huang","Yutao Zeng","Banggu Wu","Qiyang Min","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.14125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14121v1","updated":"2025-03-18T10:36:30Z","published":"2025-03-18T10:36:30Z","title":"Fundamental Limits of Matrix Sensing: Exact Asymptotics, Universality,\n  and Applications","summary":"  In the matrix sensing problem, one wishes to reconstruct a matrix from\n(possibly noisy) observations of its linear projections along given directions.\nWe consider this model in the high-dimensional limit: while previous works on\nthis model primarily focused on the recovery of low-rank matrices, we consider\nin this work more general classes of structured signal matrices with\npotentially large rank, e.g. a product of two matrices of sizes proportional to\nthe dimension. We provide rigorous asymptotic equations characterizing the\nBayes-optimal learning performance from a number of samples which is\nproportional to the number of entries in the matrix. Our proof is composed of\nthree key ingredients: $(i)$ we prove universality properties to handle\nstructured sensing matrices, related to the ''Gaussian equivalence'' phenomenon\nin statistical learning, $(ii)$ we provide a sharp characterization of\nBayes-optimal learning in generalized linear models with Gaussian data and\nstructured matrix priors, generalizing previously studied settings, and $(iii)$\nwe leverage previous works on the problem of matrix denoising. The generality\nof our results allow for a variety of applications: notably, we mathematically\nestablish predictions obtained via non-rigorous methods from statistical\nphysics in [ETB+24] regarding Bilinear Sequence Regression, a benchmark model\nfor learning from sequences of tokens, and in [MTM+24] on Bayes-optimal\nlearning in neural networks with quadratic activation function, and width\nproportional to the dimension.\n","authors":["Yizhou Xu","Antoine Maillard","Lenka Zdeborová","Florent Krzakala"],"pdf_url":"https://arxiv.org/pdf/2503.14121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14118v1","updated":"2025-03-18T10:35:30Z","published":"2025-03-18T10:35:30Z","title":"PET-MAD, a universal interatomic potential for advanced materials\n  modeling","summary":"  Machine-learning interatomic potentials (MLIPs) have greatly extended the\nreach of atomic-scale simulations, offering the accuracy of first-principles\ncalculations at a fraction of the effort. Leveraging large quantum mechanical\ndatabases and expressive architectures, recent \"universal\" models deliver\nqualitative accuracy across the periodic table but are often biased toward\nlow-energy configurations. We introduce PET-MAD, a generally applicable MLIP\ntrained on a dataset combining stable inorganic and organic solids,\nsystematically modified to enhance atomic diversity. Using a moderate but\nhighly-consistent level of electronic-structure theory, we assess PET-MAD's\naccuracy on established benchmarks and advanced simulations of six materials.\nPET-MAD rivals state-of-the-art MLIPs for inorganic solids, while also being\nreliable for molecules, organic materials, and surfaces. It is stable and fast,\nenabling, out-of-the-box, the near-quantitative study of thermal and quantum\nmechanical fluctuations, functional properties, and phase transitions. It can\nbe efficiently fine-tuned to deliver full quantum mechanical accuracy with a\nminimal number of targeted calculations.\n","authors":["Arslan Mazitov","Filippo Bigi","Matthias Kellner","Paolo Pegolo","Davide Tisi","Guillaume Fraux","Sergey Pozdnyakov","Philip Loche","Michele Ceriotti"],"pdf_url":"https://arxiv.org/pdf/2503.14118v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13839v3","updated":"2025-03-18T10:25:10Z","published":"2024-06-19T21:06:44Z","title":"RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design","summary":"  We introduce RNA-FrameFlow, the first generative model for 3D RNA backbone\ndesign. We build upon SE(3) flow matching for protein backbone generation and\nestablish protocols for data preparation and evaluation to address unique\nchallenges posed by RNA modeling. We formulate RNA structures as a set of\nrigid-body frames and associated loss functions which account for larger, more\nconformationally flexible RNA backbones (13 atoms per nucleotide) vs. proteins\n(4 atoms per residue). Toward tackling the lack of diversity in 3D RNA\ndatasets, we explore training with structural clustering and cropping\naugmentations. Additionally, we define a suite of evaluation metrics to measure\nwhether the generated RNA structures are globally self-consistent (via inverse\nfolding followed by forward folding) and locally recover RNA-specific\nstructural descriptors. The most performant version of RNA-FrameFlow generates\nlocally realistic RNA backbones of 40-150 nucleotides, over 40% of which pass\nour validity criteria as measured by a self-consistency TM-score >= 0.45, at\nwhich two RNAs have the same global fold. Open-source code:\nhttps://github.com/rish-16/rna-backbone-design\n","authors":["Rishabh Anand","Chaitanya K. Joshi","Alex Morehead","Arian R. Jamasb","Charles Harris","Simon V. Mathis","Kieran Didi","Rex Ying","Bryan Hooi","Pietro Liò"],"pdf_url":"https://arxiv.org/pdf/2406.13839v3.pdf","comment":"Oral presentation at Machine Learning in Computational Biology\n  (MLCB), 2024. Also presented as an Oral at ICML 2024 Structured Probabilistic\n  Inference & Generative Modeling Workshop, and a Spotlight at ICML 2024\n  AI4Science Workshop"},{"id":"http://arxiv.org/abs/2409.19606v3","updated":"2025-03-18T10:12:54Z","published":"2024-09-29T07:57:07Z","title":"Hyper-Connections","summary":"  We present hyper-connections, a simple yet effective method that can serve as\nan alternative to residual connections. This approach specifically addresses\ncommon drawbacks observed in residual connection variants, such as the seesaw\neffect between gradient vanishing and representation collapse. Theoretically,\nhyper-connections allow the network to adjust the strength of connections\nbetween features at different depths and dynamically rearrange layers. We\nconduct experiments focusing on the pre-training of large language models,\nincluding dense and sparse models, where hyper-connections show significant\nperformance improvements over residual connections. Additional experiments\nconducted on vision tasks also demonstrate similar improvements. We anticipate\nthat this method will be broadly applicable and beneficial across a wide range\nof AI problems.\n","authors":["Defa Zhu","Hongzhi Huang","Zihao Huang","Yutao Zeng","Yunyao Mao","Banggu Wu","Qiyang Min","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.19606v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14095v1","updated":"2025-03-18T10:12:17Z","published":"2025-03-18T10:12:17Z","title":"Towards Location-Specific Precipitation Projections Using Deep Neural\n  Networks","summary":"  Accurate precipitation estimates at individual locations are crucial for\nweather forecasting and spatial analysis. This study presents a paradigm shift\nby leveraging Deep Neural Networks (DNNs) to surpass traditional methods like\nKriging for station-specific precipitation approximation. We propose two\ninnovative NN architectures: one utilizing precipitation, elevation, and\nlocation, and another incorporating additional meteorological parameters like\nhumidity, temperature, and wind speed. Trained on a vast dataset (1980-2019),\nthese models outperform Kriging across various evaluation metrics (correlation\ncoefficient, root mean square error, bias, and skill score) on a five-year\nvalidation set. This compelling evidence demonstrates the transformative power\nof deep learning for spatial prediction, offering a robust and precise\nalternative for station-specific precipitation estimation.\n","authors":["Bipin Kumar","Bhvisy Kumar Yadav","Soumypdeep Mukhopadhyay","Rakshit Rohan","Bhupendra Bahadur Singh","Rajib Chattopadhyay","Nagraju Chilukoti","Atul Kumar Sahai"],"pdf_url":"https://arxiv.org/pdf/2503.14095v1.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.14084v1","updated":"2025-03-18T10:02:22Z","published":"2025-03-18T10:02:22Z","title":"Semantic Communication in Dynamic Channel Scenarios: Collaborative\n  Optimization of Dual-Pipeline Joint Source-Channel Coding and Personalized\n  Federated Learning","summary":"  Semantic communication is designed to tackle issues like bandwidth\nconstraints and high latency in communication systems. However, in complex\nnetwork topologies with multiple users, the enormous combinations of client\ndata and channel state information (CSI) pose significant challenges for\nexisting semantic communication architectures. To improve the generalization\nability of semantic communication models in complex scenarios while meeting the\npersonalized needs of each user in their local environments, we propose a novel\npersonalized federated learning framework with dual-pipeline joint\nsource-channel coding based on channel awareness model (PFL-DPJSCCA). Within\nthis framework, we present a method that achieves zero optimization gap for\nnon-convex loss functions. Experiments conducted under varying SNR\ndistributions validate the outstanding performance of our framework across\ndiverse datasets.\n","authors":["Xingrun Yan","Shiyuan Zuo","Yifeng Lyu","Rongfei Fan","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2503.14084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14076v1","updated":"2025-03-18T09:53:48Z","published":"2025-03-18T09:53:48Z","title":"Theoretical Foundation of Flow-Based Time Series Generation: Provable\n  Approximation, Generalization, and Efficiency","summary":"  Recent studies suggest utilizing generative models instead of traditional\nauto-regressive algorithms for time series forecasting (TSF) tasks. These\nnon-auto-regressive approaches involving different generative methods,\nincluding GAN, Diffusion, and Flow Matching for time series, have empirically\ndemonstrated high-quality generation capability and accuracy. However, we still\nlack an appropriate understanding of how it processes approximation and\ngeneralization. This paper presents the first theoretical framework from the\nperspective of flow-based generative models to relieve the knowledge of\nlimitations. In particular, we provide our insights with strict guarantees from\nthree perspectives: $\\textbf{Approximation}$, $\\textbf{Generalization}$ and\n$\\textbf{Efficiency}$. In detail, our analysis achieves the contributions as\nfollows:\n  $\\bullet$ By assuming a general data model, the fitting of the flow-based\ngenerative models is confirmed to converge to arbitrary error under the\nuniversal approximation of Diffusion Transformer (DiT).\n  $\\bullet$ Introducing a polynomial-based regularization for flow matching,\nthe generalization error thus be bounded since the generalization of polynomial\napproximation.\n  $\\bullet$ The sampling for generation is considered as an optimization\nprocess, we demonstrate its fast convergence with updating standard first-order\ngradient descent of some objective.\n","authors":["Jiangxuan Long","Zhao Song","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.14076v1.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2503.13050v2","updated":"2025-03-18T09:51:46Z","published":"2025-03-17T10:54:30Z","title":"E-Values Expand the Scope of Conformal Prediction","summary":"  Conformal prediction is a powerful framework for distribution-free\nuncertainty quantification. The standard approach to conformal prediction\nrelies on comparing the ranks of prediction scores: under exchangeability, the\nrank of a future test point cannot be too extreme relative to a calibration\nset. This rank-based method can be reformulated in terms of p-values. In this\npaper, we explore an alternative approach based on e-values, known as conformal\ne-prediction. E-values offer key advantages that cannot be achieved with\np-values, enabling new theoretical and practical capabilities. In particular,\nwe present three applications that leverage the unique strengths of e-values:\nbatch anytime-valid conformal prediction, fixed-size conformal sets with\ndata-dependent coverage, and conformal prediction under ambiguous ground truth.\nOverall, these examples demonstrate that e-value-based constructions provide a\nflexible expansion of the toolbox of conformal prediction.\n","authors":["Etienne Gauthier","Francis Bach","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2503.13050v2.pdf","comment":"Code available at: https://github.com/GauthierE/evalues-expand-cp"},{"id":"http://arxiv.org/abs/2412.13769v2","updated":"2025-03-18T09:30:51Z","published":"2024-12-18T12:06:52Z","title":"QuLTSF: Long-Term Time Series Forecasting with Quantum Machine Learning","summary":"  Long-term time series forecasting (LTSF) involves predicting a large number\nof future values of a time series based on the past values. This is an\nessential task in a wide range of domains including weather forecasting, stock\nmarket analysis and disease outbreak prediction. Over the decades LTSF\nalgorithms have transitioned from statistical models to deep learning models\nlike transformer models. Despite the complex architecture of transformer based\nLTSF models `Are Transformers Effective for Time Series Forecasting? (Zeng et\nal., 2023)' showed that simple linear models can outperform the\nstate-of-the-art transformer based LTSF models. Recently, quantum machine\nlearning (QML) is evolving as a domain to enhance the capabilities of classical\nmachine learning models. In this paper we initiate the application of QML to\nLTSF problems by proposing QuLTSF, a simple hybrid QML model for multivariate\nLTSF. Through extensive experiments on a widely used weather dataset we show\nthe advantages of QuLTSF over the state-of-the-art classical linear models, in\nterms of reduced mean squared error and mean absolute error.\n","authors":["Hari Hara Suthan Chittoor","Paul Robert Griffin","Ariel Neufeld","Jayne Thompson","Mile Gu"],"pdf_url":"https://arxiv.org/pdf/2412.13769v2.pdf","comment":"Published in ICAART 2025"},{"id":"http://arxiv.org/abs/2503.14055v1","updated":"2025-03-18T09:16:51Z","published":"2025-03-18T09:16:51Z","title":"Modular Distributed Nonconvex Learning with Error Feedback","summary":"  In this paper, we design a novel distributed learning algorithm using\nstochastic compressed communications. In detail, we pursue a modular approach,\nmerging ADMM and a gradient-based approach, benefiting from the robustness of\nthe former and the computational efficiency of the latter. Additionally, we\nintegrate a stochastic integral action (error feedback) enabling almost sure\nrejection of the compression error. We analyze the resulting method in\nnonconvex scenarios and guarantee almost sure asymptotic convergence to the set\nof stationary points of the problem. This result is obtained using\nsystem-theoretic tools based on stochastic timescale separation. We corroborate\nour findings with numerical simulations in nonconvex classification.\n","authors":["Guido Carnevale","Nicola Bastianello"],"pdf_url":"https://arxiv.org/pdf/2503.14055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14053v1","updated":"2025-03-18T09:13:24Z","published":"2025-03-18T09:13:24Z","title":"ON-Traffic: An Operator Learning Framework for Online Traffic Flow\n  Estimation and Uncertainty Quantification from Lagrangian Sensors","summary":"  Accurate traffic flow estimation and prediction are critical for the\nefficient management of transportation systems, particularly under increasing\nurbanization. Traditional methods relying on static sensors often suffer from\nlimited spatial coverage, while probe vehicles provide richer, albeit sparse\nand irregular data. This work introduces ON-Traffic, a novel deep operator\nNetwork and a receding horizon learning-based framework tailored for online\nestimation of spatio-temporal traffic state along with quantified uncertainty\nby using measurements from moving probe vehicles and downstream boundary\ninputs. Our framework is evaluated in both numerical and simulation datasets,\nshowcasing its ability to handle irregular, sparse input data, adapt to\ntime-shifted scenarios, and provide well-calibrated uncertainty estimates. The\nresults demonstrate that the model captures complex traffic phenomena,\nincluding shockwaves and congestion propagation, while maintaining robustness\nto noise and sensor dropout. These advancements present a significant step\ntoward online, adaptive traffic management systems.\n","authors":["Jake Rap","Amritam Das"],"pdf_url":"https://arxiv.org/pdf/2503.14053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17178v3","updated":"2025-03-18T09:09:29Z","published":"2025-01-24T17:01:14Z","title":"Tuning LLM Judge Design Decisions for 1/1000 of the Cost","summary":"  Evaluating Large Language Models (LLMs) often requires costly human\nannotations. To address this, LLM-based judges have been proposed, which\ncompare the outputs of two LLMs enabling the ranking of models without human\nintervention. While several approaches have been proposed, many confounding\nfactors are present between different papers. For instance the model, the\nprompt and other hyperparameters are typically changed at the same time making\napple-to-apple comparisons challenging. In this paper, we propose to\nsystematically analyze and tune hyperparameter of LLM judges. To alleviate the\nhigh cost of evaluating a judge, we propose to leverage multi-objective\nmulti-fidelity which allows to find judges that trades accuracy for cost and\nalso reduce significantly the cost of the search. Our method identifies judges\nthat not only outperform existing benchmarks in accuracy and cost-efficiency\nbut also utilize open-weight models, ensuring greater accessibility and\nreproducibility.\n","authors":["David Salinas","Omar Swelam","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2501.17178v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14045v1","updated":"2025-03-18T09:06:19Z","published":"2025-03-18T09:06:19Z","title":"Empirical risk minimization algorithm for multiclass classification of\n  S.D.E. paths","summary":"  We address the multiclass classification problem for stochastic diffusion\npaths, assuming that the classes are distinguished by their drift functions,\nwhile the diffusion coefficient remains common across all classes. In this\nsetting, we propose a classification algorithm that relies on the minimization\nof the L 2 risk. We establish rates of convergence for the resulting predictor.\nNotably, we introduce a margin assumption under which we show that our\nprocedure can achieve fast rates of convergence. Finally, a simulation study\nhighlights the numerical performance of our classification algorithm.\n","authors":["Christophe Denis","Eddy Ella Mintsa"],"pdf_url":"https://arxiv.org/pdf/2503.14045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14043v1","updated":"2025-03-18T09:04:37Z","published":"2025-03-18T09:04:37Z","title":"Learning on LLM Output Signatures for gray-box LLM Behavior Analysis","summary":"  Large Language Models (LLMs) have achieved widespread adoption, yet our\nunderstanding of their behavior remains limited, particularly in detecting data\ncontamination and hallucinations. While recently proposed probing techniques\nprovide insights through activation analysis, they require \"white-box\" access\nto model internals, often unavailable. Current \"gray-box\" approaches typically\nanalyze only the probability of the actual tokens in the sequence with simple\ntask-specific heuristics. Importantly, these methods overlook the rich\ninformation contained in the full token distribution at each processing step.\nTo address these limitations, we propose that gray-box analysis should leverage\nthe complete observable output of LLMs, consisting of both the previously used\ntoken probabilities as well as the complete token distribution sequences - a\nunified data type we term LOS (LLM Output Signature). To this end, we develop a\ntransformer-based approach to process LOS that theoretically guarantees\napproximation of existing techniques while enabling more nuanced analysis. Our\napproach achieves superior performance on hallucination and data contamination\ndetection in gray-box settings, significantly outperforming existing baselines.\nFurthermore, it demonstrates strong transfer capabilities across datasets and\nLLMs, suggesting that LOS captures fundamental patterns in LLM behavior. Our\ncode is available at: https://github.com/BarSGuy/LLM-Output-Signatures-Network.\n","authors":["Guy Bar-Shalom","Fabrizio Frasca","Derek Lim","Yoav Gelberg","Yftah Ziser","Ran El-Yaniv","Gal Chechik","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2503.14043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21301v2","updated":"2025-03-18T09:00:53Z","published":"2024-10-21T11:39:03Z","title":"Evaluating the Posterior Sampling Ability of Plug&Play Diffusion Methods\n  in Sparse-View CT","summary":"  Plug&Play (PnP) diffusion models are state-of-the-art methods in computed\ntomography (CT) reconstruction. Such methods usually consider applications\nwhere the sinogram contains a sufficient amount of information for the\nposterior distribution to be concentrated around a single mode, and\nconsequently are evaluated using image-to-image metrics such as PSNR/SSIM.\nInstead, we are interested in reconstructing compressible flow images from\nsinograms having a small number of projections, which results in a posterior\ndistribution no longer concentrated or even multimodal. Thus, in this paper, we\naim at evaluating the approximate posterior of PnP diffusion models and\nintroduce two posterior evaluation properties. We quantitatively evaluate three\nPnP diffusion methods on three different datasets for several numbers of\nprojections. We surprisingly find that, for each method, the approximate\nposterior deviates from the true posterior when the number of projections\ndecreases.\n","authors":["Liam Moroy","Guillaume Bourmaud","Frédéric Champagnat","Jean-François Giovannelli"],"pdf_url":"https://arxiv.org/pdf/2410.21301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11209v2","updated":"2025-03-18T08:42:38Z","published":"2025-03-14T08:56:30Z","title":"Clustering Items through Bandit Feedback: Finding the Right Feature out\n  of Many","summary":"  We study the problem of clustering a set of items based on bandit feedback.\nEach of the $n$ items is characterized by a feature vector, with a possibly\nlarge dimension $d$. The items are partitioned into two unknown groups such\nthat items within the same group share the same feature vector. We consider a\nsequential and adaptive setting in which, at each round, the learner selects\none item and one feature, then observes a noisy evaluation of the item's\nfeature. The learner's objective is to recover the correct partition of the\nitems, while keeping the number of observations as small as possible. We\nprovide an algorithm which relies on finding a relevant feature for the\nclustering task, leveraging the Sequential Halving algorithm. With probability\nat least $1-\\delta$, we obtain an accurate recovery of the partition and derive\nan upper bound on the budget required. Furthermore, we derive an\ninstance-dependent lower bound, which is tight in some relevant cases.\n","authors":["Maximilian Graf","Victor Thuot","Nicolas Verzelen"],"pdf_url":"https://arxiv.org/pdf/2503.11209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06652v3","updated":"2025-03-18T08:40:04Z","published":"2024-06-10T09:03:17Z","title":"Improving Generalization of Neural Vehicle Routing Problem Solvers\n  Through the Lens of Model Architecture","summary":"  Neural models produce promising results when solving Vehicle Routing Problems\n(VRPs), but often fall short in generalization. Recent attempts to enhance\nmodel generalization often incur unnecessarily large training cost or cannot be\ndirectly applied to other models solving different VRP variants. To address\nthese issues, we take a novel perspective on model architecture in this study.\nSpecifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and\na Distribution-Specific (DS) decoder to enhance the size and distribution\ngeneralization, respectively. ESF adjusts the attention weight pattern of the\nmodel towards familiar ones discovered during training when solving VRPs of\nvarying sizes. The DS decoder explicitly models VRPs of multiple training\ndistribution patterns through multiple auxiliary light decoders, expanding the\nmodel representation space to encompass a broader range of distributional\nscenarios. We conduct extensive experiments on both synthetic and widely\nrecognized real-world benchmarking datasets and compare the performance with\nseven baseline models. The results demonstrate the effectiveness of using ESF\nand DS decoder to obtain a more generalizable model and showcase their\napplicability to solve different VRP variants, i.e., travelling salesman\nproblem and capacitated VRP. Notably, our proposed generic components require\nminimal computational resources, and can be effortlessly integrated into\nconventional generalization strategies to further elevate model generalization.\n","authors":["Yubin Xiao","Di Wang","Xuan Wu","Yuesong Wu","Boyang Li","Wei Du","Liupu Wang","You Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06652v3.pdf","comment":"This work has been accepted by Neural Networks"},{"id":"http://arxiv.org/abs/2410.16750v2","updated":"2025-03-18T08:37:58Z","published":"2024-10-22T07:12:38Z","title":"Theoretical Convergence Guarantees for Variational Autoencoders","summary":"  Variational Autoencoders (VAE) are popular generative models used to sample\nfrom complex data distributions. Despite their empirical success in various\nmachine learning tasks, significant gaps remain in understanding their\ntheoretical properties, particularly regarding convergence guarantees. This\npaper aims to bridge that gap by providing non-asymptotic convergence\nguarantees for VAE trained using both Stochastic Gradient Descent and Adam\nalgorithms.We derive a convergence rate of $\\mathcal{O}(\\log n / \\sqrt{n})$,\nwhere $n$ is the number of iterations of the optimization algorithm, with\nexplicit dependencies on the batch size, the number of variational samples, and\nother key hyperparameters. Our theoretical analysis applies to both Linear VAE\nand Deep Gaussian VAE, as well as several VAE variants, including $\\beta$-VAE\nand IWAE. Additionally, we empirically illustrate the impact of hyperparameters\non convergence, offering new insights into the theoretical understanding of VAE\ntraining.\n","authors":["Sobihan Surendran","Antoine Godichon-Baggioni","Sylvain Le Corff"],"pdf_url":"https://arxiv.org/pdf/2410.16750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14024v1","updated":"2025-03-18T08:35:39Z","published":"2025-03-18T08:35:39Z","title":"Uncertainty-Aware Global-View Reconstruction for Multi-View Multi-Label\n  Feature Selection","summary":"  In recent years, multi-view multi-label learning (MVML) has gained popularity\ndue to its close resemblance to real-world scenarios. However, the challenge of\nselecting informative features to ensure both performance and efficiency\nremains a significant question in MVML. Existing methods often extract\ninformation separately from the consistency part and the complementary part,\nwhich may result in noise due to unclear segmentation. In this paper, we\npropose a unified model constructed from the perspective of global-view\nreconstruction. Additionally, while feature selection methods can discern the\nimportance of features, they typically overlook the uncertainty of samples,\nwhich is prevalent in realistic scenarios. To address this, we incorporate the\nperception of sample uncertainty during the reconstruction process to enhance\ntrustworthiness. Thus, the global-view is reconstructed through the graph\nstructure between samples, sample confidence, and the view relationship. The\naccurate mapping is established between the reconstructed view and the label\nmatrix. Experimental results demonstrate the superior performance of our method\non multi-view datasets.\n","authors":["Pingting Hao","Kunpeng Liu","Wanfu Gao"],"pdf_url":"https://arxiv.org/pdf/2503.14024v1.pdf","comment":"9 pages,5 figures, accept in AAAI 25"},{"id":"http://arxiv.org/abs/2408.12526v3","updated":"2025-03-18T08:33:28Z","published":"2024-08-22T16:31:32Z","title":"Exploiting Student Parallelism for Efficient GPU Inference of BERT-like\n  Models in Online Services","summary":"  Due to high accuracy, BERT-like models have been widely adopted by text\nmining and web searching. However, large BERT-like models suffer from\ninefficient online inference, facing the following two problems on GPUs: (1)\ntheir high accuracy relies on the large model depth, which linearly increases\nthe sequential computation on GPUs; (2) stochastic and dynamic online workloads\ncause extra costs from batching and paddings. Therefore, we present \\sys for\nthe real-world setting of GPU inference on online workloads. At its core, \\sys\nadopts stacking distillation and boosting ensemble, distilling the original\ndeep model into a group of shallow but virtually stacked student models running\nin parallel. This enables \\sys to achieve a lower model depth (e.g., two\nlayers) than the others and the lowest inference latency while maintaining\naccuracy. In addition, adaptive student pruning realizes dynamic student\nnumbers according to changing online workloads. Especially for occasional\nworkload bursts, it can temporarily decrease the student number with minimal\naccuracy loss to improve system throughput. We conduct comprehensive\nexperiments to verify the effectiveness, whose results show that \\sys\noutperforms the baselines by $4.1\\times\\sim 1.6\\times$ in latency while\nmaintaining accuracy and achieves up to $22.27\\times$ higher throughput for\nworkload bursts.\n","authors":["Weiyan Wang","Yilun Jin","Yiming Zhang","Victor Junqiu Wei","Han Tian","Li Chen","Jinbao Xue","Yangyu Tao","Di Wang","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2408.12526v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11256v2","updated":"2025-03-18T08:31:00Z","published":"2023-08-22T07:59:49Z","title":"Efficient Last-iterate Convergence Algorithms in Solving Games","summary":"  To establish last-iterate convergence for Counterfactual Regret Minimization\n(CFR) algorithms in learning a Nash equilibrium (NE) of extensive-form games\n(EFGs), recent studies reformulate learning an NE of the original EFG as\nlearning the NEs of a sequence of (perturbed) regularized EFGs. Consequently,\nproving last-iterate convergence in solving the original EFG reduces to proving\nlast-iterate convergence in solving (perturbed) regularized EFGs. However, the\nempirical convergence rates of the algorithms in these studies are suboptimal,\nsince they do not utilize Regret Matching (RM)-based CFR algorithms to solve\nperturbed EFGs, which are known the exceptionally fast empirical convergence\nrates. Additionally, since solving multiple perturbed regularized EFGs is\nrequired, fine-tuning across all such games is infeasible, making\nparameter-free algorithms highly desirable. In this paper, we prove that\nCFR$^+$, a classical parameter-free RM-based CFR algorithm, achieves\nlast-iterate convergence in learning an NE of perturbed regularized EFGs.\nLeveraging CFR$^+$ to solve perturbed regularized EFGs, we get Reward\nTransformation CFR$^+$ (RTCFR$^+$). Importantly, we extend prior work on the\nparameter-free property of CFR$^+$, enhancing its stability, which is crucial\nfor the empirical convergence of RTCFR$^+$. Experiments show that RTCFR$^+$\nsignificantly outperforms existing algorithms with theoretical last-iterate\nconvergence guarantees.\n","authors":["Linjian Meng","Youzhi Zhang","Zhenxing Ge","Shangdong Yang","Tianyu Ding","Wenbin Li","Tianpei Yang","Bo An","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2308.11256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08976v2","updated":"2025-03-18T08:30:23Z","published":"2024-12-12T06:13:32Z","title":"Identity-Preserving Pose-Guided Character Animation via Facial Landmarks\n  Transformation","summary":"  Creating realistic pose-guided image-to-video character animations while\npreserving facial identity remains challenging, especially in complex and\ndynamic scenarios such as dancing, where precise identity consistency is\ncrucial. Existing methods frequently encounter difficulties maintaining facial\ncoherence due to misalignments between facial landmarks extracted from driving\nvideos that provide head pose and expression cues and the facial geometry of\nthe reference images. To address this limitation, we introduce the Facial\nLandmarks Transformation (FLT) method, which leverages a 3D Morphable Model to\naddress this limitation. FLT converts 2D landmarks into a 3D face model,\nadjusts the 3D face model to align with the reference identity, and then\ntransforms them back into 2D landmarks to guide the image-to-video generation\nprocess. This approach ensures accurate alignment with the reference facial\ngeometry, enhancing the consistency between generated videos and reference\nimages. Experimental results demonstrate that FLT effectively preserves facial\nidentity, significantly improving pose-guided character animation models.\n","authors":["Lianrui Mu","Xingze Zhou","Wenjie Zheng","Jiangnan Ye","Haoji Hu"],"pdf_url":"https://arxiv.org/pdf/2412.08976v2.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2411.16156v2","updated":"2025-03-18T08:15:28Z","published":"2024-11-25T07:32:02Z","title":"VideoOrion: Tokenizing Object Dynamics in Videos","summary":"  We present VideoOrion, a Video Large Language Model (Video-LLM) that\nexplicitly captures the key semantic information in videos - the\nspatial-temporal dynamics of objects throughout the videos. VideoOrion employs\nexpert vision models to extract object dynamics through a detect-segment-track\npipeline, encoding them into a set of object tokens by aggregating\nspatial-temporal object features. Our method addresses the persistent challenge\nin Video-LLMs of efficiently compressing high-dimensional video data into\nsemantic tokens that are comprehensible to LLMs. Compared to prior methods\nwhich resort to downsampling the original video or aggregating visual tokens\nusing resamplers, leading to information loss and entangled semantics,\nVideoOrion not only offers a more natural and efficient way to derive compact,\ndisentangled semantic representations but also enables explicit object modeling\nof video content with minimal computational cost. Moreover, the introduced\nobject tokens naturally allow VideoOrion to accomplish video-based referring\ntasks. Experimental results show that VideoOrion can learn to make good use of\nthe object tokens, and achieves competitive results on both general video\nquestion answering and video-based referring benchmarks.\n","authors":["Yicheng Feng","Yijiang Li","Wanpeng Zhang","Hao Luo","Zihao Yue","Sipeng Zheng","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2411.16156v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08401v4","updated":"2025-03-18T08:10:45Z","published":"2024-06-12T16:50:12Z","title":"Nyström Kernel Stein Discrepancy","summary":"  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein's method\nwith the flexibility of kernel techniques, gained considerable attention.\nThrough the Stein operator, KSD allows the construction of powerful\ngoodness-of-fit tests where it is sufficient to know the target distribution up\nto a multiplicative constant. However, the typical U- and V-statistic-based KSD\nestimators suffer from a quadratic runtime complexity, which hinders their\napplication in large-scale settings. In this work, we propose a Nystr\\\"om-based\nKSD acceleration -- with runtime $\\mathcal O\\left(mn+m^3\\right)$ for $n$\nsamples and $m\\ll n$ Nystr\\\"om points -- , show its $\\sqrt{n}$-consistency with\na classical sub-Gaussian assumption, and demonstrate its applicability for\ngoodness-of-fit testing on a suite of benchmarks. We also show the $\\sqrt\nn$-consistency of the quadratic-time KSD estimator.\n","authors":["Florian Kalinke","Zoltan Szabo","Bharath K. Sriperumbudur"],"pdf_url":"https://arxiv.org/pdf/2406.08401v4.pdf","comment":"Add limitations; accepted for publication at AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.14004v1","updated":"2025-03-18T08:10:33Z","published":"2025-03-18T08:10:33Z","title":"Predicting Human Choice Between Textually Described Lotteries","summary":"  Predicting human decision-making under risk and uncertainty is a\nlong-standing challenge in cognitive science, economics, and AI. While prior\nresearch has focused on numerically described lotteries, real-world decisions\noften rely on textual descriptions. This study conducts the first large-scale\nexploration of human decision-making in such tasks using a large dataset of\none-shot binary choices between textually described lotteries. We evaluate\nmultiple computational approaches, including fine-tuning Large Language Models\n(LLMs), leveraging embeddings, and integrating behavioral theories of choice\nunder risk. Our results show that fine-tuned LLMs, specifically RoBERTa and\nGPT-4o outperform hybrid models that incorporate behavioral theory, challenging\nestablished methods in numerical settings. These findings highlight fundamental\ndifferences in how textual and numerical information influence decision-making\nand underscore the need for new modeling strategies to bridge this gap.\n","authors":["Eyal Marantz","Ori Plonsky"],"pdf_url":"https://arxiv.org/pdf/2503.14004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14002v1","updated":"2025-03-18T08:09:24Z","published":"2025-03-18T08:09:24Z","title":"MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific\n  Generative Modeling","summary":"  Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling.\n","authors":["Damian Boborzi","Phillip Mueller","Jonas Emrich","Dominik Schmid","Sebastian Mueller","Lars Mikelsons"],"pdf_url":"https://arxiv.org/pdf/2503.14002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05797v2","updated":"2025-03-18T08:03:45Z","published":"2024-06-09T14:20:55Z","title":"3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text\n  Modeling","summary":"  The integration of molecular and natural language representations has emerged\nas a focal point in molecular science, with recent advancements in Language\nModels (LMs) demonstrating significant potential for comprehensive modeling of\nboth domains. However, existing approaches face notable limitations,\nparticularly in their neglect of three-dimensional (3D) information, which is\ncrucial for understanding molecular structures and functions. While some\nefforts have been made to incorporate 3D molecular information into LMs using\nexternal structure encoding modules, significant difficulties remain, such as\ninsufficient interaction across modalities in pre-training and challenges in\nmodality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a\nunified framework designed to model molecule in both sequence and 3D structure\nspaces. The key innovation of our approach lies in mapping fine-grained 3D\nsubstructure representations into a specialized 3D token vocabulary. This\nmethodology facilitates the seamless integration of sequence and structure\nrepresentations in a tokenized format, enabling 3D-MolT5 to encode molecular\nsequences, molecular structures, and text sequences within a unified\narchitecture. Leveraging this tokenized input strategy, we build a foundation\nmodel that unifies the sequence and structure data formats. We then conduct\njoint pre-training with multi-task objectives to enhance the model's\ncomprehension of these diverse modalities within a shared representation space.\nThus, our approach significantly improves cross-modal interaction and\nalignment, addressing key challenges in previous work. Further instruction\ntuning demonstrated that our 3D-MolT5 has strong generalization ability and\nsurpasses existing methods with superior performance in multiple downstream\ntasks. Our code is available at https://github.com/QizhiPei/3D-MolT5.\n","authors":["Qizhi Pei","Rui Yan","Kaiyuan Gao","Jinhua Zhu","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2406.05797v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.13985v1","updated":"2025-03-18T07:42:11Z","published":"2025-03-18T07:42:11Z","title":"DefectFill: Realistic Defect Generation with Inpainting Diffusion Model\n  for Visual Inspection","summary":"  Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset.\n","authors":["Jaewoo Song","Daemin Park","Kanghyun Baek","Sangyub Lee","Jooyoung Choi","Eunji Kim","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2503.13985v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2405.14099v4","updated":"2025-03-18T07:40:27Z","published":"2024-05-23T02:01:05Z","title":"Automatic Differentiation is Essential in Training Neural Networks for\n  Solving Differential Equations","summary":"  Neural network-based approaches have recently shown significant promise in\nsolving partial differential equations (PDEs) in science and engineering,\nespecially in scenarios featuring complex domains or incorporation of empirical\ndata. One advantage of the neural network methods for PDEs lies in its\nautomatic differentiation (AD), which necessitates only the sample points\nthemselves, unlike traditional finite difference (FD) approximations that\nrequire nearby local points to compute derivatives. In this paper, we\nquantitatively demonstrate the advantage of AD in training neural networks. The\nconcept of truncated entropy is introduced to characterize the training\nproperty. Specifically, through comprehensive experimental and theoretical\nanalyses conducted on random feature models and two-layer neural networks, we\ndiscover that the defined truncated entropy serves as a reliable metric for\nquantifying the residual loss of random feature models and the training speed\nof neural networks for both AD and FD methods. Our experimental and theoretical\nanalyses demonstrate that, from a training perspective, AD outperforms FD in\nsolving PDEs.\n","authors":["Chuqi Chen","Yahong Yang","Yang Xiang","Wenrui Hao"],"pdf_url":"https://arxiv.org/pdf/2405.14099v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13980v1","updated":"2025-03-18T07:30:29Z","published":"2025-03-18T07:30:29Z","title":"Empowering LLMs in Decision Games through Algorithmic Data Synthesis","summary":"  Large Language Models (LLMs) have exhibited impressive capabilities across\nnumerous domains, yet they often struggle with complex reasoning and\ndecision-making tasks. Decision-making games, which inherently require\nmultifaceted reasoning logic, serve as ideal sandboxes for evaluating and\nenhancing the reasoning abilities of LLMs. In this work, we first explore\nwhether LLMs can master complex decision-making games through targeted\npost-training. To this end, we design data synthesis strategies and curate\nextensive offline datasets from two classic games, Doudizhu and Go. We further\ndevelop a suite of techniques to effectively incorporate this data into LLM\ntraining, resulting in two novel agents: Mastermind-Dou and Mastermind-Go. Our\nexperimental results demonstrate that these Mastermind LLMs achieve competitive\nperformance in their respective games. Additionally, we explore whether\nintegrating decision-making data can enhance the general reasoning abilities of\nLLMs. Our findings suggest that such post-training improves certain aspects of\nreasoning, providing valuable insights for optimizing LLM data collection and\nsynthesis strategies.\n","authors":["Haolin Wang","Xueyan Li","Yazhe Niu","Shuai Hu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2503.13980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14216v3","updated":"2025-03-18T07:30:25Z","published":"2025-01-24T03:44:16Z","title":"TFG-Flow: Training-free Guidance in Multimodal Generative Flow","summary":"  Given an unconditional generative model and a predictor for a target property\n(e.g., a classifier), the goal of training-free guidance is to generate samples\nwith desirable target properties without additional training. As a highly\nefficient technique for steering generative models toward flexible outcomes,\ntraining-free guidance has gained increasing attention in diffusion models.\nHowever, existing methods only handle data in continuous spaces, while many\nscientific applications involve both continuous and discrete data (referred to\nas multimodality). Another emerging trend is the growing use of the simple and\ngeneral flow matching framework in building generative foundation models, where\nguided generation remains under-explored. To address this, we introduce\nTFG-Flow, a novel training-free guidance method for multimodal generative flow.\nTFG-Flow addresses the curse-of-dimensionality while maintaining the property\nof unbiased sampling in guiding discrete variables. We validate TFG-Flow on\nfour molecular design tasks and show that TFG-Flow has great potential in drug\ndesign by generating molecules with desired properties.\n","authors":["Haowei Lin","Shanda Li","Haotian Ye","Yiming Yang","Stefano Ermon","Yitao Liang","Jianzhu Ma"],"pdf_url":"https://arxiv.org/pdf/2501.14216v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10118v2","updated":"2025-03-18T07:28:11Z","published":"2025-03-13T07:27:05Z","title":"An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy\n  Transfer with Differentiable Simulation","summary":"  The sim-to-real gap remains a critical challenge in robotics, hindering the\ndeployment of algorithms trained in simulation to real-world systems. This\npaper introduces a novel Real-Sim-Real (RSR) loop framework leveraging\ndifferentiable simulation to address this gap by iteratively refining\nsimulation parameters, aligning them with real-world conditions, and enabling\nrobust and efficient policy transfer. A key contribution of our work is the\ndesign of an informative cost function that encourages the collection of\ndiverse and representative real-world data, minimizing bias and maximizing the\nutility of each data point for simulation refinement. This cost function\nintegrates seamlessly into existing reinforcement learning algorithms (e.g.,\nPPO, SAC) and ensures a balanced exploration of critical regions in the real\ndomain. Furthermore, our approach is implemented on the versatile Mujoco MJX\nplatform, and our framework is compatible with a wide range of robotic systems.\nExperimental results on several robotic manipulation tasks demonstrate that our\nmethod significantly reduces the sim-to-real gap, achieving high task\nperformance and generalizability across diverse scenarios of both explicit and\nimplicit environmental uncertainties.\n","authors":["Lu Shi","Yuxuan Xu","Shiyu Wang","Jinhao Huang","Wenhao Zhao","Yufei Jia","Zike Yan","Weibin Gu","Guyue Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.10118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13279v2","updated":"2025-03-18T07:27:15Z","published":"2024-07-18T08:33:10Z","title":"Analyzing and Bridging the Gap between Maximizing Total Reward and\n  Discounted Reward in Deep Reinforcement Learning","summary":"  The optimal objective is a fundamental aspect of reinforcement learning (RL),\nas it determines how policies are evaluated and optimized. While total return\nmaximization is the ideal objective in RL, discounted return maximization is\nthe practical objective due to its stability. This can lead to a misalignment\nof objectives. To better understand the problem, we theoretically analyze the\nperformance gap between the policy maximizes the total return and the policy\nmaximizes the discounted return. Our analysis reveals that increasing the\ndiscount factor can be ineffective at eliminating this gap when environment\ncontains cyclic states,a frequent scenario. To address this issue, we propose\ntwo alternative approaches to align the objectives. The first approach achieves\nalignment by modifying the terminal state value, treating it as a tunable\nhyper-parameter with its suitable range defined through theoretical analysis.\nThe second approach focuses on calibrating the reward data in trajectories,\nenabling alignment in practical Deep RL applications using off-policy\nalgorithms. This method enhances robustness to the discount factor and improve\nperformance when the trajectory length is large. Our proposed methods\ndemonstrate that adjusting reward data can achieve alignment, providing an\ninsight that can be leveraged to design new optimization objectives to\nfundamentally enhance the performance of RL algorithms.\n","authors":["Shuyu Yin","Fei Wen","Peilin Liu","Tao Luo"],"pdf_url":"https://arxiv.org/pdf/2407.13279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13976v1","updated":"2025-03-18T07:24:55Z","published":"2025-03-18T07:24:55Z","title":"A CNN-based End-to-End Learning for RIS-assisted Communication System","summary":"  Reconfigurable intelligent surface (RIS) is an emerging technology that is\nused to improve the system performance in beyond 5G systems. In this letter, we\npropose a novel convolutional neural network (CNN)-based autoencoder to jointly\noptimize the transmitter, the receiver, and the RIS of a RIS-assisted\ncommunication system. The proposed system jointly optimizes the sub-tasks of\nthe transmitter, the receiver, and the RIS such as encoding/decoding, channel\nestimation, phase optimization, and modulation/demodulation. Numerically we\nhave shown that the bit error rate (BER) performance of the CNN-based\nautoencoder system is better than the theoretical BER performance of the\nRIS-assisted communication systems.\n","authors":["Nipuni Ginige","Nandana Rajatheva","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2503.13976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10253v2","updated":"2025-03-18T07:08:41Z","published":"2025-03-13T11:01:03Z","title":"PIMRL: Physics-Informed Multi-Scale Recurrent Learning for\n  Spatiotemporal Prediction","summary":"  Simulation of spatiotemporal systems governed by partial differential\nequations is widely applied in fields such as biology, chemistry, aerospace\ndynamics, and meteorology. Traditional numerical methods incur high\ncomputational costs due to the requirement of small time steps for accurate\npredictions. While machine learning has reduced these costs, long-term\npredictions remain challenged by error accumulation, particularly in scenarios\nwith insufficient data or varying time scales, where stability and accuracy are\ncompromised. Existing methods often neglect the effective utilization of\nmulti-scale data, leading to suboptimal robustness in predictions. To address\nthese issues, we propose a novel multi-scale learning framework, namely, the\nPhysics-Informed Multi-Scale Recurrent Learning (PIMRL), to effectively\nleverage multi-scale data for spatiotemporal dynamics prediction. The PIMRL\nframework comprises two modules: the micro-scale module embeds physical\nknowledge into neural networks via pretraining, and the macro-scale module\nadopts a data-driven approach to learn the temporal evolution of physics in the\nlatent space. Experimental results demonstrate that the PIMRL framework\nconsistently achieves state-of-the-art performance across five benchmark\ndatasets ranging from one to three dimensions, showing average improvements of\nover 9\\% in both RMSE and MAE evaluation metrics, with maximum enhancements\nreaching up to 80%.\n","authors":["Han Wan","Qi Wang","Yuan Mi","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2503.10253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13964v1","updated":"2025-03-18T06:57:21Z","published":"2025-03-18T06:57:21Z","title":"MDocAgent: A Multi-Modal Multi-Agent Framework for Document\n  Understanding","summary":"  Document Question Answering (DocQA) is a very common task. Existing methods\nusing Large Language Models (LLMs) or Large Vision Language Models (LVLMs) and\nRetrieval Augmented Generation (RAG) often prioritize information from a single\nmodal, failing to effectively integrate textual and visual cues. These\napproaches struggle with complex multi-modal reasoning, limiting their\nperformance on real-world documents. We present MDocAgent (A Multi-Modal\nMulti-Agent Framework for Document Understanding), a novel RAG and multi-agent\nframework that leverages both text and image. Our system employs five\nspecialized agents: a general agent, a critical agent, a text agent, an image\nagent and a summarizing agent. These agents engage in multi-modal context\nretrieval, combining their individual insights to achieve a more comprehensive\nunderstanding of the document's content. This collaborative approach enables\nthe system to synthesize information from both textual and visual components,\nleading to improved accuracy in question answering. Preliminary experiments on\nfive benchmarks like MMLongBench, LongDocURL demonstrate the effectiveness of\nour MDocAgent, achieve an average improvement of 12.1% compared to current\nstate-of-the-art method. This work contributes to the development of more\nrobust and comprehensive DocQA systems capable of handling the complexities of\nreal-world documents containing rich textual and visual information. Our data\nand code are available at https://github.com/aiming-lab/MDocAgent.\n","authors":["Siwei Han","Peng Xia","Ruiyi Zhang","Tong Sun","Yun Li","Hongtu Zhu","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2503.13964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12370v4","updated":"2025-03-18T06:55:49Z","published":"2024-12-16T21:56:01Z","title":"Scam Detection for Ethereum Smart Contracts: Leveraging Graph\n  Representation Learning for Secure Blockchain","summary":"  Due to the increasing abuse of fraudulent activities that result in\nsignificant financial and reputational harm, Ethereum smart contracts face a\nsignificant problem in detecting fraud. Existing monitoring methods typically\nrely on lease code analysis or physically extracted features, which suffer from\nscalability and adaptability limitations. In this study, we use graph\nrepresentation learning to observe purchase trends and find fraudulent deals.\nWe can achieve powerful categorisation performance by using innovative machine\nlearning versions and transforming Ethereum invoice data into graph structures.\nOur method addresses label imbalance through SMOTE-ENN techniques and evaluates\nmodels like Multi-Layer Perceptron ( MLP ) and Graph Convolutional Networks (\nGCN). Experimental results show that the MLP type surpasses the GCN in this\nenvironment, with domain-specific assessments closely aligned with real-world\nassessments. This study provides a scalable and efficient way to improve\nEthereum's ecosystem's confidence and security.\n","authors":["Yihong Jin","Ze Yang","Xinhe Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12370v4.pdf","comment":"Accepted to ISCAIT 2025"},{"id":"http://arxiv.org/abs/2410.12360v3","updated":"2025-03-18T06:54:45Z","published":"2024-10-16T08:23:39Z","title":"Towards Neural Scaling Laws for Time Series Foundation Models","summary":"  Scaling laws offer valuable insights into the design of time series\nfoundation models (TSFMs). However, previous research has largely focused on\nthe scaling laws of TSFMs for in-distribution (ID) data, leaving their\nout-of-distribution (OOD) scaling behavior and the influence of model\narchitectures less explored. In this work, we examine two common TSFM\narchitectures, encoder-only and decoder-only Transformers, and investigate\ntheir scaling behavior on both ID and OOD data. These models are trained and\nevaluated across varying parameter counts, compute budgets, and dataset sizes.\nOur experiments reveal that the log-likelihood loss of TSFMs exhibits similar\nscaling behavior in both OOD and ID settings. We further compare the scaling\nproperties across different architectures, incorporating two state-of-the-art\nTSFMs as case studies, showing that model architecture plays a significant role\nin scaling. The encoder-only Transformers demonstrate better scalability than\nthe decoder-only Transformers, while the architectural enhancements in the two\nadvanced TSFMs primarily improve ID performance but reduce OOD scalability.\nWhile scaling up TSFMs is expected to drive performance breakthroughs, the lack\nof a comprehensive understanding of TSFM scaling laws has hindered the\ndevelopment of a robust framework to guide model scaling. We fill this gap in\nthis work by synthesizing our findings and providing practical guidelines for\ndesigning and scaling larger TSFMs with enhanced model capabilities.\n","authors":["Qingren Yao","Chao-Han Huck Yang","Renhe Jiang","Yuxuan Liang","Ming Jin","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2410.12360v3.pdf","comment":"Accepted by the 13th International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2410.09408v3","updated":"2025-03-18T06:54:13Z","published":"2024-10-12T07:28:54Z","title":"C-Adapter: Adapting Deep Classifiers for Efficient Conformal Prediction\n  Sets","summary":"  Conformal prediction, as an emerging uncertainty quantification technique,\ntypically functions as post-hoc processing for the outputs of trained\nclassifiers. To optimize the classifier for maximum predictive efficiency,\nConformal Training rectifies the training objective with a regularization that\nminimizes the average prediction set size at a specific error rate. However,\nthe regularization term inevitably deteriorates the classification accuracy and\nleads to suboptimal efficiency of conformal predictors. To address this issue,\nwe introduce \\textbf{Conformal Adapter} (C-Adapter), an adapter-based tuning\nmethod to enhance the efficiency of conformal predictors without sacrificing\naccuracy. In particular, we implement the adapter as a class of intra\norder-preserving functions and tune it with our proposed loss that maximizes\nthe discriminability of non-conformity scores between correctly and randomly\nmatched data-label pairs. Using C-Adapter, the model tends to produce extremely\nhigh non-conformity scores for incorrect labels, thereby enhancing the\nefficiency of prediction sets across different coverage rates. Extensive\nexperiments demonstrate that C-Adapter can effectively adapt various\nclassifiers for efficient prediction sets, as well as enhance the conformal\ntraining method.\n","authors":["Kangdao Liu","Hao Zeng","Jianguo Huang","Huiping Zhuang","Chi-Man Vong","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2410.09408v3.pdf","comment":"The experimental results are not sufficient"},{"id":"http://arxiv.org/abs/2503.13954v1","updated":"2025-03-18T06:46:53Z","published":"2025-03-18T06:46:53Z","title":"Enhanced High-Dimensional Data Visualization through Adaptive\n  Multi-Scale Manifold Embedding","summary":"  To address the dual challenges of the curse of dimensionality and the\ndifficulty in separating intra-cluster and inter-cluster structures in\nhigh-dimensional manifold embedding, we proposes an Adaptive Multi-Scale\nManifold Embedding (AMSME) algorithm. By introducing ordinal distance to\nreplace traditional Euclidean distances, we theoretically demonstrate that\nordinal distance overcomes the constraints of the curse of dimensionality in\nhigh-dimensional spaces, effectively distinguishing heterogeneous samples. We\ndesign an adaptive neighborhood adjustment method to construct similarity\ngraphs that simultaneously balance intra-cluster compactness and inter-cluster\nseparability. Furthermore, we develop a two-stage embedding framework: the\nfirst stage achieves preliminary cluster separation while preserving\nconnectivity between structurally similar clusters via the similarity graph,\nand the second stage enhances inter-cluster separation through a label-driven\ndistance reweighting. Experimental results demonstrate that AMSME significantly\npreserves intra-cluster topological structures and improves inter-cluster\nseparation on real-world datasets. Additionally, leveraging its\nmulti-resolution analysis capability, AMSME discovers novel neuronal subtypes\nin the mouse lumbar dorsal root ganglion scRNA-seq dataset, with marker gene\nanalysis revealing their distinct biological roles.\n","authors":["Tianhao Ni","Bingjie Li","Zhigang Yao"],"pdf_url":"https://arxiv.org/pdf/2503.13954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09829v2","updated":"2025-03-18T06:26:34Z","published":"2025-03-12T20:47:40Z","title":"SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey","summary":"  Recent advances in deep learning and Transformers have driven major\nbreakthroughs in robotics by employing techniques such as imitation learning,\nreinforcement learning, and LLM-based multimodal perception and\ndecision-making. However, conventional deep learning and Transformer models\noften struggle to process data with inherent symmetries and invariances,\ntypically relying on large datasets or extensive data augmentation. Equivariant\nneural networks overcome these limitations by explicitly integrating symmetry\nand invariance into their architectures, leading to improved efficiency and\ngeneralization. This tutorial survey reviews a wide range of equivariant deep\nlearning and control methods for robotics, from classic to state-of-the-art,\nwith a focus on SE(3)-equivariant models that leverage the natural 3D\nrotational and translational symmetries in visual robotic manipulation and\ncontrol design. Using unified mathematical notation, we begin by reviewing key\nconcepts from group theory, along with matrix Lie groups and Lie algebras. We\nthen introduce foundational group-equivariant neural network design and show\nhow the group-equivariance can be obtained through their structure. Next, we\ndiscuss the applications of SE(3)-equivariant neural networks in robotics in\nterms of imitation learning and reinforcement learning. The SE(3)-equivariant\ncontrol design is also reviewed from the perspective of geometric control.\nFinally, we highlight the challenges and future directions of equivariant\nmethods in developing more robust, sample-efficient, and multi-modal real-world\nrobotic systems.\n","authors":["Joohwan Seo","Soochul Yoo","Junwoo Chang","Hyunseok An","Hyunwoo Ryu","Soomi Lee","Arvind Kruthiventy","Jongeun Choi","Roberto Horowitz"],"pdf_url":"https://arxiv.org/pdf/2503.09829v2.pdf","comment":"Submitted to International Journcal of Control, Automation and\n  Systems (IJCAS), Under Review"},{"id":"http://arxiv.org/abs/2503.13942v1","updated":"2025-03-18T06:14:20Z","published":"2025-03-18T06:14:20Z","title":"Structured Knowledge Accumulation: An Autonomous Framework for\n  Layer-Wise Entropy Reduction in Neural Learning","summary":"  We introduce the Structured Knowledge Accumulation (SKA) framework, which\nreinterprets entropy as a dynamic, layer-wise measure of knowledge alignment in\nneural networks. Instead of relying on traditional gradient-based optimization,\nSKA defines entropy in terms of knowledge vectors and their influence on\ndecision probabilities across multiple layers. This formulation naturally leads\nto the emergence of activation functions such as the sigmoid as a consequence\nof entropy minimization. Unlike conventional backpropagation, SKA allows each\nlayer to optimize independently by aligning its knowledge representation with\nchanges in decision probabilities. As a result, total network entropy decreases\nin a hierarchical manner, allowing knowledge structures to evolve\nprogressively. This approach provides a scalable, biologically plausible\nalternative to gradient-based learning, bridging information theory and\nartificial intelligence while offering promising applications in\nresource-constrained and parallel computing environments.\n","authors":["Bouarfa Mahi Quantiota"],"pdf_url":"https://arxiv.org/pdf/2503.13942v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.08116v2","updated":"2025-03-18T05:49:19Z","published":"2024-12-11T05:50:33Z","title":"Diffusion-based Data Augmentation and Knowledge Distillation with\n  Generated Soft Labels Solving Data Scarcity Problems of SAR Oil Spill\n  Segmentation","summary":"  Oil spills pose severe environmental risks, making early detection crucial\nfor effective response and mitigation. As Synthetic Aperture Radar (SAR) images\noperate under all-weather conditions, SAR-based oil spill segmentation enables\nfast and robust monitoring. However, when using deep learning models, SAR oil\nspill segmentation often struggles in training due to the scarcity of labeled\ndata. To address this limitation, we propose a diffusion-based data\naugmentation with knowledge transfer (DAKTer) strategy. Our DAKTer strategy\nenables a diffusion model to generate SAR oil spill images along with soft\nlabel pairs, which offer richer class probability distributions than\nsegmentation masks (i.e. hard labels). Also, for reliable joint generation of\nhigh-quality SAR images and well-aligned soft labels, we introduce an SNR-based\nbalancing factor aligning the noise corruption process of both modalilties in\ndiffusion models. By leveraging the generated SAR images and soft labels, a\nstudent segmentation model can learn robust feature representations without\nteacher models trained for the same task, improving its ability to segment oil\nspill regions. Extensive experiments demonstrate that our DAKTer strategy\neffectively transfers the knowledge of per-pixel class probabilities to the\nstudent segmentation model to distinguish the oil spill regions from other\nlook-alike regions in the SAR images. Our DAKTer strategy boosts various\nsegmentation models to achieve superior performance with large margins compared\nto other generative data augmentation methods.\n","authors":["Jaeho Moon","Jeonghwan Yun","Jaehyun Kim","Jaehyup Lee","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.08116v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.14421v1","updated":"2025-03-18T16:55:07Z","published":"2025-03-18T16:55:07Z","title":"ExDDV: A New Dataset for Explainable Deepfake Detection in Video","summary":"  The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV.\n","authors":["Vlad Hondru","Eduard Hogea","Darian Onchis","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2503.14421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14271v1","updated":"2025-03-18T14:05:52Z","published":"2025-03-18T14:05:52Z","title":"Video Streaming with Kairos: An MPC-Based ABR with Streaming-Aware\n  Throughput Prediction","summary":"  In this paper, we present Kairos, a model predictive control (MPC)-based\nadaptive bitrate (ABR) scheme that integrates streaming-aware throughput\npredictions to enhance video streaming quality. Kairos features an\nattention-based throughput predictor with buffer-aware uncertainty control,\nimproving prediction accuracy and adaptability to network conditions.\nSpecifically, we introduce a multi-time attention network to handle the\nirregularly sampled sequences in streaming data, creating uniformly spaced\nlatent representations. Additionally, we design a separate prediction network\nthat estimates future throughput at multiple percentiles and incorporates a\nbuffer-aware uncertainty adjustment module. This module dynamically selects the\nappropriate throughput percentile based on the buffer size, enhancing\nrobustness to varying network conditions. Lastly, to mitigate QoE smoothness\npenalties caused by predictors focused solely on accuracy, we introduce a\nsmoothness regularizer. By embedding streaming-aware characteristics, such as\nsampling irregularity, buffer occupancy, and smoothness, into the throughput\npredictor design, Kairos significantly improves bitrate decision-making within\nthe MPC framework. Extensive trace-driven and real-world experiments\ndemonstrate that Kairos outperforms state-of-the-art ABR schemes, achieving an\naverage QoE improvement of 1.52% to 7.28% under various network conditions.\n","authors":["Ziyu Zhong","Mufan Liu","Le Yang","Yifan Wang","Yiling Xu","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.14271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13501v2","updated":"2025-03-18T13:55:22Z","published":"2024-03-20T10:58:58Z","title":"VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis","summary":"  Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.\n","authors":["Yumeng Li","William Beluch","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2403.13501v2.pdf","comment":"Accepted at ICLR 2025. Code: https://github.com/boschresearch/VSTAR\n  and project page: https://yumengli007.github.io/VSTAR"},{"id":"http://arxiv.org/abs/2503.14220v1","updated":"2025-03-18T12:58:48Z","published":"2025-03-18T12:58:48Z","title":"musicolors: Bridging Sound and Visuals For Synesthetic Creative Musical\n  Experience","summary":"  Music visualization is an important medium that enables synesthetic\nexperiences and creative inspiration. However, previous research focused mainly\non the technical and theoretical aspects, overlooking users' everyday\ninteraction with music visualizations. This gap highlights the pressing need\nfor research on how music visualization influences users in synesthetic\ncreative experiences and where they are heading. Thus, we developed musicolors,\na web-based music visualization library available in real-time. Additionally,\nwe conducted a qualitative user study with composers, developers, and listeners\nto explore how they use musicolors to appreciate and get inspiration and craft\nthe music-visual interaction. The results show that musicolors provides a rich\nvalue of music visualization to users through sketching for musical ideas,\nintegrating visualizations with other systems or platforms, and synesthetic\nlistening. Based on these findings, we also provide guidelines for future music\nvisualizations to offer a more interactive and creative experience.\n","authors":["ChungHa Lee","Jin-Hyuk Hong"],"pdf_url":"https://arxiv.org/pdf/2503.14220v1.pdf","comment":"16 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.14154v1","updated":"2025-03-18T11:25:55Z","published":"2025-03-18T11:25:55Z","title":"RBFIM: Perceptual Quality Assessment for Compressed Point Clouds Using\n  Radial Basis Function Interpolation","summary":"  One of the main challenges in point cloud compression (PCC) is how to\nevaluate the perceived distortion so that the codec can be optimized for\nperceptual quality. Current standard practices in PCC highlight a primary\nissue: while single-feature metrics are widely used to assess compression\ndistortion, the classic method of searching point-to-point nearest neighbors\nfrequently fails to adequately build precise correspondences between point\nclouds, resulting in an ineffective capture of human perceptual features. To\novercome the related limitations, we propose a novel assessment method called\nRBFIM, utilizing radial basis function (RBF) interpolation to convert discrete\npoint features into a continuous feature function for the distorted point\ncloud. By substituting the geometry coordinates of the original point cloud\ninto the feature function, we obtain the bijective sets of point features. This\nenables an establishment of precise corresponding features between distorted\nand original point clouds and significantly improves the accuracy of quality\nassessments. Moreover, this method avoids the complexity caused by\nbidirectional searches. Extensive experiments on multiple subjective quality\ndatasets of compressed point clouds demonstrate that our RBFIM excels in\naddressing human perception tasks, thereby providing robust support for PCC\noptimization efforts.\n","authors":["Zhang Chen","Shuai Wan","Siyu Ren","Fuzheng Yang","Mengting Yu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2503.14154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04013v2","updated":"2025-03-18T08:54:41Z","published":"2024-09-06T03:53:59Z","title":"3D-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian\n  Geometric Priors","summary":"  Existing multi-view image compression methods often rely on 2D\nprojection-based similarities between views to estimate disparities. While\neffective for small disparities, such as those in stereo images, these methods\nstruggle with the more complex disparities encountered in wide-baseline\nmulti-camera systems, commonly found in virtual reality and autonomous driving\napplications. To address this limitation, we propose 3D-LMVIC, a novel\nlearning-based multi-view image compression framework that leverages 3D\nGaussian Splatting to derive geometric priors for accurate disparity\nestimation. Furthermore, we introduce a depth map compression model to minimize\ngeometric redundancy across views, along with a multi-view sequence ordering\nstrategy based on a defined distance measure between views to enhance\ncorrelations between adjacent views. Experimental results demonstrate that\n3D-LMVIC achieves superior performance compared to both traditional and\nlearning-based methods. Additionally, it significantly improves disparity\nestimation accuracy over existing two-view approaches.\n","authors":["Yujun Huang","Bin Chen","Niu Lian","Baoyi An","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2409.04013v2.pdf","comment":"17 pages, 10 figures, conference"},{"id":"http://arxiv.org/abs/2502.05130v2","updated":"2025-03-18T04:31:39Z","published":"2025-02-07T18:02:47Z","title":"Latent Swap Joint Diffusion for 2D Long-Form Latent Generation","summary":"  This paper introduces Swap Forward (SaFa), a modality-agnostic and efficient\nmethod to generate seamless and coherence long spectrum and panorama through\nlatent swap joint diffusion across multi-views. We first investigate the\nspectrum aliasing problem in spectrum-based audio generation caused by existing\njoint diffusion methods. Through a comparative analysis of the VAE latent\nrepresentation of Mel-spectra and RGB images, we identify that the failure\narises from excessive suppression of high-frequency components during the\nspectrum denoising process due to the averaging operator. To address this\nissue, we propose Self-Loop Latent Swap, a frame-level bidirectional swap\napplied to the overlapping region of adjacent views. Leveraging stepwise\ndifferentiated trajectories of adjacent subviews, this swap operator adaptively\nenhances high-frequency components and avoid spectrum distortion. Furthermore,\nto improve global cross-view consistency in non-overlapping regions, we\nintroduce Reference-Guided Latent Swap, a unidirectional latent swap operator\nthat provides a centralized reference trajectory to synchronize subview\ndiffusions. By refining swap timing and intervals, we can achieve a cross-view\nsimilarity-diversity balance in a forward-only manner. Quantitative and\nqualitative experiments demonstrate that SaFa significantly outperforms\nexisting joint diffusion methods and even training-based methods in audio\ngeneration using both U-Net and DiT models, along with effective longer length\nadaptation. It also adapts well to panorama generation, achieving comparable\nperformance with 2 $\\sim$ 20 $\\times$ faster speed and greater model\ngeneralizability. More generation demos are available at\nhttps://swapforward.github.io/\n","authors":["Yusheng Dai","Chenxi Wang","Chang Li","Chen Wang","Jun Du","Kewei Li","Ruoyu Wang","Jiefeng Ma","Lei Sun","Jianqing Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05130v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13809v1","updated":"2025-03-18T01:43:09Z","published":"2025-03-18T01:43:09Z","title":"The Immersive Archive: Archival Strategies for the Sensorama &\n  Sutherland HMD","summary":"  The Immersive Archive is an initiative dedicated to preserve and restore the\ngroundbreaking works from across Extended Reality (XR) history. Originating at\nthe University of Southern California's Mobile and Environmental Media Lab,\nthis archive is committed to developing and exhibiting simulations of\ninfluential XR devices that have shaped immersive media over time. This paper\nexamines the challenges and strategies involved in archiving seminal XR\ntechnologies, with a focus on Morton Heilig's Sensorama and Ivan Sutherland's\nHeadMounted Display. As pioneering prototypes in virtual and augmented reality,\nthese devices provide valuable insights into the evolution of immersive media,\nhighlighting both technological innovation and sensory experimentation. Through\ncollaborative archival efforts with institutions such as the HMH Moving Image\nArchive at University of Southern California and the Computer History Museum,\nthis research integrates media archaeology with digital preservation\ntechniques. Emphasis is placed on documentation practices, restoration of\nphysical artifacts and developing simulations of these historic experiences for\ncontemporary virtual reality platforms. Our interdisciplinary approach to\narchival methodologies, which captures the multisensory and interactive\nqualities of these pioneering devices, has been instrumental in developing a\nframework for future immersive media preservation initiatives. By preserving\nthe immersive essence of these early experiences, we lay the groundwork for\nfuture generations to explore and learn from the origins of immersive media.\nSafeguarding this rich legacy is essential to ensure these visionary works\ncontinue to inspire and shape the future of media landscapes.\n","authors":["Zeynep Abes","Nathan Fairchild","Spencer Lin","Michael Wahba","Katrina Xiao","Scott S. Fisher"],"pdf_url":"https://arxiv.org/pdf/2503.13809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13805v1","updated":"2025-03-18T01:32:38Z","published":"2025-03-18T01:32:38Z","title":"Text-Guided Image Invariant Feature Learning for Robust Image\n  Watermarking","summary":"  Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking.\n","authors":["Muhammad Ahtesham","Xin Zhong"],"pdf_url":"https://arxiv.org/pdf/2503.13805v1.pdf","comment":null}]},"2025-03-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.15485v1","updated":"2025-03-19T17:58:57Z","published":"2025-03-19T17:58:57Z","title":"TULIP: Towards Unified Language-Image Pretraining","summary":"  Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io\n","authors":["Zineng Tang","Long Lian","Seun Eisape","XuDong Wang","Roei Herzig","Adam Yala","Alane Suhr","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2503.15485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15484v1","updated":"2025-03-19T17:57:49Z","published":"2025-03-19T17:57:49Z","title":"Value Profiles for Encoding Human Variation","summary":"  Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.\n","authors":["Taylor Sorensen","Pushkar Mishra","Roma Patel","Michael Henry Tessler","Michiel Bakker","Georgina Evans","Iason Gabriel","Noah Goodman","Verena Rieser"],"pdf_url":"https://arxiv.org/pdf/2503.15484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15477v1","updated":"2025-03-19T17:54:41Z","published":"2025-03-19T17:54:41Z","title":"What Makes a Reward Model a Good Teacher? An Optimization Perspective","summary":"  The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.\n","authors":["Noam Razin","Zixuan Wang","Hubert Strauss","Stanley Wei","Jason D. Lee","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2503.15477v1.pdf","comment":"Code available at https://github.com/princeton-pli/what-makes-good-rm"},{"id":"http://arxiv.org/abs/2503.15469v1","updated":"2025-03-19T17:45:13Z","published":"2025-03-19T17:45:13Z","title":"Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional\n  Context-Aware Representation Learning for Enhanced Text Classification","summary":"  Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.\n","authors":["ZhengLin Lai","MengYao Liao","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2503.15469v1.pdf","comment":"11 pages,1 figure"},{"id":"http://arxiv.org/abs/2503.15463v1","updated":"2025-03-19T17:41:46Z","published":"2025-03-19T17:41:46Z","title":"From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment","summary":"  Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems.\n","authors":["Jia-Nan Li","Jian Guan","Songhao Wu","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2503.15463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15454v1","updated":"2025-03-19T17:36:35Z","published":"2025-03-19T17:36:35Z","title":"Evaluating Bias in Retrieval-Augmented Medical Question-Answering\n  Systems","summary":"  Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making.\n","authors":["Yuelyu Ji","Hang Zhang","Yanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.15454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15450v1","updated":"2025-03-19T17:31:15Z","published":"2025-03-19T17:31:15Z","title":"SkyLadder: Better and Faster Pretraining via Context Window Scheduling","summary":"  Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder.\n","authors":["Tongyao Zhu","Qian Liu","Haonan Wang","Shiqi Chen","Xiangming Gu","Tianyu Pang","Min-Yen Kan"],"pdf_url":"https://arxiv.org/pdf/2503.15450v1.pdf","comment":"22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models"},{"id":"http://arxiv.org/abs/2503.15438v1","updated":"2025-03-19T17:19:07Z","published":"2025-03-19T17:19:07Z","title":"VenusFactory: A Unified Platform for Protein Engineering Data Retrieval\n  and Language Model Fine-Tuning","summary":"  Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory.\n","authors":["Yang Tan","Chen Liu","Jingyuan Gao","Banghao Wu","Mingchen Li","Ruilin Wang","Lingrong Zhang","Huiqun Yu","Guisheng Fan","Liang Hong","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.15438v1.pdf","comment":"12 pages, 1 figure, 8 tables"},{"id":"http://arxiv.org/abs/2402.13213v3","updated":"2025-03-19T16:57:23Z","published":"2024-02-20T18:24:47Z","title":"Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A","summary":"  We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.\n","authors":["Benjamin Plaut","Nguyen X. Khanh","Tu Trinh"],"pdf_url":"https://arxiv.org/pdf/2402.13213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11197v3","updated":"2025-03-19T16:33:16Z","published":"2025-03-14T08:43:53Z","title":"Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study\n  on Audio Question Answering","summary":"  Recently, reinforcement learning (RL) has been shown to greatly enhance the\nreasoning capabilities of large language models (LLMs), and RL-based approaches\nhave been progressively applied to visual multimodal tasks. However, the audio\nmodality has largely been overlooked in these developments. Thus, we conduct a\nseries of RL explorations in audio understanding and reasoning, specifically\nfocusing on the audio question answering (AQA) task. We leverage the group\nrelative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and\nour experiments demonstrated state-of-the-art performance on the MMAU Test-mini\nbenchmark, achieving an accuracy rate of 64.5%. The main findings in this\ntechnical report are as follows: 1) The GRPO algorithm can be effectively\napplied to large audio language models (LALMs), even when the model has only\n8.2B parameters; 2) With only 38k post-training samples, RL significantly\noutperforms supervised fine-tuning (SFT), indicating that RL-based approaches\ncan be effective without large datasets; 3) The explicit reasoning process has\nnot shown significant benefits for AQA tasks, and how to efficiently utilize\ndeep thinking remains an open question for further research; 4) LALMs still lag\nfar behind humans auditory-language reasoning, suggesting that the RL-based\napproaches warrant further exploration. Our project is available at\nhttps://github.com/xiaomi-research/r1-aqa and\nhttps://huggingface.co/mispeech/r1-aqa.\n","authors":["Gang Li","Jizhong Liu","Heinrich Dinkel","Yadong Niu","Junbo Zhang","Jian Luan"],"pdf_url":"https://arxiv.org/pdf/2503.11197v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08020v2","updated":"2025-03-19T16:26:10Z","published":"2025-02-11T23:40:53Z","title":"Speculate, then Collaborate: Fusing Knowledge of Language Models during\n  Decoding","summary":"  Large Language Models (LLMs) often excel in specific domains but fall short\nin others due to the limitations of their training. Thus, enabling LLMs to\nsolve problems collaboratively by integrating their complementary knowledge\npromises to improve their performance across domains. To realize this\npotential, we introduce a novel Collaborative Speculative Decoding (CoSD)\nalgorithm that enables efficient LLM knowledge fusion at test time without\nrequiring additional model training. CoSD employs a draft model to generate\ninitial sequences and an easy-to-learn rule or decision tree to decide when to\ninvoke an assistant model to improve these drafts. CoSD not only enhances\nknowledge fusion but also improves inference efficiency, is transferable across\ndomains and models, and offers greater explainability. Experimental results\ndemonstrate that CoSD improves accuracy by up to 10\\% across benchmarks\ncompared to existing methods, providing a scalable and effective solution for\nLLM-based applications\n","authors":["Ziyao Wang","Muneeza Azmat","Ang Li","Raya Horesh","Mikhail Yurochkin"],"pdf_url":"https://arxiv.org/pdf/2502.08020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17721v2","updated":"2025-03-19T16:25:10Z","published":"2023-10-26T18:30:37Z","title":"From Transcripts to Insights: Uncovering Corporate Risks Using\n  Generative AI","summary":"  We explore the value of generative AI tools, such as ChatGPT, in helping\ninvestors uncover dimensions of corporate risk. We develop and validate\nfirm-level measures of risk exposure to political, climate, and AI-related\nrisks. Using the GPT 3.5 model to generate risk summaries and assessments from\nthe context provided by earnings call transcripts, we show that GPT-based\nmeasures possess significant information content and outperform the existing\nrisk measures in predicting (abnormal) firm-level volatility and firms' choices\nsuch as investment and innovation. Importantly, information in risk assessments\ndominates that in risk summaries, establishing the value of general AI\nknowledge. We also find that generative AI is effective at detecting emerging\nrisks, such as AI risk, which has soared in recent quarters. Our measures\nperform well both within and outside the GPT's training window and are priced\nin equity markets. Taken together, an AI-based approach to risk measurement\nprovides useful insights to users of corporate disclosures at a low cost.\n","authors":["Alex Kim","Maximilian Muhn","Valeri Nikolaev"],"pdf_url":"https://arxiv.org/pdf/2310.17721v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15374v1","updated":"2025-03-19T16:12:11Z","published":"2025-03-19T16:12:11Z","title":"Real-world validation of a multimodal LLM-powered pipeline for\n  High-Accuracy Clinical Trial Patient Matching leveraging EHR data","summary":"  Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching.\n","authors":["Anatole Callies","Quentin Bodinier","Philippe Ravaud","Kourosh Davarpanah"],"pdf_url":"https://arxiv.org/pdf/2503.15374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05206v3","updated":"2025-03-19T16:10:18Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Xudong Han","Haonan Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Tim Baldwin","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v3.pdf","comment":"47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2503.15358v1","updated":"2025-03-19T15:58:46Z","published":"2025-03-19T15:58:46Z","title":"SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation","summary":"  Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.\n","authors":["Thomas Pickard","Aline Villavicencio","Maggie Mi","Wei He","Dylan Phelps","Carolina Scarton","Marco Idiart"],"pdf_url":"https://arxiv.org/pdf/2503.15358v1.pdf","comment":"Preprint; SemEval-2025 proceedings to appear at ACL 2025"},{"id":"http://arxiv.org/abs/2412.20227v2","updated":"2025-03-19T15:56:49Z","published":"2024-12-28T17:48:33Z","title":"LLM Reasoning Engine: Specialized Training for Enhanced Mathematical\n  Reasoning","summary":"  Large Language Models (LLMs) have shown remarkable performance in various\nnatural language processing tasks but face challenges in mathematical\nreasoning, where complex problem-solving requires both linguistic understanding\nand mathematical reasoning skills. Existing approaches to address this\nchallenge often rely on ensemble methods and suffer from the problem of data\nscarcity in target domains. In this work, we present a novel method to enhance\nLLMs' capabilities in mathematical reasoning tasks. Motivated by the need to\nbridge this gap, our approach incorporates a question paraphrase strategy,\nwhich aims at diversifying the linguistic forms of mathematical questions to\nimprove generalization. Additionally, specialized training objectives are\nemployed to guide the model's learning process, focusing on enhancing its\nunderstanding of mathematical concepts and reasoning processes. We conduct\nexperiments on four datasets using different LLMs, and demonstrate the\neffectiveness of our approach in improving LLMs' performance on mathematical\nreasoning tasks. Our findings underscore the significance of our methodology in\nthe advancement of large language models and its potential implications for\nreal-world applications that require mathematical reasoning abilities.\n","authors":["Shuguang Chen","Guang Lin"],"pdf_url":"https://arxiv.org/pdf/2412.20227v2.pdf","comment":"Accepted to NAACL 2025 KnowledgeNLP"},{"id":"http://arxiv.org/abs/2503.15354v1","updated":"2025-03-19T15:56:21Z","published":"2025-03-19T15:56:21Z","title":"Optimizing Decomposition for Optimal Claim Verification","summary":"  Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.\n","authors":["Yining Lu","Noah Ziems","Hy Dang","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.15354v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15351v1","updated":"2025-03-19T15:48:57Z","published":"2025-03-19T15:48:57Z","title":"SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling\n  with Large Language Models","summary":"  In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.\n","authors":["I-Fan Lin","Faegheh Hasibi","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2503.15351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13551v2","updated":"2025-03-19T15:43:56Z","published":"2025-03-16T15:18:40Z","title":"Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models","summary":"  Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate\nsteps. In this paper, we propose a novel reward model approach, Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps from fine-grained and coarse-grained level. HRM performs better in\nassessing reasoning coherence and self-reflection, particularly when the\nprevious reasoning step is incorrect. Furthermore, to address the inefficiency\nof autonomous generating PRM training data via Monte Carlo Tree Search (MCTS),\nwe introduce a lightweight and effective data augmentation strategy called\nHierarchical Node Compression (HNC) based on node merging (combining two\nconsecutive reasoning steps into one step) in the tree structure. This approach\ndiversifies MCTS results for HRM with negligible computational overhead,\nenhancing label robustness by introducing noise. Empirical results on the\nPRM800K dataset demonstrate that HRM, in conjunction with HNC, achieves\nsuperior stability and reliability in evaluation compared to PRM. Furthermore,\ncross-domain evaluations on MATH500 and GSM8K confirm HRM's superior\ngeneralization and robustness across diverse reasoning tasks. The code for all\nexperiments will be released at https:\n//github.com/tengwang0318/hierarchial_reward_model.\n","authors":["Teng Wang","Zhangyi Jiang","Zhenqi He","Wenhan Yang","Yanan Zheng","Zeyu Li","Zifan He","Shenyang Tong","Hailei Gong"],"pdf_url":"https://arxiv.org/pdf/2503.13551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14703v3","updated":"2025-03-19T15:37:42Z","published":"2024-06-20T19:50:56Z","title":"Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics","summary":"  Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.\n","authors":["Seungbeen Lee","Seungwon Lim","Seungju Han","Giyeong Oh","Hyungjoo Chae","Jiwan Chung","Minju Kim","Beong-woo Kwak","Yeonsoo Lee","Dongha Lee","Jinyoung Yeo","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2406.14703v3.pdf","comment":"Accepted to NAACL2025 Findings"},{"id":"http://arxiv.org/abs/2503.15338v1","updated":"2025-03-19T15:34:21Z","published":"2025-03-19T15:34:21Z","title":"Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context","summary":"  Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio.\n","authors":["Junyi Ao","Dekun Chen","Xiaohai Tian","Wenjie Feng","Jun Zhang","Lu Lu","Yuxuan Wang","Haizhou Li","Zhizheng Wu"],"pdf_url":"https://arxiv.org/pdf/2503.15338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12167v2","updated":"2025-03-19T15:23:29Z","published":"2025-03-15T15:11:17Z","title":"PLM: Efficient Peripheral Language Models Hardware-Co-Designed for\n  Ubiquitous Computing","summary":"  While scaling laws have been continuously validated in large language models\n(LLMs) with increasing model parameters, the inherent tension between the\ninference demands of LLMs and the limited resources of edge devices poses a\ncritical challenge to the development of edge intelligence. Recently, numerous\nsmall language models have emerged, aiming to distill the capabilities of LLMs\ninto smaller footprints. However, these models often retain the fundamental\narchitectural principles of their larger counterparts, still imposing\nconsiderable strain on the storage and bandwidth capacities of edge devices. In\nthis paper, we introduce the PLM, a Peripheral Language Model, developed\nthrough a co-design process that jointly optimizes model architecture and edge\nsystem constraints. The PLM utilizes a Multi-head Latent Attention mechanism\nand employs the squared ReLU activation function to encourage sparsity, thereby\nreducing peak memory footprint during inference. During training, we collect\nand reorganize open-source datasets, implement a multi-phase training strategy,\nand empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning\nrate scheduler. Additionally, we incorporate Reinforcement Learning from Human\nFeedback (RLHF) by adopting the ARIES preference learning approach. Following a\ntwo-phase SFT process, this method yields performance gains of 2% in general\ntasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel\narchitecture, evaluation results demonstrate that PLM outperforms existing\nsmall language models trained on publicly available data while maintaining the\nlowest number of activated parameters. Furthermore, deployment across various\nedge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,\nvalidates PLM's suitability for peripheral applications. The PLM series models\nare publicly available at https://github.com/plm-team/PLM.\n","authors":["Cheng Deng","Luoyang Sun","Jiwen Jiang","Yongcheng Zeng","Xinjian Wu","Wenxin Zhao","Qingfa Xiao","Jiachuan Wang","Haoyang Li","Lei Chen","Lionel M. Ni","Haifeng Zhang","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2503.12167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09454v2","updated":"2025-03-19T15:23:04Z","published":"2025-03-12T14:57:08Z","title":"Explicit Learning and the LLM in Machine Translation","summary":"  This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.\n","authors":["Malik Marmonier","Rachel Bawden","Benoît Sagot"],"pdf_url":"https://arxiv.org/pdf/2503.09454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15299v1","updated":"2025-03-19T15:21:48Z","published":"2025-03-19T15:21:48Z","title":"Inside-Out: Hidden Factual Knowledge in LLMs","summary":"  This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first.\n","authors":["Zorik Gekhman","Eyal Ben David","Hadas Orgad","Eran Ofek","Yonatan Belinkov","Idan Szpector","Jonathan Herzig","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2503.15299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13857v2","updated":"2025-03-19T15:21:06Z","published":"2025-03-18T03:14:23Z","title":"Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles\n  with Large Language Model-Driven Evaluations","summary":"  Background. Systematic reviews in comparative effectiveness research require\ntimely evidence synthesis. Preprints accelerate knowledge dissemination but\nvary in quality, posing challenges for systematic reviews.\n  Methods. We propose AutoConfidence (automated confidence assessment), an\nadvanced framework for predicting preprint publication, which reduces reliance\non manual curation and expands the range of predictors, including three key\nadvancements: (1) automated data extraction using natural language processing\ntechniques, (2) semantic embeddings of titles and abstracts, and (3) large\nlanguage model (LLM)-driven evaluation scores. Additionally, we employed two\nprediction models: a random forest classifier for binary outcome and a survival\ncure model that predicts both binary outcome and publication risk over time.\n  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven\nscores, improving to 0.733 with semantic embeddings and 0.747 with article\nusage metrics. The survival cure model reached AUROC 0.716 with LLM-driven\nscores, improving to 0.731 with semantic embeddings. For publication risk\nprediction, it achieved a concordance index of 0.658, increasing to 0.667 with\nsemantic embeddings.\n  Conclusion. Our study advances the framework for preprint publication\nprediction through automated data extraction and multiple feature integration.\nBy combining semantic embeddings with LLM-driven evaluations, AutoConfidence\nenhances predictive performance while reducing manual annotation burden. The\nframework has the potential to facilitate systematic incorporation of preprint\narticles in evidence-based medicine, supporting researchers in more effective\nevaluation and utilization of preprint resources.\n","authors":["Rui Yang","Jiayi Tong","Haoyuan Wang","Hui Huang","Ziyang Hu","Peiyu Li","Nan Liu","Christopher J. Lindsell","Michael J. Pencina","Yong Chen","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2503.13857v2.pdf","comment":"28 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.15289v1","updated":"2025-03-19T15:09:39Z","published":"2025-03-19T15:09:39Z","title":"TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence\n  Tracing and Relationship Classification","summary":"  LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation.\n","authors":["Junnan Zhu","Min Xiao","Yining Wang","Feifei Zhai","Yu Zhou","Chengqing Zong"],"pdf_url":"https://arxiv.org/pdf/2503.15289v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2503.15272v1","updated":"2025-03-19T14:46:53Z","published":"2025-03-19T14:46:53Z","title":"MAMM-Refine: A Recipe for Improving Faithfulness in Generation with\n  Multi-Agent Collaboration","summary":"  Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe.\n","authors":["David Wan","Justin Chih-Yao Chen","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2503.15272v1.pdf","comment":"NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine"},{"id":"http://arxiv.org/abs/2503.15242v1","updated":"2025-03-19T14:19:57Z","published":"2025-03-19T14:19:57Z","title":"BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?","summary":"  We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.\n","authors":["Pierre Chambon","Baptiste Roziere","Benoit Sagot","Gabriel Synnaeve"],"pdf_url":"https://arxiv.org/pdf/2503.15242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13413v3","updated":"2025-03-19T14:18:01Z","published":"2025-03-17T17:42:51Z","title":"DLPO: Towards a Robust, Efficient, and Generalizable Prompt Optimization\n  Framework from a Deep-Learning Perspective","summary":"  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, largely driven by well-designed prompts. However, crafting and selecting\nsuch prompts often requires considerable human effort, significantly limiting\nits scalability. To mitigate this, recent studies have explored automated\nprompt optimization as a promising solution. Despite these efforts, existing\nmethods still face critical challenges in robustness, efficiency, and\ngeneralization. To systematically address these challenges, we first conduct an\nempirical analysis to identify the limitations of current reflection-based\nprompt optimization paradigm. Building on these insights, we propose 7\ninnovative approaches inspired by traditional deep learning paradigms for\nprompt optimization (DLPO), seamlessly integrating these concepts into\ntext-based gradient optimization. Through these advancements, we progressively\ntackle the aforementioned challenges and validate our methods through extensive\nexperimentation. We hope our study not only provides valuable guidance for\nfuture research but also offers a comprehensive understanding of the challenges\nand potential solutions in prompt optimization. Our code is available at\nhttps://github.com/sfasfaffa/DLPO.\n","authors":["Dengyun Peng","Yuhang Zhou","Qiguang Chen","Jinhao Liu","Jingjing Chen","Libo Qin"],"pdf_url":"https://arxiv.org/pdf/2503.13413v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.12896v2","updated":"2025-03-19T14:15:12Z","published":"2025-02-18T14:32:44Z","title":"None of the Others: a General Technique to Distinguish Reasoning from\n  Memorization in Multiple-Choice LLM Evaluation Benchmarks","summary":"  In LLM evaluations, reasoning is often distinguished from recall/memorization\nby performing numerical variations to math-oriented questions. Here we\nintroduce a general variation method for multiple-choice questions that\ncompletely dissociates the correct answer from previously seen tokens or\nconcepts, requiring LLMs to understand and reason (rather than memorizing) in\norder to answer correctly. Using this method, we evaluate state-of-the-art\nproprietary and open-source LLMs on two datasets available in English and\nSpanish: the public MMLU benchmark and the private UNED-Access 2024 dataset.\nResults show that all models experience remarkable accuracy drops under our\nproposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access\n2024, ranging from 10% to 93% across models. Notably, the most accurate model\nin our experimentation (OpenAI-o3-mini) is not the most robust\n(DeepSeek-R1-70B), suggesting that the best models in standard evaluations may\nnot be the ones with better reasoning capabilities. Also, we see larger\naccuracy drops in public (vs private) datasets and questions posed in their\noriginal language (vs a manual translation), which are signs of contamination\nand also point to a relevant role of recall/memorization in current LLMs'\nanswers.\n","authors":["Eva Sánchez Salido","Julio Gonzalo","Guillermo Marco"],"pdf_url":"https://arxiv.org/pdf/2502.12896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15235v1","updated":"2025-03-19T14:13:02Z","published":"2025-03-19T14:13:02Z","title":"Exploring Large Language Models for Word Games:Who is the Spy?","summary":"  Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy.\n","authors":["Chentian Wei","Jiewei Chen","Jinzhu Xu"],"pdf_url":"https://arxiv.org/pdf/2503.15235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15222v1","updated":"2025-03-19T14:01:33Z","published":"2025-03-19T14:01:33Z","title":"Model Hubs and Beyond: Analyzing Model Popularity, Performance, and\n  Documentation","summary":"  With the massive surge in ML models on platforms like Hugging Face, users\noften lose track and struggle to choose the best model for their downstream\ntasks, frequently relying on model popularity indicated by download counts,\nlikes, or recency. We investigate whether this popularity aligns with actual\nmodel performance and how the comprehensiveness of model documentation\ncorrelates with both popularity and performance. In our study, we evaluated a\ncomprehensive set of 500 Sentiment Analysis models on Hugging Face. This\nevaluation involved massive annotation efforts, with human annotators\ncompleting nearly 80,000 annotations, alongside extensive model training and\nevaluation. Our findings reveal that model popularity does not necessarily\ncorrelate with performance. Additionally, we identify critical inconsistencies\nin model card reporting: approximately 80\\% of the models analyzed lack\ndetailed information about the model, training, and evaluation processes.\nFurthermore, about 88\\% of model authors overstate their models' performance in\nthe model cards. Based on our findings, we provide a checklist of guidelines\nfor users to choose good models for downstream tasks.\n","authors":["Pritam Kadasi","Sriman Reddy","Srivathsa Vamsi Chaturvedula","Rudranshu Sen","Agnish Saha","Soumavo Sikdar","Sayani Sarkar","Suhani Mittal","Rohit Jindal","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2503.15222v1.pdf","comment":"Accepted to ICWSM'25"},{"id":"http://arxiv.org/abs/2503.15220v1","updated":"2025-03-19T14:00:55Z","published":"2025-03-19T14:00:55Z","title":"Entity-aware Cross-lingual Claim Detection for Automated Fact-checking","summary":"  Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite significant progress in the task, there remain open\nchallenges such as dealing with multilingual and multimodal data prevalent in\nonline discourse. Addressing the multilingual challenge, recent efforts have\nfocused on fine-tuning pre-trained multilingual language models. While these\nmodels can handle multiple languages, their ability to effectively transfer\ncross-lingual knowledge for detecting claims spreading on social media remains\nunder-explored. In this paper, we introduce \\textit{EX-Claim}, an entity-aware\ncross-lingual claim detection model that generalizes well to handle claims\nwritten in any language. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model significantly outperforms the\nbaselines, across 27 languages, and achieves the highest rate of knowledge\ntransfer, even with limited training data.\n","authors":["Rrubaa Panchendrarajan","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2503.15220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09587v2","updated":"2025-03-19T13:51:54Z","published":"2024-11-14T16:57:46Z","title":"BabyLM Challenge: Exploring the Effect of Variation Sets on Language\n  Model Training Efficiency","summary":"  While current large language models have achieved a remarkable success, their\ndata efficiency remains a challenge to overcome. Recently it has been suggested\nthat child-directed speech (CDS) can improve training data efficiency of modern\nlanguage models based on Transformer neural networks. However, it is not yet\nunderstood which specific properties of CDS are effective for training these\nmodels. In the context of the BabyLM Challenge, we focus on Variation Sets\n(VSs), sets of consecutive utterances expressing a similar intent with slightly\ndifferent words and structures, which are ubiquitous in CDS. To assess the\nimpact of VSs on training data efficiency, we augment CDS data with different\nproportions of artificial VSs and use these datasets to train an\nauto-regressive model, GPT-2. We find that the best proportion of VSs depends\non the evaluation benchmark: BLiMP and GLUE scores benefit from the presence of\nVSs, but EWOK scores do not. Additionally, the results vary depending on\nmultiple factors such as the number of epochs and the order of utterance\npresentation. Taken together, these findings suggest that VSs can have a\nbeneficial influence on language models, while leaving room for further\ninvestigation.\n","authors":["Akari Haga","Akiyo Fukatsu","Miyu Oba","Arianna Bisazza","Yohei Oseki"],"pdf_url":"https://arxiv.org/pdf/2411.09587v2.pdf","comment":"Accepted by BabyLM challenge 2024 at CONLL 2024 (\n  https://aclanthology.org/2024.conll-babylm.23 )"},{"id":"http://arxiv.org/abs/2503.15204v1","updated":"2025-03-19T13:47:25Z","published":"2025-03-19T13:47:25Z","title":"When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection","summary":"  Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.\n","authors":["Tittaya Mairittha","Tanakon Sawanglok","Panuwit Raden","Sorrawit Treesuk"],"pdf_url":"https://arxiv.org/pdf/2503.15204v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.09283v2","updated":"2025-03-19T13:41:21Z","published":"2024-07-12T14:13:59Z","title":"DAHRS: Divergence-Aware Hallucination-Remediated SRL Projection","summary":"  Semantic role labeling (SRL) enriches many downstream applications, e.g.,\nmachine translation, question answering, summarization, and stance/belief\ndetection. However, building multilingual SRL models is challenging due to the\nscarcity of semantically annotated corpora for multiple languages. Moreover,\nstate-of-the-art SRL projection (XSRL) based on large language models (LLMs)\nyields output that is riddled with spurious role labels. Remediation of such\nhallucinations is not straightforward due to the lack of explainability of\nLLMs. We show that hallucinated role labels are related to naturally occurring\ndivergence types that interfere with initial alignments. We implement\nDivergence-Aware Hallucination-Remediated SRL projection (DAHRS), leveraging\nlinguistically-informed alignment remediation followed by greedy First-Come\nFirst-Assign (FCFA) SRL projection. DAHRS improves the accuracy of SRL\nprojection without additional transformer-based machinery, beating XSRL in both\nhuman and automatic comparisons, and advancing beyond headwords to accommodate\nphrase-level SRL projection (e.g., EN-FR, EN-ES). Using CoNLL-2009 as our\nground truth, we achieve a higher word-level F1 over XSRL: 87.6% vs. 77.3%\n(EN-FR) and 89.0% vs. 82.7% (EN-ES). Human phrase-level assessments yield 89.1%\n(EN-FR) and 91.0% (EN-ES). We also define a divergence metric to adapt our\napproach to other language pairs (e.g., English-Tagalog).\n","authors":["Sangpil Youm","Brodie Mather","Chathuri Jayaweera","Juliana Prada","Bonnie Dorr"],"pdf_url":"https://arxiv.org/pdf/2407.09283v2.pdf","comment":"15 pages, 6 figures, Accepted to The 29th International Conference on\n  Natural Language & Information Systems (NLDB 2024)"},{"id":"http://arxiv.org/abs/2503.15176v1","updated":"2025-03-19T13:02:01Z","published":"2025-03-19T13:02:01Z","title":"A Review on Large Language Models for Visual Analytics","summary":"  This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.\n","authors":["Navya Sonal Agarwal","Sanjay Kumar Sonbhadra"],"pdf_url":"https://arxiv.org/pdf/2503.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14324v2","updated":"2025-03-19T12:58:33Z","published":"2025-03-18T14:56:46Z","title":"DualToken: Towards Unifying Visual Understanding and Generation with\n  Dual Visual Vocabularies","summary":"  The differing representation spaces required for visual understanding and\ngeneration pose a challenge in unifying them within the autoregressive paradigm\nof large language models. A vision tokenizer trained for reconstruction excels\nat capturing low-level perceptual details, making it well-suited for visual\ngeneration but lacking high-level semantic representations for understanding\ntasks. Conversely, a vision encoder trained via contrastive learning aligns\nwell with language but struggles to decode back into the pixel space for\ngeneration tasks. To bridge this gap, we propose DualToken, a method that\nunifies representations for both understanding and generation within a single\ntokenizer. However, directly integrating reconstruction and semantic objectives\nin a single tokenizer creates conflicts, leading to degraded performance in\nboth reconstruction quality and semantic performance. Instead of forcing a\nsingle codebook to handle both semantic and perceptual information, DualToken\ndisentangles them by introducing separate codebooks for high and low-level\nfeatures, effectively transforming their inherent conflict into a synergistic\nrelationship. As a result, DualToken achieves state-of-the-art performance in\nboth reconstruction and semantic tasks while demonstrating remarkable\neffectiveness in downstream MLLM understanding and generation tasks. Notably,\nwe also show that DualToken, as a unified tokenizer, surpasses the naive\ncombination of two distinct types vision encoders, providing superior\nperformance within a unified MLLM.\n","authors":["Wei Song","Yuran Wang","Zijia Song","Yadong Li","Haoze Sun","Weipeng Chen","Zenan Zhou","Jianhua Xu","Jiaqi Wang","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15169v1","updated":"2025-03-19T12:51:52Z","published":"2025-03-19T12:51:52Z","title":"Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks","summary":"  This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs.\n","authors":["Yuting Guo","Abeed Sarker"],"pdf_url":"https://arxiv.org/pdf/2503.15169v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2503.15166v1","updated":"2025-03-19T12:47:37Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.11509v2","updated":"2025-03-19T12:42:41Z","published":"2025-03-14T15:29:58Z","title":"TikZero: Zero-Shot Text-Guided Graphics Program Synthesis","summary":"  With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.\n","authors":["Jonas Belouadi","Eddy Ilg","Margret Keuper","Hideki Tanaka","Masao Utiyama","Raj Dabre","Steffen Eger","Simone Paolo Ponzetto"],"pdf_url":"https://arxiv.org/pdf/2503.11509v2.pdf","comment":"Project page: https://github.com/potamides/DeTikZify"},{"id":"http://arxiv.org/abs/2503.11302v2","updated":"2025-03-19T12:17:11Z","published":"2025-03-14T11:11:03Z","title":"Are formal and functional linguistic mechanisms dissociated in language\n  models?","summary":"  Although large language models (LLMs) are increasingly capable, these\ncapabilities are unevenly distributed: they excel at formal linguistic tasks,\nsuch as producing fluent, grammatical text, but struggle more with functional\nlinguistic tasks like reasoning and consistent fact retrieval. Inspired by\nneuroscience, recent work suggests that to succeed on both formal and\nfunctional linguistic tasks, LLMs should use different mechanisms for each;\nsuch localization could either be built-in or emerge spontaneously through\ntraining. In this paper, we ask: do current models, with fast-improving\nfunctional linguistic abilities, exhibit distinct localization of formal and\nfunctional linguistic mechanisms? We answer this by finding and comparing the\n\"circuits\", or minimal computational subgraphs, responsible for various formal\nand functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that\nwhile there is indeed little overlap between circuits for formal and functional\ntasks, there is also little overlap between formal linguistic tasks, as exists\nin the human brain. Thus, a single formal linguistic network, unified and\ndistinct from functional task circuits, remains elusive. However, in terms of\ncross-task faithfulness - the ability of one circuit to solve another's task -\nwe observe a separation between formal and functional mechanisms, suggesting\nthat shared mechanisms between formal tasks may exist.\n","authors":["Michael Hanna","Sandro Pezzelle","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2503.11302v2.pdf","comment":"35 pages, 10 figures, 3 tables. Code available at\n  https://github.com/hannamw/formal-functional-dissociation"},{"id":"http://arxiv.org/abs/2503.11280v2","updated":"2025-03-19T12:16:42Z","published":"2025-03-14T10:39:27Z","title":"High-Dimensional Interlingual Representations of Large Language Models","summary":"  Large language models (LLMs) trained on massive multilingual datasets hint at\nthe formation of interlingual constructs--a shared subspace in the\nrepresentation space. However, evidence regarding this phenomenon is mixed,\nleaving it unclear whether these models truly develop unified interlingual\nrepresentations, or present a partially aligned constructs. We explore 31\ndiverse languages varying on their resource-levels, typologies, and\ngeographical regions; and find that multilingual LLMs exhibit inconsistent\ncross-lingual alignments. To address this, we propose an interlingual\nrepresentation framework identifying both the shared interlingual semantic\nsubspace and fragmented components, existed due to representational\nlimitations. We introduce Interlingual Local Overlap (ILO) score to quantify\ninterlingual alignment by comparing the local neighborhood structures of\nhigh-dimensional representations. We utilize ILO to investigate the impact of\nsingle-language fine-tuning on the interlingual representations in multilingual\nLLMs. Our results indicate that training exclusively on a single language\ndisrupts the alignment in early layers, while freezing these layers preserves\nthe alignment of interlingual representations, leading to improved\ncross-lingual generalization. These results validate our framework and metric\nfor evaluating interlingual representation, and further underscore that\ninterlingual alignment is crucial for scalable multilingual learning.\n","authors":["Bryan Wilie","Samuel Cahyawijaya","Junxian He","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2503.11280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15176v3","updated":"2025-03-19T12:15:10Z","published":"2024-07-21T14:23:37Z","title":"ReAttention: Training-Free Infinite Context with Finite Attention Scope","summary":"  The long-context capability of the Large Language Models (LLM) has made\nsignificant breakthroughs, but the maximum supported context length in length\nextrapolation remains a critical bottleneck limiting their practical\napplications. The constraint of context length in LLMs arises from the\nself-attention mechanism, which cannot effectively and efficiently capture the\nsemantic relationships within infinitely long contexts via the limited\npre-trained positional information and attention scope. In this work, we\npropose ReAttention, a training-free approach enabling LLM based on the\nself-attention mechanism to support an infinite context with a finite attention\nscope under sufficient memory resources. ReAttention performs the\nposition-agnostic top-$k$ attention before the ordinary position-aware\nself-attention, freeing LLMs from the length extrapolation issue. We validate\nthe performance of ReAttention on the LongBench, L-Eval, and InfiniteBench and\ndemonstrate that it is on par with traditional methods. Furthermore, we also\napply ReAttention on mainstream LLMs, including LLaMA3.1-8B and\nMistral-v0.3-7B, enabling them to support context lengths of at least 1M and\neven expanding the context length of LLaMA3.2-3B-chat by 128$\\times$ to 4M\nwithout any further training in Needle-In-A-Haystack tests. We also improve the\nefficiency of ReAttention with Triton and achieve an efficient extrapolation\nwithout additional overhead. The code is available at\nhttps://github.com/OpenMOSS/ReAttention.\n","authors":["Xiaoran Liu","Ruixiao Li","Qipeng Guo","Zhigeng Liu","Yuerong Song","Kai Lv","Hang Yan","Linlin Li","Qun Liu","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2407.15176v3.pdf","comment":"21 pages, 11 figures, Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.15133v1","updated":"2025-03-19T11:48:52Z","published":"2025-03-19T11:48:52Z","title":"EmoGRACE: Aspect-based emotion analysis for social media data","summary":"  While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data.\n","authors":["Christina Zorenböhmer","Sebastian Schmidt","Bernd Resch"],"pdf_url":"https://arxiv.org/pdf/2503.15133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15128v1","updated":"2025-03-19T11:42:33Z","published":"2025-03-19T11:42:33Z","title":"Increasing the Robustness of the Fine-tuned Multilingual\n  Machine-Generated Text Detectors","summary":"  Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data.\n","authors":["Dominik Macko","Robert Moro","Ivan Srba"],"pdf_url":"https://arxiv.org/pdf/2503.15128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16457v4","updated":"2025-03-19T11:37:27Z","published":"2025-02-23T06:16:23Z","title":"Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge","summary":"  Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.\n","authors":["Heegyu Kim","Taeyang Jeon","Seungtaek Choi","Ji Hoon Hong","Dong Won Jeon","Ga-Yeon Baek","Gyeong-Won Kwak","Dong-Hee Lee","Jisu Bae","Chihoon Lee","Yunseo Kim","Seon-Jin Choi","Jin-Seong Park","Sung Beom Cho","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2502.16457v4.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2503.15124v1","updated":"2025-03-19T11:33:40Z","published":"2025-03-19T11:33:40Z","title":"Evaluating ASR Confidence Scores for Automated Error Detection in\n  User-Assisted Correction Interfaces","summary":"  Despite advances in Automatic Speech Recognition (ASR), transcription errors\npersist and require manual correction. Confidence scores, which indicate the\ncertainty of ASR results, could assist users in identifying and correcting\nerrors. This study evaluates the reliability of confidence scores for error\ndetection through a comprehensive analysis of end-to-end ASR models and a user\nstudy with 36 participants. The results show that while confidence scores\ncorrelate with transcription accuracy, their error detection performance is\nlimited. Classifiers frequently miss errors or generate many false positives,\nundermining their practical utility. Confidence-based error detection neither\nimproved correction efficiency nor was perceived as helpful by participants.\nThese findings highlight the limitations of confidence scores and the need for\nmore sophisticated approaches to improve user interaction and explainability of\nASR results.\n","authors":["Korbinian Kuhn","Verena Kersken","Gottfried Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2503.15124v1.pdf","comment":"7 pages, 1 figure, to be published in Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems (CHI EA '25)"},{"id":"http://arxiv.org/abs/2405.02732v2","updated":"2025-03-19T11:31:31Z","published":"2024-05-04T18:32:08Z","title":"Recall Them All: Retrieval-Augmented Language Models for Long Object\n  List Extraction from Long Documents","summary":"  Methods for relation extraction from text mostly focus on high precision, at\nthe cost of limited recall. High recall is crucial, though, to populate long\nlists of object entities that stand in a specific relation with a given\nsubject. Cues for relevant objects can be spread across many passages in long\ntexts. This poses the challenge of extracting long lists from long texts. We\npresent the L3X method which tackles the problem in two stages: (1)\nrecall-oriented generation using a large language model (LLM) with judicious\ntechniques for retrieval augmentation, and (2) precision-oriented\nscrutinization to validate or prune candidates. Our L3X method outperforms\nLLM-only generations by a substantial margin.\n","authors":["Sneha Singhania","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2405.02732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15117v1","updated":"2025-03-19T11:21:37Z","published":"2025-03-19T11:21:37Z","title":"Exploring Model Editing for LLM-based Aspect-Based Sentiment\n  Classification","summary":"  Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy.\n","authors":["Shichen Li","Zhongqing Wang","Zheyu Zhao","Yue Zhang","Peifeng Li"],"pdf_url":"https://arxiv.org/pdf/2503.15117v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2503.15092v1","updated":"2025-03-19T10:44:37Z","published":"2025-03-19T10:44:37Z","title":"Towards Understanding the Safety Boundaries of DeepSeek Models:\n  Evaluation and Findings","summary":"  This study presents the first comprehensive safety evaluation of the DeepSeek\nmodels, focusing on evaluating the safety risks associated with their generated\ncontent. Our evaluation encompasses DeepSeek's latest generation of large\nlanguage models, multimodal large language models, and text-to-image models,\nsystematically examining their performance regarding unsafe content generation.\nNotably, we developed a bilingual (Chinese-English) safety evaluation dataset\ntailored to Chinese sociocultural contexts, enabling a more thorough evaluation\nof the safety capabilities of Chinese-developed models. Experimental results\nindicate that despite their strong general capabilities, DeepSeek models\nexhibit significant safety vulnerabilities across multiple risk dimensions,\nincluding algorithmic discrimination and sexual content. These findings provide\ncrucial insights for understanding and improving the safety of large foundation\nmodels. Our code is available at\nhttps://github.com/NY1024/DeepSeek-Safety-Eval.\n","authors":["Zonghao Ying","Guangyi Zheng","Yongxin Huang","Deyue Zhang","Wenxin Zhang","Quanchen Zou","Aishan Liu","Xianglong Liu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.15092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15057v1","updated":"2025-03-19T09:49:22Z","published":"2025-03-19T09:49:22Z","title":"A Data-driven Investigation of Euphemistic Language: Comparing the usage\n  of \"slave\" and \"servant\" in 19th century US newspapers","summary":"  This study investigates the usage of \"slave\" and \"servant\" in the 19th\ncentury US newspapers using computational methods. While both terms were used\nto refer to enslaved African Americans, they were used in distinct ways. In the\nChronicling America corpus, we included possible OCR errors by using FastText\nembedding and excluded text reprints to consider text reprint culture in the\n19th century. Word2vec embedding was used to find semantically close words to\n\"slave\" and \"servant\" and log-odds ratio was calculated to identify\nover-represented discourse words in the Southern and Northern newspapers. We\nfound that \"slave\" is associated with socio-economic, legal, and administrative\nwords, however, \"servant\" is linked to religious words in the Northern\nnewspapers while Southern newspapers associated \"servant\" with domestic and\nfamilial words. We further found that slave discourse words in Southern\nnewspapers are more prevalent in Northern newspapers while servant discourse\nwords from each side are prevalent in their own region. This study contributes\nto the understanding of how newspapers created different discourses around\nenslaved African Americans in the 19th century US.\n","authors":["Jaihyun Park","Ryan Cordell"],"pdf_url":"https://arxiv.org/pdf/2503.15057v1.pdf","comment":"The 5th International Conference on Natural Language Processing for\n  Digital Humanities (NLP4DH)"},{"id":"http://arxiv.org/abs/2503.15055v1","updated":"2025-03-19T09:46:54Z","published":"2025-03-19T09:46:54Z","title":"ELTEX: A Framework for Domain-Driven Synthetic Data Generation","summary":"  We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains.\n","authors":["Arina Razmyslovich","Kseniia Murasheva","Sofia Sedlova","Julien Capitaine","Eugene Dmitriev"],"pdf_url":"https://arxiv.org/pdf/2503.15055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19753v3","updated":"2025-03-19T09:37:10Z","published":"2024-09-29T16:08:45Z","title":"CoTKR: Chain-of-Thought Enhanced Knowledge Rewriting for Complex\n  Knowledge Graph Question Answering","summary":"  Recent studies have explored the use of Large Language Models (LLMs) with\nRetrieval Augmented Generation (RAG) for Knowledge Graph Question Answering\n(KGQA). They typically require rewriting retrieved subgraphs into natural\nlanguage formats comprehensible to LLMs. However, when tackling complex\nquestions, the knowledge rewritten by existing methods may include irrelevant\ninformation, omit crucial details, or fail to align with the question's\nsemantics. To address them, we propose a novel rewriting method CoTKR,\nChain-of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces\nand corresponding knowledge in an interleaved manner, thereby mitigating the\nlimitations of single-step knowledge rewriting. Additionally, to bridge the\npreference gap between the knowledge rewriter and the question answering (QA)\nmodel, we propose a training strategy PAQAF, Preference Alignment from Question\nAnswering Feedback, for leveraging feedback from the QA model to further\noptimize the knowledge rewriter. We conduct experiments using various LLMs\nacross several KGQA benchmarks. Experimental results demonstrate that, compared\nwith previous knowledge rewriting methods, CoTKR generates the most beneficial\nknowledge representation for QA models, which significantly improves the\nperformance of LLMs in KGQA.\n","authors":["Yike Wu","Yi Huang","Nan Hu","Yuncheng Hua","Guilin Qi","Jiaoyan Chen","Jeff Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2409.19753v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15044v1","updated":"2025-03-19T09:32:52Z","published":"2025-03-19T09:32:52Z","title":"SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in\n  Machine-Generated Text Detection","summary":"  The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.\n","authors":["Haoyi Li","Angela Yifei Yuan","Soyeon Caren Han","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2503.15044v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.06793v2","updated":"2025-03-19T09:24:09Z","published":"2024-08-13T10:25:13Z","title":"Layerwise Recurrent Router for Mixture-of-Experts","summary":"  The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE .\n","authors":["Zihan Qiu","Zeyu Huang","Shuang Cheng","Yizhi Zhou","Zili Wang","Ivan Titov","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2408.06793v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15003v1","updated":"2025-03-19T08:52:59Z","published":"2025-03-19T08:52:59Z","title":"LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?","summary":"  Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language.\n","authors":["Amr Keleg"],"pdf_url":"https://arxiv.org/pdf/2503.15003v1.pdf","comment":"Accepted to the C3NLP workshop (Co-located with NAACL 2025)"},{"id":"http://arxiv.org/abs/2503.14996v1","updated":"2025-03-19T08:45:03Z","published":"2025-03-19T08:45:03Z","title":"Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM\n  Evaluation in Multiple-Choice Question Answering","summary":"  One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices.\n","authors":["Francesco Maria Molfese","Luca Moroni","Luca Gioffrè","Alessandro Scirè","Simone Conia","Roberto Navigli"],"pdf_url":"https://arxiv.org/pdf/2503.14996v1.pdf","comment":"17 pages (9 main), 11 figures, 21 tables"},{"id":"http://arxiv.org/abs/2503.14991v1","updated":"2025-03-19T08:36:58Z","published":"2025-03-19T08:36:58Z","title":"Inspecting the Representation Manifold of Differentially-Private Text","summary":"  Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space.\n","authors":["Stefan Arnold"],"pdf_url":"https://arxiv.org/pdf/2503.14991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14985v1","updated":"2025-03-19T08:31:39Z","published":"2025-03-19T08:31:39Z","title":"ML-Triton, A Multi-Level Compilation and Language Extension to Triton\n  GPU Programming","summary":"  In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean.\n","authors":["Dewei Wang","Wei Zhu","Liyang Ling","Ettore Tiotto","Quintin Wang","Whitney Tsang","Julian Opperman","Jacky Deng"],"pdf_url":"https://arxiv.org/pdf/2503.14985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17250v2","updated":"2025-03-19T08:24:14Z","published":"2024-10-22T17:59:56Z","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","summary":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.\n","authors":["Shota Onohara","Atsuyuki Miyai","Yuki Imajuku","Kazuki Egashira","Jeonghun Baek","Xiang Yue","Graham Neubig","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2410.17250v2.pdf","comment":"Accepted at NAACL 2025. Project page:\n  https://mmmu-japanese-benchmark.github.io/JMMMU/"},{"id":"http://arxiv.org/abs/2503.14345v2","updated":"2025-03-19T07:17:41Z","published":"2025-03-18T15:25:08Z","title":"MoonCast: High-Quality Zero-Shot Podcast Generation","summary":"  Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.\n","authors":["Zeqian Ju","Dongchao Yang","Jianwei Yu","Kai Shen","Yichong Leng","Zhengtao Wang","Xu Tan","Xinyu Zhou","Tao Qin","Xiangyang Li"],"pdf_url":"https://arxiv.org/pdf/2503.14345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12524v2","updated":"2025-03-19T07:09:24Z","published":"2025-03-16T14:39:33Z","title":"EXAONE Deep: Reasoning Enhanced Language Models","summary":"  We present EXAONE Deep series, which exhibits superior capabilities in\nvarious reasoning tasks, including math and coding benchmarks. We train our\nmodels mainly on the reasoning-specialized dataset that incorporates long\nstreams of thought processes. Evaluation results show that our smaller models,\nEXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while\nthe largest model, EXAONE Deep 32B, demonstrates competitive performance\nagainst leading open-weight models. All EXAONE Deep models are openly available\nfor research purposes and can be downloaded from\nhttps://huggingface.co/LGAI-EXAONE\n","authors":["LG AI Research","Kyunghoon Bae","Eunbi Choi","Kibong Choi","Stanley Jungkyu Choi","Yemuk Choi","Seokhee Hong","Junwon Hwang","Hyojin Jeon","Kijeong Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Hyosang Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Yongil Kim","Youchul Kim","Edward Hwayoung Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Sangha Park","Yongmin Park","Sihoon Yang","Heuiyeen Yeen","Sihyuk Yi","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2503.12524v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2412.04862,\n  arXiv:2408.03541"},{"id":"http://arxiv.org/abs/2411.05060v2","updated":"2025-03-19T06:52:27Z","published":"2024-11-07T18:47:39Z","title":"A Guide to Misinformation Detection Data and Evaluation","summary":"  Misinformation is a complex societal issue, and mitigating solutions are\ndifficult to create due to data deficiencies. To address this, we have curated\nthe largest collection of (mis)information datasets in the literature, totaling\n75. From these, we evaluated the quality of 36 datasets that consist of\nstatements or claims, as well as the 9 datasets that consist of data in purely\nparagraph form. We assess these datasets to identify those with solid\nfoundations for empirical work and those with flaws that could result in\nmisleading and non-generalizable results, such as spurious correlations, or\nexamples that are ambiguous or otherwise impossible to assess for veracity. We\nfind the latter issue is particularly severe and affects most datasets in the\nliterature. We further provide state-of-the-art baselines on all these\ndatasets, but show that regardless of label quality, categorical labels may no\nlonger give an accurate evaluation of detection model performance. Finally, we\nwe propose and highlight Evaluation Quality Assessment (EQA) as a tool to guide\nthe field toward systemic solutions rather than inadvertently propagating\nissues in evaluation. Overall, this guide aims to provide a roadmap for higher\nquality data and better grounded evaluations, ultimately improving research in\nmisinformation detection. All datasets and other artifacts are available at\nmisinfo-datasets.complexdatalab.com.\n","authors":["Camille Thibault","Jacob-Junqi Tian","Gabrielle Peloquin-Skulski","Taylor Lynn Curtis","James Zhou","Florence Laflamme","Yuxiang Guan","Reihaneh Rabbany","Jean-François Godbout","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2411.05060v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14926v1","updated":"2025-03-19T06:26:25Z","published":"2025-03-19T06:26:25Z","title":"Covering Cracks in Content Moderation: Delexicalized Distant Supervision\n  for Illicit Drug Jargon Detection","summary":"  In light of rising drug-related concerns and the increasing role of social\nmedia, sales and discussions of illicit drugs have become commonplace online.\nSocial media platforms hosting user-generated content must therefore perform\ncontent moderation, which is a difficult task due to the vast amount of jargon\nused in drug discussions. Previous works on drug jargon detection were limited\nto extracting a list of terms, but these approaches have fundamental problems\nin practical application. First, they are trivially evaded using word\nsubstitutions. Second, they cannot distinguish whether euphemistic terms such\nas \"pot\" or \"crack\" are being used as drugs or in their benign meanings. We\nargue that drug content moderation should be done using contexts rather than\nrelying on a banlist. However, manually annotated datasets for training such a\ntask are not only expensive but also prone to becoming obsolete. We present\nJEDIS, a framework for detecting illicit drug jargon terms by analyzing their\ncontexts. JEDIS utilizes a novel approach that combines distant supervision and\ndelexicalization, which allows JEDIS to be trained without human-labeled data\nwhile being robust to new terms and euphemisms. Experiments on two manually\nannotated datasets show JEDIS significantly outperforms state-of-the-art\nword-based baselines in terms of F1-score and detection coverage in drug jargon\ndetection. We also conduct qualitative analysis that demonstrates JEDIS is\nrobust against pitfalls faced by existing approaches.\n","authors":["Minkyoo Song","Eugene Jang","Jaehan Kim","Seungwon Shin"],"pdf_url":"https://arxiv.org/pdf/2503.14926v1.pdf","comment":"Accepted for publication in the KDD 2025 Research Track"},{"id":"http://arxiv.org/abs/2410.12444v2","updated":"2025-03-19T06:22:38Z","published":"2024-10-16T10:48:14Z","title":"Expanding Chatbot Knowledge in Customer Service: Context-Aware Similar\n  Question Generation Using Large Language Models","summary":"  Service chatbots play an important role in enhancing customer support by\ndelivering timely responses to diverse queries. Traditionally, these chatbots\nrely on retrieval-based methods constrained by a predefined knowledge base of\nquestion-answer (QA) pairs to guarantee reliable responses. To effectively\nhandle varied customer inquiries, augmenting the knowledge base with similar\nquestions that maintain semantic consistency and linguistic variability is\ncrucial. This paper presents methodologies for a novel approach that utilizes\nLarge Language Models (LLMs) for generating similar questions and selecting an\noptimal subset of questions for knowledge base augmentation in industrial\nchatbots. Specifically, we define the SQG task in the context of LLM training\nand propose a one-to-many objective that incorporates contextual information.\nWe also introduce an optimization framework that selects a diverse subset of\nsimilar questions within predefined resource constraints. Experimental results\ndemonstrate significant improvements over traditional methods, achieving\ngreater semantic diversity while aligning with source QA pairs, with over 120%\nrelative improvement in meeting business-specific requirements with human\nevaluation. Combined with several best practices, we provide a robust,\napplication-driven solution for enhancing chatbot performance and improving\ncustomer service satisfaction.\n","authors":["Mengze Hong","Chen Jason Zhang","Di Jiang","Yuanfeng Song","Lu Wang","Yuanqin He","Zhiyang Su","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2410.12444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10177v2","updated":"2025-03-19T06:22:21Z","published":"2025-03-13T08:58:10Z","title":"PRISM: Preference Refinement via Implicit Scene Modeling for 3D\n  Vision-Language Preference-Based Reinforcement Learning","summary":"  We propose PRISM, a novel framework designed to overcome the limitations of\n2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point\ncloud modeling and future-aware preference refinement. At its core, PRISM\nadopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and\nviewpoint biases, ensuring more stable and spatially consistent preference\nsignals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to\nincorporate long-horizon considerations, thereby preventing the short-sighted\nfeedback often seen in static preference comparisons. In contrast to\nconventional PBRL techniques, this integration of 3D perception and\nfuture-oriented reasoning leads to significant gains in preference agreement\nrates, faster policy convergence, and robust generalization across unseen\nrobotic environments. Our empirical results, spanning tasks such as robotic\nmanipulation and autonomous navigation, highlight PRISM's potential for\nreal-world applications where precise spatial understanding and reliable\nlong-term decision-making are critical. By bridging 3D geometric awareness with\nCoT-driven preference modeling, PRISM establishes a comprehensive foundation\nfor scalable, human-aligned reinforcement learning.\n","authors":["Yirong Sun","Yanjun Chen"],"pdf_url":"https://arxiv.org/pdf/2503.10177v2.pdf","comment":"I withdraw arXiv:2503.10177 due to critical computational errors\n  invalidating its conclusions and the withdrawal of consent from co-author\n  Yanjun Chen"},{"id":"http://arxiv.org/abs/2412.09049v2","updated":"2025-03-19T06:14:04Z","published":"2024-12-12T08:19:01Z","title":"Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues","summary":"  Discovering customer intentions in dialogue conversations is crucial for\nautomated service agents. Yet, existing intent clustering methods often fail to\nalign with human perceptions due to the heavy reliance on embedding distance\nmetrics and sentence embeddings. To address these limitations, we propose\nintegrating the semantic understanding capabilities of LLMs into an\n$\\textbf{LLM-in-the-loop (LLM-ITL)}$ intent clustering framework. Specifically,\nthis paper (1) investigates the effectiveness of fine-tuned LLMs in semantic\ncoherence evaluation and intent cluster naming, achieving over 95% accuracy;\n(2) designs an LLM-ITL clustering algorithm that facilitates the iterative\ndiscovery of coherent intent clusters; and (3) proposes task-specific\ntechniques tailored for customer service dialogue intent clustering. Since\nexisting English benchmarks pose limited semantic diversity and intent labels,\nwe introduced a comprehensive Chinese dialogue intent dataset, comprising over\n100,000 real customer service calls and 1,507 human-annotated intent clusters.\nThe proposed approaches significantly outperformed LLM-guided baselines,\nachieving notable improvements in clustering quality and a 12% boost in the\ndownstream intent classification task. Combined with several best practices,\nour findings highlight the potential of LLM-in-the-loop techniques for scalable\nand human-aligned problem-solving. Sample code and datasets are available at:\nhttps://anonymous.4open.science/r/Dial-in-LLM-0410.\n","authors":["Mengze Hong","Di Jiang","Yuanfeng Song","Lu Wang","Wailing Ng","Yanjie Sun","Chen Jason Zhang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.09049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14917v1","updated":"2025-03-19T05:50:21Z","published":"2025-03-19T05:50:21Z","title":"MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large\n  Language Models","summary":"  High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs.\n","authors":["Jiazheng Li","Lu Yu","Qing Cui","Zhiqiang Zhang","Jun Zhou","Yanfang Ye","Chuxu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14900v1","updated":"2025-03-19T04:58:45Z","published":"2025-03-19T04:58:45Z","title":"Deep Contrastive Unlearning for Language Models","summary":"  The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods.\n","authors":["Estrid He","Tabinda Sarwar","Ibrahim Khalil","Xun Yi","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2503.14900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14895v1","updated":"2025-03-19T04:39:45Z","published":"2025-03-19T04:39:45Z","title":"Mitigating Object Hallucinations in MLLMs via Multi-Frequency\n  Perturbations","summary":"  Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark.\n","authors":["Shuo Li","Jiajun Sun","Guodong Zheng","Xiaoran Fan","Yujiong Shen","Yi Lu","Zhiheng Xi","Yuming Yang","Wenming Tan","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2503.14895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14891v1","updated":"2025-03-19T04:36:35Z","published":"2025-03-19T04:36:35Z","title":"MetaLadder: Ascending Mathematical Solution Quality via\n  Analogical-Problem Reasoning Transfer","summary":"  Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder.\n","authors":["Honglin Lin","Zhuoshi Pan","Yu Li","Qizhi Pei","Xin Gao","Mengzhang Cai","Conghui He","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2503.14891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08598v4","updated":"2025-03-19T04:25:35Z","published":"2024-06-12T19:05:43Z","title":"Language Model Council: Democratically Benchmarking Foundation Models on\n  Highly Subjective Tasks","summary":"  As Large Language Models (LLMs) continue to evolve, evaluating them remains a\npersistent challenge. Many recent evaluations use LLMs as judges to score\noutputs from other LLMs, often relying on a single large model like GPT-4o.\nHowever, using a single LLM judge is prone to intra-model bias, and many tasks\n- such as those related to emotional intelligence, creative writing, and\npersuasiveness - may be too subjective for a single model to judge fairly. We\nintroduce the Language Model Council (LMC), where a group of LLMs collaborate\nto create tests, respond to them, and evaluate each other's responses to\nproduce a ranking in a democratic fashion. Unlike previous approaches that\nfocus on reducing cost or bias by using a panel of smaller models, our work\nexamines the benefits and nuances of a fully inclusive LLM evaluation system.\nIn a detailed case study on emotional intelligence, we deploy a council of 20\nrecent LLMs to rank each other on open-ended responses to interpersonal\nconflicts. Our results show that the LMC produces rankings that are more\nseparable and more robust, and through a user study, we show that they are more\nconsistent with human evaluations than any individual LLM judge. Using all LLMs\nfor judging can be costly, however, so we use Monte Carlo simulations and\nhand-curated sub-councils to study hypothetical council compositions and\ndiscuss the value of the incremental LLM judge.\n","authors":["Justin Zhao","Flor Miriam Plaza-del-Arco","Benjamin Genchel","Amanda Cercas Curry"],"pdf_url":"https://arxiv.org/pdf/2406.08598v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13837v2","updated":"2025-03-19T04:09:17Z","published":"2025-03-18T02:21:07Z","title":"Self-Vocabularizing Training for Neural Machine Translation","summary":"  Past vocabulary learning techniques identify relevant vocabulary before\ntraining, relying on statistical and entropy-based assumptions that largely\nneglect the role of model training. Empirically, we observe that trained\ntranslation models are induced to use a byte-pair encoding (BPE) vocabulary\nsubset distinct from the original BPE vocabulary, leading to performance\nimprovements when retrained with the induced vocabulary. In this paper, we\nanalyze this discrepancy in neural machine translation by examining vocabulary\nand entropy shifts during self-training--where each iteration generates a\nlabeled dataset by pairing source sentences with the model's predictions to\ndefine a new vocabulary. Building on these insights, we propose\nself-vocabularizing training, an iterative method that self-selects a smaller,\nmore optimal vocabulary, yielding up to a 1.49 BLEU improvement. Moreover, we\nfind that deeper model architectures lead to both an increase in unique token\nusage and a 6-8% reduction in vocabulary size.\n","authors":["Pin-Jie Lin","Ernie Chang"],"pdf_url":"https://arxiv.org/pdf/2503.13837v2.pdf","comment":"Accepted to NAACL SRW 2025"},{"id":"http://arxiv.org/abs/2503.10695v2","updated":"2025-03-19T04:07:06Z","published":"2025-03-12T05:11:11Z","title":"Introducing Verification Task of Set Consistency with Set-Consistency\n  Energy Networks","summary":"  Examining logical inconsistencies among multiple statements (such as\ncollections of sentences or question-answer pairs) is a crucial challenge in\nmachine learning, particularly for ensuring the safety and reliability of\nmodels. Traditional methods that rely on pairwise comparisons often fail to\ncapture inconsistencies that only emerge when more than two statements are\nevaluated collectively. To address this gap, we introduce the task of\nset-consistency verification, an extension of natural language inference (NLI)\nthat assesses the logical coherence of entire sets rather than isolated pairs.\nBuilding on this task, we present the Set-Consistency Energy Network\n(SC-Energy), a novel model that employs a contrastive loss framework to learn\nthe compatibility among a collection of statements. Our approach not only\nefficiently verifies inconsistencies and pinpoints the specific statements\nresponsible for logical contradictions, but also significantly outperforms\nexisting methods including prompting-based LLM models. Furthermore, we release\ntwo new datasets: Set-LConVQA and Set-SNLI for set-consistency verification\ntask.\n","authors":["Mooho Song","Hyeryung Son","Jay-Yoon Lee"],"pdf_url":"https://arxiv.org/pdf/2503.10695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14849v1","updated":"2025-03-19T03:13:37Z","published":"2025-03-19T03:13:37Z","title":"LogLLaMA: Transformer-based log anomaly detection with LLaMA","summary":"  Log anomaly detection refers to the task that distinguishes the anomalous log\nmessages from normal log messages. Transformer-based large language models\n(LLMs) are becoming popular for log anomaly detection because of their superb\nability to understand complex and long language patterns. In this paper, we\npropose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first\nfinetuned on normal log messages from three large-scale datasets to learn their\npatterns. After finetuning, the model is capable of generating successive log\nmessages given previous log messages. Our generative model is further trained\nto identify anomalous log messages using reinforcement learning (RL). The\nexperimental results show that LogLLaMA outperforms the state-of-the-art\napproaches for anomaly detection on BGL, Thunderbird, and HDFS datasets.\n","authors":["Zhuoyi Yang","Ian G. Harris"],"pdf_url":"https://arxiv.org/pdf/2503.14849v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2501.14342v2","updated":"2025-03-19T02:48:55Z","published":"2025-01-24T09:12:52Z","title":"Chain-of-Retrieval Augmented Generation","summary":"  This paper introduces an approach for training o1-like RAG models that\nretrieve and reason over relevant information step by step before generating\nthe final answer. Conventional RAG methods usually perform a single retrieval\nstep before the generation process, which limits their effectiveness in\naddressing complex queries due to imperfect retrieval results. In contrast, our\nproposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the\nmodel to dynamically reformulate the query based on the evolving state. To\ntrain CoRAG effectively, we utilize rejection sampling to automatically\ngenerate intermediate retrieval chains, thereby augmenting existing RAG\ndatasets that only provide the correct final answer. At test time, we propose\nvarious decoding strategies to scale the model's test-time compute by\ncontrolling the length and number of sampled retrieval chains. Experimental\nresults across multiple benchmarks validate the efficacy of CoRAG, particularly\nin multi-hop question answering tasks, where we observe more than 10 points\nimprovement in EM score compared to strong baselines. On the KILT benchmark,\nCoRAG establishes a new state-of-the-art performance across a diverse range of\nknowledge-intensive tasks. Furthermore, we offer comprehensive analyses to\nunderstand the scaling behavior of CoRAG, laying the groundwork for future\nresearch aimed at developing factual and grounded foundation models.\n","authors":["Liang Wang","Haonan Chen","Nan Yang","Xiaolong Huang","Zhicheng Dou","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2501.14342v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2412.18860v2","updated":"2025-03-19T02:46:34Z","published":"2024-12-25T10:08:54Z","title":"Bootstrap Your Own Context Length","summary":"  We introduce a bootstrapping approach to train long-context language models\nby exploiting their short-context capabilities only. Our method utilizes a\nsimple agent workflow to synthesize diverse long-context instruction tuning\ndata, thereby eliminating the necessity for manual data collection and\nannotation. The proposed data synthesis workflow requires only a short-context\nlanguage model, a text retriever, and a document collection, all of which are\nreadily accessible within the open-source ecosystem. Subsequently, language\nmodels are fine-tuned using the synthesized data to extend their context\nlengths. In this manner, we effectively transfer the short-context capabilities\nof language models to long-context scenarios through a bootstrapping process.\nWe conduct experiments with the open-source Llama-3 family of models and\ndemonstrate that our method can successfully extend the context length to up to\n1M tokens, achieving superior performance across various benchmarks.\n","authors":["Liang Wang","Nan Yang","Xingxing Zhang","Xiaolong Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.18860v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2503.14828v1","updated":"2025-03-19T02:06:07Z","published":"2025-03-19T02:06:07Z","title":"The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim\n  Normalization, and Retrieval","summary":"  The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings.\n","authors":["Firoj Alam","Julia Maria Struß","Tanmoy Chakraborty","Stefan Dietze","Salim Hafid","Katerina Korre","Arianna Muti","Preslav Nakov","Federico Ruggeri","Sebastian Schellhammer","Vinay Setty","Megha Sundriyal","Konstantin Todorov","Venktesh V"],"pdf_url":"https://arxiv.org/pdf/2503.14828v1.pdf","comment":"misinformation, factuality, fact-checking, fact-checkers,\n  check-worthiness, Social Media Platforms"},{"id":"http://arxiv.org/abs/2503.14827v1","updated":"2025-03-19T01:59:44Z","published":"2025-03-19T01:59:44Z","title":"MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation\n  Models","summary":"  Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/.\n","authors":["Chejian Xu","Jiawei Zhang","Zhaorun Chen","Chulin Xie","Mintong Kang","Yujin Potter","Zhun Wang","Zhuowen Yuan","Alexander Xiong","Zidi Xiong","Chenhui Zhang","Lingzhi Yuan","Yi Zeng","Peiyang Xu","Chengquan Guo","Andy Zhou","Jeffrey Ziwei Tan","Xuandong Zhao","Francesco Pinto","Zhen Xiang","Yu Gai","Zinan Lin","Dan Hendrycks","Bo Li","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2503.14827v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14797v1","updated":"2025-03-19T00:14:55Z","published":"2025-03-19T00:14:55Z","title":"FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual\n  Verification of Machine-Generated Text","summary":"  With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext.\n","authors":["Varich Boonsanong","Vidhisha Balachandran","Xiaochuang Han","Shangbin Feng","Lucy Lu Wang","Yulia Tsvetkov"],"pdf_url":"https://arxiv.org/pdf/2503.14797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17215v3","updated":"2025-03-19T00:03:30Z","published":"2024-10-22T17:40:32Z","title":"MiniPLM: Knowledge Distillation for Pre-Training Language Models","summary":"  Knowledge distillation (KD) is widely used to train small, high-performing\nstudent language models (LMs) using large teacher LMs. While effective in\nfine-tuning, KD during pre-training faces efficiency, flexibility, and\neffectiveness issues. Existing methods either incur high computational costs\ndue to online teacher inference, require tokenization matching between teacher\nand student LMs, or risk losing the difficulty and diversity of the\nteacher-generated training data. In this work, we propose MiniPLM, a KD\nframework for pre-training LMs by refining the training data distribution with\nthe teacher LM's knowledge. For efficiency, MiniPLM performs offline teacher\ninference, allowing KD for multiple student LMs without adding training costs.\nFor flexibility, MiniPLM operates solely on the training corpus, enabling KD\nacross model families. For effectiveness, MiniPLM leverages the differences\nbetween large and small LMs to enhance the training data difficulty and\ndiversity, helping student LMs acquire versatile and sophisticated knowledge.\nExtensive experiments demonstrate that MiniPLM boosts the student LMs'\nperformance on 9 common downstream tasks, improves language modeling\ncapabilities, and reduces pre-training computation. The benefit of MiniPLM\nextends to larger training scales, evidenced by the scaling curve\nextrapolation. Further analysis reveals that MiniPLM supports KD across model\nfamilies and enhances the pre-training data utilization. Our code, data, and\nmodels can be found at https://github.com/thu-coai/MiniPLM.\n","authors":["Yuxian Gu","Hao Zhou","Fandong Meng","Jie Zhou","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.17215v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.13947v2","updated":"2025-03-19T23:27:43Z","published":"2025-01-19T23:25:21Z","title":"A Comprehensive Survey on Integrating Large Language Models with\n  Knowledge-Based Methods","summary":"  The rapid development of artificial intelligence has led to marked progress\nin the field. One interesting direction for research is whether Large Language\nModels (LLMs) can be integrated with structured knowledge-based systems. This\napproach aims to combine the generative language understanding of LLMs and the\nprecise knowledge representation systems by which they are integrated. This\narticle surveys the relationship between LLMs and knowledge bases, looks at how\nthey can be applied in practice, and discusses related technical, operational,\nand ethical challenges. Utilizing a comprehensive examination of the\nliterature, the study both identifies important issues and assesses existing\nsolutions. It demonstrates the merits of incorporating generative AI into\nstructured knowledge-base systems concerning data contextualization, model\naccuracy, and utilization of knowledge resources. The findings give a full list\nof the current situation of research, point out the main gaps, and propose\nhelpful paths to take. These insights contribute to advancing AI technologies\nand support their practical deployment across various sectors.\n","authors":["Wenli Yang","Lilian Some","Michael Bain","Byeong Kang"],"pdf_url":"https://arxiv.org/pdf/2501.13947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15737v1","updated":"2025-03-19T22:59:36Z","published":"2025-03-19T22:59:36Z","title":"KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical\n  Named Entity Recognition","summary":"  Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing (NLP) that plays a crucial role in information extraction, question\nanswering, and knowledge-based systems. Traditional deep learning-based NER\nmodels often struggle with domain-specific generalization and suffer from data\nsparsity issues. In this work, we introduce Knowledge Graph distilled for Named\nEntity Recognition (KoGNER), a novel approach that integrates Knowledge Graph\n(KG) distillation into NER models to enhance entity recognition performance.\nOur framework leverages structured knowledge representations from KGs to enrich\ncontextual embeddings, thereby improving entity classification and reducing\nambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge\nDistillation, where external knowledge sources are distilled into a lightweight\nrepresentation for seamless integration with NER models, and (2) Entity-Aware\nAugmentation, which integrates contextual embeddings that have been enriched\nwith knowledge graph information directly into GNN, thereby improving the\nmodel's ability to understand and represent entity relationships. Experimental\nresults on benchmark datasets demonstrate that KoGNER achieves state-of-the-art\nperformance, outperforming finetuned NER models and LLMs by a significant\nmargin. These findings suggest that leveraging knowledge graphs as auxiliary\ninformation can significantly improve NER accuracy, making KoGNER a promising\ndirection for future research in knowledge-aware NLP.\n","authors":["Heming Zhang","Wenyu Li","Di Huang","Yinjie Tang","Yixin Chen","Philip Payne","Fuhai Li"],"pdf_url":"https://arxiv.org/pdf/2503.15737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15718v1","updated":"2025-03-19T22:07:19Z","published":"2025-03-19T22:07:19Z","title":"Am I eligible? Natural Language Inference for Clinical Trial Patient\n  Recruitment: the Patient's Point of View","summary":"  Recruiting patients to participate in clinical trials can be challenging and\ntime-consuming. Usually, participation in a clinical trial is initiated by a\nhealthcare professional and proposed to the patient. Promoting clinical trials\ndirectly to patients via online recruitment might help to reach them more\nefficiently. In this study, we address the case where a patient is initiating\ntheir own recruitment process and wants to determine whether they are eligible\nfor a given clinical trial, using their own language to describe their medical\nprofile. To study whether this creates difficulties in the patient trial\nmatching process, we design a new dataset and task, Natural Language Inference\nfor Patient Recruitment (NLI4PR), in which patient language profiles must be\nmatched to clinical trials. We create it by adapting the TREC 2022 Clinical\nTrial Track dataset, which provides patients' medical profiles, and rephrasing\nthem manually using patient language. We also use the associated clinical trial\nreports where the patients are either eligible or excluded. We prompt several\nopen-source Large Language Models on our task and achieve from 56.5 to 71.8 of\nF1 score using patient language, against 64.7 to 73.1 for the same task using\nmedical language. When using patient language, we observe only a small loss in\nperformance for the best model, suggesting that having the patient as a\nstarting point could be adopted to help recruit patients for clinical trials.\nThe corpus and code bases are all freely available on our Github and\nHuggingFace repositories.\n","authors":["Mathilde Aguiar","Pierre Zweigenbaum","Nona Naderi"],"pdf_url":"https://arxiv.org/pdf/2503.15718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09516v2","updated":"2025-03-19T21:40:12Z","published":"2025-03-12T16:26:39Z","title":"Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning","summary":"  Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities during inference to\nuse search engines is not optimal, since the LLM does not learn how to\noptimally interact with the search engine. This paper introduces Search-R1, an\nextension of the DeepSeek-R1 model where the LLM learns -- solely through\nreinforcement learning (RL) -- to autonomously generate (multiple) search\nqueries during step-by-step reasoning with real-time retrieval. Search-R1\noptimizes LLM rollouts with multi-turn search interactions, leveraging\nretrieved token masking for stable RL training and a simple outcome-based\nreward function. Experiments on seven question-answering datasets show that\nSearch-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10%\n(LLaMA3.2-3B) over strong baselines. This paper further provides empirical\ninsights into RL optimization methods, LLM choices, and response length\ndynamics in retrieval-augmented reasoning. The code and model checkpoints are\navailable at https://github.com/PeterGriffinJin/Search-R1.\n","authors":["Bowen Jin","Hansi Zeng","Zhenrui Yue","Dong Wang","Hamed Zamani","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2503.09516v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.15115v2","updated":"2025-03-19T21:39:33Z","published":"2024-11-22T18:31:47Z","title":"VideoRepair: Improving Text-to-Video Generation via Misalignment\n  Evaluation and Localized Refinement","summary":"  Recent text-to-video (T2V) diffusion models have demonstrated impressive\ngeneration capabilities across various domains. However, these models often\ngenerate videos that have misalignments with text prompts, especially when the\nprompts describe complex scenes with multiple objects and attributes. To\naddress this, we introduce VideoRepair, a novel model-agnostic, training-free\nvideo refinement framework that automatically identifies fine-grained\ntext-video misalignments and generates explicit spatial and textual feedback,\nenabling a T2V diffusion model to perform targeted, localized refinements.\nVideoRepair consists of two stages: In (1) video refinement planning, we first\ndetect misalignments by generating fine-grained evaluation questions and\nanswering them using an MLLM. Based on video evaluation outputs, we identify\naccurately generated objects and construct localized prompts to precisely\nrefine misaligned regions. In (2) localized refinement, we enhance video\nalignment by 'repairing' the misaligned regions from the original video while\npreserving the correctly generated areas. This is achieved by frame-wise region\ndecomposition using our Region-Preserving Segmentation (RPS) module. On two\npopular video generation benchmarks (EvalCrafter and T2V-CompBench),\nVideoRepair substantially outperforms recent baselines across various\ntext-video alignment metrics. We provide a comprehensive analysis of\nVideoRepair components and qualitative examples.\n","authors":["Daeun Lee","Jaehong Yoon","Jaemin Cho","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2411.15115v2.pdf","comment":"Project page: https://video-repair.github.io"},{"id":"http://arxiv.org/abs/2410.21637v3","updated":"2025-03-19T21:32:41Z","published":"2024-10-29T00:46:24Z","title":"Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase\n  Inversion","summary":"  High-quality paraphrases are easy to produce using instruction-tuned language\nmodels or specialized paraphrasing models. Although this capability has a\nvariety of benign applications, paraphrasing\nattacks$\\unicode{x2013}$paraphrases applied to machine-generated\ntexts$\\unicode{x2013}$are known to significantly degrade the performance of\nmachine-text detectors. This motivates us to consider the novel problem of\nparaphrase inversion, where, given paraphrased text, the objective is to\nrecover an approximation of the original text. The closer the approximation is\nto the original text, the better machine-text detectors will perform. We\npropose an approach which frames the problem as translation from paraphrased\ntext back to the original text, which requires examples of texts and\ncorresponding paraphrases to train the inversion model. Fortunately, such\ntraining data can easily be generated, given a corpus of original texts and one\nor more paraphrasing models. We find that language models such as GPT-4 and\nLlama-3 exhibit biases when paraphrasing which an inversion model can learn\nwith a modest amount of data. Perhaps surprisingly, we also find that such\nmodels generalize well, including to paraphrase models unseen at training time.\nFinally, we show that when combined with a paraphrased-text detector, our\ninversion models provide an effective defense against paraphrasing attacks, and\noverall our approach yields an average improvement of +22% AUROC across seven\nmachine-text detectors and three different domains.\n","authors":["Rafael Rivera Soto","Barry Chen","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2410.21637v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14052v3","updated":"2025-03-19T21:18:02Z","published":"2024-10-17T21:47:11Z","title":"From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory\n  Representation for LLMs","summary":"  Recent advancements in large language models have significantly improved\ntheir context windows, yet challenges in effective long-term memory management\nremain. We introduce MemTree, an algorithm that leverages a dynamic,\ntree-structured memory representation to optimize the organization, retrieval,\nand integration of information, akin to human cognitive schemas. MemTree\norganizes memory hierarchically, with each node encapsulating aggregated\ntextual content, corresponding semantic embeddings, and varying abstraction\nlevels across the tree's depths. Our algorithm dynamically adapts this memory\nstructure by computing and comparing semantic embeddings of new and existing\ninformation to enrich the model's context-awareness. This approach allows\nMemTree to handle complex reasoning and extended interactions more effectively\nthan traditional memory augmentation methods, which often rely on flat lookup\ntables. Evaluations on benchmarks for multi-turn dialogue understanding and\ndocument question answering show that MemTree significantly enhances\nperformance in scenarios that demand structured memory management.\n","authors":["Alireza Rezazadeh","Zichao Li","Wei Wei","Yujia Bao"],"pdf_url":"https://arxiv.org/pdf/2410.14052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08182v2","updated":"2025-03-19T20:58:37Z","published":"2024-10-10T17:55:02Z","title":"MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal\n  Models","summary":"  Existing multimodal retrieval benchmarks primarily focus on evaluating\nwhether models can retrieve and utilize external textual knowledge for question\nanswering. However, there are scenarios where retrieving visual information is\neither more beneficial or easier to access than textual data. In this paper, we\nintroduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in\nwhich we systematically identify and categorize scenarios where visually\naugmented knowledge is better than textual knowledge, for instance, more images\nfrom varying viewpoints. MRAG-Bench consists of 16,130 images and 1,353\nhuman-annotated multiple-choice questions across 9 distinct scenarios. With\nMRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large\nvision-language models (LVLMs). Our results show that all LVLMs exhibit greater\nimprovements when augmented with images compared to textual knowledge,\nconfirming that MRAG-Bench is vision-centric. Additionally, we conduct\nextensive analysis with MRAG-Bench, which offers valuable insights into\nretrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces\nchallenges in effectively leveraging retrieved knowledge, achieving only a\n5.82% improvement with ground-truth information, in contrast to a 33.16%\nimprovement observed in human participants. These findings highlight the\nimportance of MRAG-Bench in encouraging the community to enhance LVLMs' ability\nto utilize retrieved visual knowledge more effectively.\n","authors":["Wenbo Hu","Jia-Chen Gu","Zi-Yi Dou","Mohsen Fayyaz","Pan Lu","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2410.08182v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14350v2","updated":"2025-03-19T20:33:40Z","published":"2025-03-18T15:31:12Z","title":"VEGGIE: Instructional Editing and Reasoning of Video Concepts with\n  Grounded Generation","summary":"  Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing.\n","authors":["Shoubin Yu","Difan Liu","Ziqiao Ma","Yicong Hong","Yang Zhou","Hao Tan","Joyce Chai","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2503.14350v2.pdf","comment":"First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/"},{"id":"http://arxiv.org/abs/2412.18011v2","updated":"2025-03-19T19:37:12Z","published":"2024-12-23T22:08:40Z","title":"StructTest: Benchmarking LLMs' Reasoning through Compositional\n  Structured Outputs","summary":"  The rapid advancement of large language models (LLMs) demands robust,\nunbiased, and scalable evaluation methods. However, human annotations are\ncostly to scale, model-based evaluations are susceptible to stylistic biases,\nand target-answer-based benchmarks are vulnerable to data contamination and\ncheating. To address these limitations, we propose StructTest, a novel\nbenchmark that evaluates LLMs on their ability to follow compositional\ninstructions and generate structured outputs, providing an unbiased,\ncost-effective, and difficult-to-cheat evaluation framework. Assessments are\nconducted deterministically using a rule-based evaluator, which can be easily\nextended to new tasks and datasets. By testing structured outputs across\ndiverse domains including Summarization, Code, HTML, and Math, and evaluating\n17 popular LLMs, we demonstrate that StructTest remains challenging even for\ntop-performing models like Deepseek-V3/R1 and GPT-4o, establishing it as a\nrobust proxy for measuring reasoning capabilities. We believe StructTest offers\na critical and complementary approach to achieving objective and comprehensive\nmodel evaluation.\n","authors":["Hailin Chen","Fangkai Jiao","Mathieu Ravaut","Nawshad Farruque","Xuan Phi Nguyen","Chengwei Qin","Manan Dey","Bosheng Ding","Caiming Xiong","Shafiq Joty","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.18011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15664v1","updated":"2025-03-19T19:29:47Z","published":"2025-03-19T19:29:47Z","title":"Enhancing Pancreatic Cancer Staging with Large Language Models: The Role\n  of Retrieval-Augmented Generation","summary":"  Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+/RAG+ (NotebookLM\nwith REK), REK+/RAG- (Gemini 2.0 Flash with REK), and REK-/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+/RAG- (38%) and REK-/RAG- (35%). For TNM classification, REK+/RAG+ attained\n80% accuracy, exceeding REK+/RAG- (55%) and REK-/RAG- (50%). Additionally,\nREK+/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.\n","authors":["Hisashi Johno","Yuki Johno","Akitomo Amakawa","Junichi Sato","Ryota Tozuka","Atsushi Komaba","Hiroaki Watanabe","Hiroki Watanabe","Chihiro Goto","Hiroyuki Morisaka","Hiroshi Onishi","Kazunori Nakamoto"],"pdf_url":"https://arxiv.org/pdf/2503.15664v1.pdf","comment":"11 pages, 6 figures, 2 tables, 6 supplementary files"},{"id":"http://arxiv.org/abs/2503.15661v1","updated":"2025-03-19T19:26:17Z","published":"2025-03-19T19:26:17Z","title":"UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and\n  Interaction","summary":"  Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.\n","authors":["Shravan Nayak","Xiangru Jian","Kevin Qinghong Lin","Juan A. Rodriguez","Montek Kalsi","Rabiul Awal","Nicolas Chapados","M. Tamer Özsu","Aishwarya Agrawal","David Vazquez","Christopher Pal","Perouz Taslakian","Spandana Gella","Sai Rajeswar"],"pdf_url":"https://arxiv.org/pdf/2503.15661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08679v3","updated":"2025-03-19T19:20:42Z","published":"2025-03-11T17:56:30Z","title":"Chain-of-Thought Reasoning In The Wild Is Not Always Faithful","summary":"  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art\nAI capabilities. However, recent studies have shown that CoT reasoning is not\nalways faithful, i.e. CoT reasoning does not always reflect how models arrive\nat conclusions. So far, most of these studies have focused on unfaithfulness in\nunnatural contexts where an explicit bias has been introduced. In contrast, we\nshow that unfaithful CoT can occur on realistic prompts with no artificial\nbias. Our results reveal non-negligible rates of several forms of unfaithful\nreasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and\nChatGPT-4o (7.0%) all answer a notable proportion of question pairs\nunfaithfully. Specifically, we find that models rationalize their implicit\nbiases in answers to binary questions (\"implicit post-hoc rationalization\").\nFor example, when separately presented with the questions \"Is X bigger than Y?\"\nand \"Is Y bigger than X?\", models sometimes produce superficially coherent\narguments to justify answering Yes to both questions or No to both questions,\ndespite such responses being logically contradictory. We also investigate\nrestoration errors (Dziri et al., 2023), where models make and then silently\ncorrect errors in their reasoning, and unfaithful shortcuts, where models use\nclearly illogical reasoning to simplify solving problems in Putnam questions (a\nhard benchmark). Our findings raise challenges for AI safety work that relies\non monitoring CoT to detect undesired behavior.\n","authors":["Iván Arcuschin","Jett Janiak","Robert Krzyzanowski","Senthooran Rajamanoharan","Neel Nanda","Arthur Conmy"],"pdf_url":"https://arxiv.org/pdf/2503.08679v3.pdf","comment":"Accepted to the Reasoning and Planning for LLMs Workshop (ICLR 25),\n  10 main paper pages, 39 appendix pages"},{"id":"http://arxiv.org/abs/2412.20760v2","updated":"2025-03-19T19:08:17Z","published":"2024-12-30T07:09:25Z","title":"Attributing Culture-Conditioned Generations to Pretraining Corpora","summary":"  In open-ended generative tasks like narrative writing or dialogue, large\nlanguage models often exhibit cultural biases, showing limited knowledge and\ngenerating templated outputs for less prevalent cultures. Recent works show\nthat these biases may stem from uneven cultural representation in pretraining\ncorpora. This work investigates how pretraining leads to biased\nculture-conditioned generations by analyzing how models associate entities with\ncultures based on pretraining data patterns. We propose the MEMOed framework\n(MEMOrization from pretraining document) to determine whether a generation for\na culture arises from memorization. Using MEMOed on culture-conditioned\ngenerations about food and clothing for 110 cultures, we find that\nhigh-frequency cultures in pretraining data yield more generations with\nmemorized symbols, while some low-frequency cultures produce none.\nAdditionally, the model favors generating entities with extraordinarily high\nfrequency regardless of the conditioned culture, reflecting biases toward\nfrequent pretraining terms irrespective of relevance. We hope that the MEMOed\nframework and our insights will inspire more works on attributing model\nperformance on pretraining data.\n","authors":["Huihan Li","Arnav Goel","Keyu He","Xiang Ren"],"pdf_url":"https://arxiv.org/pdf/2412.20760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00890v3","updated":"2025-03-19T18:55:51Z","published":"2024-07-01T01:25:26Z","title":"Macroeconomic Forecasting with Large Language Models","summary":"  This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios\n","authors":["Andrea Carriero","Davide Pettenuzzo","Shubhranshu Shekhar"],"pdf_url":"https://arxiv.org/pdf/2407.00890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12065v2","updated":"2025-03-19T18:53:49Z","published":"2025-02-17T17:34:48Z","title":"Formalizing Complex Mathematical Statements with LLMs: A Study on\n  Mathematical Definitions","summary":"  Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.\n","authors":["Lan Zhang","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2502.12065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02650v2","updated":"2025-03-19T18:50:38Z","published":"2024-10-03T16:34:46Z","title":"Undesirable Memorization in Large Language Models: A Survey","summary":"  While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it is equally crucial to examine their associated\nrisks. Among these, privacy and security vulnerabilities are particularly\nconcerning, posing significant ethical and legal challenges. At the heart of\nthese vulnerabilities stands memorization, which refers to a model's tendency\nto store and reproduce phrases from its training data. This phenomenon has been\nshown to be a fundamental source to various privacy and security attacks\nagainst LLMs. In this paper, we provide a taxonomy of the literature on LLM\nmemorization, exploring it across three dimensions: granularity,\nretrievability, and desirability. Next, we discuss the metrics and methods used\nto quantify memorization, followed by an analysis of the causes and factors\nthat contribute to memorization phenomenon. We then explore strategies that are\nused so far to mitigate the undesirable aspects of this phenomenon. We conclude\nour survey by identifying potential research topics for the near future,\nincluding methods to balance privacy and performance, and the analysis of\nmemorization in specific LLM contexts such as conversational agents,\nretrieval-augmented generation, and diffusion language models. Given the rapid\nresearch pace in this field, we also maintain a dedicated repository of the\nreferences discussed in this survey which will be regularly updated to reflect\nthe latest developments.\n","authors":["Ali Satvaty","Suzan Verberne","Fatih Turkmen"],"pdf_url":"https://arxiv.org/pdf/2410.02650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15621v1","updated":"2025-03-19T18:10:12Z","published":"2025-03-19T18:10:12Z","title":"LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.\n","authors":["Federico Cocchi","Nicholas Moratelli","Davide Caffagni","Sara Sarto","Lorenzo Baraldi","Marcella Cornia","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.15621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15620v1","updated":"2025-03-19T18:09:19Z","published":"2025-03-19T18:09:19Z","title":"Does Context Matter? ContextualJudgeBench for Evaluating LLM-based\n  Judges in Contextual Settings","summary":"  The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy.\n","authors":["Austin Xu","Srijan Bansal","Yifei Ming","Semih Yavuz","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2503.15620v1.pdf","comment":"23 pages, 13 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.05931v2","updated":"2025-03-19T18:08:48Z","published":"2025-03-07T20:57:43Z","title":"Training and Inference Efficiency of Encoder-Decoder Speech Models","summary":"  Attention encoder-decoder model architecture is the backbone of several\nrecent top performing foundation speech models: Whisper, Seamless, OWSM, and\nCanary-1B. However, the reported data and compute requirements for their\ntraining are prohibitive for many in the research community. In this work, we\nfocus on the efficiency angle and ask the questions of whether we are training\nthese speech models efficiently, and what can we do to improve? We argue that a\nmajor, if not the most severe, detrimental factor for training efficiency is\nrelated to the sampling strategy of sequential data. We show that negligence in\nmini-batch sampling leads to more than 50% computation being spent on padding.\nTo that end, we study, profile, and optimize Canary-1B training to show gradual\nimprovement in GPU utilization leading up to 5x increase in average batch sizes\nversus its original training settings. This in turn allows us to train an\nequivalent model using 4x less GPUs in the same wall time, or leverage the\noriginal resources and train it in 2x shorter wall time. Finally, we observe\nthat the major inference bottleneck lies in the autoregressive decoder steps.\nWe find that adjusting the model architecture to transfer model parameters from\nthe decoder to the encoder results in a 3x inference speedup as measured by\ninverse real-time factor (RTFx) while preserving the accuracy and compute\nrequirements for convergence. The training code and models will be available as\nopen-source.\n","authors":["Piotr Żelasko","Kunal Dhawan","Daniel Galvez","Krishna C. Puvvada","Ankita Pasad","Nithin Rao Koluguri","Ke Hu","Vitaly Lavrukhin","Jagadeesh Balam","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2503.05931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00057v2","updated":"2025-03-19T18:04:39Z","published":"2025-02-26T20:24:01Z","title":"Improved YOLOv12 with LLM-Generated Synthetic Data for Enhanced Apple\n  Detection and Benchmarking Against YOLOv11 and YOLOv10","summary":"  This study evaluated the performance of the YOLOv12 object detection model,\nand compared against the performances YOLOv11 and YOLOv10 for apple detection\nin commercial orchards based on the model training completed entirely on\nsynthetic images generated by Large Language Models (LLMs). The YOLOv12n\nconfiguration achieved the highest precision at 0.916, the highest recall at\n0.969, and the highest mean Average Precision (mAP@50) at 0.978. In comparison,\nthe YOLOv11 series was led by YOLO11x, which achieved the highest precision at\n0.857, recall at 0.85, and mAP@50 at 0.91. For the YOLOv10 series, YOLOv10b and\nYOLOv10l both achieved the highest precision at 0.85, with YOLOv10n achieving\nthe highest recall at 0.8 and mAP@50 at 0.89. These findings demonstrated that\nYOLOv12, when trained on realistic LLM-generated datasets surpassed its\npredecessors in key performance metrics. The technique also offered a\ncost-effective solution by reducing the need for extensive manual data\ncollection in the agricultural field. In addition, this study compared the\ncomputational efficiency of all versions of YOLOv12, v11 and v10, where\nYOLOv11n reported the lowest inference time at 4.7 ms, compared to YOLOv12n's\n5.6 ms and YOLOv10n's 5.9 ms. Although YOLOv12 is new and more accurate than\nYOLOv11, and YOLOv10, YOLO11n still stays the fastest YOLO model among YOLOv10,\nYOLOv11 and YOLOv12 series of models. (Index: YOLOv12, YOLOv11, YOLOv10,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO Object detection)\n","authors":["Ranjan Sapkota","Manoj Karkee"],"pdf_url":"https://arxiv.org/pdf/2503.00057v2.pdf","comment":"8 pages, 5 Figures, 2 Tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.15485v1","updated":"2025-03-19T17:58:57Z","published":"2025-03-19T17:58:57Z","title":"TULIP: Towards Unified Language-Image Pretraining","summary":"  Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io\n","authors":["Zineng Tang","Long Lian","Seun Eisape","XuDong Wang","Roei Herzig","Adam Yala","Alane Suhr","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2503.15485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11761v3","updated":"2025-03-19T17:56:39Z","published":"2024-10-15T16:33:33Z","title":"SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology\n  Image Understanding","summary":"  Despite the progress made by multimodal large language models (MLLMs) in\ncomputational pathology, they remain limited by a predominant focus on\npatch-level analysis, missing essential contextual information at the\nwhole-slide level. The lack of large-scale instruction datasets and the\ngigapixel scale of whole slide images (WSIs) pose significant developmental\nchallenges. In this paper, we present SlideChat, the first vision-language\nassistant capable of understanding gigapixel whole-slide images, exhibiting\nexcellent multimodal conversational capability and response complex instruction\nacross diverse pathology scenarios. To support its development, we created\nSlideInstruction, the largest instruction-following dataset for WSIs consisting\nof 4.2K WSI captions and 176K VQA pairs with multiple categories. Furthermore,\nwe propose SlideBench, a multimodal benchmark that incorporates captioning and\nVQA tasks to assess SlideChat's capabilities in varied clinical settings such\nas microscopy, diagnosis. Compared to both general and specialized MLLMs,\nSlideChat exhibits exceptional capabilities achieving state-of-the-art\nperformance on 18 of 22 tasks. For example, it achieved an overall accuracy of\n81.17% on SlideBench-VQA (TCGA), and 54.15% on SlideBench-VQA (BCNB). Our code,\ndata, and model is publicly accessible at\nhttps://uni-medical.github.io/SlideChat.github.io.\n","authors":["Ying Chen","Guoan Wang","Yuanfeng Ji","Yanjun Li","Jin Ye","Tianbin Li","Ming Hu","Rongshan Yu","Yu Qiao","Junjun He"],"pdf_url":"https://arxiv.org/pdf/2410.11761v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.15475v1","updated":"2025-03-19T17:52:17Z","published":"2025-03-19T17:52:17Z","title":"Cube: A Roblox View of 3D Intelligence","summary":"  Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence.\n","authors":[" Foundation AI Team","Kiran Bhat","Nishchaie Khanna","Karun Channa","Tinghui Zhou","Yiheng Zhu","Xiaoxia Sun","Charles Shang","Anirudh Sudarshan","Maurice Chu","Daiqing Li","Kangle Deng","Jean-Philippe Fauconnier","Tijmen Verhulsdonck","Maneesh Agrawala","Kayvon Fatahalian","Alexander Weiss","Christian Reiser","Ravi Kiran Chirravuri","Ravali Kandur","Alejandro Pelaez","Akash Garg","Michael Palleschi","Jessica Wang","Skylar Litz","Leon Liu","Anying Li","David Harmon","Derek Liu","Liangjun Feng","Denis Goupil","Lukas Kuczynski","Jihyun Yoon","Naveen Marri","Peiye Zhuang","Yinan Zhang","Brian Yin","Haomiao Jiang","Marcel van Workum","Thomas Lane","Bryce Erickson","Salil Pathare","Kyle Price","Anupam Singh","David Baszucki"],"pdf_url":"https://arxiv.org/pdf/2503.15475v1.pdf","comment":"Our code and model weights can be found at:\n  https://github.com/Roblox/cube"},{"id":"http://arxiv.org/abs/2503.15474v1","updated":"2025-03-19T17:49:27Z","published":"2025-03-19T17:49:27Z","title":"Toward task-driven satellite image super-resolution","summary":"  Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution.\n","authors":["Maciej Ziaja","Pawel Kowaleczko","Daniel Kostrzewa","Nicolas Longépé","Michal Kawulok"],"pdf_url":"https://arxiv.org/pdf/2503.15474v1.pdf","comment":"Submitted to IEEE IGARSS 2024"},{"id":"http://arxiv.org/abs/2503.15470v1","updated":"2025-03-19T17:45:56Z","published":"2025-03-19T17:45:56Z","title":"EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining","summary":"  Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM.\n","authors":["Boshen Xu","Yuting Mei","Xinbi Liu","Sipeng Zheng","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2503.15470v1.pdf","comment":"Code will be released at: https://github.com/xuboshen/EgoDTM"},{"id":"http://arxiv.org/abs/2503.15465v1","updated":"2025-03-19T17:44:21Z","published":"2025-03-19T17:44:21Z","title":"FP4DiT: Towards Effective Floating Point Quantization for Diffusion\n  Transformers","summary":"  Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP.\n","authors":["Ruichen Chen","Keith G. Mills","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2503.15465v1.pdf","comment":"The code is available at https://github.com/cccrrrccc/FP4DiT"},{"id":"http://arxiv.org/abs/2503.15457v1","updated":"2025-03-19T17:36:54Z","published":"2025-03-19T17:36:54Z","title":"Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step\n  Generator","summary":"  Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.\n","authors":["Yuanzhi Zhu","Xi Wang","Stéphane Lathuilière","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2503.15457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15451v1","updated":"2025-03-19T17:32:24Z","published":"2025-03-19T17:32:24Z","title":"MotionStreamer: Streaming Motion Generation via Diffusion-based\n  Autoregressive Model in Causal Latent Space","summary":"  This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/\n","authors":["Lixing Xiao","Shunlin Lu","Huaijin Pi","Ke Fan","Liang Pan","Yueer Zhou","Ziyong Feng","Xiaowei Zhou","Sida Peng","Jingbo Wang"],"pdf_url":"https://arxiv.org/pdf/2503.15451v1.pdf","comment":"Project Page: https://zju3dv.github.io/MotionStreamer/"},{"id":"http://arxiv.org/abs/2503.15435v1","updated":"2025-03-19T17:17:44Z","published":"2025-03-19T17:17:44Z","title":"V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative\n  Perception","summary":"  LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset.\n","authors":["Baolu Li","Zongzhe Xu","Jinlong Li","Xinyu Liu","Jianwu Fang","Xiaopeng Li","Hongkai Yu"],"pdf_url":"https://arxiv.org/pdf/2503.15435v1.pdf","comment":"accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.15426v1","updated":"2025-03-19T17:08:13Z","published":"2025-03-19T17:08:13Z","title":"Visual Position Prompt for MLLM based Visual Grounding","summary":"  Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance.\n","authors":["Wei Tang","Yanpeng Sun","Qinying Gu","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2503.15426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19160v2","updated":"2025-03-19T17:06:01Z","published":"2025-01-31T14:28:10Z","title":"RMDM: Radio Map Diffusion Model with Physics Informed","summary":"  With the rapid development of wireless communication technology, the\nefficient utilization of spectrum resources, optimization of communication\nquality, and intelligent communication have become critical. Radio map\nreconstruction is essential for enabling advanced applications, yet challenges\nsuch as complex signal propagation and sparse data hinder accurate\nreconstruction. To address these issues, we propose the **Radio Map Diffusion\nModel (RMDM)**, a physics-informed framework that integrates **Physics-Informed\nNeural Networks (PINNs)** to incorporate constraints like the **Helmholtz\nequation**. RMDM employs a dual U-Net architecture: the first ensures physical\nconsistency by minimizing PDE residuals, boundary conditions, and source\nconstraints, while the second refines predictions via diffusion-based\ndenoising. By leveraging physical laws, RMDM significantly enhances accuracy,\nrobustness, and generalization. Experiments demonstrate that RMDM outperforms\nstate-of-the-art methods, achieving **NMSE of 0.0031** and **RMSE of 0.0125**\nunder the Static RM (SRM) setting, and **NMSE of 0.0047** and **RMSE of\n0.0146** under the Dynamic RM (DRM) setting. These results establish a novel\nparadigm for integrating physics-informed and data-driven approaches in radio\nmap reconstruction, particularly under sparse data conditions.\n","authors":["Haozhe Jia","Wenshuo Chen","Zhihui Huang","Hongru Xiao","Nanqian Jia","Keming Wu","Songning Lai","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2501.19160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14478v2","updated":"2025-03-19T17:03:25Z","published":"2025-03-18T17:51:34Z","title":"Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM","summary":"  Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench.\n","authors":["Xinyu Fang","Zhijian Chen","Kai Lan","Lixin Ma","Shengyuan Ding","Yingji Liang","Xiangyu Zhao","Farong Wen","Zicheng Zhang","Guofeng Zhang","Haodong Duan","Kai Chen","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2503.14478v2.pdf","comment":"Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench"},{"id":"http://arxiv.org/abs/2503.15420v1","updated":"2025-03-19T17:00:58Z","published":"2025-03-19T17:00:58Z","title":"LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding","summary":"  Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks.\n","authors":["Amirhossein Kazerouni","Soroush Mehraban","Michael Brudno","Babak Taati"],"pdf_url":"https://arxiv.org/pdf/2503.15420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15417v1","updated":"2025-03-19T16:59:32Z","published":"2025-03-19T16:59:32Z","title":"Temporal Regularization Makes Your Video Generator Stronger","summary":"  Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality.\n","authors":["Harold Haodong Chen","Haojian Huang","Xianfeng Wu","Yexin Liu","Yajing Bai","Wen-Jie Shu","Harry Yang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2503.15417v1.pdf","comment":"Project: https://haroldchen19.github.io/FluxFlow/"},{"id":"http://arxiv.org/abs/2503.15415v1","updated":"2025-03-19T16:57:00Z","published":"2025-03-19T16:57:00Z","title":"Automated Processing of eXplainable Artificial Intelligence Outputs in\n  Deep Learning Models for Fault Diagnostics of Large Infrastructures","summary":"  Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells.\n","authors":["Giovanni Floreale","Piero Baraldi","Enrico Zio","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2503.15415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15414v1","updated":"2025-03-19T16:56:34Z","published":"2025-03-19T16:56:34Z","title":"Federated Continual 3D Segmentation With Single-round Communication","summary":"  Federated learning seeks to foster collaboration among distributed clients\nwhile preserving the privacy of their local data. Traditionally, federated\nlearning methods assume a fixed setting in which client data and learning\nobjectives remain constant. However, in real-world scenarios, new clients may\njoin, and existing clients may expand the segmentation label set as task\nrequirements evolve. In such a dynamic federated analysis setup, the\nconventional federated communication strategy of model aggregation per\ncommunication round is suboptimal. As new clients join, this strategy requires\nretraining, linearly increasing communication and computation overhead. It also\nimposes requirements for synchronized communication, which is difficult to\nachieve among distributed clients. In this paper, we propose a federated\ncontinual learning strategy that employs a one-time model aggregation at the\nserver through multi-model distillation. This approach builds and updates the\nglobal model while eliminating the need for frequent server communication. When\nintegrating new data streams or onboarding new clients, this approach\nefficiently reuses previous client models, avoiding the need to retrain the\nglobal model across the entire federation. By minimizing communication load and\nbypassing the need to put unchanged clients online, our approach relaxes\nsynchronization requirements among clients, providing an efficient and scalable\nfederated analysis framework suited for real-world applications. Using\nmulti-class 3D abdominal CT segmentation as an application task, we demonstrate\nthe effectiveness of the proposed approach.\n","authors":["Can Peng","Qianhui Men","Pramit Saha","Qianye Yang","Cheng Ouyang","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2503.15414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15412v1","updated":"2025-03-19T16:56:03Z","published":"2025-03-19T16:56:03Z","title":"Learn Your Scales: Towards Scale-Consistent Generative Novel View\n  Synthesis","summary":"  Conventional depth-free multi-view datasets are captured using a moving\nmonocular camera without metric calibration. The scales of camera positions in\nthis monocular setting are ambiguous. Previous methods have acknowledged scale\nambiguity in multi-view data via various ad-hoc normalization pre-processing\nsteps, but have not directly analyzed the effect of incorrect scene scales on\ntheir application. In this paper, we seek to understand and address the effect\nof scale ambiguity when used to train generative novel view synthesis methods\n(GNVS). In GNVS, new views of a scene or object can be minimally synthesized\ngiven a single image and are, thus, unconstrained, necessitating the use of\ngenerative methods. The generative nature of these models captures all aspects\nof uncertainty, including any uncertainty of scene scales, which act as\nnuisance variables for the task. We study the effect of scene scale ambiguity\nin GNVS when sampled from a single image by isolating its effect on the\nresulting models and, based on these intuitions, define new metrics that\nmeasure the scale inconsistency of generated views. We then propose a framework\nto estimate scene scales jointly with the GNVS model in an end-to-end fashion.\nEmpirically, we show that our method reduces the scale inconsistency of\ngenerated views without the complexity or downsides of previous scale\nnormalization methods. Further, we show that removing this ambiguity improves\ngenerated image quality of the resulting GNVS model.\n","authors":["Fereshteh Forghani","Jason J. Yu","Tristan Aumentado-Armstrong","Konstantinos G. Derpanis","Marcus A. Brubaker"],"pdf_url":"https://arxiv.org/pdf/2503.15412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15406v1","updated":"2025-03-19T16:45:47Z","published":"2025-03-19T16:45:47Z","title":"Visual Persona: Foundation Model for Full-Body Human Customization","summary":"  We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks.\n","authors":["Jisu Nam","Soowon Son","Zhan Xu","Jing Shi","Difan Liu","Feng Liu","Aashish Misraa","Seungryong Kim","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.15406v1.pdf","comment":"CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona"},{"id":"http://arxiv.org/abs/2503.15404v1","updated":"2025-03-19T16:44:23Z","published":"2025-03-19T16:44:23Z","title":"Improving Adversarial Transferability on Vision Transformers via Forward\n  Propagation Refinement","summary":"  Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR.\n","authors":["Yuchen Ren","Zhengyu Zhao","Chenhao Lin","Bo Yang","Lu Zhou","Zhe Liu","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2503.15404v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2503.15402v1","updated":"2025-03-19T16:43:35Z","published":"2025-03-19T16:43:35Z","title":"Towards efficient keyword spotting using spike-based time difference\n  encoders","summary":"  Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns.\n","authors":["Alejandro Pequeño-Zurro","Lyes Khacef","Stefano Panzeri","Elisabetta Chicca"],"pdf_url":"https://arxiv.org/pdf/2503.15402v1.pdf","comment":"26 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.15390v1","updated":"2025-03-19T16:27:29Z","published":"2025-03-19T16:27:29Z","title":"FedSCA: Federated Tuning with Similarity-guided Collaborative\n  Aggregation for Heterogeneous Medical Image Segmentation","summary":"  Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance.\n","authors":["Yumin Zhang","Yan Gao","Haoran Duan","Hanqing Guo","Tejal Shah","Rajiv Ranjan","Bo Wei"],"pdf_url":"https://arxiv.org/pdf/2503.15390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14273v2","updated":"2025-03-19T16:17:19Z","published":"2025-03-18T14:09:00Z","title":"Manual Labelling Artificially Inflates Deep Learning-Based Segmentation\n  Performance on RGB Images of Closed Canopy: Validation Using TLS","summary":"  Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests.\n","authors":["Matthew J. Allen","Harry J. F. Owen","Stuart W. D. Grieve","Emily R. Lines"],"pdf_url":"https://arxiv.org/pdf/2503.14273v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.05206v3","updated":"2025-03-19T16:10:18Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Xudong Han","Haonan Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Tim Baldwin","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v3.pdf","comment":"47 pages, 3 figures, 11 tables; GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2503.15369v1","updated":"2025-03-19T16:07:04Z","published":"2025-03-19T16:07:04Z","title":"EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language\n  Models","summary":"  While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model.\n","authors":["Yinan Liang","Ziwei Wang","Xiuwei Xu","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2503.15369v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15361v1","updated":"2025-03-19T16:01:27Z","published":"2025-03-19T16:01:27Z","title":"Boosting HDR Image Reconstruction via Semantic Knowledge Transfer","summary":"  Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods.\n","authors":["Qingsen Yan","Tao Hu","Genggeng Chen","Wei Dong","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15358v1","updated":"2025-03-19T15:58:46Z","published":"2025-03-19T15:58:46Z","title":"SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity\n  Representation","summary":"  Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity.\n","authors":["Thomas Pickard","Aline Villavicencio","Maggie Mi","Wei He","Dylan Phelps","Carolina Scarton","Marco Idiart"],"pdf_url":"https://arxiv.org/pdf/2503.15358v1.pdf","comment":"Preprint; SemEval-2025 proceedings to appear at ACL 2025"},{"id":"http://arxiv.org/abs/2503.15352v1","updated":"2025-03-19T15:51:17Z","published":"2025-03-19T15:51:17Z","title":"Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for\n  Cross-modal Transfer","summary":"  Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.\n","authors":["Abhi Kamboj","Minh N. Do"],"pdf_url":"https://arxiv.org/pdf/2503.15352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15342v1","updated":"2025-03-19T15:41:32Z","published":"2025-03-19T15:41:32Z","title":"TruthLens:A Training-Free Paradigm for DeepFake Detection","summary":"  The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation.\n","authors":["Ritabrata Chakraborty","Rajatsubhra Chakraborty","Ali Khaleghi Rahimian","Thomas MacDougall"],"pdf_url":"https://arxiv.org/pdf/2503.15342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10772v2","updated":"2025-03-19T15:39:37Z","published":"2025-03-13T18:06:13Z","title":"FlowTok: Flowing Seamlessly Across Text and Image Tokens","summary":"  Bridging different modalities lies at the heart of cross-modality generation.\nWhile conventional approaches treat the text modality as a conditioning signal\nthat gradually guides the denoising process from Gaussian noise to the target\nimage modality, we explore a much simpler paradigm-directly evolving between\ntext and image modalities through flow matching. This requires projecting both\nmodalities into a shared latent space, which poses a significant challenge due\nto their inherently different representations: text is highly semantic and\nencoded as 1D tokens, whereas images are spatially redundant and represented as\n2D latent embeddings. To address this, we introduce FlowTok, a minimal\nframework that seamlessly flows across text and images by encoding images into\na compact 1D token representation. Compared to prior methods, this design\nreduces the latent space size by 3.3x at an image resolution of 256,\neliminating the need for complex conditioning mechanisms or noise scheduling.\nMoreover, FlowTok naturally extends to image-to-text generation under the same\nformulation. With its streamlined architecture centered around compact 1D\ntokens, FlowTok is highly memory-efficient, requires significantly fewer\ntraining resources, and achieves much faster sampling speeds-all while\ndelivering performance comparable to state-of-the-art models. Code will be\navailable at https://github.com/bytedance/1d-tokenizer.\n","authors":["Ju He","Qihang Yu","Qihao Liu","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2503.10772v2.pdf","comment":"Project page at https://tacju.github.io/projects/flowtok.html"},{"id":"http://arxiv.org/abs/2309.17211v2","updated":"2025-03-19T15:35:38Z","published":"2023-09-29T13:09:40Z","title":"Data-Free Dynamic Compression of CNNs for Tractable Efficiency","summary":"  To reduce the computational cost of convolutional neural networks (CNNs) on\nresource-constrained devices, structured pruning approaches have shown promise\nin lowering floating-point operations (FLOPs) without substantial drops in\naccuracy. However, most methods require fine-tuning or specific training\nprocedures to achieve a reasonable trade-off between retained accuracy and\nreduction in FLOPs, adding computational overhead and requiring training data\nto be available. To this end, we propose HASTE (Hashing for Tractable\nEfficiency), a data-free, plug-and-play convolution module that instantly\nreduces a network's test-time inference cost without training or fine-tuning.\nOur approach utilizes locality-sensitive hashing (LSH) to detect redundancies\nin the channel dimension of latent feature maps, compressing similar channels\nto reduce input and filter depth simultaneously, resulting in cheaper\nconvolutions. We demonstrate our approach on the popular vision benchmarks\nCIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a\n1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module.\n","authors":["Lukas Meiner","Jens Mehnert","Alexandru Paul Condurache"],"pdf_url":"https://arxiv.org/pdf/2309.17211v2.pdf","comment":"Accepted at VISAPP 2025"},{"id":"http://arxiv.org/abs/2503.15337v1","updated":"2025-03-19T15:33:44Z","published":"2025-03-19T15:33:44Z","title":"Recover and Match: Open-Vocabulary Multi-Label Recognition through\n  Knowledge-Constrained Optimal Transport","summary":"  Identifying multiple novel classes in an image, known as open-vocabulary\nmulti-label recognition, is a challenging task in computer vision. Recent\nstudies explore the transfer of powerful vision-language models such as CLIP.\nHowever, these approaches face two critical challenges: (1) The local semantics\nof CLIP are disrupted due to its global pre-training objectives, resulting in\nunreliable regional predictions. (2) The matching property between image\nregions and candidate labels has been neglected, relying instead on naive\nfeature aggregation such as average pooling, which leads to spurious\npredictions from irrelevant regions. In this paper, we present RAM (Recover And\nMatch), a novel framework that effectively addresses the above issues. To\ntackle the first problem, we propose Ladder Local Adapter (LLA) to enforce\nrefocusing on local regions, recovering local semantics in a memory-friendly\nway. For the second issue, we propose Knowledge-Constrained Optimal Transport\n(KCOT) to suppress meaningless matching to non-GT labels by formulating the\ntask as an optimal transport problem. As a result, RAM achieves\nstate-of-the-art performance on various datasets from three distinct domains,\nand shows great potential to boost the existing methods. Code:\nhttps://github.com/EricTan7/RAM.\n","authors":["Hao Tan","Zichang Tan","Jun Li","Ajian Liu","Jun Wan","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2503.15337v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15321v1","updated":"2025-03-19T15:27:16Z","published":"2025-03-19T15:27:16Z","title":"Euclid Quick Data Release (Q1). Active galactic nuclei identification\n  using diffusion-based inpainting of Euclid VIS images","summary":"  Light emission from galaxies exhibit diverse brightness profiles, influenced\nby factors such as galaxy type, structural features and interactions with other\ngalaxies. Elliptical galaxies feature more uniform light distributions, while\nspiral and irregular galaxies have complex, varied light profiles due to their\nstructural heterogeneity and star-forming activity. In addition, galaxies with\nan active galactic nucleus (AGN) feature intense, concentrated emission from\ngas accretion around supermassive black holes, superimposed on regular galactic\nlight, while quasi-stellar objects (QSO) are the extreme case of the AGN\nemission dominating the galaxy. The challenge of identifying AGN and QSO has\nbeen discussed many times in the literature, often requiring multi-wavelength\nobservations. This paper introduces a novel approach to identify AGN and QSO\nfrom a single image. Diffusion models have been recently developed in the\nmachine-learning literature to generate realistic-looking images of everyday\nobjects. Utilising the spatial resolving power of the Euclid VIS images, we\ncreated a diffusion model trained on one million sources, without using any\nsource pre-selection or labels. The model learns to reconstruct light\ndistributions of normal galaxies, since the population is dominated by them. We\ncondition the prediction of the central light distribution by masking the\ncentral few pixels of each source and reconstruct the light according to the\ndiffusion model. We further use this prediction to identify sources that\ndeviate from this profile by examining the reconstruction error of the few\ncentral pixels regenerated in each source's core. Our approach, solely using\nVIS imaging, features high completeness compared to traditional methods of AGN\nand QSO selection, including optical, near-infrared, mid-infrared, and X-rays.\n[abridged]\n","authors":[" Euclid Collaboration","G. Stevens","S. Fotopoulou","M. N. Bremer","T. Matamoro Zatarain","K. Jahnke","B. Margalef-Bentabol","M. Huertas-Company","M. J. Smith","M. Walmsley","M. Salvato","M. Mezcua","A. Paulino-Afonso","M. Siudek","M. Talia","F. Ricci","W. Roster","N. Aghanim","B. Altieri","S. Andreon","H. Aussel","C. Baccigalupi","M. Baldi","S. Bardelli","P. Battaglia","A. Biviano","A. Bonchi","E. Branchini","M. Brescia","J. Brinchmann","S. Camera","G. Cañas-Herrera","V. Capobianco","C. Carbone","J. Carretero","M. Castellano","G. Castignani","S. Cavuoti","K. C. Chambers","A. Cimatti","C. Colodro-Conde","G. Congedo","C. J. Conselice","L. Conversi","Y. Copin","A. Costille","F. Courbin","H. M. Courtois","M. Cropper","A. Da Silva","H. Degaudenzi","G. De Lucia","C. Dolding","H. Dole","M. Douspis","F. Dubath","X. Dupac","S. Dusini","S. Escoffier","M. Farina","S. Ferriol","K. George","C. Giocoli","B. R. Granett","A. Grazian","F. Grupp","S. V. H. Haugan","I. M. Hook","F. Hormuth","A. Hornstrup","P. Hudelot","M. Jhabvala","E. Keihänen","S. Kermiche","A. Kiessling","M. Kilbinger","B. Kubik","M. Kümmel","H. Kurki-Suonio","Q. Le Boulc'h","A. M. C. Le Brun","D. Le Mignant","P. B. Lilje","V. Lindholm","I. Lloro","G. Mainetti","D. Maino","E. Maiorano","O. Marggraf","M. Martinelli","N. Martinet","F. Marulli","R. Massey","S. Maurogordato","H. J. McCracken","E. Medinaceli","S. Mei","M. Melchior","M. Meneghetti","E. Merlin","G. Meylan","A. Mora","M. Moresco","L. Moscardini","R. Nakajima","C. Neissner","S. -M. Niemi","C. Padilla","S. Paltani","F. Pasian","K. Pedersen","W. J. Percival","V. Pettorino","G. Polenta","M. Poncet","L. A. Popa","L. Pozzetti","F. Raison","R. Rebolo","A. Renzi","J. Rhodes","G. Riccio","E. Romelli","M. Roncarelli","R. Saglia","A. G. Sánchez","D. Sapone","J. A. Schewtschenko","M. Schirmer","P. Schneider","T. Schrabback","A. Secroun","S. Serrano","P. Simon","C. Sirignano","G. Sirri","J. Skottfelt","L. Stanco","J. Steinwagner","P. Tallada-Crespí","A. N. Taylor","I. Tereno","S. Toft","R. Toledo-Moreo","F. Torradeflot","I. Tutusaus","L. Valenziano","J. Valiviita","T. Vassallo","G. Verdoes Kleijn","A. Veropalumbo","Y. Wang","J. Weller","A. Zacchei","G. Zamorani","F. M. Zerbi","I. A. Zinchenko","E. Zucca","V. Allevato","M. Ballardini","M. Bolzonella","E. Bozzo","C. Burigana","R. Cabanac","A. Cappi","J. A. Escartin Vigo","L. Gabarra","W. G. Hartley","J. Martín-Fleitas","S. Matthew","R. B. Metcalf","A. Pezzotta","M. Pöntinen","I. Risso","V. Scottez","M. Sereno","M. Tenti","M. Wiesmann","Y. Akrami","S. Alvi","I. T. Andika","S. Anselmi","M. Archidiacono","F. Atrio-Barandela","D. Bertacca","M. Bethermin","L. Bisigello","A. Blanchard","L. Blot","S. Borgani","M. L. Brown","S. Bruton","A. Calabro","F. Caro","T. Castro","F. Cogato","S. Davini","G. Desprez","A. Díaz-Sánchez","J. J. Diaz","S. Di Domizio","J. M. Diego","P. -A. Duc","A. Enia","Y. Fang","A. G. Ferrari","A. Finoguenov","A. Fontana","A. Franco","J. García-Bellido","T. Gasparetto","V. Gautard","E. Gaztanaga","F. Giacomini","F. Gianotti","M. Guidi","C. M. Gutierrez","A. Hall","S. Hemmati","H. Hildebrandt","J. Hjorth","J. J. E. Kajava","Y. Kang","V. Kansal","D. Karagiannis","C. C. Kirkpatrick","S. Kruk","L. Legrand","M. Lembo","F. Lepori","G. Leroy","J. Lesgourgues","L. Leuzzi","T. I. Liaudat","J. Macias-Perez","M. Magliocchetti","F. Mannucci","R. Maoli","C. J. A. P. Martins","L. Maurin","M. Miluzio","P. Monaco","G. Morgante","K. Naidoo","A. Navarro-Alsina","F. Passalacqua","K. Paterson","L. Patrizii","A. Pisani","D. Potter","S. Quai","M. Radovich","P. -F. Rocci","G. Rodighiero","S. Sacquegna","M. Sahlén","D. B. Sanders","E. Sarpa","A. Schneider","M. Schultheis","D. Sciotti","E. Sellentin","F. Shankar","L. C. Smith","K. Tanidis","G. Testera","R. Teyssier","S. Tosi","A. Troja","M. Tucci","C. Valieri","D. Vergani","G. Verza","N. A. Walton"],"pdf_url":"https://arxiv.org/pdf/2503.15321v1.pdf","comment":"Paper submitted as part of the A&A Special Issue `Euclid Quick Data\n  Release (Q1)', 32 pages, 26 figures"},{"id":"http://arxiv.org/abs/2503.15300v1","updated":"2025-03-19T15:22:23Z","published":"2025-03-19T15:22:23Z","title":"SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes","summary":"  Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/.\n","authors":["Weixiao Gao","Liangliang Nan","Hugo Ledoux"],"pdf_url":"https://arxiv.org/pdf/2503.15300v1.pdf","comment":"22 pages, 24 figures"},{"id":"http://arxiv.org/abs/2503.15295v1","updated":"2025-03-19T15:17:14Z","published":"2025-03-19T15:17:14Z","title":"DCA: Dividing and Conquering Amnesia in Incremental Object Detection","summary":"  Incremental object detection (IOD) aims to cultivate an object detector that\ncan continuously localize and recognize novel classes while preserving its\nperformance on previous classes. Existing methods achieve certain success by\nimproving knowledge distillation and exemplar replay for transformer-based\ndetection frameworks, but the intrinsic forgetting mechanisms remain\nunderexplored. In this paper, we dive into the cause of forgetting and discover\nforgetting imbalance between localization and recognition in transformer-based\nIOD, which means that localization is less-forgetting and can generalize to\nfuture classes, whereas catastrophic forgetting occurs primarily on\nrecognition. Based on these insights, we propose a Divide-and-Conquer Amnesia\n(DCA) strategy, which redesigns the transformer-based IOD into a\nlocalization-then-recognition process. DCA can well maintain and transfer the\nlocalization ability, leaving decoupled fragile recognition to be specially\nconquered. To reduce feature drift in recognition, we leverage semantic\nknowledge encoded in pre-trained language models to anchor class\nrepresentations within a unified feature space across incremental tasks. This\ninvolves designing a duplex classifier fusion and embedding class semantic\nfeatures into the recognition decoding process in the form of queries.\nExtensive experiments validate that our approach achieves state-of-the-art\nperformance, especially for long-term incremental scenarios. For example, under\nthe four-step setting on MS-COCO, our DCA strategy significantly improves the\nfinal AP by 6.9%.\n","authors":["Aoting Zhang","Dongbao Yang","Chang Liu","Xiaopeng Hong","Miao Shang","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.15295v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2503.15293v1","updated":"2025-03-19T15:12:26Z","published":"2025-03-19T15:12:26Z","title":"Test-Time Backdoor Detection for Object Detection Models","summary":"  Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks.\n","authors":["Hangtao Zhang","Yichen Wang","Shihui Yan","Chenyu Zhu","Ziqi Zhou","Linshan Hou","Shengshan Hu","Minghui Li","Yanjun Zhang","Leo Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15293v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2502.21201v3","updated":"2025-03-19T15:11:51Z","published":"2025-02-28T16:18:57Z","title":"The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in\n  Wildlife Behaviour Recognition","summary":"  Computer vision analysis of camera trap video footage is essential for\nwildlife conservation, as captured behaviours offer some of the earliest\nindicators of changes in population health. Recently, several high-impact\nanimal behaviour datasets and methods have been introduced to encourage their\nuse; however, the role of behaviour-correlated background information and its\nsignificant effect on out-of-distribution generalisation remain unexplored. In\nresponse, we present the PanAf-FGBG dataset, featuring 20 hours of wild\nchimpanzee behaviours, recorded at over 350 individual camera locations.\nUniquely, it pairs every video with a chimpanzee (referred to as a foreground\nvideo) with a corresponding background video (with no chimpanzee) from the same\ncamera location. We present two views of the dataset: one with overlapping\ncamera locations and one with disjoint locations. This setup enables, for the\nfirst time, direct evaluation of in-distribution and out-of-distribution\nconditions, and for the impact of backgrounds on behaviour recognition models\nto be quantified. All clips come with rich behavioural annotations and metadata\nincluding unique camera IDs and detailed textual scene descriptions.\nAdditionally, we establish several baselines and present a highly effective\nlatent-space normalisation technique that boosts out-of-distribution\nperformance by +5.42% mAP for convolutional and +3.75% mAP for\ntransformer-based models. Finally, we provide an in-depth analysis on the role\nof backgrounds in out-of-distribution behaviour recognition, including the so\nfar unexplored impact of background durations (i.e., the count of background\nframes within foreground videos).\n","authors":["Otto Brookes","Maksim Kukushkin","Majid Mirmehdi","Colleen Stephens","Paula Dieguez","Thurston C. Hicks","Sorrel Jones","Kevin Lee","Maureen S. McCarthy","Amelia Meier","Emmanuelle Normand","Erin G. Wessling","Roman M. Wittig","Kevin Langergraber","Klaus Zuberbühler","Lukas Boesch","Thomas Schmid","Mimi Arandjelovic","Hjalmar Kühl","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2502.21201v3.pdf","comment":"2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR)"},{"id":"http://arxiv.org/abs/2503.15288v1","updated":"2025-03-19T15:09:29Z","published":"2025-03-19T15:09:29Z","title":"Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning\n  for efficient CME tracking","summary":"  Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH.\n","authors":["Justin Le Louëdec","Maike Bauer","Tanja Amerstorfer","Jackie A. Davies"],"pdf_url":"https://arxiv.org/pdf/2503.15288v1.pdf","comment":"24 pages, 11 figures, 1 tables, submitted to AGU Space Weather on\n  14th Marc 2025"},{"id":"http://arxiv.org/abs/2503.15285v1","updated":"2025-03-19T15:04:01Z","published":"2025-03-19T15:04:01Z","title":"PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration\n  between LiDAR Point Cloud and Camera Image","summary":"  The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%.\n","authors":["Yuanchao Yue","Zhengxin Li","Wei Zhang","Hui Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.15285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15284v1","updated":"2025-03-19T15:03:41Z","published":"2025-03-19T15:03:41Z","title":"EdgeRegNet: Edge Feature-based Multimodal Registration Network between\n  Images and LiDAR Point Clouds","summary":"  Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance.\n","authors":["Yuanchao Yue","Hui Yuan","Qinglong Miao","Xiaolong Mao","Raouf Hamzaoui","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2503.15284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15283v1","updated":"2025-03-19T15:03:19Z","published":"2025-03-19T15:03:19Z","title":"TF-TI2I: Training-Free Text-and-Image-to-Image Generation via\n  Multi-Modal Implicit-Context Learning in Text-to-Image Models","summary":"  Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks.\n","authors":["Teng-Fang Hsiao","Bo-Kai Ruan","Yi-Lun Wu","Tzu-Ling Lin","Hong-Han Shuai"],"pdf_url":"https://arxiv.org/pdf/2503.15283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12083v2","updated":"2025-03-19T15:02:48Z","published":"2024-12-16T18:52:56Z","title":"IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and\n  Illuminations","summary":"  Capturing geometric and material information from images remains a\nfundamental challenge in computer vision and graphics. Traditional\noptimization-based methods often require hours of computational time to\nreconstruct geometry, material properties, and environmental lighting from\ndense multi-view inputs, while still struggling with inherent ambiguities\nbetween lighting and material. On the other hand, learning-based approaches\nleverage rich material priors from existing 3D object datasets but face\nchallenges with maintaining multi-view consistency. In this paper, we introduce\nIDArb, a diffusion-based model designed to perform intrinsic decomposition on\nan arbitrary number of images under varying illuminations. Our method achieves\naccurate and multi-view consistent estimation on surface normals and material\nproperties. This is made possible through a novel cross-view, cross-domain\nattention module and an illumination-augmented, view-adaptive training\nstrategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides\nlarge-scale multi-view intrinsic data and renderings under diverse lighting\nconditions, supporting robust training. Extensive experiments demonstrate that\nIDArb outperforms state-of-the-art methods both qualitatively and\nquantitatively. Moreover, our approach facilitates a range of downstream tasks,\nincluding single-image relighting, photometric stereo, and 3D reconstruction,\nhighlighting its broad applications in realistic 3D content creation.\n","authors":["Zhibing Li","Tong Wu","Jing Tan","Mengchen Zhang","Jiaqi Wang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2412.12083v2.pdf","comment":"ICLR 2025. Project Page: https://lizb6626.github.io/IDArb/"},{"id":"http://arxiv.org/abs/2503.15275v1","updated":"2025-03-19T14:51:27Z","published":"2025-03-19T14:51:27Z","title":"Challenges and Trends in Egocentric Vision: A Survey","summary":"  With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield.\n","authors":["Xiang Li","Heqian Qiu","Lanxiao Wang","Hanwen Zhang","Chenghao Qi","Linfeng Han","Huiyu Xiong","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2503.15275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13393v2","updated":"2025-03-19T14:49:31Z","published":"2024-12-18T00:10:00Z","title":"MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction\n  in the Wild","summary":"  Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MaskHand, a novel generative\nmasked model for hand mesh recovery that synthesizes plausible 3D hand meshes\nby learning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MaskHand consists of two key components: (1) a\nVQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a\nlatent space, and (2) a Context-Guided Masked Transformer that randomly masks\nout pose tokens and learns their joint distribution, conditioned on corrupted\ntoken sequence, image context, and 2D pose cues. This learned distribution\nfacilitates confidence-guided sampling during inference, producing mesh\nreconstructions with low uncertainty and high precision. Extensive evaluations\non benchmark and real-world datasets demonstrate that MaskHand achieves\nstate-of-the-art accuracy, robustness, and realism in 3D hand mesh\nreconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html.\n","authors":["Muhammad Usama Saleem","Ekkasit Pinyoanuntapong","Mayur Jagdishbhai Patel","Hongfei Xue","Ahmed Helmy","Srijan Das","Pu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15979v2","updated":"2025-03-19T14:43:51Z","published":"2024-04-24T16:54:39Z","title":"On the Fourier analysis in the SO(3) space : EquiLoPO Network","summary":"  Analyzing volumetric data with rotational invariance or equivariance is an\nactive topic in current research. Existing deep-learning approaches utilize\neither group convolutional networks limited to discrete rotations or steerable\nconvolutional networks with constrained filter structures. This work proposes a\nnovel equivariant neural network architecture that achieves analytical\nEquivariance to Local Pattern Orientation on the continuous SO(3) group while\nallowing unconstrained trainable filters - EquiLoPO Network. Our key\ninnovations are a group convolutional operation leveraging irreducible\nrepresentations as the Fourier basis and a local activation function in the\nSO(3) space that provides a well-defined mapping from input to output\nfunctions, preserving equivariance. By integrating these operations into a\nResNet-style architecture, we propose a model that overcomes the limitations of\nprior methods. A comprehensive evaluation on diverse 3D medical imaging\ndatasets from MedMNIST3D demonstrates the effectiveness of our approach, which\nconsistently outperforms state of the art. This work suggests the benefits of\ntrue rotational equivariance on SO(3) and flexible unconstrained filters\nenabled by the local activation function, providing a flexible framework for\nequivariant deep learning on volumetric data with potential applications across\ndomains. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPO/-/tree/main/EquiLoPO.\n","authors":["Dmitrii Zhemchuzhnikov","Sergei Grudinin"],"pdf_url":"https://arxiv.org/pdf/2404.15979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11236v3","updated":"2025-03-19T14:41:25Z","published":"2024-10-15T03:43:51Z","title":"Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward\n  Modeling","summary":"  In this paper, we focus on the task of conditional image generation, where an\nimage is synthesized according to user instructions. The critical challenge\nunderpinning this task is ensuring both the fidelity of the generated images\nand their semantic alignment with the provided conditions. To tackle this\nissue, previous studies have employed supervised perceptual losses derived from\npre-trained models, i.e., reward models, to enforce alignment between the\ncondition and the generated result. However, we observe one inherent\nshortcoming: considering the diversity of synthesized images, the reward model\nusually provides inaccurate feedback when encountering newly generated data,\nwhich can undermine the training process. To address this limitation, we\npropose an uncertainty-aware reward modeling, called Ctrl-U, including\nuncertainty estimation and uncertainty-aware regularization, designed to reduce\nthe adverse effects of imprecise feedback from the reward model. Given the\ninherent cognitive uncertainty within reward models, even images generated\nunder identical conditions often result in a relatively large discrepancy in\nreward loss. Inspired by the observation, we explicitly leverage such\nprediction variance as an uncertainty indicator. Based on the uncertainty\nestimation, we regularize the model training by adaptively rectifying the\nreward. In particular, rewards with lower uncertainty receive higher loss\nweights, while those with higher uncertainty are given reduced weights to allow\nfor larger variability. The proposed uncertainty regularization facilitates\nreward fine-tuning through consistency construction. Extensive experiments\nvalidate the effectiveness of our methodology in improving the controllability\nand generation quality, as well as its scalability across diverse conditional\nscenarios. Codes are publicly available at\nhttps://grenoble-zhang.github.io/Ctrl-U-Page/.\n","authors":["Guiyu Zhang","Huan-ang Gao","Zijian Jiang","Hao Zhao","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.11236v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.15265v1","updated":"2025-03-19T14:39:30Z","published":"2025-03-19T14:39:30Z","title":"DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement\n  Learning","summary":"  Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/\n","authors":["Ruowen Zhao","Junliang Ye","Zhengyi Wang","Guangce Liu","Yiwen Chen","Yikai Wang","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.15265v1.pdf","comment":"Project page: https://zhaorw02.github.io/DeepMesh/"},{"id":"http://arxiv.org/abs/2503.15264v1","updated":"2025-03-19T14:37:21Z","published":"2025-03-19T14:37:21Z","title":"LEGION: Learning to Ground and Explain for Synthetic Image Detection","summary":"  The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased.\n","authors":["Hengrui Kang","Siwei Wen","Zichen Wen","Junyan Ye","Weijia Li","Peilin Feng","Baichuan Zhou","Bin Wang","Dahua Lin","Linfeng Zhang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2503.15264v1.pdf","comment":"Project Page: https://opendatalab.github.io/LEGION"},{"id":"http://arxiv.org/abs/2503.15260v1","updated":"2025-03-19T14:32:14Z","published":"2025-03-19T14:32:14Z","title":"DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation","summary":"  Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods.\n","authors":["Lei Shi","Xi Fang","Naiyu Wang","Junxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.15260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05040v2","updated":"2025-03-19T14:27:29Z","published":"2025-02-07T16:07:51Z","title":"GaussRender: Learning 3D Occupancy with Gaussian Rendering","summary":"  Understanding the 3D geometry and semantics of driving scenes is critical for\nsafe autonomous driving. Recent advances in 3D occupancy prediction have\nimproved scene representation but often suffer from spatial inconsistencies,\nleading to floating artifacts and poor surface localization. Existing\nvoxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence. In\nthis paper, we propose GaussRender, a module that improves 3D occupancy\nlearning by enforcing projective consistency. Our key idea is to project both\npredicted and ground-truth 3D occupancy into 2D camera views, where we apply\nsupervision. Our method penalizes 3D configurations that produce inconsistent\n2D projections, thereby enforcing a more coherent 3D structure. To achieve this\nefficiently, we leverage differentiable rendering with Gaussian splatting.\nGaussRender seamlessly integrates with existing architectures while maintaining\nefficiency and requiring no inference-time modifications. Extensive evaluations\non multiple benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes,\nSSCBench-KITTI360) demonstrate that GaussRender significantly improves\ngeometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc,\nSymphonies), achieving state-of-the-art results, particularly on\nsurface-sensitive metrics. The code is open-sourced at\nhttps://github.com/valeoai/GaussRender.\n","authors":["Loïck Chambon","Eloi Zablocki","Alexandre Boulch","Mickaël Chen","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2502.05040v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08352v2","updated":"2025-03-19T14:18:14Z","published":"2025-03-11T12:06:57Z","title":"Mitigating Ambiguities in 3D Classification with Gaussian Splatting","summary":"  3D classification with point cloud input is a fundamental problem in 3D\nvision. However, due to the discrete nature and the insufficient material\ndescription of point cloud representations, there are ambiguities in\ndistinguishing wire-like and flat surfaces, as well as transparent or\nreflective objects. To address these issues, we propose Gaussian Splatting (GS)\npoint cloud-based 3D classification. We find that the scale and rotation\ncoefficients in the GS point cloud help characterize surface types.\nSpecifically, wire-like surfaces consist of multiple slender Gaussian\nellipsoids, while flat surfaces are composed of a few flat Gaussian ellipsoids.\nAdditionally, the opacity in the GS point cloud represents the transparency\ncharacteristics of objects. As a result, ambiguities in point cloud-based 3D\nclassification can be mitigated utilizing GS point cloud as input. To verify\nthe effectiveness of GS point cloud input, we construct the first real-world GS\npoint cloud dataset in the community, which includes 20 categories with 200\nobjects in each category. Experiments not only validate the superiority of GS\npoint cloud input, especially in distinguishing ambiguous objects, but also\ndemonstrate the generalization ability across different classification methods.\n","authors":["Ruiqi Zhang","Hao Zhu","Jingyi Zhao","Qi Zhang","Xun Cao","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2503.08352v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15234v1","updated":"2025-03-19T14:13:02Z","published":"2025-03-19T14:13:02Z","title":"CoE: Chain-of-Explanation via Automatic Visual Concept Circuit\n  Description and Polysemanticity Quantification","summary":"  Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores.\n","authors":["Wenlong Yu","Qilong Wang","Chuang Liu","Dong Li","Qinghua Hu"],"pdf_url":"https://arxiv.org/pdf/2503.15234v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.14493v2","updated":"2025-03-19T14:10:18Z","published":"2025-03-18T17:58:03Z","title":"State Space Model Meets Transformer: A New Paradigm for 3D Object\n  Detection","summary":"  DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets.\n","authors":["Chuxin Wang","Wenfei Yang","Xiang Liu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14493v2.pdf","comment":"Accepted by ICLR 2025. Project url:\n  https://chuxwa.github.io/project_DEST/"},{"id":"http://arxiv.org/abs/2502.19634v2","updated":"2025-03-19T13:55:33Z","published":"2025-02-26T23:57:34Z","title":"MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language\n  Models (VLMs) via Reinforcement Learning","summary":"  Reasoning is a critical frontier for advancing medical image analysis, where\ntransparency and trustworthiness play a central role in both clinician trust\nand regulatory approval. Although Medical Visual Language Models (VLMs) show\npromise for radiological tasks, most existing VLMs merely produce final answers\nwithout revealing the underlying reasoning. To address this gap, we introduce\nMedVLM-R1, a medical VLM that explicitly generates natural language reasoning\nto enhance transparency and trustworthiness. Instead of relying on supervised\nfine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs a\nreinforcement learning framework that incentivizes the model to discover\nhuman-interpretable reasoning paths without using any reasoning references.\nDespite limited training data (600 visual question answering samples) and model\nparameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI,\nCT, and X-ray benchmarks, outperforming larger models trained on over a million\nsamples. It also demonstrates robust domain generalization under\nout-of-distribution tasks. By unifying medical image analysis with explicit\nreasoning, MedVLM-R1 marks a pivotal step toward trustworthy and interpretable\nAI in clinical practice. Inference model is available at:\nhttps://huggingface.co/JZPeterPan/MedVLM-R1.\n","authors":["Jiazhen Pan","Che Liu","Junde Wu","Fenglin Liu","Jiayuan Zhu","Hongwei Bran Li","Chen Chen","Cheng Ouyang","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2502.19634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15211v1","updated":"2025-03-19T13:51:00Z","published":"2025-03-19T13:51:00Z","title":"GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector","summary":"  We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object\ndetector enhanced by neural radiance fields. The key to accurate 3D object\ndetection is in effective voxel representation. However, due to occlusion and\nlack of 3D information, constructing 3D features from multi-view 2D images is\nchallenging. Addressing that, we introduce a unique 3D positional information\nembedded voxel optimization mechanism to fuse multi-view features. To\nprioritize neural field reconstruction in object regions, we also devise a\ndouble importance sampling scheme for the NeRF branch of our detector. We\nadditionally propose an opacity optimization module for precise voxel opacity\nprediction by enforcing multi-view consistency constraints. Moreover, to\nfurther improve voxel density consistency across multiple perspectives, we\nincorporate ray distance as a weighting factor to minimize cumulative ray\nerrors. Our unique modules synergetically form an end-to-end neural model that\nestablishes new state-of-the-art in NeRF-based multi-view 3D detection,\nverified with extensive experiments on ScanNet and ARKITScenes. Code will be\navailable at https://github.com/ZechuanLi/GO-N3RDet.\n","authors":["Zechuan Li","Hongshan Yu","Yihao Ding","Jinhao Qiao","Basim Azam","Naveed Akhtar"],"pdf_url":"https://arxiv.org/pdf/2503.15211v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.15208v1","updated":"2025-03-19T13:49:48Z","published":"2025-03-19T13:49:48Z","title":"DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D\n  Driving Scene Generation","summary":"  Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. A key challenge lies in finding\nan efficient and generalizable geometric representation that seamlessly\nconnects temporal and spatial synthesis. To address this, we propose DiST-4D,\nthe first disentangled spatiotemporal diffusion framework for 4D driving scene\ngeneration, which leverages metric depth as the core geometric representation.\nDiST-4D decomposes the problem into two diffusion processes: DiST-T, which\npredicts future metric depth and multi-view RGB sequences directly from past\nobservations, and DiST-S, which enables spatial NVS by training only on\nexisting viewpoints while enforcing cycle consistency. This cycle consistency\nmechanism introduces a forward-backward rendering constraint, reducing the\ngeneralization gap between observed and unseen viewpoints. Metric depth is\nessential for both accurate reliable forecasting and accurate spatial NVS, as\nit provides a view-consistent geometric representation that generalizes well to\nunseen perspectives. Experiments demonstrate that DiST-4D achieves\nstate-of-the-art performance in both temporal prediction and NVS tasks, while\nalso delivering competitive performance in planning-related evaluations.\n","authors":["Jiazhe Guo","Yikang Ding","Xiwu Chen","Shuo Chen","Bohan Li","Yingshuang Zou","Xiaoyang Lyu","Feiyang Tan","Xiaojuan Qi","Zhiheng Li","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18212v2","updated":"2025-03-19T13:48:18Z","published":"2024-04-28T15:07:53Z","title":"Paint by Inpaint: Learning to Add Image Objects by Removing Them First","summary":"  Image editing has advanced significantly with the introduction of\ntext-conditioned diffusion models. Despite this progress, seamlessly adding\nobjects to images based on textual instructions without requiring user-provided\ninput masks remains a challenge. We address this by leveraging the insight that\nremoving objects (Inpaint) is significantly simpler than its inverse process of\nadding them (Paint), attributed to inpainting models that benefit from\nsegmentation mask guidance. Capitalizing on this realization, by implementing\nan automated and extensive pipeline, we curate a filtered large-scale image\ndataset containing pairs of images and their corresponding object-removed\nversions. Using these pairs, we train a diffusion model to inverse the\ninpainting process, effectively adding objects into images. Unlike other\nediting datasets, ours features natural target images instead of synthetic ones\nwhile ensuring source-target consistency by construction. Additionally, we\nutilize a large Vision-Language Model to provide detailed descriptions of the\nremoved objects and a Large Language Model to convert these descriptions into\ndiverse, natural-language instructions. Our quantitative and qualitative\nresults show that the trained model surpasses existing models in both object\naddition and general editing tasks. Visit our project page for the released\ndataset and trained models: https://rotsteinnoam.github.io/Paint-by-Inpaint.\n","authors":["Navve Wasserman","Noam Rotstein","Roy Ganz","Ron Kimmel"],"pdf_url":"https://arxiv.org/pdf/2404.18212v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10579v3","updated":"2025-03-19T13:40:18Z","published":"2023-05-17T21:27:27Z","title":"MultiPlaneNeRF: Neural Radiance Field with Non-Trainable Representation","summary":"  NeRF is a popular model that efficiently represents 3D objects from 2D\nimages. However, vanilla NeRF has some important limitations. NeRF must be\ntrained on each object separately. The training time is long since we encode\nthe object's shape and color in neural network weights. Moreover, NeRF does not\ngeneralize well to unseen data. In this paper, we present MultiPlaneNeRF -- a\nmodel that simultaneously solves the above problems. Our model works directly\non 2D images. We project 3D points on 2D images to produce non-trainable\nrepresentations. The projection step is not parametrized and a very shallow\ndecoder can efficiently process the representation. Furthermore, we can train\nMultiPlaneNeRF on a large data set and force our implicit decoder to generalize\nacross many objects. Consequently, we can only replace the 2D images (without\nadditional training) to produce a NeRF representation of the new object. In the\nexperimental section, we demonstrate that MultiPlaneNeRF achieves results\ncomparable to state-of-the-art models for synthesizing new views and has\ngeneralization properties. Additionally, MultiPlane decoder can be used as a\ncomponent in large generative models like GANs.\n","authors":["Dominik Zimny","Artur Kasymov","Adam Kania","Jacek Tabor","Maciej Zięba","Marcin Mazur","Przemysław Spurek"],"pdf_url":"https://arxiv.org/pdf/2305.10579v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15197v1","updated":"2025-03-19T13:37:52Z","published":"2025-03-19T13:37:52Z","title":"Detect-and-Guide: Self-regulation of Diffusion Models for Safe\n  Text-to-Image Generation via Guideline Token Optimization","summary":"  Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts.\n","authors":["Feifei Li","Mi Zhang","Yiming Sun","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2503.15197v1.pdf","comment":"CVPR25"},{"id":"http://arxiv.org/abs/2503.15195v1","updated":"2025-03-19T13:33:29Z","published":"2025-03-19T13:33:29Z","title":"Benchmarking Large Language Models for Handwritten Text Recognition","summary":"  Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.\n","authors":["Giorgia Crosilla","Lukas Klic","Giovanni Colavizza"],"pdf_url":"https://arxiv.org/pdf/2503.15195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09082v3","updated":"2025-03-19T13:31:16Z","published":"2024-12-12T09:08:13Z","title":"Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and\n  Method","summary":"  Existing Vision-Language Navigation (VLN) methods primarily focus on\nsingle-stage navigation, limiting their effectiveness in multi-stage and\nlong-horizon tasks within complex and dynamic environments. To address these\nlimitations, we propose a novel VLN task, named Long-Horizon Vision-Language\nNavigation (LH-VLN), which emphasizes long-term planning and decision\nconsistency across consecutive subtasks. Furthermore, to support LH-VLN, we\ndevelop an automated data generation platform NavGen, which constructs datasets\nwith complex task structures and improves data utility through a bidirectional,\nmulti-granularity generation approach. To accurately evaluate complex tasks, we\nconstruct the Long-Horizon Planning and Reasoning in VLN (LHPR-VLN) benchmark\nconsisting of 3,260 tasks with an average of 150 task steps, serving as the\nfirst dataset specifically designed for the long-horizon vision-language\nnavigation task. Furthermore, we propose Independent Success Rate (ISR),\nConditional Success Rate (CSR), and CSR weight by Ground Truth (CGT) metrics,\nto provide fine-grained assessments of task completion. To improve model\nadaptability in complex tasks, we propose a novel Multi-Granularity Dynamic\nMemory (MGDM) module that integrates short-term memory blurring with long-term\nmemory retrieval to enable flexible navigation in dynamic environments. Our\nplatform, benchmark and method supply LH-VLN with a robust data generation\npipeline, comprehensive model evaluation dataset, reasonable metrics, and a\nnovel VLN model, establishing a foundational framework for advancing LH-VLN.\n","authors":["Xinshuai Song","Weixing Chen","Yang Liu","Weikai Chen","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2412.09082v3.pdf","comment":"Accepted by CVPR 2025. A novel Long-Horizon Vision-Language\n  Navigation task, project page: https://hcplab-sysu.github.io/LH-VLN/"},{"id":"http://arxiv.org/abs/2411.16819v3","updated":"2025-03-19T13:30:44Z","published":"2024-11-25T16:41:45Z","title":"Pathways on the Image Manifold: Image Editing via Video Generation","summary":"  Recent advances in image editing, driven by image diffusion models, have\nshown remarkable progress. However, significant challenges remain, as these\nmodels often struggle to follow complex edit instructions accurately and\nfrequently compromise fidelity by altering key elements of the original image.\nSimultaneously, video generation has made remarkable strides, with models that\neffectively function as consistent and continuous world simulators. In this\npaper, we propose merging these two fields by utilizing image-to-video models\nfor image editing. We reformulate image editing as a temporal process, using\npretrained video models to create smooth transitions from the original image to\nthe desired edit. This approach traverses the image manifold continuously,\nensuring consistent edits while preserving the original image's key aspects.\nOur approach achieves state-of-the-art results on text-based image editing,\ndemonstrating significant improvements in both edit accuracy and image\npreservation. Visit our project page:\nhttps://rotsteinnoam.github.io/Frame2Frame.\n","authors":["Noam Rotstein","Gal Yona","Daniel Silver","Roy Velich","David Bensaïd","Ron Kimmel"],"pdf_url":"https://arxiv.org/pdf/2411.16819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13684v2","updated":"2025-03-19T13:22:39Z","published":"2024-12-18T10:19:12Z","title":"MMO-IG: Multi-Class and Multi-Scale Object Image Generation for Remote\n  Sensing","summary":"  The rapid advancement of deep generative models (DGMs) has significantly\nadvanced research in computer vision, providing a cost-effective alternative to\nacquiring vast quantities of expensive imagery. However, existing methods\npredominantly focus on synthesizing remote sensing (RS) images aligned with\nreal images in a global layout view, which limits their applicability in RS\nimage object detection (RSIOD) research. To address these challenges, we\npropose a multi-class and multi-scale object image generator based on DGMs,\ntermed MMO-IG, designed to generate RS images with supervised object labels\nfrom global and local aspects simultaneously. Specifically, from the local\nview, MMO-IG encodes various RS instances using an iso-spacing instance map\n(ISIM). During the generation process, it decodes each instance region with\niso-spacing value in ISIM-corresponding to both background and foreground\ninstances-to produce RS images through the denoising process of diffusion\nmodels. Considering the complex interdependencies among MMOs, we construct a\nspatial-cross dependency knowledge graph (SCDKG). This ensures a realistic and\nreliable multidirectional distribution among MMOs for region embedding, thereby\nreducing the discrepancy between source and target domains. Besides, we propose\na structured object distribution instruction (SODI) to guide the generation of\nsynthesized RS image content from a global aspect with SCDKG-based ISIM\ntogether. Extensive experimental results demonstrate that our MMO-IG exhibits\nsuperior generation capabilities for RS images with dense MMO-supervised\nlabels, and RS detectors pre-trained with MMO-IG show excellent performance on\nreal-world datasets.\n","authors":["Chuang Yang","Bingxuan Zhao","Qing Zhou","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15185v1","updated":"2025-03-19T13:14:57Z","published":"2025-03-19T13:14:57Z","title":"3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware\n  View Transformation","summary":"  The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution.\n","authors":["Gyeongrok Oh","Sungjune Kim","Heeju Ko","Hyung-gun Chi","Jinkyu Kim","Dongwook Lee","Daehyun Ji","Sungjoon Choi","Sujin Jang","Sangpil Kim"],"pdf_url":"https://arxiv.org/pdf/2503.15185v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2410.06385v2","updated":"2025-03-19T13:12:12Z","published":"2024-10-08T21:33:02Z","title":"Skin Cancer Machine Learning Model Tone Bias","summary":"  Background: Many open-source skin cancer image datasets are the result of\nclinical trials conducted in countries with lighter skin tones. Due to this\ntone imbalance, machine learning models derived from these datasets can perform\nwell at detecting skin cancer for lighter skin tones. Any tone bias in these\nmodels could introduce fairness concerns and reduce public trust in the\nartificial intelligence health field.\n  Methods: We examine a subset of images from the International Skin Imaging\nCollaboration (ISIC) archive that provide tone information. The subset has a\nsignificant tone imbalance. These imbalances could explain a model's tone bias.\nTo address this, we train models using the imbalanced dataset and a balanced\ndataset to compare against. The datasets are used to train a deep convolutional\nneural network model to classify the images as malignant or benign. We then\nevaluate the models' disparate impact, based on selection rate, relative to\ndark or light skin tone.\n  Results: Using the imbalanced dataset, we found that the model is\nsignificantly better at detecting malignant images in lighter tone resulting in\na disparate impact of 0.577. Using the balanced dataset, we found that the\nmodel is also significantly better at detecting malignant images in lighter\nversus darker tones with a disparate impact of 0.684. Using the imbalanced or\nbalanced dataset to train the model still results in a disparate impact well\nbelow the standard threshold of 0.80 which suggests the model is biased with\nrespect to skin tone.\n  Conclusion: The results show that typical skin cancer machine learning models\ncan be tone biased. These results provide evidence that diagnosis or tone\nimbalance is not the cause of the bias. Other techniques will be necessary to\nidentify and address the bias in these models, an area of future investigation.\n","authors":["James Pope","Md Hassanuzzaman","William Chapman","Huw Day","Mingmar Sherpa","Omar Emara","Nirmala Adhikari","Ayush Joshi"],"pdf_url":"https://arxiv.org/pdf/2410.06385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15176v1","updated":"2025-03-19T13:02:01Z","published":"2025-03-19T13:02:01Z","title":"A Review on Large Language Models for Visual Analytics","summary":"  This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration.\n","authors":["Navya Sonal Agarwal","Sanjay Kumar Sonbhadra"],"pdf_url":"https://arxiv.org/pdf/2503.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14324v2","updated":"2025-03-19T12:58:33Z","published":"2025-03-18T14:56:46Z","title":"DualToken: Towards Unifying Visual Understanding and Generation with\n  Dual Visual Vocabularies","summary":"  The differing representation spaces required for visual understanding and\ngeneration pose a challenge in unifying them within the autoregressive paradigm\nof large language models. A vision tokenizer trained for reconstruction excels\nat capturing low-level perceptual details, making it well-suited for visual\ngeneration but lacking high-level semantic representations for understanding\ntasks. Conversely, a vision encoder trained via contrastive learning aligns\nwell with language but struggles to decode back into the pixel space for\ngeneration tasks. To bridge this gap, we propose DualToken, a method that\nunifies representations for both understanding and generation within a single\ntokenizer. However, directly integrating reconstruction and semantic objectives\nin a single tokenizer creates conflicts, leading to degraded performance in\nboth reconstruction quality and semantic performance. Instead of forcing a\nsingle codebook to handle both semantic and perceptual information, DualToken\ndisentangles them by introducing separate codebooks for high and low-level\nfeatures, effectively transforming their inherent conflict into a synergistic\nrelationship. As a result, DualToken achieves state-of-the-art performance in\nboth reconstruction and semantic tasks while demonstrating remarkable\neffectiveness in downstream MLLM understanding and generation tasks. Notably,\nwe also show that DualToken, as a unified tokenizer, surpasses the naive\ncombination of two distinct types vision encoders, providing superior\nperformance within a unified MLLM.\n","authors":["Wei Song","Yuran Wang","Zijia Song","Yadong Li","Haoze Sun","Weipeng Chen","Zenan Zhou","Jianhua Xu","Jiaqi Wang","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2503.14324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15168v1","updated":"2025-03-19T12:50:40Z","published":"2025-03-19T12:50:40Z","title":"World Models in Artificial Intelligence: Sensing, Learning, and\n  Reasoning Like a Child","summary":"  World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.\n","authors":["Javier Del Ser","Jesus L. Lobo","Heimo Müller","Andreas Holzinger"],"pdf_url":"https://arxiv.org/pdf/2503.15168v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2411.15778v4","updated":"2025-03-19T12:48:55Z","published":"2024-11-24T10:58:48Z","title":"Enhancing the automatic segmentation and analysis of 3D liver\n  vasculature models","summary":"  Surgical assessment of liver cancer patients requires identification of the\nvessel trees from medical images. Specifically, the venous trees - the portal\n(perfusing) and the hepatic (draining) trees are important for understanding\nthe liver anatomy and disease state, and perform surgery planning. This\nresearch aims to improve the 3D segmentation, skeletonization, and subsequent\nanalysis of vessel trees, by creating an automatic pipeline based on deep\nlearning and image processing techniques.\n  The first part of this work explores the impact of differentiable\nskeletonization methods such as ClDice and morphological skeletonization loss,\non the overall liver vessel segmentation performance. To this aim, it studies\nhow to improve vessel tree connectivity.\n  The second part of this study converts a single class vessel segmentation\ninto multi-class ones, separating the two venous trees. It builds on the\nprevious two-class vessel segmentation model, which vessel tree outputs might\nbe entangled, and on connected components and skeleton analyses of the trees.\n  After providing sub-labeling of the specific anatomical branches of each\nvenous tree, these algorithms also enable a morphometric analysis of the vessel\ntrees by extracting various geometrical markers.\n  In conclusion, we propose a method that successfully improves current\nskeletonization methods, for extensive vascular trees that contain vessels of\ndifferent calibers. The separation algorithm creates a clean multi-class\nsegmentation of the vessels, validated by surgeons to provide low error. A new,\npublicly shared high-quality liver vessel dataset of 77 cases is thus created.\nFinally a method to annotate vessel trees according to anatomy is provided,\nenabling a unique liver vessel morphometry analysis.\n","authors":["Yassine Machta","Omar Ali","Kevin Hakkakian","Ana Vlasceanu","Amaury Facque","Nicolas Golse","Irene Vignon-Clementel"],"pdf_url":"https://arxiv.org/pdf/2411.15778v4.pdf","comment":"Paper presented at MICCAI 2024 Workshop: ADSMI. This work was done in\n  the context of an internship at Simbiotx, Inria"},{"id":"http://arxiv.org/abs/2503.15166v1","updated":"2025-03-19T12:47:37Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.11509v2","updated":"2025-03-19T12:42:41Z","published":"2025-03-14T15:29:58Z","title":"TikZero: Zero-Shot Text-Guided Graphics Program Synthesis","summary":"  With the rise of generative AI, synthesizing figures from text captions\nbecomes a compelling application. However, achieving high geometric precision\nand editability requires representing figures as graphics programs in languages\nlike TikZ, and aligned training data (i.e., graphics programs with captions)\nremains scarce. Meanwhile, large amounts of unaligned graphics programs and\ncaptioned raster images are more readily available. We reconcile these\ndisparate data sources by presenting TikZero, which decouples graphics program\ngeneration from text understanding by using image representations as an\nintermediary bridge. It enables independent training on graphics programs and\ncaptioned images and allows for zero-shot text-guided graphics program\nsynthesis during inference. We show that our method substantially outperforms\nbaselines that can only operate with caption-aligned graphics programs.\nFurthermore, when leveraging caption-aligned graphics programs as a\ncomplementary training signal, TikZero matches or exceeds the performance of\nmuch larger models, including commercial systems like GPT-4o. Our code,\ndatasets, and select models are publicly available.\n","authors":["Jonas Belouadi","Eddy Ilg","Margret Keuper","Hideki Tanaka","Masao Utiyama","Raj Dabre","Steffen Eger","Simone Paolo Ponzetto"],"pdf_url":"https://arxiv.org/pdf/2503.11509v2.pdf","comment":"Project page: https://github.com/potamides/DeTikZify"},{"id":"http://arxiv.org/abs/2503.15161v1","updated":"2025-03-19T12:38:04Z","published":"2025-03-19T12:38:04Z","title":"UltraFlwr -- An Efficient Federated Medical and Surgical Object\n  Detection Framework","summary":"  Object detection shows promise for medical and surgical applications such as\ncell counting and tool tracking. However, its faces multiple real-world edge\ndeployment challenges including limited high-quality annotated data, data\nsharing restrictions, and computational constraints. In this work, we introduce\nUltraFlwr, a framework for federated medical and surgical object detection. By\nleveraging Federated Learning (FL), UltraFlwr enables decentralized model\ntraining across multiple sites without sharing raw data. To further enhance\nUltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation\n(PA) strategies specifically designed for YOLO models in FL. YOLO-PA\nsignificantly reduces communication overhead by up to 83% per round while\nmaintaining performance comparable to Full Aggregation (FA) strategies. Our\nextensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate\nthat YOLO-PA not only provides better client models compared to client-wise\ncentralized training and FA strategies, but also facilitates efficient training\nand deployment across resource-constrained edge devices. Further, we also\nestablish one of the first benchmarks in federated medical and surgical object\ndetection. This paper advances the feasibility of training and deploying\ndetection models on the edge, making federated object detection more practical\nfor time-critical and resource-constrained medical and surgical applications.\nUltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr.\n","authors":["Yang Li","Soumya Snigdha Kundu","Maxence Boels","Toktam Mahmoodi","Sebastien Ourselin","Tom Vercauteren","Prokar Dasgupta","Jonathan Shapey","Alejandro Granados"],"pdf_url":"https://arxiv.org/pdf/2503.15161v1.pdf","comment":"10 pages, 2 figures, under review @ MICCAI"},{"id":"http://arxiv.org/abs/2412.10831v3","updated":"2025-03-19T12:36:47Z","published":"2024-12-14T13:28:40Z","title":"Low-Biased General Annotated Dataset Generation","summary":"  Pre-training backbone networks on a general annotated dataset (e.g.,\nImageNet) that comprises numerous manually collected images with category\nannotations has proven to be indispensable for enhancing the generalization\ncapacity of downstream visual tasks. However, those manually collected images\noften exhibit bias, which is non-transferable across either categories or\ndomains, thus causing the model's generalization capacity degeneration. To\nmitigate this problem, we present a low-biased general annotated dataset\ngeneration framework (lbGen). Instead of expensive manual collection, we aim at\ndirectly generating low-biased images with category annotations. To achieve\nthis goal, we propose to leverage the advantage of a multimodal foundation\nmodel (e.g., CLIP), in terms of aligning images in a low-biased semantic space\ndefined by language. Specifically, we develop a bi-level semantic alignment\nloss, which not only forces all generated images to be consistent with the\nsemantic distribution of all categories belonging to the target dataset in an\nadversarial learning manner, but also requires each generated image to match\nthe semantic description of its category name. In addition, we further cast an\nexisting image quality scoring model into a quality assurance loss to preserve\nthe quality of the generated image. By leveraging these two loss functions, we\ncan obtain a low-biased image generation model by simply fine-tuning a\npre-trained diffusion model using only all category names in the target dataset\nas input. Experimental results confirm that, compared with the manually labeled\ndataset or other synthetic datasets, the utilization of our generated\nlow-biased dataset leads to stable generalization capacity enhancement of\ndifferent backbone networks across various tasks, especially in tasks where the\nmanually labeled samples are scarce.\n","authors":["Dengyang Jiang","Haoyu Wang","Lei Zhang","Wei Wei","Guang Dai","Mengmeng Wang","Jingdong Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10831v3.pdf","comment":"CVPR2025 Accepted Paper"},{"id":"http://arxiv.org/abs/2503.15156v1","updated":"2025-03-19T12:24:29Z","published":"2025-03-19T12:24:29Z","title":"ARC: Anchored Representation Clouds for High-Resolution INR\n  Classification","summary":"  Implicit neural representations (INRs) encode signals in neural network\nweights as a memory-efficient representation, decoupling sampling resolution\nfrom the associated resource costs. Current INR image classification methods\nare demonstrated on low-resolution data and are sensitive to image-space\ntransformations. We attribute these issues to the global, fully-connected MLP\nneural network architecture encoding of current INRs, which lack mechanisms for\nlocal representation: MLPs are sensitive to absolute image location and\nstruggle with high-frequency details. We propose ARC: Anchored Representation\nClouds, a novel INR architecture that explicitly anchors latent vectors locally\nin image-space. By introducing spatial structure to the latent vectors, ARC\ncaptures local image data which in our testing leads to state-of-the-art\nimplicit image classification of both low- and high-resolution images and\nincreased robustness against image-space translation. Code can be found at\nhttps://github.com/JLuij/anchored_representation_clouds.\n","authors":["Joost Luijmes","Alexander Gielisse","Roman Knyazhitskiy","Jan van Gemert"],"pdf_url":"https://arxiv.org/pdf/2503.15156v1.pdf","comment":"Accepted at the ICLR 2025 Workshop on Neural Network Weights as a New\n  Data Modality"},{"id":"http://arxiv.org/abs/2503.12303v3","updated":"2025-03-19T12:22:00Z","published":"2025-03-16T00:25:13Z","title":"Towards Self-Improving Systematic Cognition for Next-Generation\n  Foundation MLLMs","summary":"  Despite their impressive capabilities, Multimodal Large Language Models\n(MLLMs) face challenges with fine-grained perception and complex reasoning.\nPrevalent multimodal pre-training approaches focus on enhancing perception by\ntraining on high-quality image captions due to the extremely high cost of\ncollecting chain-of-thought (CoT) reasoning data for improving reasoning. While\nleveraging advanced MLLMs for caption generation enhances scalability, the\noutputs often lack comprehensiveness and accuracy. In this paper, we introduce\nSelf-Improving cognition (SIcog), a self-learning framework designed to\nconstruct next-generation foundation MLLMs by enhancing their systematic\ncognitive capabilities through multimodal pre-training with self-generated\ndata. Specifically, we propose Chain-of-Description, an approach that improves\nan MLLM's systematic perception by enabling step-by-step visual understanding,\nensuring greater comprehensiveness and accuracy. Additionally, we adopt a\nstructured CoT reasoning technique to enable MLLMs to integrate in-depth\nmultimodal reasoning. To construct a next-generation foundation MLLM with\nself-improved cognition, SIcog first equips an MLLM with systematic perception\nand reasoning abilities using minimal external annotations. The enhanced models\nthen generate detailed captions and CoT reasoning data, which are further\ncurated through self-consistency. This curated data is ultimately used for\nmultimodal pre-training to develop next-generation foundation models. Extensive\nexperiments on both low- and high-resolution MLLMs across diverse benchmarks\ndemonstrate that, with merely 213K self-generated pre-training samples, SIcog\nproduces next-generation foundation MLLMs with significantly improved\ncognition, achieving benchmark-leading performance compared to prevalent\npre-training approaches.\n","authors":["Xiaoying Zhang","Da Peng","Yipeng Zhang","Zonghao Guo","Chengyue Wu","Chi Chen","Wei Ke","Helen Meng","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2503.12303v3.pdf","comment":"38 pages. Preprint, work in progress"},{"id":"http://arxiv.org/abs/2503.11851v2","updated":"2025-03-19T12:18:48Z","published":"2025-03-14T20:28:20Z","title":"DCAT: Dual Cross-Attention Fusion for Disease Classification in\n  Radiological Images with Uncertainty Estimation","summary":"  Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.\n","authors":["Jutika Borah","Hidam Kumarjit Singh"],"pdf_url":"https://arxiv.org/pdf/2503.11851v2.pdf","comment":"18 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.01715v2","updated":"2025-03-19T12:10:34Z","published":"2025-03-03T16:31:55Z","title":"KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via\n  KeyFrame Interpolation","summary":"  Current audio-driven facial animation methods achieve impressive results for\nshort videos but suffer from error accumulation and identity drift when\nextended to longer durations. Existing methods attempt to mitigate this through\nexternal spatial control, increasing long-term consistency but compromising the\nnaturalness of motion. We propose KeyFace, a novel two-stage diffusion-based\nframework, to address these issues. In the first stage, keyframes are generated\nat a low frame rate, conditioned on audio input and an identity frame, to\ncapture essential facial expressions and movements over extended periods of\ntime. In the second stage, an interpolation model fills in the gaps between\nkeyframes, ensuring smooth transitions and temporal coherence. To further\nenhance realism, we incorporate continuous emotion representations and handle a\nwide range of non-speech vocalizations (NSVs), such as laughter and sighs. We\nalso introduce two new evaluation metrics for assessing lip synchronization and\nNSV generation. Experimental results show that KeyFace outperforms\nstate-of-the-art methods in generating natural, coherent facial animations over\nextended durations, successfully encompassing NSVs and continuous emotions.\n","authors":["Antoni Bigata","Michał Stypułkowski","Rodrigo Mira","Stella Bounareli","Konstantinos Vougioukas","Zoe Landgraf","Nikita Drobyshev","Maciej Zieba","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2503.01715v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15144v1","updated":"2025-03-19T12:09:45Z","published":"2025-03-19T12:09:45Z","title":"PointSFDA: Source-free Domain Adaptation for Point Cloud Completion","summary":"  Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}.\n","authors":["Xing He","Zhe Zhu","Liangliang Nan","Honghua Chen","Jing Qin","Mingqiang Wei"],"pdf_url":"https://arxiv.org/pdf/2503.15144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15141v1","updated":"2025-03-19T12:06:50Z","published":"2025-03-19T12:06:50Z","title":"Object-Centric Pretraining via Target Encoder Bootstrapping","summary":"  Object-centric representation learning has recently been successfully applied\nto real-world datasets. This success can be attributed to pretrained\nnon-object-centric foundation models, whose features serve as reconstruction\ntargets for slot attention. However, targets must remain frozen throughout the\ntraining, which sets an upper bound on the performance object-centric models\ncan attain. Attempts to update the target encoder by bootstrapping result in\nlarge performance drops, which can be attributed to its lack of object-centric\ninductive biases, causing the object-centric model's encoder to drift away from\nrepresentations useful as reconstruction targets. To address these limitations,\nwe propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a\nself-distillation setup for training object-centric models from scratch, on\nreal-world data, for the first time ever. In OCEBO, the target encoder is\nupdated as an exponential moving average of the object-centric model, thus\nexplicitly being enriched with object-centric inductive biases introduced by\nslot attention while removing the upper bound on performance present in other\nmodels. We mitigate the slot collapse caused by random initialization of the\ntarget encoder by introducing a novel cross-view patch filtering approach that\nlimits the supervision to sufficiently informative patches. When pretrained on\n241k images from COCO, OCEBO achieves unsupervised object discovery performance\ncomparable to that of object-centric models with frozen non-object-centric\ntarget encoders pretrained on hundreds of millions of images. The code and\npretrained models are publicly available at https://github.com/djukicn/ocebo.\n","authors":["Nikola Đukić","Tim Lebailly","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2503.15141v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2412.14672v2","updated":"2025-03-19T12:04:30Z","published":"2024-12-19T09:24:10Z","title":"FiVL: A Framework for Improved Vision-Language Alignment through the\n  Lens of Training, Evaluation and Explainability","summary":"  Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. Furthermore, current\nvision-language benchmarks are not specifically measuring the degree to which\nthe answer require the visual input. This limitation makes it challenging to\nconfirm that the image is truly necessary, particularly in tasks like visual\nquestion answering. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nalso evaluate their effectiveness in achieving it. We demonstrate the value of\nour datasets through three approaches. First, we introduce a novel training\ntask based on our augmented training dataset, resulting in better performance\nthan the baseline. Second, we present benchmarks to assess the model's ability\nto use image as substantive evidence, rather than relying solely on linguistic\npriors. Finally, we identify attention heads with the strongest vision-language\nalignment, enabling explainability on visual-driven hallucinations. The code is\navailable at https://github.com/IntelLabs/fivl.\n","authors":["Estelle Aflalo","Gabriela Ben Melech Stan","Tiep Le","Man Luo","Shachar Rosenman","Sayak Paul","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.14672v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15138v1","updated":"2025-03-19T11:59:14Z","published":"2025-03-19T11:59:14Z","title":"VideoGen-of-Thought: Step-by-step generating multi-shot video with\n  minimal manual intervention","summary":"  Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.\n","authors":["Mingzhe Zheng","Yongqi Xu","Haojian Huang","Xuran Ma","Yexin Liu","Wenjie Shu","Yatian Pang","Feilong Tang","Qifeng Chen","Harry Yang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2503.15138v1.pdf","comment":"Code: https://github.com/DuNGEOnmassster/VideoGen-of-Thought.git;\n  Webpage: https://cheliosoops.github.io/VGoT/"},{"id":"http://arxiv.org/abs/2410.11666v4","updated":"2025-03-19T11:57:01Z","published":"2024-10-15T14:53:07Z","title":"DORNet: A Degradation Oriented and Regularized Network for Blind Depth\n  Super-Resolution","summary":"  Recent RGB-guided depth super-resolution methods have achieved impressive\nperformance under the assumption of fixed and known degradation (e.g., bicubic\ndownsampling). However, in real-world scenarios, captured depth data often\nsuffer from unconventional and unknown degradation due to sensor limitations\nand complex imaging environments (e.g., low reflective surfaces, varying\nillumination). Consequently, the performance of these methods significantly\ndeclines when real-world degradation deviate from their assumptions. In this\npaper, we propose the Degradation Oriented and Regularized Network (DORNet), a\nnovel framework designed to adaptively address unknown degradation in\nreal-world scenes through implicit degradation representations. Our approach\nbegins with the development of a self-supervised degradation learning strategy,\nwhich models the degradation representations of low-resolution depth data using\nrouting selection-based degradation regularization. To facilitate effective\nRGB-D fusion, we further introduce a degradation-oriented feature\ntransformation module that selectively propagates RGB content into the depth\ndata based on the learned degradation priors. Extensive experimental results on\nboth real and synthetic datasets demonstrate the superiority of our DORNet in\nhandling unknown degradation, outperforming existing methods. The code is\navailable at https://github.com/yanzq95/DORNet.\n","authors":["Zhengxue Wang","Zhiqiang Yan","Jinshan Pan","Guangwei Gao","Kai Zhang","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2410.11666v4.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2408.07246v4","updated":"2025-03-19T11:46:58Z","published":"2024-08-14T01:16:40Z","title":"ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area","summary":"  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n","authors":["Junxian Li","Di Zhang","Xunzhi Wang","Zeying Hao","Jingdi Lei","Qian Tan","Cai Zhou","Wei Liu","Yaotian Yang","Xinrui Xiong","Weiyun Wang","Zhe Chen","Wenhai Wang","Wei Li","Shufei Zhang","Mao Su","Wanli Ouyang","Yuqiang Li","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07246v4.pdf","comment":"11 pages, updated version"},{"id":"http://arxiv.org/abs/2503.15126v1","updated":"2025-03-19T11:38:14Z","published":"2025-03-19T11:38:14Z","title":"Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action\n  Segmentation","summary":"  Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results.\n","authors":["Haoyu Ji","Bowen Chen","Weihong Ren","Wenze Huang","Zhihao Yang","Zhiyong Wang","Honghai Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13926v2","updated":"2025-03-19T11:29:13Z","published":"2025-03-18T05:43:42Z","title":"Learning Shape-Independent Transformation via Spherical Representations\n  for Category-Level Object Pose Estimation","summary":"  Category-level object pose estimation aims to determine the pose and size of\nnovel objects in specific categories. Existing correspondence-based approaches\ntypically adopt point-based representations to establish the correspondences\nbetween primitive observed points and normalized object coordinates. However,\ndue to the inherent shape-dependence of canonical coordinates, these methods\nsuffer from semantic incoherence across diverse object shapes. To resolve this\nissue, we innovatively leverage the sphere as a shared proxy shape of objects\nto learn shape-independent transformation via spherical representations. Based\non this insight, we introduce a novel architecture called SpherePose, which\nyields precise correspondence prediction through three core designs. Firstly,\nWe endow the point-wise feature extraction with SO(3)-invariance, which\nfacilitates robust mapping between camera coordinate space and object\ncoordinate space regardless of rotation transformation. Secondly, the spherical\nattention mechanism is designed to propagate and integrate features among\nspherical anchors from a comprehensive perspective, thus mitigating the\ninterference of noise and incomplete point cloud. Lastly, a hyperbolic\ncorrespondence loss function is designed to distinguish subtle distinctions,\nwhich can promote the precision of correspondence prediction. Experimental\nresults on CAMERA25, REAL275 and HouseCat6D benchmarks demonstrate the superior\nperformance of our method, verifying the effectiveness of spherical\nrepresentations and architectural innovations.\n","authors":["Huan Ren","Wenfei Yang","Xiang Liu","Shifeng Zhang","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13926v2.pdf","comment":"Accepted by ICLR 2025. Project page is available at\n  https://renhuan1999.github.io/SpherePose"},{"id":"http://arxiv.org/abs/2412.06014v2","updated":"2025-03-19T11:26:14Z","published":"2024-12-08T18:16:13Z","title":"Post-hoc Probabilistic Vision-Language Models","summary":"  Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable\nsuccess in classification, retrieval, and generative tasks. For this, VLMs\ndeterministically map images and text descriptions to a joint latent space in\nwhich their similarity is assessed using the cosine similarity. However, a\ndeterministic mapping of inputs fails to capture uncertainties over concepts\narising from domain shifts when used in downstream tasks. In this work, we\npropose post-hoc uncertainty estimation in VLMs that does not require\nadditional training. Our method leverages a Bayesian posterior approximation\nover the last layers in VLMs and analytically quantifies uncertainties over\ncosine similarities. We demonstrate its effectiveness for uncertainty\nquantification and support set selection in active learning. Compared to\nbaselines, we obtain improved and well-calibrated predictive uncertainties,\ninterpretable uncertainty estimates, and sample-efficient active learning. Our\nresults show promise for safety-critical applications of large-scale models.\n","authors":["Anton Baumann","Rui Li","Marcus Klasson","Santeri Mentu","Shyamgopal Karthik","Zeynep Akata","Arno Solin","Martin Trapp"],"pdf_url":"https://arxiv.org/pdf/2412.06014v2.pdf","comment":"Project page: https://aaltoml.github.io/BayesVLM/"},{"id":"http://arxiv.org/abs/2503.15110v1","updated":"2025-03-19T11:07:01Z","published":"2025-03-19T11:07:01Z","title":"GIVEPose: Gradual Intra-class Variation Elimination for RGB-based\n  Category-Level Object Pose Estimation","summary":"  Recent advances in RGBD-based category-level object pose estimation have been\nlimited by their reliance on precise depth information, restricting their\nbroader applicability. In response, RGB-based methods have been developed.\nAmong these methods, geometry-guided pose regression that originated from\ninstance-level tasks has demonstrated strong performance. However, we argue\nthat the NOCS map is an inadequate intermediate representation for\ngeometry-guided pose regression method, as its many-to-one correspondence with\ncategory-level pose introduces redundant instance-specific information,\nresulting in suboptimal results. This paper identifies the intra-class\nvariation problem inherent in pose regression based solely on the NOCS map and\nproposes the Intra-class Variation-Free Consensus (IVFC) map, a novel\ncoordinate representation generated from the category-level consensus model. By\nleveraging the complementary strengths of the NOCS map and the IVFC map, we\nintroduce GIVEPose, a framework that implements Gradual Intra-class Variation\nElimination for category-level object pose estimation. Extensive evaluations on\nboth synthetic and real-world datasets demonstrate that GIVEPose significantly\noutperforms existing state-of-the-art RGB-based approaches, achieving\nsubstantial improvements in category-level object pose estimation. Our code is\navailable at https://github.com/ziqin-h/GIVEPose.\n","authors":["Zinqin Huang","Gu Wang","Chenyangguang Zhang","Ruida Zhang","Xiu Li","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2503.15110v1.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2503.15106v1","updated":"2025-03-19T11:04:37Z","published":"2025-03-19T11:04:37Z","title":"Distilling 3D distinctive local descriptors for 6D pose estimation","summary":"  Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/\n","authors":["Amir Hamza","Andrea Caraffa","Davide Boscaini","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2503.15106v1.pdf","comment":"Project Website: https://tev-fbk.github.io/dGeDi/"},{"id":"http://arxiv.org/abs/2411.17385v3","updated":"2025-03-19T11:00:12Z","published":"2024-11-26T12:44:17Z","title":"DepthCues: Evaluating Monocular Depth Perception in Large Vision Models","summary":"  Large-scale pre-trained vision models are becoming increasingly prevalent,\noffering expressive and generalizable visual representations that benefit\nvarious downstream tasks. Recent studies on the emergent properties of these\nmodels have revealed their high-level geometric understanding, in particular in\nthe context of depth perception. However, it remains unclear how depth\nperception arises in these models without explicit depth supervision provided\nduring pre-training. To investigate this, we examine whether the monocular\ndepth cues, similar to those used by the human visual system, emerge in these\nmodels. We introduce a new benchmark, DepthCues, designed to evaluate depth cue\nunderstanding, and present findings across 20 diverse and representative\npre-trained vision models. Our analysis shows that human-like depth cues emerge\nin more recent larger models. We also explore enhancing depth perception in\nlarge vision models by fine-tuning on DepthCues, and find that even without\ndense depth supervision, this improves depth estimation. To support further\nresearch, our benchmark and evaluation code will be made publicly available for\nstudying depth perception in vision models.\n","authors":["Duolikun Danier","Mehmet Aygün","Changjian Li","Hakan Bilen","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2411.17385v3.pdf","comment":"Accepted to CVPR 2025. Project page:\n  https://danier97.github.io/depthcues/"},{"id":"http://arxiv.org/abs/2503.15096v1","updated":"2025-03-19T10:50:03Z","published":"2025-03-19T10:50:03Z","title":"When the Future Becomes the Past: Taming Temporal Correspondence for\n  Self-supervised Video Representation Learning","summary":"  The past decade has witnessed notable achievements in self-supervised\nlearning for video tasks. Recent efforts typically adopt the Masked Video\nModeling (MVM) paradigm, leading to significant progress on multiple video\ntasks. However, two critical challenges remain: 1) Without human annotations,\nthe random temporal sampling introduces uncertainty, increasing the difficulty\nof model training. 2) Previous MVM methods primarily recover the masked patches\nin the pixel space, leading to insufficient information compression for\ndownstream tasks. To address these challenges jointly, we propose a\nself-supervised framework that leverages Temporal Correspondence for video\nRepresentation learning (T-CoRe). For challenge 1), we propose a sandwich\nsampling strategy that selects two auxiliary frames to reduce reconstruction\nuncertainty in a two-side-squeezing manner. Addressing challenge 2), we\nintroduce an auxiliary branch into a self-distillation architecture to restore\nrepresentations in the latent space, generating high-level semantic\nrepresentations enriched with temporal information. Experiments of T-CoRe\nconsistently present superior performance across several downstream tasks,\ndemonstrating its effectiveness for video representation learning. The code is\navailable at https://github.com/yafeng19/T-CORE.\n","authors":["Yang Liu","Qianqian Xu","Peisong Wen","Siran Dai","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2503.15096v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2502.03272v2","updated":"2025-03-19T10:42:32Z","published":"2025-02-05T15:29:28Z","title":"Deep Learning Pipeline for Fully Automated Myocardial Infarct\n  Segmentation from Clinical Cardiac MR Scans","summary":"  Purpose: To develop and evaluate a deep learning-based method that allows to\nperform myocardial infarct segmentation in a fully-automated way.\n  Materials and Methods: For this retrospective study, a cascaded framework of\ntwo and three-dimensional convolutional neural networks (CNNs), specialized on\nidentifying ischemic myocardial scars on late gadolinium enhancement (LGE)\ncardiac magnetic resonance (CMR) images, was trained on an in-house training\ndataset consisting of 144 examinations. On a separate test dataset from the\nsame institution, including images from 152 examinations obtained between 2021\nand 2023, a quantitative comparison between artificial intelligence (AI)-based\nsegmentations and manual segmentations was performed. Further, qualitative\nassessment of segmentation accuracy was evaluated for both human and\nAI-generated contours by two CMR experts in a blinded experiment.\n  Results: Excellent agreement could be found between manually and\nautomatically calculated infarct volumes ($\\rho_c$ = 0.9). The qualitative\nevaluation showed that compared to human-based measurements, the experts rated\nthe AI-based segmentations to better represent the actual extent of infarction\nsignificantly (p < 0.001) more often (33.4% AI, 25.1% human, 41.5% equal). On\nthe contrary, for segmentation of microvascular obstruction (MVO), manual\nmeasurements were still preferred (11.3% AI, 55.6% human, 33.1% equal).\n  Conclusion: This fully-automated segmentation pipeline enables CMR infarct\nsize to be calculated in a very short time and without requiring any\npre-processing of the input images while matching the segmentation quality of\ntrained human observers. In a blinded experiment, experts preferred automated\ninfarct segmentations more often than manual segmentations, paving the way for\na potential clinical application.\n","authors":["Matthias Schwab","Mathias Pamminger","Christian Kremser","Markus Haltmeier","Agnes Mayr"],"pdf_url":"https://arxiv.org/pdf/2502.03272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16898v2","updated":"2025-03-19T10:40:34Z","published":"2024-11-25T20:07:07Z","title":"MonoGSDF: Exploring Monocular Geometric Cues for Gaussian\n  Splatting-Guided Implicit Surface Reconstruction","summary":"  Accurate meshing from monocular images remains a key challenge in 3D vision.\nWhile state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at\nsynthesizing photorealistic novel views through rasterization-based rendering,\ntheir reliance on sparse, explicit primitives severely limits their ability to\nrecover watertight and topologically consistent 3D surfaces.We introduce\nMonoGSDF, a novel method that couples Gaussian-based primitives with a neural\nSigned Distance Field (SDF) for high-quality reconstruction. During training,\nthe SDF guides Gaussians' spatial distribution, while at inference, Gaussians\nserve as priors to reconstruct surfaces, eliminating the need for\nmemory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a\nscaling strategy for robust generalization. A multi-resolution training scheme\nfurther refines details and monocular geometric cues from off-the-shelf\nestimators enhance reconstruction quality. Experiments on real-world datasets\nshow MonoGSDF outperforms prior methods while maintaining efficiency.\n","authors":["Kunyi Li","Michael Niemeyer","Zeyu Chen","Nassir Navab","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2411.16898v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15091v1","updated":"2025-03-19T10:40:28Z","published":"2025-03-19T10:40:28Z","title":"Intelligent Spatial Perception by Building Hierarchical 3D Scene Graphs\n  for Indoor Scenarios with the Help of LLMs","summary":"  This paper addresses the high demand in advanced intelligent robot navigation\nfor a more holistic understanding of spatial environments, by introducing a\nnovel system that harnesses the capabilities of Large Language Models (LLMs) to\nconstruct hierarchical 3D Scene Graphs (3DSGs) for indoor scenarios. The\nproposed framework constructs 3DSGs consisting of a fundamental layer with rich\nmetric-semantic information, an object layer featuring precise point-cloud\nrepresentation of object nodes as well as visual descriptors, and higher layers\nof room, floor, and building nodes. Thanks to the innovative application of\nLLMs, not only object nodes but also nodes of higher layers, e.g., room nodes,\nare annotated in an intelligent and accurate manner. A polling mechanism for\nroom classification using LLMs is proposed to enhance the accuracy and\nreliability of the room node annotation. Thorough numerical experiments\ndemonstrate the system's ability to integrate semantic descriptions with\ngeometric data, creating an accurate and comprehensive representation of the\nenvironment instrumental for context-aware navigation and task planning.\n","authors":["Yao Cheng","Zhe Han","Fengyang Jiang","Huaizhen Wang","Fengyu Zhou","Qingshan Yin","Lei Wei"],"pdf_url":"https://arxiv.org/pdf/2503.15091v1.pdf","comment":"accepted by WRC SARA 2024"},{"id":"http://arxiv.org/abs/2503.15087v1","updated":"2025-03-19T10:38:25Z","published":"2025-03-19T10:38:25Z","title":"An Investigation of Beam Density on LiDAR Object Detection Performance","summary":"  Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference.\n","authors":["Christoph Griesbacher","Christian Fruhwirth-Reisinger"],"pdf_url":"https://arxiv.org/pdf/2503.15087v1.pdf","comment":"Accepted by CVWW 2025"},{"id":"http://arxiv.org/abs/2407.07356v2","updated":"2025-03-19T10:22:15Z","published":"2024-07-10T04:27:06Z","title":"Video In-context Learning: Autoregressive Transformers are Zero-Shot\n  Video Imitators","summary":"  People interact with the real-world largely dependent on visual signal, which\nare ubiquitous and illustrate detailed demonstrations. In this paper, we\nexplore utilizing visual signals as a new interface for models to interact with\nthe environment. Specifically, we choose videos as a representative visual\nsignal. And by training autoregressive Transformers on video datasets in a\nself-supervised objective, we find that the model emerges a zero-shot\ncapability to infer the semantics from a demonstration video, and imitate the\nsemantics to an unseen scenario. This allows the models to perform unseen tasks\nby watching the demonstration video in an in-context manner, without further\nfine-tuning. To validate the imitation capacity, we design various evaluation\nmetrics including both objective and subjective measures. The results show that\nour models can generate high-quality video clips that accurately align with the\nsemantic guidance provided by the demonstration videos, and we also show that\nthe imitation capacity follows the scaling law. Code and models have been\nopen-sourced.\n","authors":["Wentao Zhang","Junliang Guo","Tianyu He","Li Zhao","Linli Xu","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2407.07356v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.15070v1","updated":"2025-03-19T10:08:29Z","published":"2025-03-19T10:08:29Z","title":"MultiBARF: Integrating Imagery of Different Wavelength Regions by Using\n  Neural Radiance Fields","summary":"  Optical sensor applications have become popular through digital\ntransformation. Linking observed data to real-world locations and combining\ndifferent image sensors is essential to make the applications practical and\nefficient. However, data preparation to try different sensor combinations\nrequires high sensing and image processing expertise. To make data preparation\neasier for users unfamiliar with sensing and image processing, we have\ndeveloped MultiBARF. This method replaces the co-registration and geometric\ncalibration by synthesizing pairs of two different sensor images and depth\nimages at assigned viewpoints. Our method extends Bundle Adjusting Neural\nRadiance Fields(BARF), a deep neural network-based novel view synthesis method,\nfor the two imagers. Through experiments on visible light and thermographic\nimages, we demonstrate that our method superimposes two color channels of those\nsensor images on NeRF.\n","authors":["Kana Kurata","Hitoshi Niigaki","Xiaojun Wu","Ryuichi Tanida"],"pdf_url":"https://arxiv.org/pdf/2503.15070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08591v2","updated":"2025-03-19T10:05:05Z","published":"2024-12-11T18:10:21Z","title":"RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied\n  Navigation","summary":"  Vision-and-Language Navigation (VLN) suffers from the limited diversity and\nscale of training data, primarily constrained by the manual curation of\nexisting simulators. To address this, we introduce RoomTour3D, a\nvideo-instruction dataset derived from web-based room tour videos that capture\nreal-world indoor spaces and human walking demonstrations. Unlike existing VLN\ndatasets, RoomTour3D leverages the scale and diversity of online videos to\ngenerate open-ended human walking trajectories and open-world navigable\ninstructions. To compensate for the lack of navigation data in online videos,\nwe perform 3D reconstruction and obtain 3D trajectories of walking paths\naugmented with additional information on the room types, object locations and\n3D shape of surrounding scenes. Our dataset includes $\\sim$100K open-ended\ndescription-enriched trajectories with $\\sim$200K instructions, and 17K\naction-enriched trajectories from 1847 room tour environments. We demonstrate\nexperimentally that RoomTour3D enables significant improvements across multiple\nVLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D\nfacilitates the development of trainable zero-shot VLN agents, showcasing the\npotential and challenges of advancing towards open-world navigation.\n","authors":["Mingfei Han","Liang Ma","Kamila Zhumakhanova","Ekaterina Radionova","Jingyi Zhang","Xiaojun Chang","Xiaodan Liang","Ivan Laptev"],"pdf_url":"https://arxiv.org/pdf/2412.08591v2.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2408.15045v3","updated":"2025-03-19T10:05:04Z","published":"2024-08-27T13:13:38Z","title":"DocLayLLM: An Efficient Multi-modal Extension of Large Language Models\n  for Text-rich Document Understanding","summary":"  Text-rich document understanding (TDU) requires comprehensive analysis of\ndocuments containing substantial textual content and complex layouts. While\nMultimodal Large Language Models (MLLMs) have achieved fast progress in this\ndomain, existing approaches either demand significant computational resources\nor struggle with effective multi-modal integration. In this paper, we introduce\nDocLayLLM, an efficient multi-modal extension of LLMs specifically designed for\nTDU. By lightly integrating visual patch tokens and 2D positional tokens into\nLLMs' input and encoding the document content using the LLMs themselves, we\nfully take advantage of the document comprehension capability of LLMs and\nenhance their perception of OCR information. We have also deeply considered the\nrole of chain-of-thought (CoT) and innovatively proposed the techniques of CoT\nPre-training and CoT Annealing. Our DocLayLLM can achieve remarkable\nperformances with lightweight training settings, showcasing its efficiency and\neffectiveness. Experimental results demonstrate that our DocLayLLM outperforms\nexisting OCR-dependent methods and OCR-free competitors. Code and model are\navailable at https://github.com/whlscut/DocLayLLM.\n","authors":["Wenhui Liao","Jiapeng Wang","Hongliang Li","Chengyu Wang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2408.15045v3.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2412.13871v2","updated":"2025-03-19T10:04:22Z","published":"2024-12-18T14:07:46Z","title":"LLaVA-UHD v2: an MLLM Integrating High-Resolution Semantic Pyramid via\n  Hierarchical Window Transformer","summary":"  Vision transformers (ViTs) are widely employed in multimodal large language\nmodels (MLLMs) for visual encoding. However, they exhibit inferior performance\non tasks regarding fine-grained visual perception. We attribute this to the\nlimitations of ViTs in capturing diverse multi-modal visual levels, such as\nlow-level details. To address this issue, we present LLaVA-UHD v2, an MLLM with\nadvanced perception abilities by introducing a well-designed vision-language\nprojector, the Hierarchical window (Hiwin) transformer. Hiwin transformer\nenhances MLLM's ability to capture diverse multi-modal visual granularities, by\nincorporating our constructed high-resolution semantic pyramid. Specifically,\nHiwin transformer comprises two key modules: (i) a visual detail injection\nmodule, which progressively injects low-level visual details into high-level\nlanguage-aligned semantics features, thereby forming an inverse semantic\npyramid (ISP), and (ii) a hierarchical window attention module, which leverages\ncross-scale windows to condense multi-level semantics from the ISP. Extensive\nexperiments show that LLaVA-UHD v2 outperforms compared MLLMs on a wide range\nof benchmarks. Notably, our design achieves an average boost of 3.7% across 14\nbenchmarks compared with the baseline method, 9.3% on DocVQA for instance. All\nthe data and code will be publicly available to facilitate future research.\n","authors":["Yipeng Zhang","Yifan Liu","Zonghao Guo","Yidan Zhang","Xuesong Yang","Xiaoying Zhang","Chi Chen","Jun Song","Bo Zheng","Yuan Yao","Zhiyuan Liu","Tat-Seng Chua","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2412.13871v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04234v2","updated":"2025-03-19T10:00:35Z","published":"2024-12-05T15:10:13Z","title":"DEIM: DETR with Improved Matching for Fast Convergence","summary":"  We introduce DEIM, an innovative and efficient training framework designed to\naccelerate convergence in real-time object detection with Transformer-based\narchitectures (DETR). To mitigate the sparse supervision inherent in one-to-one\n(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This\napproach increases the number of positive samples per image by incorporating\nadditional targets, using standard data augmentation techniques. While Dense\nO2O matching speeds up convergence, it also introduces numerous low-quality\nmatches that could affect performance. To address this, we propose the\nMatchability-Aware Loss (MAL), a novel loss function that optimizes matches\nacross various quality levels, enhancing the effectiveness of Dense O2O.\nExtensive experiments on the COCO dataset validate the efficacy of DEIM. When\nintegrated with RT-DETR and D-FINE, it consistently boosts performance while\nreducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves\n53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,\nDEIM-trained real-time models outperform leading real-time object detectors,\nwith DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78\nFPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We\nbelieve DEIM sets a new baseline for advancements in real-time object\ndetection. Our code and pre-trained models are available at\nhttps://github.com/ShihuaHuang95/DEIM.\n","authors":["Shihua Huang","Zhichao Lu","Xiaodong Cun","Yongjun Yu","Xiao Zhou","Xi Shen"],"pdf_url":"https://arxiv.org/pdf/2412.04234v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.05214v2","updated":"2025-03-19T09:54:47Z","published":"2025-03-07T08:09:59Z","title":"Gaussian Random Fields as an Abstract Representation of Patient Metadata\n  for Multimodal Medical Image Segmentation","summary":"  The growing rate of chronic wound occurrence, especially in patients with\ndiabetes, has become a concerning trend in recent years. Chronic wounds are\ndifficult and costly to treat, and have become a serious burden on health care\nsystems worldwide. Chronic wounds can have devastating consequences for the\npatient, with infection often leading to reduced quality of life and increased\nmortality risk. Innovative deep learning methods for the detection and\nmonitoring of such wounds have the potential to reduce the impact to both\npatient and clinician. We present a novel multimodal segmentation method which\nallows for the introduction of patient metadata into the training workflow\nwhereby the patient data are expressed as Gaussian random fields. Our results\nindicate that the proposed method improved performance when utilising multiple\nmodels, each trained on different metadata categories. Using the Diabetic Foot\nUlcer Challenge 2022 test set, when compared to the baseline results\n(intersection over union = 0.4670, Dice similarity coefficient = 0.5908) we\ndemonstrate improvements of +0.0220 and +0.0229 for intersection over union and\nDice similarity coefficient respectively. This paper presents the first study\nto focus on integrating patient data into a chronic wound segmentation\nworkflow. Our results show significant performance gains when training\nindividual models using specific metadata categories, followed by average\nmerging of prediction masks using distance transforms. All source code for this\nstudy is available at:\nhttps://github.com/mmu-dermatology-research/multimodal-grf\n","authors":["Bill Cassidy","Christian McBride","Connah Kendrick","Neil D. Reeves","Joseph M. Pappachan","Shaghayegh Raad","Moi Hoon Yap"],"pdf_url":"https://arxiv.org/pdf/2503.05214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15060v1","updated":"2025-03-19T09:53:11Z","published":"2025-03-19T09:53:11Z","title":"Conjuring Positive Pairs for Efficient Unification of Representation\n  Learning and Image Synthesis","summary":"  While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.\n","authors":["Imanol G. Estepa","Jesús M. Rodríguez-de-Vera","Ignacio Sarasúa","Bhalaji Nagarajan","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2503.15060v1.pdf","comment":"The source code is available in https://github.com/ImaGonEs/Sorcen"},{"id":"http://arxiv.org/abs/2503.15058v1","updated":"2025-03-19T09:50:32Z","published":"2025-03-19T09:50:32Z","title":"Texture-Aware StarGAN for CT data harmonisation","summary":"  Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN.\n","authors":["Francesco Di Feola","Ludovica Pompilio","Cecilia Assolito","Valerio Guarrasi","Paolo Soda"],"pdf_url":"https://arxiv.org/pdf/2503.15058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15056v1","updated":"2025-03-19T09:48:04Z","published":"2025-03-19T09:48:04Z","title":"Single-Step Bidirectional Unpaired Image Translation Using Implicit\n  Bridge Consistency Distillation","summary":"  Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https://hyn2028.github.io/project_page/IBCD/index.html\n","authors":["Suhyeon Lee","Kwanyoung Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2503.15056v1.pdf","comment":"25 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.12927v2","updated":"2025-03-19T09:27:16Z","published":"2025-03-17T08:38:46Z","title":"MMLNB: Multi-Modal Learning for Neuroblastoma Subtyping Classification\n  Assisted with Textual Description Generation","summary":"  Neuroblastoma (NB), a leading cause of childhood cancer mortality, exhibits\nsignificant histopathological variability, necessitating precise subtyping for\naccurate prognosis and treatment. Traditional diagnostic methods rely on\nsubjective evaluations that are time-consuming and inconsistent. To address\nthese challenges, we introduce MMLNB, a multi-modal learning (MML) model that\nintegrates pathological images with generated textual descriptions to improve\nclassification accuracy and interpretability. The approach follows a two-stage\nprocess. First, we fine-tune a Vision-Language Model (VLM) to enhance\npathology-aware text generation. Second, the fine-tuned VLM generates textual\ndescriptions, using a dual-branch architecture to independently extract visual\nand textual features. These features are fused via Progressive Robust\nMulti-Modal Fusion (PRMF) Block for stable training. Experimental results show\nthat the MMLNB model is more accurate than the single modal model. Ablation\nstudies demonstrate the importance of multi-modal fusion, fine-tuning, and the\nPRMF mechanism. This research creates a scalable AI-driven framework for\ndigital pathology, enhancing reliability and interpretability in NB subtyping\nclassification. Our source code is available at\nhttps://github.com/HovChen/MMLNB.\n","authors":["Huangwei Chen","Yifei Chen","Zhenyu Yan","Mingyang Ding","Chenlei Li","Zhu Zhu","Feiwei Qin"],"pdf_url":"https://arxiv.org/pdf/2503.12927v2.pdf","comment":"25 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.15029v1","updated":"2025-03-19T09:23:09Z","published":"2025-03-19T09:23:09Z","title":"DRoPE: Directional Rotary Position Embedding for Efficient Agent\n  Interaction Modeling","summary":"  Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps://drope-traj.github.io/.\n","authors":["Jianbo Zhao","Taiyu Ban","Zhihao Liu","Hangning Zhou","Xiyang Wang","Qibin Zhou","Hailong Qin","Mu Yang","Lei Liu","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2503.15029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15024v1","updated":"2025-03-19T09:21:44Z","published":"2025-03-19T09:21:44Z","title":"Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for\n  Large Vision Language Models","summary":"  Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps://Forensics-Bench.github.io/.\n","authors":["Jin Wang","Chenghui Lv","Xian Li","Shichao Dong","Huadong Li","kelu Yao","Chao Li","Wenqi Shao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2503.15024v1.pdf","comment":"31 pages, 19 figures"},{"id":"http://arxiv.org/abs/2503.15023v1","updated":"2025-03-19T09:20:42Z","published":"2025-03-19T09:20:42Z","title":"Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of\n  Handwritten Arabic Script","summary":"  Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications.\n","authors":["Chaouki Boufenar","Mehdi Ayoub Rabiai","Boualem Nadjib Zahaf","Khelil Rafik Ouaras"],"pdf_url":"https://arxiv.org/pdf/2503.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15022v1","updated":"2025-03-19T09:20:35Z","published":"2025-03-19T09:20:35Z","title":"xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D\n  motion","summary":"  Object discovery, which refers to the task of localizing objects without\nhuman annotations, has gained significant attention in 2D image analysis.\nHowever, despite this growing interest, it remains under-explored in 3D data,\nwhere approaches rely exclusively on 3D motion, despite its several challenges.\nIn this paper, we present a novel framework that leverages advances in 2D\nobject discovery which are based on 2D motion to exploit the advantages of such\nmotion cues being more flexible and generalizable and to bridge the gap between\n2D and 3D modalities. Our primary contributions are twofold: (i) we introduce\nDIOD-3D, the first baseline for multi-object discovery in 3D data using 2D\nmotion, incorporating scene completion as an auxiliary task to enable dense\nobject localization from sparse input data; (ii) we develop xMOD, a cross-modal\ntraining framework that integrates 2D and 3D data while always using 2D motion\ncues. xMOD employs a teacher-student training paradigm across the two\nmodalities to mitigate confirmation bias by leveraging the domain gap. During\ninference, the model supports both RGB-only and point cloud-only inputs.\nAdditionally, we propose a late-fusion technique tailored to our pipeline that\nfurther enhances performance when both modalities are available at inference.\nWe evaluate our approach extensively on synthetic (TRIP-PD) and challenging\nreal-world datasets (KITTI and Waymo). Notably, our approach yields a\nsubstantial performance improvement compared with the 2D object discovery\nstate-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50\nscore. The code is available at https://github.com/CEA-LIST/xMOD\n","authors":["Saad Lahlali","Sandra Kara","Hejer Ammar","Florian Chabot","Nicolas Granger","Hervé Le Borgne","Quoc-Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2503.15022v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15019v1","updated":"2025-03-19T09:16:08Z","published":"2025-03-19T09:16:08Z","title":"Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene","summary":"  The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method.\n","authors":["Shengqiong Wu","Hao Fei","Jingkang Yang","Xiangtai Li","Juncheng Li","Hanwang Zhang","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2503.15019v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15017v1","updated":"2025-03-19T09:13:06Z","published":"2025-03-19T09:13:06Z","title":"Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired\n  Training","summary":"  Unpaired training has been verified as one of the most effective paradigms\nfor real scene dehazing by learning from unpaired real-world hazy and clear\nimages. Although numerous studies have been proposed, current methods\ndemonstrate limited generalization for various real scenes due to limited\nfeature representation and insufficient use of real-world prior. Inspired by\nthe strong generative capabilities of diffusion models in producing both hazy\nand clear images, we exploit diffusion prior for real-world image dehazing, and\npropose an unpaired framework named Diff-Dehazer. Specifically, we leverage\ndiffusion prior as bijective mapping learners within the CycleGAN, a classic\nunpaired learning framework. Considering that physical priors contain pivotal\nstatistics information of real-world data, we further excavate real-world\nknowledge by integrating physical priors into our framework. Furthermore, we\nintroduce a new perspective for adequately leveraging the representation\nability of diffusion models by removing degradation in image and text\nmodalities, so as to improve the dehazing effect. Extensive experiments on\nmultiple real-world datasets demonstrate the superior performance of our\nmethod. Our code https://github.com/ywxjm/Diff-Dehazer.\n","authors":["Yunwei Lan","Zhigao Cui","Chang Liu","Jialun Peng","Nian Wang","Xin Luo","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15017v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2503.15016v1","updated":"2025-03-19T09:12:56Z","published":"2025-03-19T09:12:56Z","title":"Manifold Learning for Hyperspectral Images","summary":"  Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results.\n","authors":["Fethi Harkat","Tiphaine Deuberet","Guillaume Gey","Valérie Perrier","Kévin Polisano"],"pdf_url":"https://arxiv.org/pdf/2503.15016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12793v2","updated":"2025-03-19T09:12:16Z","published":"2025-03-17T04:01:37Z","title":"Improving Generalization of Universal Adversarial Perturbation via\n  Dynamic Maximin Optimization","summary":"  Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.\n","authors":["Yechao Zhang","Yingzhe Xu","Junyu Shi","Leo Yu Zhang","Shengshan Hu","Minghui Li","Yanjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12793v2.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2412.11365v3","updated":"2025-03-19T09:04:14Z","published":"2024-12-16T01:37:51Z","title":"BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video\n  with Non-uniform Motions","summary":"  Existing Video Frame interpolation (VFI) models tend to suffer from\ntime-to-location ambiguity when trained with video of non-uniform motions, such\nas accelerating, decelerating, and changing directions, which often yield\nblurred interpolated frames. In this paper, we propose (i) a novel motion\ndescription map, Bidirectional Motion field (BiM), to effectively describe\nnon-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware\nUpsampling Network (CAUN) for precise optical flow estimation; and (iii)\nKnowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise\nthe motion estimation of VFI model with VFI-centric teacher flows. The proposed\nVFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.\nExtensive experiments show that our BiM-VFI model significantly surpasses the\nrecent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and\nSTLPIPS respectively, yielding interpolated frames with much fewer blurs at\narbitrary time instances.\n","authors":["Wonyong Seo","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2412.11365v3.pdf","comment":"The last two authors are co-corresponding authors"},{"id":"http://arxiv.org/abs/2503.15008v1","updated":"2025-03-19T08:59:02Z","published":"2025-03-19T08:59:02Z","title":"A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary\n  Learning for Breast Cancer Detection","summary":"  Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.\n","authors":["Aamir Mehmood","Yue Hu","Saddam Hussain Khan"],"pdf_url":"https://arxiv.org/pdf/2503.15008v1.pdf","comment":"12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986"},{"id":"http://arxiv.org/abs/2503.14329v2","updated":"2025-03-19T08:55:21Z","published":"2025-03-18T15:01:47Z","title":"EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference\n  Alignment","summary":"  Dexterous robotic hands often struggle to generalize effectively in complex\nenvironments due to the limitations of models trained on low-diversity data.\nHowever, the real world presents an inherently unbounded range of scenarios,\nmaking it impractical to account for every possible variation. A natural\nsolution is to enable robots learning from experience in complex environments,\nan approach akin to evolution, where systems improve through continuous\nfeedback, learning from both failures and successes, and iterating toward\noptimal performance. Motivated by this, we propose EvolvingGrasp, an\nevolutionary grasp generation method that continuously enhances grasping\nperformance through efficient preference alignment. Specifically, we introduce\nHandpose wise Preference Optimization (HPO), which allows the model to\ncontinuously align with preferences from both positive and negative feedback\nwhile progressively refining its grasping strategies. To further enhance\nefficiency and reliability during online adjustments, we incorporate a\nPhysics-aware Consistency Model within HPO, which accelerates inference,\nreduces the number of timesteps needed for preference finetuning, and ensures\nphysical plausibility throughout the process. Extensive experiments across four\nbenchmark datasets demonstrate state of the art performance of our method in\ngrasp success rate and sampling efficiency. Our results validate that\nEvolvingGrasp enables evolutionary grasp generation, ensuring robust,\nphysically feasible, and preference-aligned grasping in both simulation and\nreal scenarios.\n","authors":["Yufei Zhu","Yiming Zhong","Zemin Yang","Peishan Cong","Jingyi Yu","Xinge Zhu","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2503.14329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15005v1","updated":"2025-03-19T08:55:06Z","published":"2025-03-19T08:55:06Z","title":"Universal Scene Graph Generation","summary":"  Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance.\n","authors":["Shengqiong Wu","Hao Fei","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2503.15005v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15004v1","updated":"2025-03-19T08:54:14Z","published":"2025-03-19T08:54:14Z","title":"Semantic Segmentation of Transparent and Opaque Drinking Glasses with\n  the Help of Zero-shot Learning","summary":"  Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset.\n","authors":["Annalena Blänsdorf","Tristan Wirth","Arne Rak","Thomas Pöllabauer","Volker Knauthe","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2503.15004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15001v1","updated":"2025-03-19T08:52:04Z","published":"2025-03-19T08:52:04Z","title":"Low-Complexity Patch-based No-Reference Point Cloud Quality Metric\n  exploiting Weighted Structure and Texture Features","summary":"  During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA.\n","authors":["Michael Neri","Federica Battisti"],"pdf_url":"https://arxiv.org/pdf/2503.15001v1.pdf","comment":"Accepted for publication in IEEE Transactions on Broadcasting. Code\n  at https://github.com/michaelneri/PST-PCQA"},{"id":"http://arxiv.org/abs/2503.14998v1","updated":"2025-03-19T08:49:55Z","published":"2025-03-19T08:49:55Z","title":"TGV: Tabular Data-Guided Learning of Visual Cardiac Representations","summary":"  Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub.\n","authors":["Marta Hasny","Maxime Di Folco","Keno Bressem","Julia Schnabel"],"pdf_url":"https://arxiv.org/pdf/2503.14998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16204v4","updated":"2025-03-19T08:45:31Z","published":"2023-12-23T11:10:43Z","title":"Learning from Mistakes: Iterative Prompt Relabeling for Text-to-Image\n  Diffusion Model Training","summary":"  Diffusion models have shown impressive performance in many domains. However,\nthe model's capability to follow natural language instructions (e.g., spatial\nrelationships between objects, generating complex scenes) is still\nunsatisfactory. In this work, we propose Iterative Prompt Relabeling (IPR), a\nnovel algorithm that aligns images to text through iterative image sampling and\nprompt relabeling with feedback. IPR first samples a batch of images\nconditioned on the text, then relabels the text prompts of unmatched text-image\npairs with classifier feedback. We conduct thorough experiments on SDv2 and\nSDXL, testing their capability to follow instructions on spatial relations.\nWith IPR, we improved up to 15.22% (absolute improvement) on the challenging\nspatial relation VISOR benchmark, demonstrating superior performance compared\nto previous RL methods. Our code is publicly available at\nhttps://github.com/xinyan-cxy/IPR-RLDF.\n","authors":["Xinyan Chen","Jiaxin Ge","Tianjun Zhang","Jiaming Liu","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.16204v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14990v1","updated":"2025-03-19T08:36:20Z","published":"2025-03-19T08:36:20Z","title":"Disentangling Modes and Interference in the Spectrogram of\n  Multicomponent Signals","summary":"  In this paper, we investigate how the spectrogram of multicomponent signals\ncan be decomposed into a mode part and an interference part. We explore two\napproaches: (i) a variational method inspired by texture-geometry decomposition\nin image processing, and (ii) a supervised learning approach using a U-Net\narchitecture, trained on a dataset encompassing diverse interference patterns\nand noise conditions. Once the interference component is identified, we explain\nhow it enables us to define a criterion to locally adapt the window length used\nin the definition of the spectrogram, for the sake of improving ridge detection\nin the presence of close modes. Numerical experiments illustrate the advantages\nand limitations of both approaches for spectrogram decomposition, highlighting\ntheir potential for enhancing time-frequency analysis in the presence of strong\ninterference.\n","authors":["Kévin Polisano","Sylvain Meignen","Nils Laurent","Hubert Leterme"],"pdf_url":"https://arxiv.org/pdf/2503.14990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14983v1","updated":"2025-03-19T08:27:41Z","published":"2025-03-19T08:27:41Z","title":"Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised\n  Learning in Medical Image Segmentation","summary":"  Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS.\n","authors":["Zanting Ye","Xiaolong Niu","Xuanbin Wu","Wenxiang Yi","Yuan Chang","Lijun Lu"],"pdf_url":"https://arxiv.org/pdf/2503.14983v1.pdf","comment":"18 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.13265v2","updated":"2025-03-19T08:26:31Z","published":"2025-03-17T15:18:38Z","title":"FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View\n  Synthesis","summary":"  Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming,\nfrom single images is challenging due to a lack of 3D data. To this end, we\nintroduce FlexWorld, a novel framework consisting of two key components: (1) a\nstrong video-to-video (V2V) diffusion model to generate high-quality novel view\nimages from incomplete input rendered from a coarse scene, and (2) a\nprogressive expansion process to construct a complete 3D scene. In particular,\nleveraging an advanced pre-trained video model and accurate depth-estimated\ntraining pairs, our V2V model can generate novel views under large camera pose\nvariations. Building upon it, FlexWorld progressively generates new 3D content\nand integrates it into the global scene through geometry-aware scene fusion.\nExtensive experiments demonstrate the effectiveness of FlexWorld in generating\nhigh-quality novel view videos and flexible-view 3D scenes from single images,\nachieving superior visual quality under multiple popular metrics and datasets\ncompared to existing state-of-the-art methods. Qualitatively, we highlight that\nFlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg}\nrotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld.\n","authors":["Luxi Chen","Zihan Zhou","Min Zhao","Yikai Wang","Ge Zhang","Wenhao Huang","Hao Sun","Ji-Rong Wen","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2503.13265v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17250v2","updated":"2025-03-19T08:24:14Z","published":"2024-10-22T17:59:56Z","title":"JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding\n  Benchmark for Culture-aware Evaluation","summary":"  Accelerating research on Large Multimodal Models (LMMs) in non-English\nlanguages is crucial for enhancing user experiences across broader populations.\nIn this paper, we introduce JMMMU (Japanese MMMU), the first large-scale\nJapanese benchmark designed to evaluate LMMs on expert-level tasks based on the\nJapanese cultural context. To facilitate comprehensive culture-aware\nevaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA)\nsubset, where the culture-independent subjects (e.g., Math) are selected and\ntranslated into Japanese, enabling one-to-one comparison with its English\ncounterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly\ncrafted subjects that reflect Japanese cultural context. Using the CA subset,\nwe observe performance drop in many LMMs when evaluated in Japanese, which is\npurely attributable to language variation. Using the CS subset, we reveal their\ninadequate Japanese cultural understanding. Further, by combining both subsets,\nwe identify that some LMMs perform well on the CA subset but not on the CS\nsubset, exposing a shallow understanding of the Japanese language that lacks\ndepth in cultural understanding. We hope this work will not only help advance\nLMM performance in Japanese but also serve as a guideline to create\nhigh-standard, culturally diverse benchmarks for multilingual LMM development.\nThe project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.\n","authors":["Shota Onohara","Atsuyuki Miyai","Yuki Imajuku","Kazuki Egashira","Jeonghun Baek","Xiang Yue","Graham Neubig","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2410.17250v2.pdf","comment":"Accepted at NAACL 2025. Project page:\n  https://mmmu-japanese-benchmark.github.io/JMMMU/"},{"id":"http://arxiv.org/abs/2503.06894v2","updated":"2025-03-19T08:18:22Z","published":"2025-03-10T03:50:25Z","title":"A Deep Learning Approach for Augmenting Perceptional Understanding of\n  Histopathology Images","summary":"  In Recent Years, Digital Technologies Have Made Significant Strides In\nAugmenting-Human-Health, Cognition, And Perception, Particularly Within The\nField Of Computational-Pathology. This Paper Presents A Novel Approach To\nEnhancing The Analysis Of Histopathology Images By Leveraging A\nMult-modal-Model That Combines Vision Transformers (Vit) With Gpt-2 For Image\nCaptioning. The Model Is Fine-Tuned On The Specialized Arch-Dataset, Which\nIncludes Dense Image Captions Derived From Clinical And Academic Resources, To\nCapture The Complexities Of Pathology Images Such As Tissue Morphologies,\nStaining Variations, And Pathological Conditions. By Generating Accurate,\nContextually Captions, The Model Augments The Cognitive Capabilities Of\nHealthcare Professionals, Enabling More Efficient Disease Classification,\nSegmentation, And Detection. The Model Enhances The Perception Of Subtle\nPathological Features In Images That Might Otherwise Go Unnoticed, Thereby\nImproving Diagnostic Accuracy. Our Approach Demonstrates The Potential For\nDigital Technologies To Augment Human Cognitive Abilities In Medical Image\nAnalysis, Providing Steps Toward More Personalized And Accurate Healthcare\nOutcomes.\n","authors":["Xiaoqian Hu"],"pdf_url":"https://arxiv.org/pdf/2503.06894v2.pdf","comment":"Accepted by International Conference on Semantic & Natural Language\n  Processing (SNLP 2025)"},{"id":"http://arxiv.org/abs/2503.14979v1","updated":"2025-03-19T08:17:48Z","published":"2025-03-19T08:17:48Z","title":"One-Shot Medical Video Object Segmentation via Temporal Contrastive\n  Memory Networks","summary":"  Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps://github.com/MedAITech/TCMN.\n","authors":["Yaxiong Chen","Junjian Hu","Chunlei Li","Zixuan Zheng","Jingliang Hu","Yilei Shi","Shengwu Xiong","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2503.14979v1.pdf","comment":"MICCAI 2024 Workshop"},{"id":"http://arxiv.org/abs/2503.14975v1","updated":"2025-03-19T08:10:49Z","published":"2025-03-19T08:10:49Z","title":"Taming Flow Matching with Unbalanced Optimal Transport into Fast\n  Pansharpening","summary":"  Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM.\n","authors":["Zihan Cao","Yu Zhong","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2503.14975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14974v1","updated":"2025-03-19T08:09:32Z","published":"2025-03-19T08:09:32Z","title":"Language-based Image Colorization: A Benchmark and Beyond","summary":"  Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo.\n","authors":["Yifan Li","Shuai Yang","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14501v2","updated":"2025-03-19T08:05:50Z","published":"2025-03-18T17:59:51Z","title":"Advances in 4D Generation: A Survey","summary":"  Generative artificial intelligence (AI) has made significant progress across\nvarious domains in recent years. Building on the rapid advancements in 2D,\nvideo, and 3D content generation fields, 4D generation has emerged as a novel\nand rapidly evolving research area, attracting growing attention. 4D generation\nfocuses on creating dynamic 3D assets with spatiotemporal consistency based on\nuser input, offering greater creative freedom and richer immersive experiences.\nThis paper presents a comprehensive survey of the 4D generation field,\nsystematically summarizing its core technologies, developmental trajectory, key\nchallenges, and practical applications, while also exploring potential future\nresearch directions. The survey begins by introducing various fundamental 4D\nrepresentation models, followed by a review of 4D generation frameworks built\nupon these representations and the key technologies that incorporate motion and\ngeometry priors into 4D assets. We summarize five major challenges of 4D\ngeneration: consistency, controllability, diversity, efficiency, and fidelity,\naccompanied by an outline of existing solutions to address these issues. We\nsystematically analyze applications of 4D generation, spanning dynamic object\ngeneration, scene generation, digital human synthesis, 4D editing, and\nautonomous driving. Finally, we provide an in-depth discussion of the obstacles\ncurrently hindering the development of the 4D generation. This survey offers a\nclear and comprehensive overview of 4D generation, aiming to stimulate further\nexploration and innovation in this rapidly evolving field. Our code is publicly\navailable at: https://github.com/MiaoQiaowei/Awesome-4D.\n","authors":["Qiaowei Miao","Kehan Li","Jinsheng Quan","Zhiyuan Min","Shaojie Ma","Yichao Xu","Yi Yang","Yawei Luo"],"pdf_url":"https://arxiv.org/pdf/2503.14501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14966v1","updated":"2025-03-19T07:58:43Z","published":"2025-03-19T07:58:43Z","title":"Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models","summary":"  Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V.\n","authors":["Tingxiu Chen","Yilei Shi","Zixuan Zheng","Bingcong Yan","Jingliang Hu","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2503.14966v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2409.12620v4","updated":"2025-03-19T07:54:57Z","published":"2024-09-19T09:50:03Z","title":"Accurate Automatic 3D Annotation of Traffic Lights and Signs for\n  Autonomous Driving","summary":"  3D detection of traffic management objects, such as traffic lights and road\nsigns, is vital for self-driving cars, particularly for address-to-address\nnavigation where vehicles encounter numerous intersections with these static\nobjects. This paper introduces a novel method for automatically generating\naccurate and temporally consistent 3D bounding box annotations for traffic\nlights and signs, effective up to a range of 200 meters. These annotations are\nsuitable for training real-time models used in self-driving cars, which need a\nlarge amount of training data. The proposed method relies only on RGB images\nwith 2D bounding boxes of traffic management objects, which can be\nautomatically obtained using an off-the-shelf image-space detector neural\nnetwork, along with GNSS/INS data, eliminating the need for LiDAR point cloud\ndata.\n","authors":["Sándor Kunsági-Máté","Levente Pető","Lehel Seres","Tamás Matuszka"],"pdf_url":"https://arxiv.org/pdf/2409.12620v4.pdf","comment":"Accepted at the 2nd Workshop on Vision-Centric Autonomous Driving\n  (VCAD) as part of ECCV 2024"},{"id":"http://arxiv.org/abs/2503.14960v1","updated":"2025-03-19T07:54:52Z","published":"2025-03-19T07:54:52Z","title":"Body-Hand Modality Expertized Networks with Cross-attention for\n  Fine-grained Skeleton Action Recognition","summary":"  Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods.\n","authors":["Seungyeon Cho","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2503.14960v1.pdf","comment":"7 figures, 8 pages"},{"id":"http://arxiv.org/abs/2503.14958v1","updated":"2025-03-19T07:51:14Z","published":"2025-03-19T07:51:14Z","title":"Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot\n  Medical Video Object Segmentation via Spatiotemporal Consistency Relearning","summary":"  Few-shot video object segmentation aims to reduce annotation costs; however,\nexisting methods still require abundant dense frame annotations for training,\nwhich are scarce in the medical domain. We investigate an extremely low-data\nregime that utilizes annotations from only a few video frames and leverages\nexisting labeled images to minimize costly video annotations. Specifically, we\npropose a two-phase framework. First, we learn a few-shot segmentation model\nusing labeled images. Subsequently, to improve performance without full\nsupervision, we introduce a spatiotemporal consistency relearning approach on\nmedical videos that enforces consistency between consecutive frames.\nConstraints are also enforced between the image model and relearning model at\nboth feature and prediction levels. Experiments demonstrate the superiority of\nour approach over state-of-the-art few-shot segmentation methods. Our model\nbridges the gap between abundant annotated medical images and scarce, sparsely\nlabeled medical videos to achieve strong video segmentation performance in this\nlow data regime. Code is available at https://github.com/MedAITech/RAB.\n","authors":["Zixuan Zheng","Yilei Shi","Chunlei Li","Jingliang Hu","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2503.14958v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2503.14957v1","updated":"2025-03-19T07:49:14Z","published":"2025-03-19T07:49:14Z","title":"Neuro Symbolic Knowledge Reasoning for Procedural Video Question\n  Answering","summary":"  This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon.\n","authors":["Thanh-Son Nguyen","Hong Yang","Tzeh Yuan Neoh","Hao Zhang","Ee Yeo Keat","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2503.14957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14955v1","updated":"2025-03-19T07:46:25Z","published":"2025-03-19T07:46:25Z","title":"Depth-Aware Range Image-Based Model for Point Cloud Segmentation","summary":"  Point cloud segmentation (PCS) aims to separate points into different and\nmeaningful groups. The task plays an important role in robotics because PCS\nenables robots to understand their physical environments directly. To process\nsparse and large-scale outdoor point clouds in real time, range image-based\nmodels are commonly adopted. However, in a range image, the lack of explicit\ndepth information inevitably causes some separate objects in 3D space to touch\neach other, bringing difficulty for the range image-based models in correctly\nsegmenting the objects. Moreover, previous PCS models are usually derived from\nthe existing color image-based models and unable to make full use of the\nimplicit but ordered depth information inherent in the range image, thereby\nachieving inferior performance. In this paper, we propose Depth-Aware Module\n(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the\nrange image by explicitly modelling the interdependence among channels. Fast\nFMVNet V3 incorporates DAM by integrating it into the last block in each\narchitecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,\nand SemanticPOSS demonstrate that DAM brings a significant improvement for Fast\nFMVNet V3 with negligible computational cost.\n","authors":["Bike Chen","Antti Tikanmäki","Juha Röning"],"pdf_url":"https://arxiv.org/pdf/2503.14955v1.pdf","comment":"No Comments"},{"id":"http://arxiv.org/abs/2503.14953v1","updated":"2025-03-19T07:42:24Z","published":"2025-03-19T07:42:24Z","title":"Aligning Information Capacity Between Vision and Language via\n  Dense-to-Sparse Feature Distillation for Image-Text Matching","summary":"  Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods.\n","authors":["Yang Liu","Wentao Feng","Zhuoyao Liu","Shudong Huang","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2503.14953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09361v2","updated":"2025-03-19T07:33:47Z","published":"2024-11-14T11:08:54Z","title":"Time-to-Event Pretraining for 3D Medical Imaging","summary":"  With the rise of medical foundation models and the growing availability of\nimaging data, scalable pretraining techniques offer a promising way to identify\nimaging biomarkers predictive of future disease risk. While current\nself-supervised methods for 3D medical imaging models capture local structural\nfeatures like organ morphology, they fail to link pixel biomarkers with\nlong-term health outcomes due to a missing context problem. Current approaches\nlack the temporal context necessary to identify biomarkers correlated with\ndisease progression, as they rely on supervision derived only from images and\nconcurrent text descriptions. To address this, we introduce time-to-event\npretraining, a pretraining framework for 3D medical imaging models that\nleverages large-scale temporal supervision from paired, longitudinal electronic\nhealth records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D\nimages) and time-to-event distributions across thousands of EHR-derived tasks,\nour method improves outcome prediction, achieving an average AUROC increase of\n23.7% and a 29.4% gain in Harrell's C-index across 8 benchmark tasks.\nImportantly, these gains are achieved without sacrificing diagnostic\nclassification performance. This study lays the foundation for integrating\nlongitudinal EHR and 3D imaging data to advance clinical risk prediction.\n","authors":["Zepeng Huo","Jason Alan Fries","Alejandro Lozano","Jeya Maria Jose Valanarasu","Ethan Steinberg","Louis Blankemeier","Akshay S. Chaudhari","Curtis Langlotz","Nigam H. Shah"],"pdf_url":"https://arxiv.org/pdf/2411.09361v2.pdf","comment":"34 pages, 19 figures"},{"id":"http://arxiv.org/abs/2503.14950v1","updated":"2025-03-19T07:29:02Z","published":"2025-03-19T07:29:02Z","title":"USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and\n  Scene Depth Estimation using Features from a Pre-trained Image Segmentation\n  network","summary":"  The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data.\n","authors":["Joseph Emmanuel DL Dayo","Prospero C. Naval Jr"],"pdf_url":"https://arxiv.org/pdf/2503.14950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11221v2","updated":"2025-03-19T07:26:14Z","published":"2025-03-14T09:12:03Z","title":"Toward Generalized Image Quality Assessment: Relaxing the Perfect\n  Reference Quality Assumption","summary":"  Full-reference image quality assessment (FR-IQA) generally assumes that\nreference images are of perfect quality. However, this assumption is flawed due\nto the sensor and optical limitations of modern imaging systems. Moreover,\nrecent generative enhancement methods are capable of producing images of higher\nquality than their original. All of these challenge the effectiveness and\napplicability of current FR-IQA models. To relax the assumption of perfect\nreference image quality, we build a large-scale IQA database, namely DiffIQA,\ncontaining approximately 180,000 images generated by a diffusion-based image\nenhancer with adjustable hyper-parameters. Each image is annotated by human\nsubjects as either worse, similar, or better quality compared to its reference.\nBuilding on this, we present a generalized FR-IQA model, namely Adaptive\nFidelity-Naturalness Evaluator (A-FINE), to accurately assess and adaptively\ncombine the fidelity and naturalness of a test image. A-FINE aligns well with\nstandard FR-IQA when the reference image is much more natural than the test\nimage. We demonstrate by extensive experiments that A-FINE surpasses standard\nFR-IQA models on well-established IQA datasets and our newly created DiffIQA.\nTo further validate A-FINE, we additionally construct a super-resolution IQA\nbenchmark (SRIQA-Bench), encompassing test images derived from ten\nstate-of-the-art SR methods with reliable human quality annotations. Tests on\nSRIQA-Bench re-affirm the advantages of A-FINE. The code and dataset are\navailable at https://tianhewu.github.io/A-FINE-page.github.io/.\n","authors":["Du Chen","Tianhe Wu","Kede Ma","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.11221v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14948v1","updated":"2025-03-19T07:25:21Z","published":"2025-03-19T07:25:21Z","title":"ChatStitch: Visualizing Through Structures via Surround-View\n  Unsupervised Deep Image Stitching with Collaborative LLM-Agents","summary":"  Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively.\n","authors":["Hao Liang","Zhipeng Dong","Yi Yang","Mengyin Fu"],"pdf_url":"https://arxiv.org/pdf/2503.14948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14945v1","updated":"2025-03-19T07:20:16Z","published":"2025-03-19T07:20:16Z","title":"Generating Multimodal Driving Scenes via Next-Scene Prediction","summary":"  Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements.\n","authors":["Yanhao Wu","Haoyang Zhang","Tianwei Lin","Lichao Huang","Shujie Luo","Rui Wu","Congpei Qiu","Wei Ke","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14944v1","updated":"2025-03-19T07:20:02Z","published":"2025-03-19T07:20:02Z","title":"MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with\n  Language Guidance","summary":"  Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https://github.com/294coder/MMAIF.\n","authors":["Zihan Cao","Yu Zhong","Ziqi Wang","Liang-Jian Deng"],"pdf_url":"https://arxiv.org/pdf/2503.14944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14943v1","updated":"2025-03-19T07:19:50Z","published":"2025-03-19T07:19:50Z","title":"3D Engine-ready Photorealistic Avatars via Dynamic Textures","summary":"  As the digital and physical worlds become more intertwined, there has been a\nlot of interest in digital avatars that closely resemble their real-world\ncounterparts. Current digitization methods used in 3D production pipelines\nrequire costly capture setups, making them impractical for mass usage among\ncommon consumers. Recent academic literature has found success in\nreconstructing humans from limited data using implicit representations (e.g.,\nvoxels used in NeRFs), which are able to produce impressive videos. However,\nthese methods are incompatible with traditional rendering pipelines, making it\ndifficult to use them in applications such as games. In this work, we propose\nan end-to-end pipeline that builds explicitly-represented photorealistic 3D\navatars using standard 3D assets. Our key idea is the use of\ndynamically-generated textures to enhance the realism and visually mask\ndeficiencies in the underlying mesh geometry. This allows for seamless\nintegration with current graphics pipelines while achieving comparable visual\nquality to state-of-the-art 3D avatar generation methods.\n","authors":["Yifan Wang","Ivan Molodetskikh","Ondrej Texler","Dimitar Dinev"],"pdf_url":"https://arxiv.org/pdf/2503.14943v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14941v1","updated":"2025-03-19T07:15:41Z","published":"2025-03-19T07:15:41Z","title":"UPME: An Unsupervised Peer Review Framework for Multimodal Large\n  Language Model Evaluation","summary":"  Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences.\n","authors":["Qihui Zhang","Munan Ning","Zheyuan Liu","Yanbo Wang","Jiayi Ye","Yue Huang","Shuo Yang","Xiao Chen","Yibing Song","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.14941v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.14939v1","updated":"2025-03-19T07:07:43Z","published":"2025-03-19T07:07:43Z","title":"VisNumBench: Evaluating Number Sense of Multimodal Large Language Models","summary":"  Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps://wwwtttjjj.github.io/VisNumBench/.\n","authors":["Tengjin Weng","Jingyi Wang","Wenhao Jiang","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2503.14939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12765v2","updated":"2025-03-19T07:05:40Z","published":"2024-02-20T07:12:22Z","title":"GOOD: Towards Domain Generalized Orientated Object Detection","summary":"  Oriented object detection has been rapidly developed in the past few years,\nbut most of these methods assume the training and testing images are under the\nsame statistical distribution, which is far from reality. In this paper, we\npropose the task of domain generalized oriented object detection, which intends\nto explore the generalization of oriented object detectors on arbitrary unseen\ntarget domains. Learning domain generalized oriented object detectors is\nparticularly challenging, as the cross-domain style variation not only\nnegatively impacts the content representation, but also leads to unreliable\norientation predictions. To address these challenges, we propose a generalized\noriented object detector (GOOD). After style hallucination by the emerging\ncontrastive language-image pre-training (CLIP), it consists of two key\ncomponents, namely, rotation-aware content consistency learning (RAC) and style\nconsistency learning (SEC). The proposed RAC allows the oriented object\ndetector to learn stable orientation representation from style-diversified\nsamples. The proposed SEC further stabilizes the generalization ability of\ncontent representation from different image styles. Extensive experiments on\nmultiple cross-domain settings show the state-of-the-art performance of GOOD.\nSource code will be publicly available.\n","authors":["Qi Bi","Beichen Zhou","Jingjun Yi","Wei Ji","Haolan Zhan","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2402.12765v2.pdf","comment":"18 pages. accepted by ISPRS"},{"id":"http://arxiv.org/abs/2503.14938v1","updated":"2025-03-19T07:04:24Z","published":"2025-03-19T07:04:24Z","title":"Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot\n  Remote Sensing Scene Classification","summary":"  Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization.\n","authors":["Zhong Ji","Ci Liu","Jingren Liu","Chen Tang","Yanwei Pang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2503.14938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11361v3","updated":"2025-03-19T07:04:00Z","published":"2024-11-18T08:12:54Z","title":"Scalable Autoregressive Monocular Depth Estimation","summary":"  This paper shows that the autoregressive model is an effective and scalable\nmonocular depth estimator. Our idea is simple: We tackle the monocular depth\nestimation (MDE) task with an autoregressive prediction paradigm, based on two\ncore designs. First, our depth autoregressive model (DAR) treats the depth map\nof different resolutions as a set of tokens, and conducts the low-to-high\nresolution autoregressive objective with a patch-wise casual mask. Second, our\nDAR recursively discretizes the entire depth range into more compact intervals,\nand attains the coarse-to-fine granularity autoregressive objective in an\nordinal-regression manner. By coupling these two autoregressive objectives, our\nDAR establishes new state-of-the-art (SOTA) on KITTI and NYU Depth v2 by clear\nmargins. Further, our scalable approach allows us to scale the model up to 2.0B\nand achieve the best RMSE of 1.799 on the KITTI dataset (5% improvement)\ncompared to 1.896 by the current SOTA (Depth Anything). DAR further showcases\nzero-shot generalization ability on unseen datasets. These results suggest that\nDAR yields superior performance with an autoregressive prediction paradigm,\nproviding a promising approach to equip modern autoregressive large models\n(e.g., GPT-4o) with depth estimation capabilities.\n","authors":["Jinhong Wang","Jian Liu","Dongqi Tang","Weiqiang Wang","Wentong Li","Danny Chen","Jintai Chen","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11361v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.11117v2","updated":"2025-03-19T06:56:19Z","published":"2025-03-14T06:29:47Z","title":"Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied\n  Question Answering","summary":"  Embodied Question Answering (EQA) is a challenging task in embodied\nintelligence that requires agents to dynamically explore 3D environments,\nactively gather visual information, and perform multi-step reasoning to answer\nquestions. However, current EQA approaches suffer from critical limitations in\nexploration efficiency, dataset design, and evaluation metrics. Moreover,\nexisting datasets often introduce biases or prior knowledge, leading to\ndisembodied reasoning, while frontier-based exploration strategies struggle in\ncluttered environments and fail to ensure fine-grained exploration of\ntask-relevant areas. To address these challenges, we construct the\nEXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the\nlargest dataset designed specifically to evaluate both exploration and\nreasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories\nand 2,044 question-trajectory pairs. To improve exploration efficiency, we\npropose Fine-EQA, a hybrid exploration model that integrates frontier-based and\ngoal-oriented navigation to guide agents toward task-relevant regions more\neffectively. Additionally, we introduce a novel evaluation metric,\nExploration-Answer Consistency (EAC), which ensures faithful assessment by\nmeasuring the alignment between answer grounding and exploration reliability.\nExtensive experimental comparisons with state-of-the-art EQA models demonstrate\nthe effectiveness of our EXPRESS-Bench in advancing embodied exploration and\nquestion reasoning.\n","authors":["Kaixuan Jiang","Yang Liu","Weixing Chen","Jingzhou Luo","Ziliang Chen","Ling Pan","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2503.11117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14935v1","updated":"2025-03-19T06:42:32Z","published":"2025-03-19T06:42:32Z","title":"FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion\n  Understanding","summary":"  Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}.\n","authors":["Chongjun Tu","Lin Zhang","Pengtao Chen","Peng Ye","Xianfang Zeng","Wei Cheng","Gang Yu","Tao Chen"],"pdf_url":"https://arxiv.org/pdf/2503.14935v1.pdf","comment":"FAVOR-Bench project page: https://favor-bench.github.io/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.15204v1","updated":"2025-03-19T13:47:25Z","published":"2025-03-19T13:47:25Z","title":"When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection","summary":"  Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security.\n","authors":["Tittaya Mairittha","Tanakon Sawanglok","Panuwit Raden","Sorrawit Treesuk"],"pdf_url":"https://arxiv.org/pdf/2503.15204v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.15191v1","updated":"2025-03-19T13:21:49Z","published":"2025-03-19T13:21:49Z","title":"Optimizing Retrieval Strategies for Financial Question Answering\n  Documents in Retrieval-Augmented Generation Systems","summary":"  Retrieval-Augmented Generation (RAG) has emerged as a promising framework to\nmitigate hallucinations in Large Language Models (LLMs), yet its overall\nperformance is dependent on the underlying retrieval system. In the finance\ndomain, documents such as 10-K reports pose distinct challenges due to\ndomain-specific vocabulary and multi-hierarchical tabular data. In this work,\nwe introduce an efficient, end-to-end RAG pipeline that enhances retrieval for\nfinancial documents through a three-phase approach: pre-retrieval, retrieval,\nand post-retrieval. In the pre-retrieval phase, various query and corpus\npreprocessing techniques are employed to enrich input data. During the\nretrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with\ndomain-specific knowledge and implemented a hybrid retrieval strategy that\ncombines dense and sparse representations. Finally, the post-retrieval phase\nleverages Direct Preference Optimization (DPO) training and document selection\nmethods to further refine the results. Evaluations on seven financial question\nanswering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,\nand MultiHiertt-demonstrate substantial improvements in retrieval performance,\nleading to more accurate and contextually appropriate generation. These\nfindings highlight the critical role of tailored retrieval techniques in\nadvancing the effectiveness of RAG systems for financial applications. A fully\nreplicable pipeline is available on GitHub:\nhttps://github.com/seohyunwoo-0407/GAR.\n","authors":["Sejong Kim","Hyunseo Song","Hyunwoo Seo","Hyunjun Kim"],"pdf_url":"https://arxiv.org/pdf/2503.15191v1.pdf","comment":"15 pages, 3 figures, 11 tables. Accepted at ICLR 2025 Workshop on\n  Advances in Financial AI. Code available at\n  https://github.com/seohyunwoo-0407/GAR"},{"id":"http://arxiv.org/abs/2405.02732v2","updated":"2025-03-19T11:31:31Z","published":"2024-05-04T18:32:08Z","title":"Recall Them All: Retrieval-Augmented Language Models for Long Object\n  List Extraction from Long Documents","summary":"  Methods for relation extraction from text mostly focus on high precision, at\nthe cost of limited recall. High recall is crucial, though, to populate long\nlists of object entities that stand in a specific relation with a given\nsubject. Cues for relevant objects can be spread across many passages in long\ntexts. This poses the challenge of extracting long lists from long texts. We\npresent the L3X method which tackles the problem in two stages: (1)\nrecall-oriented generation using a large language model (LLM) with judicious\ntechniques for retrieval augmentation, and (2) precision-oriented\nscrutinization to validate or prune candidates. Our L3X method outperforms\nLLM-only generations by a substantial margin.\n","authors":["Sneha Singhania","Simon Razniewski","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2405.02732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20298v3","updated":"2025-03-19T07:11:37Z","published":"2024-03-29T17:15:21Z","title":"Review-Based Hyperbolic Cross-Domain Recommendation","summary":"  The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.\n","authors":["Yoonhyuk Choi","Jiho Choi","Taewook Ko","Chong-Kwon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.20298v3.pdf","comment":"WSDM '25"},{"id":"http://arxiv.org/abs/2502.02430v2","updated":"2025-03-19T04:49:08Z","published":"2025-02-04T15:55:10Z","title":"A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals","summary":"  Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.\n","authors":["Róbert Busa-Fekete","Julian Zimmert","András György","Linhai Qiu","Tzu-Wei Sung","Hao Shen","Hyomin Choi","Sharmila Subramaniam","Li Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.02430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14887v1","updated":"2025-03-19T04:30:20Z","published":"2025-03-19T04:30:20Z","title":"Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense\n  Retrieval","summary":"  Pseudo-relevance feedback (PRF) refines queries by leveraging initially\nretrieved documents to improve retrieval effectiveness. In this paper, we\ninvestigate how large language models (LLMs) can facilitate PRF for zero-shot\nLLM-based dense retrieval, extending the recently proposed PromptReps method.\nSpecifically, our approach uses LLMs to extract salient passage features-such\nas keywords and summaries-from top-ranked documents, which are then integrated\ninto PromptReps to produce enhanced query representations. Experiments on\npassage retrieval benchmarks demonstrate that incorporating PRF significantly\nboosts retrieval performance. Notably, smaller rankers with PRF can match the\neffectiveness of larger rankers without PRF, highlighting PRF's potential to\nimprove LLM-driven search while maintaining an efficient balance between\neffectiveness and resource usage.\n","authors":["Hang Li","Xiao Wang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2503.14887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14342v2","updated":"2025-03-19T02:48:55Z","published":"2025-01-24T09:12:52Z","title":"Chain-of-Retrieval Augmented Generation","summary":"  This paper introduces an approach for training o1-like RAG models that\nretrieve and reason over relevant information step by step before generating\nthe final answer. Conventional RAG methods usually perform a single retrieval\nstep before the generation process, which limits their effectiveness in\naddressing complex queries due to imperfect retrieval results. In contrast, our\nproposed method, CoRAG (Chain-of-Retrieval Augmented Generation), allows the\nmodel to dynamically reformulate the query based on the evolving state. To\ntrain CoRAG effectively, we utilize rejection sampling to automatically\ngenerate intermediate retrieval chains, thereby augmenting existing RAG\ndatasets that only provide the correct final answer. At test time, we propose\nvarious decoding strategies to scale the model's test-time compute by\ncontrolling the length and number of sampled retrieval chains. Experimental\nresults across multiple benchmarks validate the efficacy of CoRAG, particularly\nin multi-hop question answering tasks, where we observe more than 10 points\nimprovement in EM score compared to strong baselines. On the KILT benchmark,\nCoRAG establishes a new state-of-the-art performance across a diverse range of\nknowledge-intensive tasks. Furthermore, we offer comprehensive analyses to\nunderstand the scaling behavior of CoRAG, laying the groundwork for future\nresearch aimed at developing factual and grounded foundation models.\n","authors":["Liang Wang","Haonan Chen","Nan Yang","Xiaolong Huang","Zhicheng Dou","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2501.14342v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2412.18860v2","updated":"2025-03-19T02:46:34Z","published":"2024-12-25T10:08:54Z","title":"Bootstrap Your Own Context Length","summary":"  We introduce a bootstrapping approach to train long-context language models\nby exploiting their short-context capabilities only. Our method utilizes a\nsimple agent workflow to synthesize diverse long-context instruction tuning\ndata, thereby eliminating the necessity for manual data collection and\nannotation. The proposed data synthesis workflow requires only a short-context\nlanguage model, a text retriever, and a document collection, all of which are\nreadily accessible within the open-source ecosystem. Subsequently, language\nmodels are fine-tuned using the synthesized data to extend their context\nlengths. In this manner, we effectively transfer the short-context capabilities\nof language models to long-context scenarios through a bootstrapping process.\nWe conduct experiments with the open-source Llama-3 family of models and\ndemonstrate that our method can successfully extend the context length to up to\n1M tokens, achieving superior performance across various benchmarks.\n","authors":["Liang Wang","Nan Yang","Xingxing Zhang","Xiaolong Huang","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2412.18860v2.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2501.11870v2","updated":"2025-03-19T01:12:41Z","published":"2025-01-21T03:56:23Z","title":"Coarse-to-Fine Lightweight Meta-Embedding for ID-Based Recommendation","summary":"  The state-of-the-art recommendation systems have shifted the attention to\nefficient recommendation, e.g., on-device recommendation, under memory\nconstraints. To this end, the existing methods either focused on the\nlightweight embeddings for both users and items, or involved on-device systems\nenjoying the compact embeddings to enhance reusability and reduces space\ncomplexity. However, they focus solely on the coarse granularity of embedding,\nwhile overlook the fine-grained semantic nuances, to adversarially downgrade\nthe efficacy of meta-embeddings in capturing the intricate relationship over\nboth user and item, consequently resulting into the suboptimal recommendations.\nIn this paper, we aim to study how the meta-embedding can efficiently learn\nvaried grained semantics, together with how the fine-grained meta-embedding can\nstrengthen the representation of coarse-grained meta-embedding. To answer these\nquestions, we develop a novel graph neural networks (GNNs) based recommender\nwhere each user and item serves as the node, linked directly to coarse-grained\nvirtual nodes and indirectly to fine-grained virtual nodes, ensuring different\ngrained semantic learning, while disclosing: 1) In contrast to coarse-grained\nsemantics, fine-grained semantics are well captured through sparse\nmeta-embeddings, which adaptively 2) balance the embedding uniqueness and\nmemory constraint. Additionally, the initialization method come up upon\nSparsePCA, along with a soft thresholding activation function to render the\nsparseness of the meta-embeddings. We propose a weight bridging update strategy\nthat focuses on matching each coarse-grained meta-embedding with several\nfine-grained meta-embeddings based on the users/items' semantics. Extensive\nexperiments substantiate our method's superiority over existing baselines. Our\ncode is available at https://github.com/htyjers/C2F-MetaEmbed.\n","authors":["Yang Wang","Haipeng Liu","Zeqian Yi","Biao Qian","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2501.11870v2.pdf","comment":"16 pages, 6 figures, accepted to appear at Science China Information\n  Sciences"},{"id":"http://arxiv.org/abs/2503.14813v1","updated":"2025-03-19T01:01:28Z","published":"2025-03-19T01:01:28Z","title":"Scaled Supervision is an Implicit Lipschitz Regularizer","summary":"  In modern social media, recommender systems (RecSys) rely on the\nclick-through rate (CTR) as the standard metric to evaluate user engagement.\nCTR prediction is traditionally framed as a binary classification task to\npredict whether a user will interact with a given item. However, this approach\noverlooks the complexity of real-world social modeling, where the user, item,\nand their interactive features change dynamically in fast-paced online\nenvironments. This dynamic nature often leads to model instability, reflected\nin overfitting short-term fluctuations rather than higher-level interactive\npatterns. While overfitting calls for more scaled and refined supervisions,\ncurrent solutions often rely on binary labels that overly simplify fine-grained\nuser preferences through the thresholding process, which significantly reduces\nthe richness of the supervision. Therefore, we aim to alleviate the overfitting\nproblem by increasing the supervision bandwidth in CTR training. Specifically,\n(i) theoretically, we formulate the impact of fine-grained preferences on model\nstability as a Lipschitz constrain; (ii) empirically, we discover that scaling\nthe supervision bandwidth can act as an implicit Lipschitz regularizer, stably\noptimizing existing CTR models to achieve better generalizability. Extensive\nexperiments show that this scaled supervision significantly and consistently\nimproves the optimization process and the performance of existing CTR models,\neven without the need for additional hyperparameter tuning.\n","authors":["Zhongyu Ouyang","Chunhui Zhang","Yaning Jia","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2503.14813v1.pdf","comment":"Accepted to the International AAAI Conference on Web and Social Media\n  (ICWSM 2025)"},{"id":"http://arxiv.org/abs/2503.14802v1","updated":"2025-03-19T00:28:54Z","published":"2025-03-19T00:28:54Z","title":"Graph-Based Re-ranking: Emerging Techniques, Limitations, and\n  Opportunities","summary":"  Knowledge graphs have emerged to be promising datastore candidates for\ncontext augmentation during Retrieval Augmented Generation (RAG). As a result,\ntechniques in graph representation learning have been simultaneously explored\nalongside principal neural information retrieval approaches, such as two-phased\nretrieval, also known as re-ranking. While Graph Neural Networks (GNNs) have\nbeen proposed to demonstrate proficiency in graph learning for re-ranking,\nthere are ongoing limitations in modeling and evaluating input graph structures\nfor training and evaluation for passage and document ranking tasks. In this\nsurvey, we review emerging GNN-based ranking model architectures along with\ntheir corresponding graph representation construction methodologies. We\nconclude by providing recommendations on future research based on\ncommunity-wide challenges and opportunities.\n","authors":["Md Shahir Zaoad","Niamat Zawad","Priyanka Ranade","Richard Krogman","Latifur Khan","James Holt"],"pdf_url":"https://arxiv.org/pdf/2503.14802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14800v1","updated":"2025-03-19T00:24:01Z","published":"2025-03-19T00:24:01Z","title":"Long Context Modeling with Ranked Memory-Augmented Retrieval","summary":"  Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.\n","authors":["Ghadir Alselwi","Hao Xue","Shoaib Jameel","Basem Suleiman","Flora D. Salim","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2503.14800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09516v2","updated":"2025-03-19T21:40:12Z","published":"2025-03-12T16:26:39Z","title":"Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning","summary":"  Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Prompting advanced LLMs with reasoning capabilities during inference to\nuse search engines is not optimal, since the LLM does not learn how to\noptimally interact with the search engine. This paper introduces Search-R1, an\nextension of the DeepSeek-R1 model where the LLM learns -- solely through\nreinforcement learning (RL) -- to autonomously generate (multiple) search\nqueries during step-by-step reasoning with real-time retrieval. Search-R1\noptimizes LLM rollouts with multi-turn search interactions, leveraging\nretrieved token masking for stable RL training and a simple outcome-based\nreward function. Experiments on seven question-answering datasets show that\nSearch-R1 improves performance by 26% (Qwen2.5-7B), 21% (Qwen2.5-3B), and 10%\n(LLaMA3.2-3B) over strong baselines. This paper further provides empirical\ninsights into RL optimization methods, LLM choices, and response length\ndynamics in retrieval-augmented reasoning. The code and model checkpoints are\navailable at https://github.com/PeterGriffinJin/Search-R1.\n","authors":["Bowen Jin","Hansi Zeng","Zhenrui Yue","Dong Wang","Hamed Zamani","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2503.09516v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2503.15681v1","updated":"2025-03-19T20:25:56Z","published":"2025-03-19T20:25:56Z","title":"Narrative Trails: A Method for Coherent Storyline Extraction via Maximum\n  Capacity Path Optimization","summary":"  Traditional information retrieval is primarily concerned with finding\nrelevant information from large datasets without imposing a structure within\nthe retrieved pieces of data. However, structuring information in the form of\nnarratives--ordered sets of documents that form coherent storylines--allows us\nto identify, interpret, and share insights about the connections and\nrelationships between the ideas presented in the data. Despite their\nsignificance, current approaches for algorithmically extracting storylines from\ndata are scarce, with existing methods primarily relying on intricate\nword-based heuristics and auxiliary document structures. Moreover, many of\nthese methods are difficult to scale to large datasets and general contexts, as\nthey are designed to extract storylines for narrow tasks. In this paper, we\npropose Narrative Trails, an efficient, general-purpose method for extracting\ncoherent storylines in large text corpora. Specifically, our method uses the\nsemantic-level information embedded in the latent space of deep learning models\nto build a sparse coherence graph and extract narratives that maximize the\nminimum coherence of the storylines. By quantitatively evaluating our proposed\nmethods on two distinct narrative extraction tasks, we show the\ngeneralizability and scalability of Narrative Trails in multiple contexts while\nalso simplifying the extraction pipeline.\n","authors":["Fausto German","Brian Keith","Chris North"],"pdf_url":"https://arxiv.org/pdf/2503.15681v1.pdf","comment":"Eighth Text2Story Workshop at the 47th European Conference on\n  Information Retrieval (ECIR 2025). The code for our algorithm, evaluations,\n  and examples are available at\n  https://github.com/faustogerman/narrative-trails"},{"id":"http://arxiv.org/abs/2503.15571v1","updated":"2025-03-19T11:01:00Z","published":"2025-03-19T11:01:00Z","title":"LLM-Aided Customizable Profiling of Code Data Based On Programming\n  Language Concepts","summary":"  Data profiling is critical in machine learning for generating descriptive\nstatistics, supporting both deeper understanding and downstream tasks like data\nvaluation and curation. This work addresses profiling specifically in the\ncontext of code datasets for Large Language Models (code-LLMs), where data\nquality directly influences tasks such as code generation and summarization.\nCharacterizing code datasets in terms of programming language concepts enables\nbetter insights and targeted data curation. Our proposed methodology decomposes\ncode data profiling into two phases: (1) an offline phase where LLMs are\nleveraged to derive and learn rules for extracting syntactic and semantic\nconcepts across various programming languages, including previously unseen or\nlow-resource languages, and (2) an online deterministic phase applying these\nderived rules for efficient real-time analysis. This hybrid approach is\ncustomizable, extensible to new syntactic and semantic constructs, and scalable\nto multiple languages. Experimentally, our LLM-aided method achieves a mean\naccuracy of 90.33% for syntactic extraction rules and semantic classification\naccuracies averaging 80% and 77% across languages and semantic concepts,\nrespectively.\n","authors":["Pankaj Thorat","Adnan Qidwai","Adrija Dhar","Aishwariya Chakraborty","Anand Eswaran","Hima Patel","Praveen Jayachandran"],"pdf_url":"https://arxiv.org/pdf/2503.15571v1.pdf","comment":"21 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.15485v1","updated":"2025-03-19T17:58:57Z","published":"2025-03-19T17:58:57Z","title":"TULIP: Towards Unified Language-Image Pretraining","summary":"  Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io\n","authors":["Zineng Tang","Long Lian","Seun Eisape","XuDong Wang","Roei Herzig","Adam Yala","Alane Suhr","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2503.15485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15484v1","updated":"2025-03-19T17:57:49Z","published":"2025-03-19T17:57:49Z","title":"Value Profiles for Encoding Human Variation","summary":"  Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information.\n","authors":["Taylor Sorensen","Pushkar Mishra","Roma Patel","Michael Henry Tessler","Michiel Bakker","Georgina Evans","Iason Gabriel","Noah Goodman","Verena Rieser"],"pdf_url":"https://arxiv.org/pdf/2503.15484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15482v1","updated":"2025-03-19T17:57:11Z","published":"2025-03-19T17:57:11Z","title":"Natural Quantization of Neural Networks","summary":"  We propose a natural quantization of a standard neural network, where the\nneurons correspond to qubits and the activation functions are implemented via\nquantum gates and measurements. The simplest quantized neural network\ncorresponds to applying single-qubit rotations, with the rotation angles being\ndependent on the weights and measurement outcomes of the previous layer. This\nrealization has the advantage of being smoothly tunable from the purely\nclassical limit with no quantum uncertainty (thereby reproducing the classical\nneural network exactly) to a quantum case, where superpositions introduce an\nintrinsic uncertainty in the network. We benchmark this architecture on a\nsubset of the standard MNIST dataset and find a regime of \"quantum advantage,\"\nwhere the validation error rate in the quantum realization is smaller than that\nin the classical model. We also consider another approach where quantumness is\nintroduced via weak measurements of ancilla qubits entangled with the neuron\nqubits. This quantum neural network also allows for smooth tuning of the degree\nof quantumness by controlling an entanglement angle, $g$, with $g=\\frac\\pi 2$\nreplicating the classical regime. We find that validation error is also\nminimized within the quantum regime in this approach. We also observe a quantum\ntransition, with sharp loss of the quantum network's ability to learn at a\ncritical point $g_c$. The proposed quantum neural networks are readily\nrealizable in present-day quantum computers on commercial datasets.\n","authors":["Richard Barney","Djamil Lakhdar-Hamina","Victor Galitski"],"pdf_url":"https://arxiv.org/pdf/2503.15482v1.pdf","comment":"7 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.15481v1","updated":"2025-03-19T17:56:14Z","published":"2025-03-19T17:56:14Z","title":"Learning to Play Piano in the Real World","summary":"  Towards the grand challenge of achieving human-level manipulation in robots,\nplaying piano is a compelling testbed that requires strategic, precise, and\nflowing movements. Over the years, several works demonstrated hand-designed\ncontrollers on real world piano playing, while other works evaluated robot\nlearning approaches on simulated piano scenarios. In this paper, we develop the\nfirst piano playing robotic system that makes use of learning approaches while\nalso being deployed on a real world dexterous robot. Specifically, we make use\nof Sim2Real to train a policy in simulation using reinforcement learning before\ndeploying the learned policy on a real world dexterous robot. In our\nexperiments, we thoroughly evaluate the interplay between domain randomization\nand the accuracy of the dynamics model used in simulation. Moreover, we\nevaluate the robot's performance across multiple songs with varying complexity\nto study the generalization of our learned policy. By providing a\nproof-of-concept of learning to play piano in the real world, we want to\nencourage the community to adopt piano playing as a compelling benchmark\ntowards human-level manipulation. We open-source our code and show additional\nvideos at https://lasr.org/research/learning-to-play-piano .\n","authors":["Yves-Simon Zeulner","Sandeep Selvaraj","Roberto Calandra"],"pdf_url":"https://arxiv.org/pdf/2503.15481v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.15478v1","updated":"2025-03-19T17:55:08Z","published":"2025-03-19T17:55:08Z","title":"SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning\n  Tasks","summary":"  Large language model (LLM) agents need to perform multi-turn interactions in\nreal-world tasks. However, existing multi-turn RL algorithms for optimizing LLM\nagents fail to perform effective credit assignment over multiple turns while\nleveraging the generalization capabilities of LLMs and it remains unclear how\nto develop such algorithms. To study this, we first introduce a new benchmark,\nColBench, where an LLM agent interacts with a human collaborator over multiple\nturns to solve realistic tasks in backend programming and frontend design.\nBuilding on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with\nStep-WisE Evaluation from Training-time information), that uses a carefully\ndesigned optimization objective to train a critic model with access to\nadditional training-time information. The critic provides step-level rewards\nfor improving the policy model. Our experiments demonstrate that SWEET-RL\nachieves a 6% absolute improvement in success and win rates on ColBench\ncompared to other state-of-the-art multi-turn RL algorithms, enabling\nLlama-3.1-8B to match or exceed the performance of GPT4-o in realistic\ncollaborative content creation.\n","authors":["Yifei Zhou","Song Jiang","Yuandong Tian","Jason Weston","Sergey Levine","Sainbayar Sukhbaatar","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2503.15478v1.pdf","comment":"29 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.15477v1","updated":"2025-03-19T17:54:41Z","published":"2025-03-19T17:54:41Z","title":"What Makes a Reward Model a Good Teacher? An Optimization Perspective","summary":"  The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization.\n","authors":["Noam Razin","Zixuan Wang","Hubert Strauss","Stanley Wei","Jason D. Lee","Sanjeev Arora"],"pdf_url":"https://arxiv.org/pdf/2503.15477v1.pdf","comment":"Code available at https://github.com/princeton-pli/what-makes-good-rm"},{"id":"http://arxiv.org/abs/2503.15457v1","updated":"2025-03-19T17:36:54Z","published":"2025-03-19T17:36:54Z","title":"Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step\n  Generator","summary":"  Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling.\n","authors":["Yuanzhi Zhu","Xi Wang","Stéphane Lathuilière","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2503.15457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15456v1","updated":"2025-03-19T17:36:53Z","published":"2025-03-19T17:36:53Z","title":"Temporal Encoding Strategies for Energy Time Series Prediction","summary":"  In contemporary power systems, energy consumption prediction plays a crucial\nrole in maintaining grid stability and resource allocation enabling power\ncompanies to minimize energy waste and avoid overloading the grid. While there\nare several research works on energy optimization, they often fail to address\nthe complexities of real-time fluctuations and the cyclic pattern of energy\nconsumption. This work proposes a novel approach to enhance the accuracy of\npredictive models by employing sinusoidal encoding on periodic features of\ntime-series data. To demonstrate the increase in performance, several\nstatistical and ensemble machine learning models were trained on an energy\ndemand dataset, using the proposed sinusoidal encoding. The performance of\nthese models was then benchmarked against identical models trained on\ntraditional encoding methods. The results demonstrated a 12.6% improvement of\nRoot Mean Squared Error (from 0.5497 to 0.4802) and a 7.8% increase in the R^2\nscore (from 0.7530 to 0.8118), indicating that the proposed encoding better\ncaptures the cyclic nature of temporal patterns than traditional methods. The\nproposed methodology significantly improves prediction accuracy while\nmaintaining computational efficiency, making it suitable for real-time\napplications in smart grid systems.\n","authors":["Aayam Bansal","Keertan Balaji","Zeus Lalani"],"pdf_url":"https://arxiv.org/pdf/2503.15456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08202v2","updated":"2025-03-19T17:33:33Z","published":"2025-02-12T08:32:10Z","title":"Privacy amplification by random allocation","summary":"  We consider the privacy guarantees of an algorithm in which a user's data is\nused in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$\ndifferentially private steps. We demonstrate that the privacy guarantees of\nthis sampling scheme can be upper bound by the privacy guarantees of the\nwell-studied independent (or Poisson) subsampling in which each step uses the\nuser's data with probability $(1+ o(1))k/t $. Further, we provide two\nadditional analysis techniques that lead to numerical improvements in some\nparameter regimes. The case of $k=1$ has been previously studied in the context\nof DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024);\nChoquette-Choo et al. (2024). Privacy analysis of Balle et al. (2020) relies on\nprivacy amplification by shuffling which leads to overly conservative bounds.\nPrivacy analysis of Chua et al. (2024a); Choquette-Choo et al. (2024) relies on\nMonte Carlo simulations that are computationally prohibitive in many practical\nscenarios and have additional inherent limitations.\n","authors":["Vitaly Feldman","Moshe Shenfeld"],"pdf_url":"https://arxiv.org/pdf/2502.08202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15448v1","updated":"2025-03-19T17:29:21Z","published":"2025-03-19T17:29:21Z","title":"Reducing Communication Overhead in Federated Learning for Network\n  Anomaly Detection with Adaptive Client Selection","summary":"  Communication overhead in federated learning (FL) poses a significant\nchallenge for network anomaly detection systems, where diverse client\nconfigurations and network conditions impact efficiency and detection accuracy.\nExisting approaches attempt optimization individually but struggle to balance\nreduced overhead with performance. This paper presents an adaptive FL framework\ncombining batch size optimization, client selection, and asynchronous updates\nfor efficient anomaly detection. Using UNSW-NB15 for general network traffic\nand ROAD for automotive networks, our framework reduces communication overhead\nby 97.6% (700.0s to 16.8s) while maintaining comparable accuracy (95.10% vs.\n95.12%). The Mann-Whitney U test confirms significant improvements (p < 0.05).\nProfiling analysis reveals efficiency gains via reduced GPU operations and\nmemory transfers, ensuring robust detection across varying client conditions.\n","authors":["William Marfo","Deepak Tosh","Shirley Moore","Joshua Suetterlein","Joseph Manzano"],"pdf_url":"https://arxiv.org/pdf/2503.15448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15441v1","updated":"2025-03-19T17:21:51Z","published":"2025-03-19T17:21:51Z","title":"A discontinuity-capturing neural network with categorical embedding and\n  its application to anisotropic elliptic interface problems","summary":"  In this paper, we propose a discontinuity-capturing shallow neural network\nwith categorical embedding to represent piecewise smooth functions. The network\ncomprises three hidden layers, a discontinuity-capturing layer, a categorical\nembedding layer, and a fully-connected layer. Under such a design, we show that\na piecewise smooth function, even with a large number of pieces, can be\napproximated by a single neural network with high prediction accuracy. We then\nleverage the proposed network model to solve anisotropic elliptic interface\nproblems. The network is trained by minimizing the mean squared error loss of\nthe system. Our results show that, despite its simple and shallow structure,\nthe proposed neural network model exhibits comparable efficiency and accuracy\nto traditional grid-based numerical methods.\n","authors":["Wei-Fan Hu","Te-Sheng Lin","Ming-Chih Lai"],"pdf_url":"https://arxiv.org/pdf/2503.15441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15432v1","updated":"2025-03-19T17:14:02Z","published":"2025-03-19T17:14:02Z","title":"Accurate, transferable, and verifiable machine-learned interatomic\n  potentials for layered materials","summary":"  Twisted layered van-der-Waals materials often exhibit unique electronic and\noptical properties absent in their non-twisted counterparts. Unfortunately,\npredicting such properties is hindered by the difficulty in determining the\natomic structure in materials displaying large moir\\'e domains. Here, we\nintroduce a split machine-learned interatomic potential and dataset curation\napproach that separates intralayer and interlayer interactions and\nsignificantly improves model accuracy -- with a tenfold increase in energy and\nforce prediction accuracy relative to conventional models. We further\ndemonstrate that traditional MLIP validation metrics -- force and energy errors\n-- are inadequate for moir\\'e structures and develop a more holistic,\nphysically-motivated metric based on the distribution of stacking\nconfigurations. This metric effectively compares the entirety of large-scale\nmoir\\'e domains between two structures instead of relying on conventional\nmeasures evaluated on smaller commensurate cells. Finally, we establish that\none-dimensional instead of two-dimensional moir\\'e structures can serve as\nefficient surrogate systems for validating MLIPs, allowing for a practical\nmodel validation protocol against explicit DFT calculations. Applying our\nframework to HfS2/GaS bilayers reveals that accurate structural predictions\ndirectly translate into reliable electronic properties. Our model-agnostic\napproach integrates seamlessly with various intralayer and interlayer\ninteraction models, enabling computationally tractable relaxation of moir\\'e\nmaterials, from bilayer to complex multilayers, with rigorously validated\naccuracy.\n","authors":["Johnathan D. Georgaras","Akash Ramdas","Chung Hsuan Shan","Elena Halsted"," Berwyn","Tianshu Li","Felipe H. da Jornada"],"pdf_url":"https://arxiv.org/pdf/2503.15432v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.11566v2","updated":"2025-03-19T17:10:51Z","published":"2024-05-19T14:30:57Z","title":"Uncertainty-Aware PPG-2-ECG for Enhanced Cardiovascular Diagnosis using\n  Diffusion Models","summary":"  Analyzing the cardiovascular system condition via Electrocardiography (ECG)\nis a common and highly effective approach, and it has been practiced and\nperfected over many decades. ECG sensing is non-invasive and relatively easy to\nacquire, and yet it is still cumbersome for holter monitoring tests that may\nspan over hours and even days. A possible alternative in this context is\nPhotoplethysmography (PPG): An optically-based signal that measures blood\nvolume fluctuations, as typically sensed by conventional ``wearable devices''.\nWhile PPG presents clear advantages in acquisition, convenience, and\ncost-effectiveness, ECG provides more comprehensive information, allowing for a\nmore precise detection of heart conditions. This implies that a conversion from\nPPG to ECG, as recently discussed in the literature, inherently involves an\nunavoidable level of uncertainty. In this paper we introduce a novel\nmethodology for addressing the PPG-2-ECG conversion, and offer an enhanced\nclassification of cardiovascular conditions using the given PPG, all while\ntaking into account the uncertainties arising from the conversion process. We\nprovide a mathematical justification for our proposed computational approach,\nand present empirical studies demonstrating its superior performance compared\nto state-of-the-art baseline methods.\n","authors":["Omer Belhasin","Idan Kligvasser","George Leifman","Regev Cohen","Erin Rainaldi","Li-Fang Cheng","Nishant Verma","Paul Varghese","Ehud Rivlin","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2405.11566v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01130v2","updated":"2025-03-19T17:02:40Z","published":"2024-06-03T09:17:35Z","title":"SAVA: Scalable Learning-Agnostic Data Valuation","summary":"  Selecting data for training machine learning models is crucial since large,\nweb-scraped, real datasets contain noisy artifacts that affect the quality and\nrelevance of individual data points. These noisy artifacts will impact model\nperformance. We formulate this problem as a data valuation task, assigning a\nvalue to data points in the training set according to how similar or dissimilar\nthey are to a clean and curated validation set. Recently, LAVA demonstrated the\nuse of optimal transport (OT) between a large noisy training dataset and a\nclean validation set, to value training data efficiently, without the\ndependency on model performance. However, the LAVA algorithm requires the\nentire dataset as an input, this limits its application to larger datasets.\nInspired by the scalability of stochastic (gradient) approaches which carry out\ncomputations on batches of data points instead of the entire dataset, we\nanalogously propose SAVA, a scalable variant of LAVA with its computation on\nbatches of data points. Intuitively, SAVA follows the same scheme as LAVA which\nleverages the hierarchically defined OT for data valuation. However, while LAVA\nprocesses the whole dataset, SAVA divides the dataset into batches of data\npoints, and carries out the OT problem computation on those batches. Moreover,\nour theoretical derivations on the trade-off of using entropic regularization\nfor OT problems include refinements of prior work. We perform extensive\nexperiments, to demonstrate that SAVA can scale to large datasets with millions\nof data points and does not trade off data valuation performance.\n","authors":["Samuel Kessler","Tam Le","Vu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.01130v2.pdf","comment":"Accepted at ICLR 2025. 27 pages, 12 figures"},{"id":"http://arxiv.org/abs/2503.15420v1","updated":"2025-03-19T17:00:58Z","published":"2025-03-19T17:00:58Z","title":"LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding","summary":"  Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks.\n","authors":["Amirhossein Kazerouni","Soroush Mehraban","Michael Brudno","Babak Taati"],"pdf_url":"https://arxiv.org/pdf/2503.15420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12936v2","updated":"2025-03-19T16:58:12Z","published":"2024-08-23T09:36:09Z","title":"Smooth InfoMax -- Towards easier Post-Hoc interpretability","summary":"  We introduce Smooth InfoMax (SIM), a novel method for self-supervised\nrepresentation learning that incorporates an interpretability constraint into\nthe learned representations at various depths of the neural network. SIM's\narchitecture is split up into probabilistic modules, each locally optimized\nusing the InfoNCE bound. Inspired by VAEs, the representations from these\nmodules are designed to be samples from Gaussian distributions and are further\nconstrained to be close to the standard normal distribution. This results in a\nsmooth and predictable space, enabling traversal of the latent space through a\ndecoder for easier post-hoc analysis of the learned representations. We\nevaluate SIM's performance on sequential speech data, showing that it performs\ncompetitively with its less interpretable counterpart, Greedy InfoMax (GIM).\nMoreover, we provide insights into SIM's internal representations,\ndemonstrating that the contained information is less entangled throughout the\nrepresentation and more concentrated in a smaller subset of the dimensions.\nThis further highlights the improved interpretability of SIM.\n","authors":["Fabian Denoodt","Bart de Boer","José Oramas"],"pdf_url":"https://arxiv.org/pdf/2408.12936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13213v3","updated":"2025-03-19T16:57:23Z","published":"2024-02-20T18:24:47Z","title":"Probabilities of Chat LLMs Are Miscalibrated but Still Predict\n  Correctness on Multiple-Choice Q&A","summary":"  We study 15 large language models (LLMs) fine-tuned for chat and find that\ntheir maximum softmax probabilities (MSPs) are consistently miscalibrated on\nmultiple-choice Q&A. However, those MSPs might still encode useful uncertainty\ninformation. Specifically, we hypothesized that wrong answers would be\nassociated with smaller MSPs compared to correct answers. Via rigorous\nstatistical testing, we show that this hypothesis holds for models which\nperform well on the underlying Q&A task. We also find a strong direction\ncorrelation between Q&A accuracy and MSP correctness prediction, while finding\nno correlation between Q&A accuracy and calibration error. This suggests that\nwithin the current fine-tuning paradigm, we can expect correctness prediction\nbut not calibration to improve as LLM capabilities progress. To demonstrate the\nutility of correctness prediction, we show that when models have the option to\nabstain, performance can be improved by selectively abstaining based on the MSP\nof the initial model response, using only a small amount of labeled data to\nchoose the MSP threshold.\n","authors":["Benjamin Plaut","Nguyen X. Khanh","Tu Trinh"],"pdf_url":"https://arxiv.org/pdf/2402.13213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03436v2","updated":"2025-03-19T16:54:44Z","published":"2024-07-03T18:27:26Z","title":"A Role of Environmental Complexity on Representation Learning in Deep\n  Reinforcement Learning Agents","summary":"  We developed a simulated environment to train deep reinforcement learning\nagents on a shortcut usage navigation task, motivated by the Dual Solutions\nParadigm test used for human navigators. We manipulated the frequency with\nwhich agents were exposed to a shortcut and a navigation cue, to investigate\nhow these factors influence shortcut usage development. We find that all agents\nrapidly achieve optimal performance in closed shortcut trials once initial\nlearning starts. However, their navigation speed and shortcut usage when it is\nopen happen faster in agents with higher shortcut exposure. Analysis of the\nagents' artificial neural networks activity revealed that frequent presentation\nof a cue initially resulted in better encoding of the cue in the activity of\nindividual nodes, compared to agents who encountered the cue less often.\nHowever, stronger cue representations were ultimately formed through the use of\nthe cue in the context of navigation planning, rather than simply through\nexposure. We found that in all agents, spatial representations develop early in\ntraining and subsequently stabilize before navigation strategies fully develop,\nsuggesting that having spatially consistent activations is necessary for basic\nnavigation, but insufficient for advanced strategies. Further, using new\nanalysis techniques, we found that the planned trajectory rather than the\nagent's immediate location is encoded in the agent's networks. Moreover, the\nencoding is represented at the population rather than the individual node\nlevel. These techniques could have broader applications in studying neural\nactivity across populations of neurons or network nodes beyond individual\nactivity patterns.\n","authors":["Andrew Liu","Alla Borisyuk"],"pdf_url":"https://arxiv.org/pdf/2407.03436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12480v2","updated":"2025-03-19T16:48:03Z","published":"2024-12-17T02:33:45Z","title":"Subversion Strategy Eval: Can language models statelessly strategize to\n  subvert control protocols?","summary":"  An AI control protocol is a plan for usefully deploying AI systems that\nprevents an AI from intentionally causing some unacceptable outcome. Previous\nwork evaluated protocols by subverting them using an AI following a\nhuman-written strategy. This paper investigates how well AI systems can\ngenerate and act on their own strategies for subverting control protocols\nwhilst operating statelessly (without shared memory between contexts). To do\nthis, an AI system may need to reliably generate optimal plans in each context,\ntake actions with well-calibrated probabilities, and coordinate plans with\nother instances of itself without communicating. We develop Subversion Strategy\nEval, a suite of eight environments, covering a range of protocols and\nstrategic capabilities, and six sets of affordances that help isolate\nindividual capabilities. We implement the evaluation in Inspect-AI and release\nit open-source. We evaluate Claude 3.5 models, including helpful-only versions,\nas well as OpenAI reasoning models. None of the models demonstrate substantial\ncapability in strategizing to subvert control protocols statelessly. However,\nproviding models with additional affordances, such as the ability to share a\nplan between contexts, can substantially improve performance. We hope our\nevaluations can act as a leading indicator for when models are capable of\nsubverting control protocols and also relax the worst-case assumption of\nperfect strategic ability in AI control evaluations.\n","authors":["Alex Mallen","Charlie Griffin","Misha Wagner","Alessandro Abate","Buck Shlegeris"],"pdf_url":"https://arxiv.org/pdf/2412.12480v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15407v1","updated":"2025-03-19T16:47:56Z","published":"2025-03-19T16:47:56Z","title":"Exploiting Prior Knowledge in Preferential Learning of Individualized\n  Autonomous Vehicle Driving Styles","summary":"  Trajectory planning for automated vehicles commonly employs optimization over\na moving horizon - Model Predictive Control - where the cost function\ncritically influences the resulting driving style. However, finding a suitable\ncost function that results in a driving style preferred by passengers remains\nan ongoing challenge. We employ preferential Bayesian optimization to learn the\ncost function by iteratively querying a passenger's preference. Due to\nincreasing dimensionality of the parameter space, preference learning\napproaches might struggle to find a suitable optimum with a limited number of\nexperiments and expose the passenger to discomfort when exploring the parameter\nspace. We address these challenges by incorporating prior knowledge into the\npreferential Bayesian optimization framework. Our method constructs a virtual\ndecision maker from real-world human driving data to guide parameter sampling.\nIn a simulation experiment, we achieve faster convergence of the\nprior-knowledge-informed learning procedure compared to existing preferential\nBayesian optimization approaches and reduce the number of inadequate driving\nstyles sampled.\n","authors":["Lukas Theiner","Sebastian Hirt","Alexander Steinke","Rolf Findeisen"],"pdf_url":"https://arxiv.org/pdf/2503.15407v1.pdf","comment":"6 pages, 6 figures, accepted for ECC 2025"},{"id":"http://arxiv.org/abs/2503.15403v1","updated":"2025-03-19T16:44:21Z","published":"2025-03-19T16:44:21Z","title":"HQNN-FSP: A Hybrid Classical-Quantum Neural Network for Regression-Based\n  Financial Stock Market Prediction","summary":"  Financial time-series forecasting remains a challenging task due to complex\ntemporal dependencies and market fluctuations. This study explores the\npotential of hybrid quantum-classical approaches to assist in financial trend\nprediction by leveraging quantum resources for improved feature representation\nand learning. A custom Quantum Neural Network (QNN) regressor is introduced,\ndesigned with a novel ansatz tailored for financial applications. Two hybrid\noptimization strategies are proposed: (1) a sequential approach where classical\nrecurrent models (RNN/LSTM) extract temporal dependencies before quantum\nprocessing, and (2) a joint learning framework that optimizes classical and\nquantum parameters simultaneously. Systematic evaluation using TimeSeriesSplit,\nk-fold cross-validation, and predictive error analysis highlights the ability\nof these hybrid models to integrate quantum computing into financial\nforecasting workflows. The findings demonstrate how quantum-assisted learning\ncan contribute to financial modeling, offering insights into the practical role\nof quantum resources in time-series analysis.\n","authors":["Prashant Kumar Choudhary","Nouhaila Innan","Muhammad Shafique","Rajeev Singh"],"pdf_url":"https://arxiv.org/pdf/2503.15403v1.pdf","comment":"11 pages and 11 figures"},{"id":"http://arxiv.org/abs/2503.10412v3","updated":"2025-03-19T16:39:02Z","published":"2025-03-13T14:35:47Z","title":"dFLMoE: Decentralized Federated Learning via Mixture of Experts for\n  Medical Data Analysis","summary":"  Federated learning has wide applications in the medical field. It enables\nknowledge sharing among different healthcare institutes while protecting\npatients' privacy. However, existing federated learning systems are typically\ncentralized, requiring clients to upload client-specific knowledge to a central\nserver for aggregation. This centralized approach would integrate the knowledge\nfrom each client into a centralized server, and the knowledge would be already\nundermined during the centralized integration before it reaches back to each\nclient. Besides, the centralized approach also creates a dependency on the\ncentral server, which may affect training stability if the server malfunctions\nor connections are unstable. To address these issues, we propose a\ndecentralized federated learning framework named dFLMoE. In our framework,\nclients directly exchange lightweight head models with each other. After\nexchanging, each client treats both local and received head models as\nindividual experts, and utilizes a client-specific Mixture of Experts (MoE)\napproach to make collective decisions. This design not only reduces the\nknowledge damage with client-specific aggregations but also removes the\ndependency on the central server to enhance the robustness of the framework. We\nvalidate our framework on multiple medical tasks, demonstrating that our method\nevidently outperforms state-of-the-art approaches under both model homogeneity\nand heterogeneity settings.\n","authors":["Luyuan Xie","Tianyu Luan","Wenyuan Cai","Guochen Yan","Zhaoyu Chen","Nan Xi","Yuejian Fang","Qingni Shen","Zhonghai Wu","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.10412v3.pdf","comment":"One of the authors, Wenyuan Cai, currently requests not to make the\n  paper public. Before we officially release the paper, we request to withdraw\n  the submission"},{"id":"http://arxiv.org/abs/2503.04715v4","updated":"2025-03-19T16:28:25Z","published":"2025-03-06T18:58:29Z","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining","summary":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.09% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/\n","authors":["Houyi Li","Wenzhen Zheng","Jingcheng Hu","Qiufeng Wang","Hanshan Zhang","Zili Wang","Shijie Xuyang","Yuantao Fan","Shuigeng Zhou","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.04715v4.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2402.09122v4","updated":"2025-03-19T16:25:15Z","published":"2024-02-14T12:18:23Z","title":"Weighted-Sum of Gaussian Process Latent Variable Models","summary":"  This work develops a Bayesian non-parametric approach to signal separation\nwhere the signals may vary according to latent variables. Our key contribution\nis to augment Gaussian Process Latent Variable Models (GPLVMs) for the case\nwhere each data point comprises the weighted sum of a known number of pure\ncomponent signals, observed across several input locations. Our framework\nallows arbitrary non-linear variations in the signals while being able to\nincorporate useful priors for the linear weights, such as summing-to-one. Our\ncontributions are particularly relevant to spectroscopy, where changing\nconditions may cause the underlying pure component signals to vary from sample\nto sample. To demonstrate the applicability to both spectroscopy and other\ndomains, we consider several applications: a near-infrared spectroscopy dataset\nwith varying temperatures, a simulated dataset for identifying flow\nconfiguration through a pipe, and a dataset for determining the type of rock\nfrom its reflectance.\n","authors":["James Odgers","Ruby Sedgwick","Chrysoula Kappatou","Ruth Misener","Sarah Filippi"],"pdf_url":"https://arxiv.org/pdf/2402.09122v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15371v1","updated":"2025-03-19T16:10:17Z","published":"2025-03-19T16:10:17Z","title":"Geometrically-Aware One-Shot Skill Transfer of Category-Level Objects","summary":"  Robotic manipulation of unfamiliar objects in new environments is challenging\nand requires extensive training or laborious pre-programming. We propose a new\nskill transfer framework, which enables a robot to transfer complex object\nmanipulation skills and constraints from a single human demonstration. Our\napproach addresses the challenge of skill acquisition and task execution by\nderiving geometric representations from demonstrations focusing on\nobject-centric interactions. By leveraging the Functional Maps (FM) framework,\nwe efficiently map interaction functions between objects and their\nenvironments, allowing the robot to replicate task operations across objects of\nsimilar topologies or categories, even when they have significantly different\nshapes. Additionally, our method incorporates a Task-Space Imitation Algorithm\n(TSIA) which generates smooth, geometrically-aware robot paths to ensure the\ntransferred skills adhere to the demonstrated task constraints. We validate the\neffectiveness and adaptability of our approach through extensive experiments,\ndemonstrating successful skill transfer and task execution in diverse\nreal-world environments without requiring additional training.\n","authors":["Cristiana de Farias","Luis Figueredo","Riddhiman Laha","Maxime Adjigble","Brahim Tamadazte","Rustam Stolkin","Sami Haddadin","Naresh Marturi"],"pdf_url":"https://arxiv.org/pdf/2503.15371v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.18750v2","updated":"2025-03-19T16:08:36Z","published":"2024-12-25T02:48:53Z","title":"The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization","summary":"  Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications.\n","authors":["Md Nakhla Rafi","Dong Jae Kim","Tse-Hsun Chen","Shaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.18750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15368v1","updated":"2025-03-19T16:06:50Z","published":"2025-03-19T16:06:50Z","title":"Online Imitation Learning for Manipulation via Decaying Relative\n  Correction through Teleoperation","summary":"  Teleoperated robotic manipulators enable the collection of demonstration\ndata, which can be used to train control policies through imitation learning.\nHowever, such methods can require significant amounts of training data to\ndevelop robust policies or adapt them to new and unseen tasks. While expert\nfeedback can significantly enhance policy performance, providing continuous\nfeedback can be cognitively demanding and time-consuming for experts. To\naddress this challenge, we propose to use a cable-driven teleoperation system\nwhich can provide spatial corrections with 6 degree of freedom to the\ntrajectories generated by a policy model. Specifically, we propose a correction\nmethod termed Decaying Relative Correction (DRC) which is based upon the\nspatial offset vector provided by the expert and exists temporarily, and which\nreduces the intervention steps required by an expert. Our results demonstrate\nthat DRC reduces the required expert intervention rate by 30\\% compared to a\nstandard absolute corrective method. Furthermore, we show that integrating DRC\nwithin an online imitation learning framework rapidly increases the success\nrate of manipulation tasks such as raspberry harvesting and cloth wiping.\n","authors":["Cheng Pan","Hung Hon Cheng","Josie Hughes"],"pdf_url":"https://arxiv.org/pdf/2503.15368v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15367v1","updated":"2025-03-19T16:05:52Z","published":"2025-03-19T16:05:52Z","title":"FedBEns: One-Shot Federated Learning based on Bayesian Ensemble","summary":"  One-Shot Federated Learning (FL) is a recent paradigm that enables multiple\nclients to cooperatively learn a global model in a single round of\ncommunication with a central server. In this paper, we analyze the One-Shot FL\nproblem through the lens of Bayesian inference and propose FedBEns, an\nalgorithm that leverages the inherent multimodality of local loss functions to\nfind better global models. Our algorithm leverages a mixture of Laplace\napproximations for the clients' local posteriors, which the server then\naggregates to infer the global model. We conduct extensive experiments on\nvarious datasets, demonstrating that the proposed method outperforms competing\nbaselines that typically rely on unimodal approximations of the local losses.\n","authors":["Jacopo Talpini","Marco Savi","Giovanni Neglia"],"pdf_url":"https://arxiv.org/pdf/2503.15367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15355v1","updated":"2025-03-19T15:57:03Z","published":"2025-03-19T15:57:03Z","title":"Robustness of Nonlinear Representation Learning","summary":"  We study the problem of unsupervised representation learning in slightly\nmisspecified settings, and thus formalize the study of robustness of nonlinear\nrepresentation learning. We focus on the case where the mixing is close to a\nlocal isometry in a suitable distance and show based on existing rigidity\nresults that the mixing can be identified up to linear transformations and\nsmall errors. In a second step, we investigate Independent Component Analysis\n(ICA) with observations generated according to $x=f(s)=As+h(s)$ where $A$ is an\ninvertible mixing matrix and $h$ a small perturbation. We show that we can\napproximately recover the matrix $A$ and the independent components. Together,\nthese two results show approximate identifiability of nonlinear ICA with almost\nisometric mixing functions. Those results are a step towards identifiability\nresults for unsupervised representation learning for real-world data that do\nnot follow restrictive model classes.\n","authors":["Simon Buchholz","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2503.15355v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2503.15352v1","updated":"2025-03-19T15:51:17Z","published":"2025-03-19T15:51:17Z","title":"Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for\n  Cross-modal Transfer","summary":"  Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning.\n","authors":["Abhi Kamboj","Minh N. Do"],"pdf_url":"https://arxiv.org/pdf/2503.15352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17211v2","updated":"2025-03-19T15:35:38Z","published":"2023-09-29T13:09:40Z","title":"Data-Free Dynamic Compression of CNNs for Tractable Efficiency","summary":"  To reduce the computational cost of convolutional neural networks (CNNs) on\nresource-constrained devices, structured pruning approaches have shown promise\nin lowering floating-point operations (FLOPs) without substantial drops in\naccuracy. However, most methods require fine-tuning or specific training\nprocedures to achieve a reasonable trade-off between retained accuracy and\nreduction in FLOPs, adding computational overhead and requiring training data\nto be available. To this end, we propose HASTE (Hashing for Tractable\nEfficiency), a data-free, plug-and-play convolution module that instantly\nreduces a network's test-time inference cost without training or fine-tuning.\nOur approach utilizes locality-sensitive hashing (LSH) to detect redundancies\nin the channel dimension of latent feature maps, compressing similar channels\nto reduce input and filter depth simultaneously, resulting in cheaper\nconvolutions. We demonstrate our approach on the popular vision benchmarks\nCIFAR-10 and ImageNet, where we achieve a 46.72% reduction in FLOPs with only a\n1.25% loss in accuracy by swapping the convolution modules in a ResNet34 on\nCIFAR-10 for our HASTE module.\n","authors":["Lukas Meiner","Jens Mehnert","Alexandru Paul Condurache"],"pdf_url":"https://arxiv.org/pdf/2309.17211v2.pdf","comment":"Accepted at VISAPP 2025"},{"id":"http://arxiv.org/abs/2503.15294v1","updated":"2025-03-19T15:17:13Z","published":"2025-03-19T15:17:13Z","title":"Borsuk-Ulam and Replicable Learning of Large-Margin Halfspaces","summary":"  Recent advances in learning theory have established that, for total concepts,\nlist replicability, global stability, differentially private (DP) learnability,\nand shared-randomness replicability coincide precisely with the finiteness of\nthe Littlestone dimension. Does the same hold for partial concept classes?\n  We answer this question by studying the large-margin half-spaces class, which\nhas bounded Littlestone dimension and is purely DP-learnable and\nshared-randomness replicable even in high dimensions.\n  We prove that the list replicability number of $\\gamma$-margin half-spaces\nsatisfies \\[ \\frac{d}{2} + 1 \\le \\mathrm{LR}(H_{\\gamma}^d) \\le d, \\] which\nincreases with the dimension $d$. This reveals a surprising separation for\npartial concepts: list replicability and global stability do not follow from\nbounded Littlestone dimension, DP-learnability, or shared-randomness\nreplicability.\n  By applying our main theorem, we also answer the following open problems.\n  - We prove that any disambiguation of an infinite-dimensional large-margin\nhalf-space to a total concept class has unbounded Littlestone dimension,\nanswering an open question of Alon et al. (FOCS '21). - We prove that the\nmaximum list-replicability number of any *finite* set of points and homogeneous\nhalf-spaces in $d$-dimensional Euclidean space is $d$, resolving a problem of\nChase et al. (FOCS '23). - We prove that any disambiguation of the Gap Hamming\nDistance problem in the large gap regime has unbounded public-coin randomized\ncommunication complexity. This answers an open problem of Fang et al. (STOC\n'25).\n  We prove the lower bound via a topological argument involving the local\nBorsuk-Ulam theorem of Chase et al. (STOC '24). For the upper bound, we design\na learning rule that relies on certain triangulations of the cross-polytope and\nrecent results on the generalization properties of SVM.\n","authors":["Ari Blondal","Hamed Hatami","Pooya Hatami","Chavdar Lalov","Sivan Tretiak"],"pdf_url":"https://arxiv.org/pdf/2503.15294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09304v3","updated":"2025-03-19T15:14:41Z","published":"2022-07-19T14:38:52Z","title":"A sharp uniform-in-time error estimate for Stochastic Gradient Langevin\n  Dynamics","summary":"  We establish a sharp uniform-in-time error estimate for the Stochastic\nGradient Langevin Dynamics (SGLD), which is a widely-used sampling algorithm.\nUnder mild assumptions, we obtain a uniform-in-time $O(\\eta^2)$ bound for the\nKL-divergence between the SGLD iteration and the Langevin diffusion, where\n$\\eta$ is the step size (or learning rate). Our analysis is also valid for\nvarying step sizes. Consequently, we are able to derive an $O(\\eta)$ bound for\nthe distance between the invariant measures of the SGLD iteration and the\nLangevin diffusion, in terms of Wasserstein or total variation distances. Our\nresult can be viewed as a significant improvement compared with existing\nanalysis for SGLD in related literature.\n","authors":["Lei Li","Yuliang Wang"],"pdf_url":"https://arxiv.org/pdf/2207.09304v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14396v2","updated":"2025-03-19T15:09:41Z","published":"2025-03-18T16:36:59Z","title":"Technical Report: Aggregation on Learnable Manifolds for Asynchronous\n  Federated Optimization","summary":"  In Federated Learning (FL), a primary challenge to the server-side\naggregation of client models is device heterogeneity in both loss landscape\ngeometry and computational capacity. This issue can be particularly pronounced\nin clinical contexts where variations in data distribution (aggravated by class\nimbalance), infrastructure requirements, and sample sizes are common. We\npropose AsyncManifold, a novel asynchronous FL framework to address these\nissues by taking advantage of underlying solution space geometry at each of the\nlocal training, delay-correction, and aggregation stages. Our proposal is\naccompanied by a convergence proof in a general form and, motivated through\nexploratory studies of local behaviour, a proof-of-concept algorithm which\nperforms aggregation along non-linear mode connections and hence avoids\nbarriers to convergence that techniques based on linear interpolation will\nencounter.\n","authors":["Archie Licudi"],"pdf_url":"https://arxiv.org/pdf/2503.14396v2.pdf","comment":"22 pages, 3 figures. Preliminary technical project report [v2] fixed\n  abstract wording"},{"id":"http://arxiv.org/abs/2503.15288v1","updated":"2025-03-19T15:09:29Z","published":"2025-03-19T15:09:29Z","title":"Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning\n  for efficient CME tracking","summary":"  Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH.\n","authors":["Justin Le Louëdec","Maike Bauer","Tanja Amerstorfer","Jackie A. Davies"],"pdf_url":"https://arxiv.org/pdf/2503.15288v1.pdf","comment":"24 pages, 11 figures, 1 tables, submitted to AGU Space Weather on\n  14th Marc 2025"},{"id":"http://arxiv.org/abs/2409.09990v2","updated":"2025-03-19T15:04:38Z","published":"2024-09-16T04:46:22Z","title":"SHIRE: Enhancing Sample Efficiency using Human Intuition in\n  REinforcement Learning","summary":"  The ability of neural networks to perform robotic perception and control\ntasks such as depth and optical flow estimation, simultaneous localization and\nmapping (SLAM), and automatic control has led to their widespread adoption in\nrecent years. Deep Reinforcement Learning has been used extensively in these\nsettings, as it does not have the unsustainable training costs associated with\nsupervised learning. However, DeepRL suffers from poor sample efficiency, i.e.,\nit requires a large number of environmental interactions to converge to an\nacceptable solution. Modern RL algorithms such as Deep Q Learning and Soft\nActor-Critic attempt to remedy this shortcoming but can not provide the\nexplainability required in applications such as autonomous robotics. Humans\nintuitively understand the long-time-horizon sequential tasks common in\nrobotics. Properly using such intuition can make RL policies more explainable\nwhile enhancing their sample efficiency. In this work, we propose SHIRE, a\nnovel framework for encoding human intuition using Probabilistic Graphical\nModels (PGMs) and using it in the Deep RL training pipeline to enhance sample\nefficiency. Our framework achieves 25-78% sample efficiency gains across the\nenvironments we evaluate at negligible overhead cost. Additionally, by teaching\nRL agents the encoded elementary behavior, SHIRE enhances policy\nexplainability. A real-world demonstration further highlights the efficacy of\npolicies trained using our framework.\n","authors":["Amogh Joshi","Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2409.09990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13393v2","updated":"2025-03-19T14:49:31Z","published":"2024-12-18T00:10:00Z","title":"MaskHand: Generative Masked Modeling for Robust Hand Mesh Reconstruction\n  in the Wild","summary":"  Reconstructing a 3D hand mesh from a single RGB image is challenging due to\ncomplex articulations, self-occlusions, and depth ambiguities. Traditional\ndiscriminative methods, which learn a deterministic mapping from a 2D image to\na single 3D mesh, often struggle with the inherent ambiguities in 2D-to-3D\nmapping. To address this challenge, we propose MaskHand, a novel generative\nmasked model for hand mesh recovery that synthesizes plausible 3D hand meshes\nby learning and sampling from the probabilistic distribution of the ambiguous\n2D-to-3D mapping process. MaskHand consists of two key components: (1) a\nVQ-MANO, which encodes 3D hand articulations as discrete pose tokens in a\nlatent space, and (2) a Context-Guided Masked Transformer that randomly masks\nout pose tokens and learns their joint distribution, conditioned on corrupted\ntoken sequence, image context, and 2D pose cues. This learned distribution\nfacilitates confidence-guided sampling during inference, producing mesh\nreconstructions with low uncertainty and high precision. Extensive evaluations\non benchmark and real-world datasets demonstrate that MaskHand achieves\nstate-of-the-art accuracy, robustness, and realism in 3D hand mesh\nreconstruction. Project website:\nhttps://m-usamasaleem.github.io/publication/MaskHand/MaskHand.html.\n","authors":["Muhammad Usama Saleem","Ekkasit Pinyoanuntapong","Mayur Jagdishbhai Patel","Hongfei Xue","Ahmed Helmy","Srijan Das","Pu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.13393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15979v2","updated":"2025-03-19T14:43:51Z","published":"2024-04-24T16:54:39Z","title":"On the Fourier analysis in the SO(3) space : EquiLoPO Network","summary":"  Analyzing volumetric data with rotational invariance or equivariance is an\nactive topic in current research. Existing deep-learning approaches utilize\neither group convolutional networks limited to discrete rotations or steerable\nconvolutional networks with constrained filter structures. This work proposes a\nnovel equivariant neural network architecture that achieves analytical\nEquivariance to Local Pattern Orientation on the continuous SO(3) group while\nallowing unconstrained trainable filters - EquiLoPO Network. Our key\ninnovations are a group convolutional operation leveraging irreducible\nrepresentations as the Fourier basis and a local activation function in the\nSO(3) space that provides a well-defined mapping from input to output\nfunctions, preserving equivariance. By integrating these operations into a\nResNet-style architecture, we propose a model that overcomes the limitations of\nprior methods. A comprehensive evaluation on diverse 3D medical imaging\ndatasets from MedMNIST3D demonstrates the effectiveness of our approach, which\nconsistently outperforms state of the art. This work suggests the benefits of\ntrue rotational equivariance on SO(3) and flexible unconstrained filters\nenabled by the local activation function, providing a flexible framework for\nequivariant deep learning on volumetric data with potential applications across\ndomains. Our code is publicly available at\nhttps://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPO/-/tree/main/EquiLoPO.\n","authors":["Dmitrii Zhemchuzhnikov","Sergei Grudinin"],"pdf_url":"https://arxiv.org/pdf/2404.15979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15267v1","updated":"2025-03-19T14:43:12Z","published":"2025-03-19T14:43:12Z","title":"Learning to quantify graph nodes","summary":"  Network Quantification is the problem of estimating the class proportions in\nunlabeled subsets of graph nodes. When prior probability shift is at play, this\ntask cannot be effectively addressed by first classifying the nodes and then\ncounting the class predictions. In addition, unlike non-relational\nquantification on i.i.d. datapoints, Network Quantification demands enhanced\nflexibility to capture a broad range of connectivity patterns, resilience to\nthe challenge of heterophily, and efficiency to scale to larger networks. To\nmeet these stringent requirements we introduce XNQ, a novel method that\nsynergizes the flexibility and efficiency of the unsupervised node embeddings\ncomputed by randomized recursive Graph Neural Networks, with an\nExpectation-Maximization algorithm that provides a robust quantification-aware\nadjustment to the output probabilities of a calibrated node classifier. We\nvalidate the design choices underpinning our method through comprehensive\nablation experiments. In an extensive evaluation, we find that our approach\nconsistently and significantly improves on the best Network Quantification\nmethods to date, thereby setting the new state of the art for this challenging\ntask. Simultaneously, it provides a training speed-up of up to 10x-100x over\nother graph learning based methods.\n","authors":["Alessio Micheli","Alejandro Moreo","Marco Podda","Fabrizio Sebastiani","William Simoni","Domenico Tortorella"],"pdf_url":"https://arxiv.org/pdf/2503.15267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09722v2","updated":"2025-03-19T14:37:53Z","published":"2025-03-12T18:11:37Z","title":"The Pitfalls of Imitation Learning when Actions are Continuous","summary":"  We study the problem of imitating an expert demonstrator in a discrete-time,\ncontinuous state-and-action control system. We show that, even if the dynamics\nare stable (i.e. contracting exponentially quickly), and the expert is smooth\nand deterministic, any smooth, deterministic imitator policy necessarily\nsuffers error on execution that is exponentially larger, as a function of\nproblem horizon, than the error under the distribution of expert training data.\nOur negative result applies to both behavior cloning and offline-RL algorithms,\nunless they produce highly \"improper\" imitator policies--those which are\nnon-smooth, non-Markovian, or which exhibit highly state-dependent\nstochasticity--or unless the expert trajectory distribution is sufficiently\n\"spread.\" We provide experimental evidence of the benefits of these more\ncomplex policy parameterizations, explicating the benefits of today's popular\npolicy parameterizations in robot learning (e.g. action-chunking and Diffusion\nPolicies). We also establish a host of complementary negative and positive\nresults for imitation in control systems.\n","authors":["Max Simchowitz","Daniel Pfrommer","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2503.09722v2.pdf","comment":"98 pages, 2 figures, updated introduction"},{"id":"http://arxiv.org/abs/2410.18113v3","updated":"2025-03-19T14:36:56Z","published":"2024-10-09T04:47:22Z","title":"Scalable Co-Clustering for Large-Scale Data through Dynamic Partitioning\n  and Hierarchical Merging","summary":"  Co-clustering simultaneously clusters rows and columns, revealing more\nfine-grained groups. However, existing co-clustering methods suffer from poor\nscalability and cannot handle large-scale data. This paper presents a novel and\nscalable co-clustering method designed to uncover intricate patterns in\nhigh-dimensional, large-scale datasets. Specifically, we first propose a large\nmatrix partitioning algorithm that partitions a large matrix into smaller\nsubmatrices, enabling parallel co-clustering. This method employs a\nprobabilistic model to optimize the configuration of submatrices, balancing the\ncomputational efficiency and depth of analysis. Additionally, we propose a\nhierarchical co-cluster merging algorithm that efficiently identifies and\nmerges co-clusters from these submatrices, enhancing the robustness and\nreliability of the process. Extensive evaluations validate the effectiveness\nand efficiency of our method. Experimental results demonstrate a significant\nreduction in computation time, with an approximate 83% decrease for dense\nmatrices and up to 30% for sparse matrices.\n","authors":["Zihan Wu","Zhaoke Huang","Hong Yan"],"pdf_url":"https://arxiv.org/pdf/2410.18113v3.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.15259v1","updated":"2025-03-19T14:31:09Z","published":"2025-03-19T14:31:09Z","title":"Fast MLE and MAPE-Based Device Activity Detection for Grant-Free Access\n  via PSCA and PSCA-Net","summary":"  Fast and accurate device activity detection is the critical challenge in\ngrant-free access for supporting massive machine-type communications (mMTC) and\nultra-reliable low-latency communications (URLLC) in 5G and beyond. The\nstate-of-the-art methods have unsatisfactory error rates or computation times.\nTo address these outstanding issues, we propose new maximum likelihood\nestimation (MLE) and maximum a posterior estimation (MAPE) based device\nactivity detection methods for known and unknown pathloss that achieve superior\nerror rate and computation time tradeoffs using optimization and deep learning\ntechniques. Specifically, we investigate four non-convex optimization problems\nfor MLE and MAPE in the two pathloss cases, with one MAPE problem being\nformulated for the first time. For each non-convex problem, we develop an\ninnovative parallel iterative algorithm using the parallel successive convex\napproximation (PSCA) method. Each PSCA-based algorithm allows parallel\ncomputations, uses up to the objective function's second-order information,\nconverges to the problem's stationary points, and has a low per-iteration\ncomputational complexity compared to the state-of-the-art algorithms. Then, for\neach PSCA-based iterative algorithm, we present a deep unrolling neural network\nimplementation, called PSCA-Net, to further reduce the computation time. Each\nPSCA-Net elegantly marries the underlying PSCA-based algorithm's parallel\ncomputation mechanism with the parallelizable neural network architecture and\neffectively optimizes its step sizes based on vast data samples to speed up the\nconvergence. Numerical results demonstrate that the proposed methods can\nsignificantly reduce the error rate and computation time compared to the\nstate-of-the-art methods, revealing their significant values for grant-free\naccess.\n","authors":["Bowen Tan","Ying Cui"],"pdf_url":"https://arxiv.org/pdf/2503.15259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14286v2","updated":"2025-03-19T14:25:30Z","published":"2025-03-18T14:23:37Z","title":"Tapered Off-Policy REINFORCE: Stable and efficient reinforcement\n  learning for LLMs","summary":"  We propose a new algorithm for fine-tuning large language models using\nreinforcement learning. Tapered Off-Policy REINFORCE (TOPR) uses an asymmetric,\ntapered variant of importance sampling to speed up learning while maintaining\nstable learning dynamics, even without the use of KL regularization. TOPR can\nbe applied in a fully offline fashion, allows the handling of positive and\nnegative examples in a unified framework, and benefits from the\nimplementational simplicity that is typical of Monte Carlo algorithms. We\ndemonstrate the effectiveness of our approach with a series of experiments on\nthe GSM8K and MATH reasoning benchmarks, finding performance gains for training\nboth a model for solution generation and as a generative verifier. We show that\nproperly leveraging positive and negative examples alike in the off-policy\nregime simultaneously increases test-time accuracy and training data\nefficiency, all the while avoiding the ``wasted inference'' that comes with\ndiscarding negative examples. We find that this advantage persists over\nmultiple iterations of training and can be amplified by dataset curation\ntechniques, enabling us to match 70B-parameter model performance with 8B\nlanguage models. As a corollary to this work, we find that REINFORCE's baseline\nparameter plays an important and unexpected role in defining dataset\ncomposition in the presence of negative examples, and is consequently critical\nin driving off-policy performance.\n","authors":["Nicolas Le Roux","Marc G. Bellemare","Jonathan Lebensold","Arnaud Bergeron","Joshua Greaves","Alex Fréchette","Carolyne Pelletier","Eric Thibodeau-Laufer","Sándor Toth","Sam Work"],"pdf_url":"https://arxiv.org/pdf/2503.14286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15250v1","updated":"2025-03-19T14:24:20Z","published":"2025-03-19T14:24:20Z","title":"ImputeGAP: A Comprehensive Library for Time Series Imputation","summary":"  With the prevalence of sensor failures, imputation--the process of estimating\nmissing values--has emerged as the cornerstone of time series data preparation.\nWhile numerous imputation algorithms have been developed to address these data\ngaps, existing libraries provide limited support. Furthermore, they often lack\nthe ability to simulate realistic patterns of time series missing data and fail\nto account for the impact of imputation on subsequent downstream analysis.\n  This paper introduces ImputeGAP, a comprehensive library for time series\nimputation that supports a diverse range of imputation methods and modular\nmissing data simulation catering to datasets with varying characteristics. The\nlibrary includes extensive customization options, such as automated\nhyperparameter tuning, benchmarking, explainability, downstream evaluation, and\ncompatibility with popular time series frameworks.\n","authors":["Quentin Nater","Mourad Khayati","Jacques Pasquier"],"pdf_url":"https://arxiv.org/pdf/2503.15250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00267v2","updated":"2025-03-19T14:23:52Z","published":"2023-12-01T00:54:02Z","title":"Sample Efficient Reinforcement Learning from Human Feedback via Active\n  Exploration","summary":"  Preference-based feedback is important for many applications in machine\nlearning where evaluation of a reward function is not feasible. Notable recent\nexamples arise in preference alignment for large language models, including in\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO). For many applications of preference alignment, the cost of\nacquiring human feedback can be substantial. In this work, we take advantage of\nthe fact that one can often choose contexts at which to obtain human feedback\nto most efficiently identify a good policy, and formalize the setting as an\nactive contextual dueling bandit problem. We propose an active exploration\nalgorithm to efficiently select the data and provide theoretical proof that it\nhas a polynomial worst-case regret bound. We extend the setting and methodology\nfor practical use in preference alignment of large language models. We provide\ntwo extensions, an online and an offline approach. Our method outperforms the\nbaselines with limited samples of human preferences on several language models\nand four real-world datasets including two new datasets that we contribute to\nthe literature.\n","authors":["Viraj Mehta","Syrine Belakaria","Vikramjeet Das","Ojash Neopane","Yijia Dai","Ilija Bogunovic","Barbara Engelhardt","Stefano Ermon","Jeff Schneider","Willie Neiswanger"],"pdf_url":"https://arxiv.org/pdf/2312.00267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15225v1","updated":"2025-03-19T14:03:20Z","published":"2025-03-19T14:03:20Z","title":"A Personalized Data-Driven Generative Model of Human Motion","summary":"  The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities - such as rehabilitation therapy, sports, and\nmanufacturing - is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. However, existing\nmodels only provide simplified descriptions of human motor behavior. In this\nwork, we propose a fully data-driven approach, based on Long Short-Term Memory\nneural networks, to generate original motion that captures the unique\ncharacteristics of specific individuals. We validate the architecture using\nreal data of scalar oscillatory motion. Extensive analyses show that our model\neffectively replicates the velocity distribution and amplitude envelopes of the\nindividual it was trained on, remaining different from other individuals, and\noutperforming state-of-the-art models in terms of similarity to human data.\n","authors":["Angelo Di Porzio","Marco Coraggio"],"pdf_url":"https://arxiv.org/pdf/2503.15225v1.pdf","comment":"6 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.15221v1","updated":"2025-03-19T14:01:16Z","published":"2025-03-19T14:01:16Z","title":"A Foundation Model for Patient Behavior Monitoring and Suicide Detection","summary":"  Foundation models (FMs) have achieved remarkable success across various\ndomains, yet their adoption in healthcare remains limited. While significant\nadvances have been made in medical imaging, genetic biomarkers, and time series\nfrom electronic health records, the potential of FMs for patient behavior\nmonitoring through wearable devices remains underexplored. These datasets are\ninherently heterogeneous, multisource, and often exhibit high rates of missing\ndata, posing unique challenges. This paper introduces a novel FM based on a\nmodified vector quantized variational autoencoder (VQ-VAE), specifically\ndesigned to process real-world data from wearable devices. We demonstrate that\nour pretrained FM, trained on a broad cohort of psychiatric patients, performs\ndownstream tasks via its latent representation without fine-tuning on a\nheld-out cohort of suicidal patients. To illustrate this, we develop a\nprobabilistic change-point detection algorithm for suicide detection and\ndemonstrate the FM's effectiveness in predicting emotional states. Our results\nshow that the discrete latent structure of the VQ-VAE outperforms a\nstate-of-the-art Informer architecture in unsupervised suicide detection, while\nmatching its performance in supervised emotion prediction when the latent\ndimensionality is increased, though at the cost of reduced unsupervised\naccuracy. This trade-off highlights the need for future FMs to integrate hybrid\ndiscrete-continuous structures for balanced performance across tasks.\n","authors":["Rodrigo Oliver","Josué Pérez-Sabater","Leire Paz-Arbaizar","Alejandro Lancho","Antonio Artés","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2503.15221v1.pdf","comment":"10 pages (31 with appendices), 6 figures (13 with appendices);\n  submitted to UAI 2025"},{"id":"http://arxiv.org/abs/2210.00025v4","updated":"2025-03-19T13:51:24Z","published":"2022-09-30T18:03:31Z","title":"Artificial Replay: A Meta-Algorithm for Harnessing Historical Data in\n  Bandits","summary":"  Most real-world deployments of bandit algorithms exist somewhere in between\nthe offline and online set-up, where some historical data is available upfront\nand additional data is collected dynamically online. How best to incorporate\nhistorical data to \"warm start\" bandit algorithms is an open question: naively\ninitializing reward estimates using all historical samples can suffer from\nspurious data and imbalanced data coverage, leading to data inefficiency\n(amount of historical data used) - particularly for continuous action spaces.\nTo address these challenges, we propose ArtificialReplay, a meta-algorithm for\nincorporating historical data into any arbitrary base bandit algorithm. We show\nthat ArtificialReplay uses only a fraction of the historical data compared to a\nfull warm-start approach, while still achieving identical regret for base\nalgorithms that satisfy independence of irrelevant data (IIData), a novel and\nbroadly applicable property that we introduce. We complement these theoretical\nresults with experiments on K-armed bandits and continuous combinatorial\nbandits, on which we model green security domains using real poaching data. Our\nresults show the practical benefits of ArtificialReplay for improving data\nefficiency, including for base algorithms that do not satisfy IIData.\n","authors":["Siddhartha Banerjee","Sean R. Sinclair","Milind Tambe","Lily Xu","Christina Lee Yu"],"pdf_url":"https://arxiv.org/pdf/2210.00025v4.pdf","comment":"55 pages (30 pages main paper), 9 figures"},{"id":"http://arxiv.org/abs/2503.15210v1","updated":"2025-03-19T13:50:19Z","published":"2025-03-19T13:50:19Z","title":"Online federated learning framework for classification","summary":"  In this paper, we develop a novel online federated learning framework for\nclassification, designed to handle streaming data from multiple clients while\nensuring data privacy and computational efficiency. Our method leverages the\ngeneralized distance-weighted discriminant technique, making it robust to both\nhomogeneous and heterogeneous data distributions across clients. In particular,\nwe develop a new optimization algorithm based on the Majorization-Minimization\nprinciple, integrated with a renewable estimation procedure, enabling efficient\nmodel updates without full retraining. We provide a theoretical guarantee for\nthe convergence of our estimator, proving its consistency and asymptotic\nnormality under standard regularity conditions. In addition, we establish that\nour method achieves Bayesian risk consistency, ensuring its reliability for\nclassification tasks in federated environments. We further incorporate\ndifferential privacy mechanisms to enhance data security, protecting client\ninformation while maintaining model performance. Extensive numerical\nexperiments on both simulated and real-world datasets demonstrate that our\napproach delivers high classification accuracy, significant computational\nefficiency gains, and substantial savings in data storage requirements compared\nto existing methods.\n","authors":["Wenxing Guo","Jinhan Xie","Jianya Lu","Bei jiang","Hongsheng Dai","Linglong Kong"],"pdf_url":"https://arxiv.org/pdf/2503.15210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15209v1","updated":"2025-03-19T13:49:50Z","published":"2025-03-19T13:49:50Z","title":"Kolmogorov-Arnold Network for Transistor Compact Modeling","summary":"  Neural network (NN)-based transistor compact modeling has recently emerged as\na transformative solution for accelerating device modeling and SPICE circuit\nsimulations. However, conventional NN architectures, despite their widespread\nadoption in state-of-the-art methods, primarily function as black-box problem\nsolvers. This lack of interpretability significantly limits their capacity to\nextract and convey meaningful insights into learned data patterns, posing a\nmajor barrier to their broader adoption in critical modeling tasks. This work\nintroduces, for the first time, Kolmogorov-Arnold network (KAN) for the\ntransistor - a groundbreaking NN architecture that seamlessly integrates\ninterpretability with high precision in physics-based function modeling. We\nsystematically evaluate the performance of KAN and Fourier KAN for FinFET\ncompact modeling, benchmarking them against the golden industry-standard\ncompact model and the widely used MLP architecture. Our results reveal that KAN\nand FKAN consistently achieve superior prediction accuracy for critical figures\nof merit, including gate current, drain charge, and source charge. Furthermore,\nwe demonstrate and improve the unique ability of KAN to derive symbolic\nformulas from learned data patterns - a capability that not only enhances\ninterpretability but also facilitates in-depth transistor analysis and\noptimization. This work highlights the transformative potential of KAN in\nbridging the gap between interpretability and precision in NN-driven transistor\ncompact modeling. By providing a robust and transparent approach to transistor\nmodeling, KAN represents a pivotal advancement for the semiconductor industry\nas it navigates the challenges of advanced technology scaling.\n","authors":["Rodion Novkin","Hussam Amrouch"],"pdf_url":"https://arxiv.org/pdf/2503.15209v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.03258v2","updated":"2025-03-19T13:48:52Z","published":"2024-12-04T11:57:36Z","title":"Learning on One Mode: Addressing Multi-modality in Offline Reinforcement\n  Learning","summary":"  Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without interacting with the environment. A common challenge is\nhandling multi-modal action distributions, where multiple behaviours are\nrepresented in the data. Existing methods often assume unimodal behaviour\npolicies, leading to suboptimal performance when this assumption is violated.\nWe propose weighted imitation Learning on One Mode (LOM), a novel approach that\nfocuses on learning from a single, promising mode of the behaviour policy. By\nusing a Gaussian mixture model to identify modes and selecting the best mode\nbased on expected returns, LOM avoids the pitfalls of averaging over\nconflicting actions. Theoretically, we show that LOM improves performance while\nmaintaining simplicity in policy learning. Empirically, LOM outperforms\nexisting methods on standard D4RL benchmarks and demonstrates its effectiveness\nin complex, multi-modal scenarios.\n","authors":["Mianchu Wang","Yue Jin","Giovanni Montana"],"pdf_url":"https://arxiv.org/pdf/2412.03258v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2501.18423v2","updated":"2025-03-19T13:45:31Z","published":"2025-01-30T15:25:30Z","title":"DeepExtractor: Time-domain reconstruction of signals and glitches in\n  gravitational wave data with deep learning","summary":"  Gravitational wave (GW) interferometers, detect faint signals from distant\nastrophysical events, such as binary black hole mergers. However, their high\nsensitivity also makes them susceptible to background noise, which can obscure\nthese signals. This noise often includes transient artifacts called \"glitches\"\nthat can mimic astrophysical signals or mask their characteristics. Fast and\naccurate reconstruction of both signals and glitches is crucial for reliable\nscientific inference. In this study, we present DeepExtractor, a deep learning\nframework designed to reconstruct signals and glitches with power exceeding\ninterferometer noise, regardless of their source. We design DeepExtractor to\nmodel the inherent noise distribution of GW interferometers, following\nconventional assumptions that the noise is Gaussian and stationary over short\ntime scales. It operates by predicting and subtracting the noise component of\nthe data, retaining only the clean reconstruction. Our approach achieves\nsuperior generalization capabilities for arbitrary signals and glitches\ncompared to methods that directly map inputs to the clean training waveforms.\nWe validate DeepExtractor's effectiveness through three experiments: (1)\nreconstructing simulated glitches injected into simulated detector noise, (2)\ncomparing performance with the state-of-the-art BayesWave algorithm, and (3)\nanalyzing real data from the Gravity Spy dataset to demonstrate effective\nglitch subtraction from LIGO strain data. DeepExtractor achieves a median\nmismatch of only 0.9% for simulated glitches, outperforming several deep\nlearning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch\nrecovery, offering a dramatic computational speedup by reconstructing one\nglitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's\nprocessing time of approx. one hour per glitch.\n","authors":["Tom Dooney","Harsh Narola","Stefano Bromuri","R. Lyana Curier","Chris Van Den Broeck","Sarah Caudill","Daniel Stanley Tan"],"pdf_url":"https://arxiv.org/pdf/2501.18423v2.pdf","comment":"22 pages, 16 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.15200v1","updated":"2025-03-19T13:38:29Z","published":"2025-03-19T13:38:29Z","title":"Partially Observable Reinforcement Learning with Memory Traces","summary":"  Partially observable environments present a considerable computational\nchallenge in reinforcement learning due to the need to consider long histories.\nLearning with a finite window of observations quickly becomes intractable as\nthe window length grows. In this work, we introduce memory traces. Inspired by\neligibility traces, these are compact representations of the history of\nobservations in the form of exponential moving averages. We prove sample\ncomplexity bounds for the problem of offline on-policy evaluation that quantify\nthe value errors achieved with memory traces for the class of Lipschitz\ncontinuous value estimates. We establish a close connection to the window\napproach, and demonstrate that, in certain environments, learning with memory\ntraces is significantly more sample efficient. Finally, we underline the\neffectiveness of memory traces empirically in online reinforcement learning\nexperiments for both value prediction and control.\n","authors":["Onno Eberhard","Michael Muehlebach","Claire Vernade"],"pdf_url":"https://arxiv.org/pdf/2503.15200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16819v3","updated":"2025-03-19T13:30:44Z","published":"2024-11-25T16:41:45Z","title":"Pathways on the Image Manifold: Image Editing via Video Generation","summary":"  Recent advances in image editing, driven by image diffusion models, have\nshown remarkable progress. However, significant challenges remain, as these\nmodels often struggle to follow complex edit instructions accurately and\nfrequently compromise fidelity by altering key elements of the original image.\nSimultaneously, video generation has made remarkable strides, with models that\neffectively function as consistent and continuous world simulators. In this\npaper, we propose merging these two fields by utilizing image-to-video models\nfor image editing. We reformulate image editing as a temporal process, using\npretrained video models to create smooth transitions from the original image to\nthe desired edit. This approach traverses the image manifold continuously,\nensuring consistent edits while preserving the original image's key aspects.\nOur approach achieves state-of-the-art results on text-based image editing,\ndemonstrating significant improvements in both edit accuracy and image\npreservation. Visit our project page:\nhttps://rotsteinnoam.github.io/Frame2Frame.\n","authors":["Noam Rotstein","Gal Yona","Daniel Silver","Roy Velich","David Bensaïd","Ron Kimmel"],"pdf_url":"https://arxiv.org/pdf/2411.16819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15190v1","updated":"2025-03-19T13:21:18Z","published":"2025-03-19T13:21:18Z","title":"Learning Topology Actions for Power Grid Control: A Graph-Based\n  Soft-Label Imitation Learning Approach","summary":"  The rising proportion of renewable energy in the electricity mix introduces\nsignificant operational challenges for power grid operators. Effective power\ngrid management demands adaptive decision-making strategies capable of handling\ndynamic conditions. With the increase in complexity, more and more Deep\nLearning (DL) approaches have been proposed to find suitable grid topologies\nfor congestion management. In this work, we contribute to this research by\nintroducing a novel Imitation Learning (IL) approach that leverages soft labels\nderived from simulated topological action outcomes, thereby capturing multiple\nviable actions per state. Unlike traditional IL methods that rely on hard\nlabels to enforce a single optimal action, our method constructs soft labels\nover actions, by leveraging effective actions that prove suitable in resolving\ngrid congestion. To further enhance decision-making, we integrate Graph Neural\nNetworks (GNNs) to encode the structural properties of power grids, ensuring\nthat the topology-aware representations contribute to better agent performance.\nOur approach significantly outperforms state-of-the-art baselines, all of which\nuse only topological actions, as well as feedforward and GNN-based\narchitectures with hard labels. Most notably, it achieves a 17% better\nperformance compared to the greedy expert agent from which the imitation\ntargets were derived.\n","authors":["Mohamed Hassouna","Clara Holzhüter","Malte Lehna","Matthijs de Jong","Jan Viebahn","Bernhard Sick","Christoph Scholz"],"pdf_url":"https://arxiv.org/pdf/2503.15190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04053v4","updated":"2025-03-19T13:15:56Z","published":"2022-07-08T10:37:22Z","title":"On the Need and Applicability of Causality for Fairness: A Unified\n  Framework for AI Auditing and Legal Analysis","summary":"  As Artificial Intelligence (AI) increasingly influences decisions in critical\nsocietal sectors, understanding and establishing causality becomes essential\nfor evaluating the fairness of automated systems. This article explores the\nsignificance of causal reasoning in addressing algorithmic discrimination,\nemphasizing both legal and societal perspectives. By reviewing landmark cases\nand regulatory frameworks, particularly within the European Union, we\nillustrate the challenges inherent in proving causal claims when confronted\nwith opaque AI decision-making processes. The discussion outlines practical\nobstacles and methodological limitations in applying causal inference to\nreal-world fairness scenarios, proposing actionable solutions to enhance\ntransparency, accountability, and fairness in algorithm-driven decisions.\n","authors":["Ruta Binkyte","Ljupcho Grozdanovski","Sami Zhioua"],"pdf_url":"https://arxiv.org/pdf/2207.04053v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19383v3","updated":"2025-03-19T13:04:36Z","published":"2024-05-29T08:48:52Z","title":"Network Analytics for Anti-Money Laundering -- A Systematic Literature\n  Review and Experimental Evaluation","summary":"  Money laundering presents a pervasive challenge, burdening society by\nfinancing illegal activities. The use of network information is increasingly\nbeing explored to more effectively combat money laundering, given it involves\nconnected parties. This led to a surge in research on network analytics (NA)\nfor anti-money laundering (AML). The literature on NA for AML is, however,\nfragmented and a comprehensive overview of existing work is missing. This\nresults in limited understanding of the methods to apply and their comparative\ndetection power. Therefore, this paper presents an extensive and unique\nliterature review, based on 97 papers from Web of Science and Scopus, resulting\nin a taxonomy following a recently proposed fraud analytics framework. We\nconclude that most research relies on expert-based rules and manual features,\nwhile deep learning methods have been gaining traction. This paper also\npresents a comprehensive framework to evaluate and compare the performance of\nprominent NA methods in a standardized setup. We apply it on two publicly\navailable data sets, comparing manual feature engineering, random walk-based,\nand deep learning methods. We conclude that (1) network analytics increases the\npredictive power, but caution is needed when applying GNNs based on the class\nimbalance and network topology, and that (2) care should be taken with\nopen-source data as this can give overly optimistic results. The open-source\nimplementation facilitates researchers and practitioners to extend upon the\nresults and experiment on proprietary data, promoting a standardized approach\nfor the analysis and evaluation of network analytics for AML.\n","authors":["Bruno Deprez","Toon Vanderschueren","Bart Baesens","Tim Verdonck","Wouter Verbeke"],"pdf_url":"https://arxiv.org/pdf/2405.19383v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15177v1","updated":"2025-03-19T13:02:23Z","published":"2025-03-19T13:02:23Z","title":"Food Delivery Time Prediction in Indian Cities Using Machine Learning\n  Models","summary":"  Accurate prediction of food delivery times significantly impacts customer\nsatisfaction, operational efficiency, and profitability in food delivery\nservices. However, existing studies primarily utilize static historical data\nand often overlook dynamic, real-time contextual factors crucial for precise\nprediction, particularly in densely populated Indian cities. This research\naddresses these gaps by integrating real-time contextual variables such as\ntraffic density, weather conditions, local events, and geospatial data\n(restaurant and delivery location coordinates) into predictive models. We\nsystematically compare various machine learning algorithms, including Linear\nRegression, Decision Trees, Bagging, Random Forest, XGBoost, and LightGBM, on a\ncomprehensive food delivery dataset specific to Indian urban contexts. Rigorous\ndata preprocessing and feature selection significantly enhanced model\nperformance. Experimental results demonstrate that the LightGBM model achieves\nsuperior predictive accuracy, with an R2 score of 0.76 and Mean Squared Error\n(MSE) of 20.59, outperforming traditional baseline approaches. Our study thus\nprovides actionable insights for improving logistics strategies in complex\nurban environments. The complete methodology and code are publicly available\nfor reproducibility and further research.\n","authors":["Ananya Garg","Mohmmad Ayaan","Swara Parekh","Vikranth Udandarao"],"pdf_url":"https://arxiv.org/pdf/2503.15177v1.pdf","comment":"for code implementation, check\n  https://github.com/Vikranth3140/Food-Delivery-Time-Prediction"},{"id":"http://arxiv.org/abs/2410.19464v3","updated":"2025-03-19T12:59:43Z","published":"2024-10-25T10:48:41Z","title":"LOCAL: Learning with Orientation Matrix to Infer Causal Structure from\n  Time Series Data","summary":"  Discovering the underlying Directed Acyclic Graph (DAG) from time series\nobservational data is highly challenging due to the dynamic nature and complex\nnonlinear interactions between variables. Existing methods typically search for\nthe optimal DAG by optimizing an objective function but face scalability\nchallenges, as their computational demands grow exponentially with the\ndimensional expansion of variables. To this end, we propose LOCAL, a highly\nefficient, easy-to-implement, and constraint-free method for recovering dynamic\ncausal structures. LOCAL is the first attempt to formulate a quasi-maximum\nlikelihood-based score function for learning the dynamic DAG equivalent to the\nground truth. Building on this, we introduce two adaptive modules that enhance\nthe algebraic characterization of acyclicity: Asymptotic Causal Mask Learning\n(ACML) and Dynamic Graph Parameter Learning (DGPL). ACML constructs causal\nmasks using learnable priority vectors and the Gumbel-Sigmoid function,\nensuring DAG formation while optimizing computational efficiency. DGPL\ntransforms causal learning into decomposed matrix products, capturing dynamic\ncausal structure in high-dimensional data and improving interpretability.\nExtensive experiments on synthetic and real-world datasets demonstrate that\nLOCAL significantly outperforms existing methods and highlight LOCAL's\npotential as a robust and efficient method for dynamic causal discovery.\n","authors":["Jiajun Zhang","Boyang Qiang","Xiaoyu Guo","Weiwei Xing","Yue Cheng","Witold Pedrycz"],"pdf_url":"https://arxiv.org/pdf/2410.19464v3.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.15172v1","updated":"2025-03-19T12:56:23Z","published":"2025-03-19T12:56:23Z","title":"Multi-Agent Actor-Critic with Harmonic Annealing Pruning for Dynamic\n  Spectrum Access Systems","summary":"  Multi-Agent Deep Reinforcement Learning (MADRL) has emerged as a powerful\ntool for optimizing decentralized decision-making systems in complex settings,\nsuch as Dynamic Spectrum Access (DSA). However, deploying deep learning models\non resource-constrained edge devices remains challenging due to their high\ncomputational cost. To address this challenge, in this paper, we present a\nnovel sparse recurrent MARL framework integrating gradual neural network\npruning into the independent actor global critic paradigm. Additionally, we\nintroduce a harmonic annealing sparsity scheduler, which achieves comparable,\nand in certain cases superior, performance to standard linear and polynomial\npruning schedulers at large sparsities. Our experimental investigation\ndemonstrates that the proposed DSA framework can discover superior policies,\nunder diverse training conditions, outperforming conventional DSA, MADRL\nbaselines, and state-of-the-art pruning techniques.\n","authors":["George Stamatelis","Angelos-Nikolaos Kanatas","George C. Alexandropoulos"],"pdf_url":"https://arxiv.org/pdf/2503.15172v1.pdf","comment":"5 pages, 3 figures, 1 table, submited to an IEEE conference"},{"id":"http://arxiv.org/abs/2503.15168v1","updated":"2025-03-19T12:50:40Z","published":"2025-03-19T12:50:40Z","title":"World Models in Artificial Intelligence: Sensing, Learning, and\n  Reasoning Like a Child","summary":"  World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities.\n","authors":["Javier Del Ser","Jesus L. Lobo","Heimo Müller","Andreas Holzinger"],"pdf_url":"https://arxiv.org/pdf/2503.15168v1.pdf","comment":"11 pages, 1 figure"},{"id":"http://arxiv.org/abs/2503.15166v1","updated":"2025-03-19T12:47:37Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.15163v1","updated":"2025-03-19T12:42:37Z","published":"2025-03-19T12:42:37Z","title":"Global Group Fairness in Federated Learning via Function Tracking","summary":"  We investigate group fairness regularizers in federated learning, aiming to\ntrain a globally fair model in a distributed setting. Ensuring global fairness\nin distributed training presents unique challenges, as fairness regularizers\ntypically involve probability metrics between distributions across all clients\nand are not naturally separable by client. To address this, we introduce a\nfunction-tracking scheme for the global fairness regularizer based on a Maximum\nMean Discrepancy (MMD), which incurs a small communication overhead. This\nscheme seamlessly integrates into most federated learning algorithms while\npreserving rigorous convergence guarantees, as demonstrated in the context of\nFedAvg. Additionally, when enforcing differential privacy, the kernel-based MMD\nregularization enables straightforward analysis through a change of kernel,\nleveraging an intuitive interpretation of kernel convolution. Numerical\nexperiments confirm our theoretical insights.\n","authors":["Yves Rychener","Daniel Kuhn","Yifan Hu"],"pdf_url":"https://arxiv.org/pdf/2503.15163v1.pdf","comment":"The paper is accepted to AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.11851v2","updated":"2025-03-19T12:18:48Z","published":"2025-03-14T20:28:20Z","title":"DCAT: Dual Cross-Attention Fusion for Disease Classification in\n  Radiological Images with Uncertainty Estimation","summary":"  Accurate and reliable image classification is crucial in radiology, where\ndiagnostic decisions significantly impact patient outcomes. Conventional deep\nlearning models tend to produce overconfident predictions despite underlying\nuncertainties, potentially leading to misdiagnoses. Attention mechanisms have\nemerged as powerful tools in deep learning, enabling models to focus on\nrelevant parts of the input data. Combined with feature fusion, they can be\neffective in addressing uncertainty challenges. Cross-attention has become\nincreasingly important in medical image analysis for capturing dependencies\nacross features and modalities. This paper proposes a novel dual\ncross-attention fusion model for medical image analysis by addressing key\nchallenges in feature integration and interpretability. Our approach introduces\na bidirectional cross-attention mechanism with refined channel and spatial\nattention that dynamically fuses feature maps from EfficientNetB4 and ResNet34\nleveraging multi-network contextual dependencies. The refined features through\nchannel and spatial attention highlights discriminative patterns crucial for\naccurate classification. The proposed model achieved AUC of 99.75%, 100%,\n99.93% and 98.69% and AUPR of 99.81%, 100%, 99.97%, and 96.36% on Covid-19,\nTuberculosis, Pneumonia Chest X-ray images and Retinal OCT images respectively.\nThe entropy values and several high uncertain samples give an interpretable\nvisualization from the model enhancing transparency. By combining multi-scale\nfeature extraction, bidirectional attention and uncertainty estimation, our\nproposed model strongly impacts medical image analysis.\n","authors":["Jutika Borah","Hidam Kumarjit Singh"],"pdf_url":"https://arxiv.org/pdf/2503.11851v2.pdf","comment":"18 pages, 8 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.15150v1","updated":"2025-03-19T12:16:54Z","published":"2025-03-19T12:16:54Z","title":"Preference Construction: A Bayesian Interactive Preference Elicitation\n  Framework Based on Monte Carlo Tree Search","summary":"  We present a novel preference learning framework to capture participant\npreferences efficiently within limited interaction rounds. It involves three\nmain contributions. First, we develop a variational Bayesian approach to infer\nthe participant's preference model by estimating posterior distributions and\nmanaging uncertainty from limited information. Second, we propose an adaptive\nquestioning policy that maximizes cumulative uncertainty reduction, formulating\nquestioning as a finite Markov decision process and using Monte Carlo Tree\nSearch to prioritize promising question trajectories. By considering long-term\neffects and leveraging the efficiency of the Bayesian approach, the policy\navoids shortsightedness. Third, we apply the framework to Multiple Criteria\nDecision Aiding, with pairwise comparison as the preference information and an\nadditive value function as the preference model. We integrate the\nreparameterization trick to address high-variance issues, enhancing robustness\nand efficiency. Computational studies on real-world and synthetic datasets\ndemonstrate the framework's practical usability, outperforming baselines in\ncapturing preferences and achieving superior uncertainty reduction within\nlimited interactions.\n","authors":["Yan Wang","Jiapeng Liu","Milosz Kadziński","Xiuwu Liao"],"pdf_url":"https://arxiv.org/pdf/2503.15150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15149v1","updated":"2025-03-19T12:15:35Z","published":"2025-03-19T12:15:35Z","title":"Machine learning surrogate models of many-body dispersion interactions\n  in polymer melts","summary":"  Accurate prediction of many-body dispersion (MBD) interactions is essential\nfor understanding the van der Waals forces that govern the behavior of many\ncomplex molecular systems. However, the high computational cost of MBD\ncalculations limits their direct application in large-scale simulations. In\nthis work, we introduce a machine learning surrogate model specifically\ndesigned to predict MBD forces in polymer melts, a system that demands accurate\nMBD description and offers structural advantages for machine learning\napproaches. Our model is based on a trimmed SchNet architecture that\nselectively retains the most relevant atomic connections and incorporates\ntrainable radial basis functions for geometric encoding. We validate our\nsurrogate model on datasets from polyethylene, polypropylene, and polyvinyl\nchloride melts, demonstrating high predictive accuracy and robust\ngeneralization across diverse polymer systems. In addition, the model captures\nkey physical features, such as the characteristic decay behavior of MBD\ninteractions, providing valuable insights for optimizing cutoff strategies.\nCharacterized by high computational efficiency, our surrogate model enables\npractical incorporation of MBD effects into large-scale molecular simulations.\n","authors":["Zhaoxiang Shen","Raúl I. Sosa","Jakub Lengiewicz","Alexandre Tkatchenko","Stéphane P. A. Bordas"],"pdf_url":"https://arxiv.org/pdf/2503.15149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01588v2","updated":"2025-03-19T11:59:53Z","published":"2024-10-02T14:20:30Z","title":"DynFrs: An Efficient Framework for Machine Unlearning in Random Forest","summary":"  Random Forests are widely recognized for establishing efficacy in\nclassification and regression tasks, standing out in various domains such as\nmedical diagnosis, finance, and personalized recommendations. These domains,\nhowever, are inherently sensitive to privacy concerns, as personal and\nconfidential data are involved. With increasing demand for the right to be\nforgotten, particularly under regulations such as GDPR and CCPA, the ability to\nperform machine unlearning has become crucial for Random Forests. However,\ninsufficient attention was paid to this topic, and existing approaches face\ndifficulties in being applied to real-world scenarios. Addressing this gap, we\npropose the DynFrs framework designed to enable efficient machine unlearning in\nRandom Forests while preserving predictive accuracy. Dynfrs leverages\nsubsampling method Occ(q) and a lazy tag strategy Lzy, and is still adaptable\nto any Random Forest variant. In essence, Occ(q) ensures that each sample in\nthe training set occurs only in a proportion of trees so that the impact of\ndeleting samples is limited, and Lzy delays the reconstruction of a tree node\nuntil necessary, thereby avoiding unnecessary modifications on tree structures.\nIn experiments, applying Dynfrs on Extremely Randomized Trees yields\nsubstantial improvements, achieving orders of magnitude faster unlearning\nperformance and better predictive accuracy than existing machine unlearning\nmethods for Random Forests.\n","authors":["Shurong Wang","Zhuoyang Shen","Xinbao Qiao","Tongning Zhang","Meng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01588v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2408.07246v4","updated":"2025-03-19T11:46:58Z","published":"2024-08-14T01:16:40Z","title":"ChemVLM: Exploring the Power of Multimodal Large Language Models in\n  Chemistry Area","summary":"  Large Language Models (LLMs) have achieved remarkable success and have been\napplied across various scientific fields, including chemistry. However, many\nchemical tasks require the processing of visual information, which cannot be\nsuccessfully handled by existing chemical LLMs. This brings a growing need for\nmodels capable of integrating multimodal information in the chemical domain. In\nthis paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal\nlarge language model specifically designed for chemical applications. ChemVLM\nis trained on a carefully curated bilingual multimodal dataset that enhances\nits ability to understand both textual and visual chemical information,\nincluding molecular structures, reactions, and chemistry examination questions.\nWe develop three datasets for comprehensive evaluation, tailored to Chemical\nOptical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and\nMultimodal Molecule Understanding tasks. We benchmark ChemVLM against a range\nof open-source and proprietary multimodal large language models on various\ntasks. Experimental results demonstrate that ChemVLM achieves competitive\nperformance across all evaluated tasks. Our model can be found at\nhttps://huggingface.co/AI4Chem/ChemVLM-26B.\n","authors":["Junxian Li","Di Zhang","Xunzhi Wang","Zeying Hao","Jingdi Lei","Qian Tan","Cai Zhou","Wei Liu","Yaotian Yang","Xinrui Xiong","Weiyun Wang","Zhe Chen","Wenhai Wang","Wei Li","Shufei Zhang","Mao Su","Wanli Ouyang","Yuqiang Li","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.07246v4.pdf","comment":"11 pages, updated version"},{"id":"http://arxiv.org/abs/2412.06014v2","updated":"2025-03-19T11:26:14Z","published":"2024-12-08T18:16:13Z","title":"Post-hoc Probabilistic Vision-Language Models","summary":"  Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable\nsuccess in classification, retrieval, and generative tasks. For this, VLMs\ndeterministically map images and text descriptions to a joint latent space in\nwhich their similarity is assessed using the cosine similarity. However, a\ndeterministic mapping of inputs fails to capture uncertainties over concepts\narising from domain shifts when used in downstream tasks. In this work, we\npropose post-hoc uncertainty estimation in VLMs that does not require\nadditional training. Our method leverages a Bayesian posterior approximation\nover the last layers in VLMs and analytically quantifies uncertainties over\ncosine similarities. We demonstrate its effectiveness for uncertainty\nquantification and support set selection in active learning. Compared to\nbaselines, we obtain improved and well-calibrated predictive uncertainties,\ninterpretable uncertainty estimates, and sample-efficient active learning. Our\nresults show promise for safety-critical applications of large-scale models.\n","authors":["Anton Baumann","Rui Li","Marcus Klasson","Santeri Mentu","Shyamgopal Karthik","Zeynep Akata","Arno Solin","Martin Trapp"],"pdf_url":"https://arxiv.org/pdf/2412.06014v2.pdf","comment":"Project page: https://aaltoml.github.io/BayesVLM/"},{"id":"http://arxiv.org/abs/2411.12873v4","updated":"2025-03-19T11:23:33Z","published":"2024-11-19T21:36:04Z","title":"Tensor-Based Foundations of Ordinary Least Squares and Neural Network\n  Regression Models","summary":"  This article introduces a novel approach to the mathematical development of\nOrdinary Least Squares and Neural Network regression models, diverging from\ntraditional methods in current Machine Learning literature. By leveraging\nTensor Analysis and fundamental matrix computations, the theoretical\nfoundations of both models are meticulously detailed and extended to their\ncomplete algorithmic forms. The study culminates in the presentation of three\nalgorithms, including a streamlined version of the Backpropagation Algorithm\nfor Neural Networks, illustrating the benefits of this new mathematical\napproach.\n","authors":["Roberto Dias Algarte"],"pdf_url":"https://arxiv.org/pdf/2411.12873v4.pdf","comment":"16 pages, 3 algorithms"},{"id":"http://arxiv.org/abs/2503.15114v1","updated":"2025-03-19T11:14:16Z","published":"2025-03-19T11:14:16Z","title":"DeCaFlow: A Deconfounding Causal Generative Model","summary":"  Causal generative models (CGMs) have recently emerged as capable approaches\nto simulate the causal mechanisms generating our observations, enabling causal\ninference. Unfortunately, existing approaches either are overly restrictive,\nassuming the absence of hidden confounders, or lack generality, being tailored\nto a particular query and graph. In this work, we introduce DeCaFlow, a CGM\nthat accounts for hidden confounders in a single amortized training process\nusing only observational data and the causal graph. Importantly, DeCaFlow can\nprovably identify all causal queries with a valid adjustment set or\nsufficiently informative proxy variables. Remarkably, for the first time to our\nknowledge, we show that a confounded counterfactual query is identifiable, and\nthus solvable by DeCaFlow, as long as its interventional counterpart is as\nwell. Our empirical results on diverse settings (including the Ecoli70 dataset,\nwith 3 independent hidden confounders, tens of observed variables and hundreds\nof causal queries) show that DeCaFlow outperforms existing approaches, while\ndemonstrating its out-of-the-box flexibility.\n","authors":["Alejandro Almodóvar","Adrián Javaloy","Juan Parras","Santiago Zazo","Isabel Valera"],"pdf_url":"https://arxiv.org/pdf/2503.15114v1.pdf","comment":"32 pages, 22 figures. Under submission"},{"id":"http://arxiv.org/abs/2503.15111v1","updated":"2025-03-19T11:10:28Z","published":"2025-03-19T11:10:28Z","title":"FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking","summary":"  In Federated Learning (FL), weighted aggregation of local models is conducted\nto generate a new global model, and the aggregation weights are typically\nnormalized to 1. A recent study identifies the global weight shrinking effect\nin FL, indicating an enhancement in the global model's generalization when the\nsum of weights (i.e., the shrinking factor) is smaller than 1, where how to\nlearn the shrinking factor becomes crucial. However, principled approaches to\nthis solution have not been carefully studied from the adequate consideration\nof privacy concerns and layer-wise distinctions. To this end, we propose a\nnovel model aggregation strategy, Federated Learning with Adaptive Layer-wise\nWeight Shrinking (FedLWS), which adaptively designs the shrinking factor in a\nlayer-wise manner and avoids optimizing the shrinking factors on a proxy\ndataset. We initially explored the factors affecting the shrinking factor\nduring the training process. Then we calculate the layer-wise shrinking factors\nby considering the distinctions among each layer of the global model. FedLWS\ncan be easily incorporated with various existing methods due to its\nflexibility. Extensive experiments under diverse scenarios demonstrate the\nsuperiority of our method over several state-of-the-art approaches, providing a\npromising tool for enhancing the global model in FL.\n","authors":["Changlong Shi","Jinmeng Li","He Zhao","Dan dan Guo","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2503.15111v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2503.15108v1","updated":"2025-03-19T11:05:42Z","published":"2025-03-19T11:05:42Z","title":"VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making","summary":"  While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.\n","authors":["Mohamed Salim Aissi","Clemence Grislain","Mohamed Chetouani","Olivier Sigaud","Laure Soulier","Nicolas Thome"],"pdf_url":"https://arxiv.org/pdf/2503.15108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15107v1","updated":"2025-03-19T11:04:53Z","published":"2025-03-19T11:04:53Z","title":"Interpretability of Graph Neural Networks to Assert Effects of Global\n  Change Drivers on Ecological Networks","summary":"  Pollinators play a crucial role for plant reproduction, either in natural\necosystem or in human-modified landscape. Global change drivers,including\nclimate change or land use modifications, can alter the plant-pollinator\ninteractions. To assert the potential influence of global change drivers on\npollination, large-scale interactions, climate and land use data are required.\nWhile recent machine learning methods, such as graph neural networks (GNNs),\nallow the analysis of such datasets, interpreting their results can be\nchallenging. We explore existing methods for interpreting GNNs in order to\nhighlight the effects of various environmental covariates on pollination\nnetwork connectivity. A large simulation study is performed to confirm whether\nthese methods can detect the interactive effect between a covariate and a genus\nof plant on connectivity, and whether the application of debiasing techniques\ninfluences the estimation of these effects. An application on the Spipoll\ndataset, with and without accounting for sampling effects, highlights the\npotential impact of land use on network connectivity and shows that accounting\nfor sampling effects partially alters the estimation of these effects.\n","authors":["Emre Anakok","Pierre Barbillon","Colin Fontaine","Elisa Thebault"],"pdf_url":"https://arxiv.org/pdf/2503.15107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15105v1","updated":"2025-03-19T11:04:36Z","published":"2025-03-19T11:04:36Z","title":"Control, Optimal Transport and Neural Differential Equations in\n  Supervised Learning","summary":"  From the perspective of control theory, neural differential equations (neural\nODEs) have become an important tool for supervised learning. In the fundamental\nwork of Ruiz-Balet and Zuazua (SIAM REVIEW 2023), the authors pose an open\nproblem regarding the connection between control theory, optimal transport\ntheory, and neural differential equations. More precisely, they inquire how one\ncan quantify the closeness of the optimal flows in neural transport equations\nto the true dynamic optimal transport. In this work, we propose a construction\nof neural differential equations that converge to the true dynamic optimal\ntransport in the limit, providing a significant step in solving the formerly\nmentioned open problem.\n","authors":["Minh-Nhat Phung","Minh-Binh Tran"],"pdf_url":"https://arxiv.org/pdf/2503.15105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10610v2","updated":"2025-03-19T11:03:26Z","published":"2024-08-20T07:42:42Z","title":"On the Approximability of Stationary Processes using the ARMA Model","summary":"  Within the theoretical literature on stationary random variables, pure Moving\nAverage models and pure Autoregressive models have a rich body of work, but the\ncorresponding literature on Autoregressive Moving Average (ARMA) models is very\nsparse. We attempt to fill certain gaps in this sparse line of work. Central to\nour observations is the spectral lemma connecting supnorm based function\napproximation on the unit circle to random variable approximation. This method\nallows us to provide quantitative approximation bounds in contrast with the\nqualitative boundedness and stability guarantees associated with unit root\ntests. Using the spectral lemma we first identify a class of stationary\nprocesses where approximation guarantees are feasible. This turns a known\nheuristic argument motivating ARMA models based on rational approximations into\na rigorous result. Second, we identify an idealized stationary random process\nfor which we conjecture that a good ARMA approximation is not possible. Third,\nwe calculate exact approximation bounds for an example process, and a\nconstructive proof that, for a given order, Pad\\'e approximations do not always\ncorrespond to the best ARMA approximation. Unlike prior literature, our\napproach uses the generating function of the random process rather than the\nspectral measure, and further our results focus on approximation error of the\nrandom variable rather than the prediction error as in some classical infimum\nresults by Szego, Kolmogorov, and Wiener.\n","authors":["Anand Ganesh","Babhrubahan Bose","Anand Rajagopalan"],"pdf_url":"https://arxiv.org/pdf/2408.10610v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2410.15176v2","updated":"2025-03-19T11:00:48Z","published":"2024-10-19T18:35:52Z","title":"Adaptive Pruning with Module Robustness Sensitivity: Balancing\n  Compression and Robustness","summary":"  Neural network pruning has traditionally focused on weight-based criteria to\nachieve model compression, frequently overlooking the crucial balance between\nadversarial robustness and accuracy. Existing approaches often fail to preserve\nrobustness in pruned networks, leaving them more susceptible to adversarial\nattacks. This paper introduces Module Robustness Sensitivity (MRS), a novel\nmetric that quantifies layer-wise sensitivity to adversarial perturbations and\ndynamically informs pruning decisions. Leveraging MRS, we propose Module Robust\nPruning and Fine-Tuning (MRPF), an adaptive pruning algorithm compatible with\nany adversarial training method, offering both flexibility and scalability.\nExtensive experiments on SVHN, CIFAR, and Tiny-ImageNet across diverse\narchitectures, including ResNet, VGG, and MobileViT, demonstrate that MRPF\nsignificantly enhances adversarial robustness while maintaining competitive\naccuracy and computational efficiency. Furthermore, MRPF consistently\noutperforms state-of-the-art structured pruning methods in balancing\nrobustness, accuracy, and compression. This work establishes a practical and\ngeneralizable framework for robust pruning, addressing the long-standing\ntrade-off between model compression and robustness preservation.\n","authors":["Lincen Bai","Hedi Tabia","Raúl Santos-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2410.15176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15095v1","updated":"2025-03-19T10:48:26Z","published":"2025-03-19T10:48:26Z","title":"Diffusion-Based Forecasting for Uncertainty-Aware Model Predictive\n  Control","summary":"  We propose Diffusion-Informed Model Predictive Control (D-I MPC), a generic\nframework for uncertainty-aware prediction and decision-making in partially\nobservable stochastic systems by integrating diffusion-based time series\nforecasting models in Model Predictive Control algorithms. In our approach, a\ndiffusion-based time series forecasting model is used to probabilistically\nestimate the evolution of the system's stochastic components. These forecasts\nare then incorporated into MPC algorithms to estimate future trajectories and\noptimize action selection under the uncertainty of the future. We evaluate the\nframework on the task of energy arbitrage, where a Battery Energy Storage\nSystem participates in the day-ahead electricity market of the New York state.\nExperimental results indicate that our model-based approach with a\ndiffusion-based forecaster significantly outperforms both implementations with\nclassical forecasting methods and model-free reinforcement learning baselines.\n","authors":["Stelios Zarifis","Ioannis Kordonis","Petros Maragos"],"pdf_url":"https://arxiv.org/pdf/2503.15095v1.pdf","comment":"5 pages, 3 figures, 3 tables. This version is submitted to the 33rd\n  European Signal Processing Conference (EUSIPCO 2025), to be held in Isola\n  delle Femmine - Palermo - Italy, on September 8-12, 2025"},{"id":"http://arxiv.org/abs/2503.15089v1","updated":"2025-03-19T10:40:07Z","published":"2025-03-19T10:40:07Z","title":"Continual Contrastive Learning on Tabular Data with Out of Distribution","summary":"  Out-of-distribution (OOD) prediction remains a significant challenge in\nmachine learning, particularly for tabular data where traditional methods often\nfail to generalize beyond their training distribution. This paper introduces\nTabular Continual Contrastive Learning (TCCL), a novel framework designed to\naddress OOD challenges in tabular data processing. TCCL integrates contrastive\nlearning principles with continual learning mechanisms, featuring a\nthree-component architecture: an Encoder for data transformation, a Decoder for\nrepresentation learning, and a Learner Head. We evaluate TCCL against 14\nbaseline models, including state-of-the-art deep learning approaches and\ngradient-boosted decision trees (GBDT), across eight diverse tabular datasets.\nOur experimental results demonstrate that TCCL consistently outperforms\nexisting methods in both classification and regression tasks on OOD data, with\nparticular strength in handling distribution shifts. These findings suggest\nthat TCCL represents a significant advancement in handling OOD scenarios for\ntabular data.\n","authors":["Achmad Ginanjar","Xue Li","Priyanka Singh","Wen Hua"],"pdf_url":"https://arxiv.org/pdf/2503.15089v1.pdf","comment":"accepeted on esann 2025"},{"id":"http://arxiv.org/abs/2503.14240v2","updated":"2025-03-19T10:33:40Z","published":"2025-03-18T13:22:52Z","title":"Persistent Homology-induced Graph Ensembles for Time Series Regressions","summary":"  The effectiveness of Spatio-temporal Graph Neural Networks (STGNNs) in\ntime-series applications is often limited by their dependence on fixed,\nhand-crafted input graph structures. Motivated by insights from the Topological\nData Analysis (TDA) paradigm, of which real-world data exhibits multi-scale\npatterns, we construct several graphs using Persistent Homology Filtration -- a\nmathematical framework describing the multiscale structural properties of data\npoints. Then, we use the constructed graphs as an input to create an ensemble\nof Graph Neural Networks. The ensemble aggregates the signals from the\nindividual learners via an attention-based routing mechanism, thus\nsystematically encoding the inherent multiscale structures of data. Four\ndifferent real-world experiments on seismic activity prediction and traffic\nforecasting (PEMS-BAY, METR-LA) demonstrate that our approach consistently\noutperforms single-graph baselines while providing interpretable insights.\n","authors":["Viet The Nguyen","Duy Anh Pham","An Thai Le","Jans Peter","Gunther Gust"],"pdf_url":"https://arxiv.org/pdf/2503.14240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06183v2","updated":"2025-03-19T10:10:55Z","published":"2025-03-08T11:59:12Z","title":"Lightweight Software Kernels and Hardware Extensions for Efficient\n  Sparse Deep Neural Networks on Microcontrollers","summary":"  The acceleration of pruned Deep Neural Networks (DNNs) on edge devices such\nas Microcontrollers (MCUs) is a challenging task, given the tight area- and\npower-constraints of these devices. In this work, we propose a three-fold\ncontribution to address this problem. First, we design a set of optimized\nsoftware kernels for N:M pruned layers, targeting ultra-low-power, multicore\nRISC-V MCUs, which are up to 2.1x and 3.4x faster than their dense counterparts\nat 1:8 and 1:16 sparsity, respectively. Then, we implement a lightweight\nInstruction-Set Architecture (ISA) extension to accelerate the indirect load\nand non-zero indices decompression operations required by our kernels,\nobtaining up to 1.9x extra speedup, at the cost of a 5% area overhead. Lastly,\nwe extend an open-source DNN compiler to utilize our sparse kernels for\ncomplete networks, showing speedups of 3.21x and 1.81x on a ResNet18 and a\nVision Transformer (ViT), with less than 1.5% accuracy drop compared to a dense\nbaseline.\n","authors":["Francesco Daghero","Daniele Jahier Pagliari","Francesco Conti","Luca Benini","Massimo Poncino","Alessio Burrello"],"pdf_url":"https://arxiv.org/pdf/2503.06183v2.pdf","comment":"Accepted at MLSys 2025"},{"id":"http://arxiv.org/abs/2410.17760v2","updated":"2025-03-19T09:34:15Z","published":"2024-10-23T10:56:05Z","title":"Topology meets Machine Learning: An Introduction using the Euler\n  Characteristic Transform","summary":"  This overview article makes the case for how topological concepts can enrich\nresearch in machine learning. Using the Euler Characteristic Transform (ECT), a\ngeometrical-topological invariant, as a running example, I present different\nuse cases that result in more efficient models for analyzing point clouds,\ngraphs, and meshes. Moreover, I outline a vision for how topological concepts\ncould be used in the future, comprising (1) the learning of functions on\ntopological spaces, (2) the building of hybrid models that imbue neural\nnetworks with knowledge about the topological information in data, and (3) the\nanalysis of qualitative properties of neural networks. With current research\nalready addressing some of these aspects, this article thus serves as an\nintroduction and invitation to this nascent area of research.\n","authors":["Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.17760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15036v1","updated":"2025-03-19T09:25:54Z","published":"2025-03-19T09:25:54Z","title":"Multivariate Gaussian Topic Modelling: A novel approach to discover\n  topics with greater semantic coherence","summary":"  An important aspect of text mining involves information retrieval in form of\ndiscovery of semantic themes (topics) from documents using topic modelling.\nWhile generative topic models like Latent Dirichlet Allocation (LDA) elegantly\nmodel topics as probability distributions and are useful in identifying latent\ntopics from large document corpora with minimal supervision, they suffer from\ndifficulty in topic interpretability and reduced performance in shorter texts.\nHere we propose a novel Multivariate Gaussian Topic modelling (MGD) approach.\nIn this approach topics are presented as Multivariate Gaussian Distributions\nand documents as Gaussian Mixture Models. Using EM algorithm, the various\nconstituent Multivariate Gaussian Distributions and their corresponding\nparameters are identified. Analysis of the parameters helps identify the\nkeywords having the highest variance and mean contributions to the topic, and\nfrom these key-words topic annotations are carried out. This approach is first\napplied on a synthetic dataset to demonstrate the interpretability benefits\nvis-\\`a-vis LDA. A real-world application of this topic model is demonstrated\nin analysis of risks and hazards at a petrochemical plant by applying the model\non safety incident reports to identify the major latent hazards plaguing the\nplant. This model achieves a higher mean topic coherence of 0.436 vis-\\`a-vis\n0.294 for LDA.\n","authors":["Satyajeet Sahoo","Jhareswar Maiti","Virendra Kumar Tewari"],"pdf_url":"https://arxiv.org/pdf/2503.15036v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2503.15016v1","updated":"2025-03-19T09:12:56Z","published":"2025-03-19T09:12:56Z","title":"Manifold Learning for Hyperspectral Images","summary":"  Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results.\n","authors":["Fethi Harkat","Tiphaine Deuberet","Guillaume Gey","Valérie Perrier","Kévin Polisano"],"pdf_url":"https://arxiv.org/pdf/2503.15016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.12793v2","updated":"2025-03-19T09:12:16Z","published":"2025-03-17T04:01:37Z","title":"Improving Generalization of Universal Adversarial Perturbation via\n  Dynamic Maximin Optimization","summary":"  Deep neural networks (DNNs) are susceptible to universal adversarial\nperturbations (UAPs). These perturbations are meticulously designed to fool the\ntarget model universally across all sample classes. Unlike instance-specific\nadversarial examples (AEs), generating UAPs is more complex because they must\nbe generalized across a wide range of data samples and models. Our research\nreveals that existing universal attack methods, which optimize UAPs using DNNs\nwith static model parameter snapshots, do not fully leverage the potential of\nDNNs to generate more effective UAPs. Rather than optimizing UAPs against\nstatic DNN models with a fixed training set, we suggest using dynamic\nmodel-data pairs to generate UAPs. In particular, we introduce a dynamic\nmaximin optimization strategy, aiming to optimize the UAP across a variety of\noptimal model-data pairs. We term this approach DM-UAP. DM-UAP utilizes an\niterative max-min-min optimization framework that refines the model-data pairs,\ncoupled with a curriculum UAP learning algorithm to examine the combined space\nof model parameters and data thoroughly. Comprehensive experiments on the\nImageNet dataset demonstrate that the proposed DM-UAP markedly enhances both\ncross-sample universality and cross-model transferability of UAPs. Using only\n500 samples for UAP generation, DM-UAP outperforms the state-of-the-art\napproach with an average increase in fooling ratio of 12.108%.\n","authors":["Yechao Zhang","Yingzhe Xu","Junyu Shi","Leo Yu Zhang","Shengshan Hu","Minghui Li","Yanjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.12793v2.pdf","comment":"Accepted in AAAI 2025"},{"id":"http://arxiv.org/abs/2503.15013v1","updated":"2025-03-19T09:10:43Z","published":"2025-03-19T09:10:43Z","title":"Ambient Noise Full Waveform Inversion with Neural Operators","summary":"  Numerical simulations of seismic wave propagation are crucial for\ninvestigating velocity structures and improving seismic hazard assessment.\nHowever, standard methods such as finite difference or finite element are\ncomputationally expensive. Recent studies have shown that a new class of\nmachine learning models, called neural operators, can solve the elastodynamic\nwave equation orders of magnitude faster than conventional methods. Full\nwaveform inversion is a prime beneficiary of the accelerated simulations.\nNeural operators, as end-to-end differentiable operators, combined with\nautomatic differentiation, provide an alternative approach to the adjoint-state\nmethod. Since neural operators do not involve the Born approximation, when used\nfor full waveform inversion they have the potential to include additional\nphases and alleviate cycle-skipping problems present in traditional\nadjoint-state formulations. In this study, we demonstrate the application of\nneural operators for full waveform inversion on a real seismic dataset, which\nconsists of several nodal transects collected across the San Gabriel, Chino,\nand San Bernardino basins in the Los Angeles metropolitan area.\n","authors":["Caifeng Zou","Zachary E. Ross","Robert W. Clayton","Fan-Chi Lin","Kamyar Azizzadenesheli"],"pdf_url":"https://arxiv.org/pdf/2503.15013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.01896v2","updated":"2025-03-19T09:07:12Z","published":"2024-03-04T09:55:43Z","title":"Robustness bounds on the successful adversarial examples in\n  probabilistic models: Implications from Gaussian processes","summary":"  Adversarial example (AE) is an attack method for machine learning, which is\ncrafted by adding imperceptible perturbation to the data inducing\nmisclassification. In the current paper, we investigated the upper bound of the\nprobability of successful AEs based on the Gaussian Process (GP)\nclassification, a probabilistic inference model. We proved a new upper bound of\nthe probability of a successful AE attack that depends on AE's perturbation\nnorm, the kernel function used in GP, and the distance of the closest pair with\ndifferent labels in the training dataset. Surprisingly, the upper bound is\ndetermined regardless of the distribution of the sample dataset. We showed that\nour theoretical result was confirmed through the experiment using ImageNet. In\naddition, we showed that changing the parameters of the kernel function induces\na change of the upper bound of the probability of successful AEs.\n","authors":["Hiroaki Maeshima","Akira Otsuka"],"pdf_url":"https://arxiv.org/pdf/2403.01896v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15008v1","updated":"2025-03-19T08:59:02Z","published":"2025-03-19T08:59:02Z","title":"A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary\n  Learning for Breast Cancer Detection","summary":"  Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis.\n","authors":["Aamir Mehmood","Yue Hu","Saddam Hussain Khan"],"pdf_url":"https://arxiv.org/pdf/2503.15008v1.pdf","comment":"12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986"},{"id":"http://arxiv.org/abs/2410.12677v2","updated":"2025-03-19T08:55:53Z","published":"2024-10-16T15:41:08Z","title":"Efficient Optimization Algorithms for Linear Adversarial Training","summary":"  Adversarial training can be used to learn models that are robust against\nperturbations. For linear models, it can be formulated as a convex optimization\nproblem. Compared to methods proposed in the context of deep learning,\nleveraging the optimization structure allows significantly faster convergence\nrates. Still, the use of generic convex solvers can be inefficient for\nlarge-scale problems. Here, we propose tailored optimization algorithms for the\nadversarial training of linear models, which render large-scale regression and\nclassification problems more tractable. For regression problems, we propose a\nfamily of solvers based on iterative ridge regression and, for classification,\na family of solvers based on projected gradient descent. The methods are based\non extended variable reformulations of the original problem. We illustrate\ntheir efficiency in numerical examples.\n","authors":["Antônio H. RIbeiro","Thomas B. Schön","Dave Zahariah","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2410.12677v2.pdf","comment":"Paper accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.15002v1","updated":"2025-03-19T08:52:23Z","published":"2025-03-19T08:52:23Z","title":"Scalable Trajectory-User Linking with Dual-Stream Representation\n  Networks","summary":"  Trajectory-user linking (TUL) aims to match anonymous trajectories to the\nmost likely users who generated them, offering benefits for a wide range of\nreal-world spatio-temporal applications. However, existing TUL methods are\nlimited by high model complexity and poor learning of the effective\nrepresentations of trajectories, rendering them ineffective in handling\nlarge-scale user trajectory data. In this work, we propose a novel\n$\\underline{Scal}$abl$\\underline{e}$ Trajectory-User Linking with dual-stream\nrepresentation networks for large-scale $\\underline{TUL}$ problem, named\nScaleTUL. Specifically, ScaleTUL generates two views using temporal and spatial\naugmentations to exploit supervised contrastive learning framework to\neffectively capture the irregularities of trajectories. In each view, a\ndual-stream trajectory encoder, consisting of a long-term encoder and a\nshort-term encoder, is designed to learn unified trajectory representations\nthat fuse different temporal-spatial dependencies. Then, a TUL layer is used to\nassociate the trajectories with the corresponding users in the representation\nspace using a two-stage training model. Experimental results on check-in\nmobility datasets from three real-world cities and the nationwide U.S.\ndemonstrate the superiority of ScaleTUL over state-of-the-art baselines for\nlarge-scale TUL tasks.\n","authors":["Hao Zhang","Wei Chen","Xingyu Zhao","Jianpeng Qi","Guiyuan Jiang","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2503.15002v1.pdf","comment":"The paper has been accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.19459v2","updated":"2025-03-19T08:43:16Z","published":"2025-02-26T10:25:32Z","title":"ArtGS: Building Interactable Replicas of Complex Articulated Objects via\n  Gaussian Splatting","summary":"  Building articulated objects is a key challenge in computer vision. Existing\nmethods often fail to effectively integrate information across different object\nstates, limiting the accuracy of part-mesh reconstruction and part dynamics\nmodeling, particularly for complex multi-part articulated objects. We introduce\nArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient\nrepresentation to address these issues. Our method incorporates canonical\nGaussians with coarse-to-fine initialization and updates for aligning\narticulated part information across different object states, and employs a\nskinning-inspired part dynamics modeling module to improve both part-mesh\nreconstruction and articulation learning. Extensive experiments on both\nsynthetic and real-world datasets, including a new benchmark for complex\nmulti-part objects, demonstrate that ArtGS achieves state-of-the-art\nperformance in joint parameter estimation and part mesh reconstruction. Our\napproach significantly improves reconstruction quality and efficiency,\nespecially for multi-part articulated objects. Additionally, we provide\ncomprehensive analyses of our design choices, validating the effectiveness of\neach component to highlight potential areas for future improvement. Our work is\nmade publicly available at: https://articulate-gs.github.io.\n","authors":["Yu Liu","Baoxiong Jia","Ruijie Lu","Junfeng Ni","Song-Chun Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2502.19459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01949v2","updated":"2025-03-19T08:34:29Z","published":"2024-10-02T18:51:38Z","title":"Discrete Copula Diffusion","summary":"  Discrete diffusion models have recently shown significant progress in\nmodeling complex data, such as natural languages and DNA sequences. However,\nunlike diffusion models for continuous data, which can generate high-quality\nsamples in just a few denoising steps, modern discrete diffusion models still\nrequire hundreds or even thousands of denoising steps to perform well. In this\npaper, we identify a fundamental limitation that prevents discrete diffusion\nmodels from achieving strong performance with fewer steps -- they fail to\ncapture dependencies between output variables at each denoising step. To\naddress this issue, we provide a formal explanation and introduce a general\napproach to supplement the missing dependency information by incorporating\nanother deep generative model, termed the copula model. Our method does not\nrequire fine-tuning either the diffusion model or the copula model, yet it\nenables high-quality sample generation with significantly fewer denoising\nsteps. When we apply this approach to autoregressive copula models, the\ncombined model outperforms both models individually in unconditional and\nconditional text generation. Specifically, the hybrid model achieves better\n(un)conditional text generation using 8 to 32 times fewer denoising steps than\nthe diffusion model alone. In addition to presenting an effective discrete\ndiffusion generation algorithm, this paper emphasizes the importance of\nmodeling inter-variable dependencies in discrete diffusion.\n","authors":["Anji Liu","Oliver Broadrick","Mathias Niepert","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2410.01949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07414v2","updated":"2025-03-19T08:34:26Z","published":"2024-11-11T22:36:50Z","title":"Comparing Targeting Strategies for Maximizing Social Welfare with\n  Limited Resources","summary":"  Machine learning is increasingly used to select which individuals receive\nlimited-resource interventions in domains such as human services, education,\ndevelopment, and more. However, it is often not apparent what the right\nquantity is for models to predict. Policymakers rarely have access to data from\na randomized controlled trial (RCT) that would enable accurate estimates of\nwhich individuals would benefit more from the intervention, while observational\ndata creates a substantial risk of bias in treatment effect estimates.\nPractitioners instead commonly use a technique termed ``risk-based targeting\"\nwhere the model is just used to predict each individual's status quo outcome\n(an easier, non-causal task). Those with higher predicted risk are offered\ntreatment. There is currently almost no empirical evidence to inform which\nchoices lead to the most effective machine learning-informed targeting\nstrategies in social domains. In this work, we use data from 5 real-world RCTs\nin a variety of domains to empirically assess such choices. We find that when\ntreatment effects can be estimated with high accuracy (which we simulate by\nallowing the model to partially observe outcomes in advance), treatment effect\nbased targeting substantially outperforms risk-based targeting, even when\ntreatment effect estimates are biased. Moreover, these results hold even when\nthe policymaker has strong normative preferences for assisting higher-risk\nindividuals. However, the features and data actually available in most RCTs we\nexamine do not suffice for accurate estimates of heterogeneous treatment\neffects. Our results suggest treatment effect targeting has significant\npotential benefits, but realizing these benefits requires improvements to data\ncollection and model training beyond what is currently common in practice.\n","authors":["Vibhhu Sharma","Bryan Wilder"],"pdf_url":"https://arxiv.org/pdf/2411.07414v2.pdf","comment":"Accepted to ICLR 2025 as a Poster"},{"id":"http://arxiv.org/abs/2503.14980v1","updated":"2025-03-19T08:21:22Z","published":"2025-03-19T08:21:22Z","title":"Embedding spatial context in urban traffic forecasting with contrastive\n  pre-training","summary":"  Urban traffic forecasting is a commonly encountered problem, with\nwide-ranging applications in fields such as urban planning, civil engineering\nand transport. In this paper, we study the enhancement of traffic forecasting\nwith pre-training, focusing on spatio-temporal graph methods. While various\nmachine learning methods to solve traffic forecasting problems have been\nexplored and extensively studied, there is a gap of a more contextual approach:\nstudying how relevant non-traffic data can improve prediction performance on\ntraffic forecasting problems. We call this data spatial context. We introduce a\nnovel method of combining road and traffic information through the notion of a\ntraffic quotient graph, a quotient graph formed from road geometry and traffic\nsensors. We also define a way to encode this relationship in the form of a\ngeometric encoder, pre-trained using contrastive learning methods and enhanced\nwith OpenStreetMap data. We introduce and discuss ways to integrate this\ngeometric encoder with existing graph neural network (GNN)-based traffic\nforecasting models, using a contrastive pre-training paradigm. We demonstrate\nthe potential for this hybrid model to improve generalisation and performance\nwith zero additional traffic data. Code for this paper is available at\nhttps://github.com/mattchrlw/forecasting-on-new-roads.\n","authors":["Matthew Low","Arian Prabowo","Hao Xue","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2503.14980v1.pdf","comment":"21 pages with references, 10 figures"},{"id":"http://arxiv.org/abs/2503.14976v1","updated":"2025-03-19T08:10:54Z","published":"2025-03-19T08:10:54Z","title":"Application of linear regression method to the deep reinforcement\n  learning in continuous action cases","summary":"  The linear regression (LR) method offers the advantage that optimal\nparameters can be calculated relatively easily, although its representation\ncapability is limited than that of the deep learning technique. To improve deep\nreinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was\nproposed by Levine et al., which combines Deep Q Network (DQN) with LR method.\nHowever, the LS-DQN method assumes that the actions are discrete. In this\nstudy, we propose the Double Least Squares Deep Deterministic Policy Gradient\n(DLS-DDPG) method to address this limitation. This method combines the LR\nmethod with the Deep Deterministic Policy Gradient (DDPG) technique, one of the\nrepresentative deep reinforcement learning algorithms for continuous action\ncases. Numerical experiments conducted in MuJoCo environments showed that the\nLR update improved performance at least in some tasks, although there are\ndifficulties such as the inability to make the regularization terms small.\n","authors":["Hisato Komatsu"],"pdf_url":"https://arxiv.org/pdf/2503.14976v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2308.10711v2","updated":"2025-03-19T07:59:35Z","published":"2023-08-21T13:24:52Z","title":"Relax and penalize: a new bilevel approach to mixed-binary\n  hyperparameter optimization","summary":"  In recent years, bilevel approaches have become very popular to efficiently\nestimate high-dimensional hyperparameters of machine learning models. However,\nto date, binary parameters are handled by continuous relaxation and rounding\nstrategies, which could lead to inconsistent solutions. In this context, we\ntackle the challenging optimization of mixed-binary hyperparameters by\nresorting to an equivalent continuous bilevel reformulation based on an\nappropriate penalty term. We propose an algorithmic framework that, under\nsuitable assumptions, is guaranteed to provide mixed-binary solutions.\nMoreover, the generality of the method allows to safely use existing continuous\nbilevel solvers within the proposed framework. We evaluate the performance of\nour approach for two specific machine learning problems, i.e., the estimation\nof the group-sparsity structure in regression problems and the data\ndistillation problem. The reported results show that our method is competitive\nwith state-of-the-art approaches based on relaxation and rounding\n","authors":["Sara Venturini","Marianna de Santis","Jordan Patracone","Francesco Rinaldi","Saverio Salzo","Martin Schmidt"],"pdf_url":"https://arxiv.org/pdf/2308.10711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14963v1","updated":"2025-03-19T07:57:08Z","published":"2025-03-19T07:57:08Z","title":"Continual Multimodal Contrastive Learning","summary":"  Multimodal contrastive learning (MCL) advances in aligning different\nmodalities and generating multimodal representations in a joint space. By\nleveraging contrastive learning across diverse modalities, large-scale\nmultimodal data enhances representational quality. However, a critical yet\noften overlooked challenge remains: multimodal data is rarely collected in a\nsingle process, and training from scratch is computationally expensive.\nInstead, emergent multimodal data can be used to optimize existing models\ngradually, \\textit{i.e.}, models are trained on a sequence of modality pair\ndata. We define this problem as Continual Multimodal Contrastive Learning\n(CMCL), an underexplored yet crucial research direction at the intersection of\nmultimodal and continual learning. In this paper, we formulate CMCL through two\nspecialized principles of stability and plasticity. We theoretically derive a\nnovel optimization-based method, which projects updated gradients from dual\nsides onto subspaces where any gradient is prevented from interfering with the\npreviously learned knowledge. Two upper bounds provide theoretical insights on\nboth stability and plasticity in our solution. Beyond our theoretical\ncontributions, we conduct experiments on multiple datasets by comparing our\nmethod against advanced continual learning baselines. The empirical results\nfurther support our claims and demonstrate the efficacy of our method. The code\nwill be publicly available.\n","authors":["Xiaohao Liu","Xiaobo Xia","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2503.14963v1.pdf","comment":"36 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2302.02528v2","updated":"2025-03-19T07:37:00Z","published":"2023-02-06T01:59:16Z","title":"Personalized Interpretable Classification","summary":"  How to interpret a data mining model has received much attention recently,\nbecause people may distrust a black-box predictive model if they do not\nunderstand how the model works. Hence, it will be trustworthy if a model can\nprovide transparent illustrations on how to make the decision. Although many\nrule-based interpretable classification algorithms have been proposed, all\nthese existing solutions cannot directly construct an interpretable model to\nprovide personalized prediction for each individual test sample. In this paper,\nwe make a first step towards formally introducing personalized interpretable\nclassification as a new data mining problem to the literature. In addition to\nthe problem formulation on this new issue, we present a greedy algorithm called\nPIC (Personalized Interpretable Classifier) to identify a personalized rule for\neach individual test sample. To improve the running efficiency, a fast\napproximate algorithm called fPIC is presented as well. To demonstrate the\nnecessity, feasibility and advantages of such a personalized interpretable\nclassification method, we conduct a series of empirical studies on real data\nsets. The experimental results show that: (1) The new problem formulation\nenables us to find interesting rules for test samples that may be missed by\nexisting non-personalized classifiers. (2) Our algorithms can achieve the\nsame-level predictive accuracy as those state-of-the-art (SOTA) interpretable\nclassifiers. (3) On a real data set for predicting breast cancer metastasis,\nsuch personalized interpretable classifiers can outperform SOTA methods in\nterms of both accuracy and interpretability.\n","authors":["Zengyou He","Pengju Li","Yifan Tang","Lianyu Hu","Mudi Jiang","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2302.02528v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13491v2","updated":"2025-03-19T07:34:50Z","published":"2025-03-10T13:31:42Z","title":"FLP-XR: Future Location Prediction on Extreme Scale Maritime Data in\n  Real-time","summary":"  Movements of maritime vessels are inherently complex and challenging to model\ndue to the dynamic and often unpredictable nature of maritime operations. Even\nwithin structured maritime environments, such as shipping lanes and port\napproaches, where vessels adhere to navigational rules and predefined sea\nroutes, uncovering underlying patterns is far from trivial. The necessity for\naccurate modeling of the mobility of maritime vessels arises from the numerous\napplications it serves, including risk assessment for collision avoidance,\noptimization of shipping routes, and efficient port management. This paper\nintroduces FLP-XR, a model that leverages maritime mobility data to construct a\nrobust framework that offers precise predictions while ensuring extremely fast\ntraining and inference capabilities. We demonstrate the efficiency of our\napproach through an extensive experimental study using three real-world AIS\ndatasets. According to the experimental results, FLP-XR outperforms the current\nstate-of-the-art in many cases, whereas it performs 2-3 orders of magnitude\nfaster in terms of training and inference.\n","authors":["George S. Theodoropoulos","Andreas Patakis","Andreas Tritsarolis","Yannis Theodoridis"],"pdf_url":"https://arxiv.org/pdf/2503.13491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09361v2","updated":"2025-03-19T07:33:47Z","published":"2024-11-14T11:08:54Z","title":"Time-to-Event Pretraining for 3D Medical Imaging","summary":"  With the rise of medical foundation models and the growing availability of\nimaging data, scalable pretraining techniques offer a promising way to identify\nimaging biomarkers predictive of future disease risk. While current\nself-supervised methods for 3D medical imaging models capture local structural\nfeatures like organ morphology, they fail to link pixel biomarkers with\nlong-term health outcomes due to a missing context problem. Current approaches\nlack the temporal context necessary to identify biomarkers correlated with\ndisease progression, as they rely on supervision derived only from images and\nconcurrent text descriptions. To address this, we introduce time-to-event\npretraining, a pretraining framework for 3D medical imaging models that\nleverages large-scale temporal supervision from paired, longitudinal electronic\nhealth records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D\nimages) and time-to-event distributions across thousands of EHR-derived tasks,\nour method improves outcome prediction, achieving an average AUROC increase of\n23.7% and a 29.4% gain in Harrell's C-index across 8 benchmark tasks.\nImportantly, these gains are achieved without sacrificing diagnostic\nclassification performance. This study lays the foundation for integrating\nlongitudinal EHR and 3D imaging data to advance clinical risk prediction.\n","authors":["Zepeng Huo","Jason Alan Fries","Alejandro Lozano","Jeya Maria Jose Valanarasu","Ethan Steinberg","Louis Blankemeier","Akshay S. Chaudhari","Curtis Langlotz","Nigam H. Shah"],"pdf_url":"https://arxiv.org/pdf/2411.09361v2.pdf","comment":"34 pages, 19 figures"},{"id":"http://arxiv.org/abs/2503.03866v2","updated":"2025-03-19T07:23:37Z","published":"2025-03-05T19:55:10Z","title":"Learning to Negotiate via Voluntary Commitment","summary":"  The partial alignment and conflict of autonomous agents lead to mixed-motive\nscenarios in many real-world applications. However, agents may fail to\ncooperate in practice even when cooperation yields a better outcome. One well\nknown reason for this failure comes from non-credible commitments. To\nfacilitate commitments among agents for better cooperation, we define Markov\nCommitment Games (MCGs), a variant of commitment games, where agents can\nvoluntarily commit to their proposed future plans. Based on MCGs, we propose a\nlearnable commitment protocol via policy gradients. We further propose\nincentive-compatible learning to accelerate convergence to equilibria with\nbetter social welfare. Experimental results in challenging mixed-motive tasks\ndemonstrate faster empirical convergence and higher returns for our method\ncompared with its counterparts. Our code is available at\nhttps://github.com/shuhui-zhu/DCL.\n","authors":["Shuhui Zhu","Baoxiang Wang","Sriram Ganapathi Subramanian","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2503.03866v2.pdf","comment":"Accepted by AISTATS 2025"},{"id":"http://arxiv.org/abs/2503.14345v2","updated":"2025-03-19T07:17:41Z","published":"2025-03-18T15:25:08Z","title":"MoonCast: High-Quality Zero-Shot Podcast Generation","summary":"  Recent advances in text-to-speech synthesis have achieved notable success in\ngenerating high-quality short utterances for individual speakers. However,\nthese systems still face challenges when extending their capabilities to long,\nmulti-speaker, and spontaneous dialogues, typical of real-world scenarios such\nas podcasts. These limitations arise from two primary challenges: 1) long\nspeech: podcasts typically span several minutes, exceeding the upper limit of\nmost existing work; 2) spontaneity: podcasts are marked by their spontaneous,\noral nature, which sharply contrasts with formal, written contexts; existing\nworks often fall short in capturing this spontaneity. In this paper, we propose\nMoonCast, a solution for high-quality zero-shot podcast generation, aiming to\nsynthesize natural podcast-style speech from text-only sources (e.g., stories,\ntechnical reports, news in TXT, PDF, or Web URL formats) using the voices of\nunseen speakers. To generate long audio, we adopt a long-context language\nmodel-based audio modeling approach utilizing large-scale long-context speech\ndata. To enhance spontaneity, we utilize a podcast generation module to\ngenerate scripts with spontaneous details, which have been empirically shown to\nbe as crucial as the text-to-speech modeling itself. Experiments demonstrate\nthat MoonCast outperforms baselines, with particularly notable improvements in\nspontaneity and coherence.\n","authors":["Zeqian Ju","Dongchao Yang","Jianwei Yu","Kai Shen","Yichong Leng","Zhengtao Wang","Xu Tan","Xinyu Zhou","Tao Qin","Xiangyang Li"],"pdf_url":"https://arxiv.org/pdf/2503.14345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03988v2","updated":"2025-03-19T07:13:59Z","published":"2024-10-05T00:43:09Z","title":"Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate\n  Regression","summary":"  We examine the implicit bias of mirror flow in univariate least squares error\nregression with wide and shallow neural networks. For a broad class of\npotential functions, we show that mirror flow exhibits lazy training and has\nthe same implicit bias as ordinary gradient flow when the network width tends\nto infinity. For ReLU networks, we characterize this bias through a variational\nproblem in function space. Our analysis includes prior results for ordinary\ngradient flow as a special case and lifts limitations which required either an\nintractable adjustment of the training data or networks with skip connections.\nWe further introduce scaled potentials and show that for these, mirror flow\nstill exhibits lazy training but is not in the kernel regime. For networks with\nabsolute value activations, we show that mirror flow with scaled potentials\ninduces a rich class of biases, which generally cannot be captured by an RKHS\nnorm. A takeaway is that whereas the parameter initialization determines how\nstrongly the curvature of the learned function is penalized at different\nlocations of the input space, the scaled potential determines how the different\nmagnitudes of the curvature are penalized.\n","authors":["Shuang Liang","Guido Montúfar"],"pdf_url":"https://arxiv.org/pdf/2410.03988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20298v3","updated":"2025-03-19T07:11:37Z","published":"2024-03-29T17:15:21Z","title":"Review-Based Hyperbolic Cross-Domain Recommendation","summary":"  The issue of data sparsity poses a significant challenge to recommender\nsystems. In response to this, algorithms that leverage side information such as\nreview texts have been proposed. Furthermore, Cross-Domain Recommendation\n(CDR), which captures domain-shareable knowledge and transfers it from a richer\ndomain (source) to a sparser one (target), has received notable attention.\nNevertheless, the majority of existing methodologies assume a Euclidean\nembedding space, encountering difficulties in accurately representing richer\ntext information and managing complex interactions between users and items.\nThis paper advocates a hyperbolic CDR approach based on review texts for\nmodeling user-item relationships. We first emphasize that conventional\ndistance-based domain alignment techniques may cause problems because small\nmodifications in hyperbolic geometry result in magnified perturbations,\nultimately leading to the collapse of hierarchical structures. To address this\nchallenge, we propose hierarchy-aware embedding and domain alignment schemes\nthat adjust the scale to extract domain-shareable information without\ndisrupting structural forms. The process involves the initial embedding of\nreview texts in hyperbolic space, followed by feature extraction incorporating\ndegree-based normalization and structure alignment. We conducted extensive\nexperiments to substantiate the efficiency, robustness, and scalability of our\nproposed model in comparison to state-of-the-art baselines.\n","authors":["Yoonhyuk Choi","Jiho Choi","Taewook Ko","Chong-Kwon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.20298v3.pdf","comment":"WSDM '25"},{"id":"http://arxiv.org/abs/2503.14937v1","updated":"2025-03-19T06:48:18Z","published":"2025-03-19T06:48:18Z","title":"Proceedings of the 3rd Italian Conference on Big Data and Data Science\n  (ITADATA2024)","summary":"  Proceedings of the 3rd Italian Conference on Big Data and Data Science\n(ITADATA2024), held in Pisa, Italy, September 17-19, 2024.\n  The Italian Conference on Big Data and Data Science (ITADATA2024) is the\nannual event supported by the CINI Big Data National Laboratory and ISTI CNR\nthat aims to put together Italian researchers and professionals from academia,\nindustry, government, and public administration working in the field of big\ndata and data science, as well as related fields (e.g., security and privacy,\nHPC, Cloud).\n  ITADATA2024 covered research on all theoretical and practical aspects of Big\nData and data science including data governance, data processing, data\nanalysis, data reporting, data protection, as well as experimental studies and\nlessons learned. In particular, ITADATA2024 focused on\n  - Data spaces\n  - Data processing life cycle\n  - Machine learning and Large Language Models\n  - Applications of big data and data science in healthcare, finance, industry\n5.0, and beyond\n  - Data science for social network analysis\n","authors":["Nicola Bena","Claudia Diamantini","Michela Natilli","Luigi Romano","Giovanni Stilo","Valentina Pansanella","Claudio A. Ardagna","Anna Monreale","Roberto Trasarti"],"pdf_url":"https://arxiv.org/pdf/2503.14937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14936v1","updated":"2025-03-19T06:44:29Z","published":"2025-03-19T06:44:29Z","title":"Enhancing Code LLM Training with Programmer Attention","summary":"  Human attention provides valuable yet underexploited signals for code LLM\ntraining, offering a perspective beyond purely machine-driven attention.\nDespite the complexity and cost of collecting eye-tracking data, there has also\nbeen limited progress in systematically using these signals for code LLM\ntraining. To address both issues, we propose a cohesive pipeline spanning\naugmentation and reward-based fine-tuning. Specifically, we introduce (1) an\neye-tracking path augmentation method to expand programmer attention datasets,\n(2) a pattern abstraction step that refines raw fixations into learnable\nattention motifs, and (3) a reward-guided strategy for integrating these\ninsights directly into a CodeT5 supervised fine-tuning process. Our experiments\nyield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization,\nunderscoring how uniting human and machine attention can boost code\nintelligence. We hope this work encourages broader exploration of human-centric\nmethods in next-generation AI4SE.\n","authors":["Yifan Zhang","Chen Huang","Zachary Karas","Dung Thuy Nguyen","Kevin Leach","Yu Huang"],"pdf_url":"https://arxiv.org/pdf/2503.14936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17270v2","updated":"2025-03-19T06:42:21Z","published":"2024-10-07T13:51:58Z","title":"MOFFlow: Flow Matching for Structure Prediction of Metal-Organic\n  Frameworks","summary":"  Metal-organic frameworks (MOFs) are a class of crystalline materials with\npromising applications in many areas such as carbon capture and drug delivery.\nIn this work, we introduce MOFFlow, the first deep generative model tailored\nfor MOF structure prediction. Existing approaches, including ab initio\ncalculations and even deep generative models, struggle with the complexity of\nMOF structures due to the large number of atoms in the unit cells. To address\nthis limitation, we propose a novel Riemannian flow matching framework that\nreduces the dimensionality of the problem by treating the metal nodes and\norganic linkers as rigid bodies, capitalizing on the inherent modularity of\nMOFs. By operating in the $SE(3)$ space, MOFFlow effectively captures the\nroto-translational dynamics of these rigid components in a scalable way. Our\nexperiment demonstrates that MOFFlow accurately predicts MOF structures\ncontaining several hundred atoms, significantly outperforming conventional\nmethods and state-of-the-art machine learning baselines while being much\nfaster.\n","authors":["Nayoung Kim","Seongsu Kim","Minsu Kim","Jinkyoo Park","Sungsoo Ahn"],"pdf_url":"https://arxiv.org/pdf/2410.17270v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.14932v1","updated":"2025-03-19T06:38:51Z","published":"2025-03-19T06:38:51Z","title":"Prada: Black-Box LLM Adaptation with Private Data on\n  Resource-Constrained Devices","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\nabilities in various natural language processing tasks. However, adapting these\nmodels to specialized domains using private datasets stored on\nresource-constrained edge devices, such as smartphones and personal computers,\nremains challenging due to significant privacy concerns and limited\ncomputational resources. Existing model adaptation methods either compromise\ndata privacy by requiring data transmission or jeopardize model privacy by\nexposing proprietary LLM parameters. To address these challenges, we propose\nPrada, a novel privacy-preserving and efficient black-box LLM adaptation system\nusing private on-device datasets. Prada employs a lightweight proxy model\nfine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During\ninference, Prada leverages the logits offset, i.e., difference in outputs\nbetween the base and adapted proxy models, to iteratively refine outputs from a\nremote black-box LLM. This offset-based adaptation approach preserves both data\nprivacy and model privacy, as there is no need to share sensitive data or\nproprietary model parameters. Furthermore, we incorporate speculative decoding\nto further speed up the inference process of Prada, making the system\npractically deployable on bandwidth-constrained edge devices, enabling a more\npractical deployment of Prada. Extensive experiments on various downstream\ntasks demonstrate that Prada achieves performance comparable to centralized\nfine-tuning methods while significantly reducing computational overhead by up\nto 60% and communication costs by up to 80%.\n","authors":["Ziyao Wang","Yexiao He","Zheyu Shen","Yu Li","Guoheng Sun","Myungjin Lee","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2503.14932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14929v1","updated":"2025-03-19T06:29:15Z","published":"2025-03-19T06:29:15Z","title":"ACE: A Cardinality Estimator for Set-Valued Queries","summary":"  Cardinality estimation is a fundamental functionality in database systems.\nMost existing cardinality estimators focus on handling predicates over numeric\nor categorical data. They have largely omitted an important data type,\nset-valued data, which frequently occur in contemporary applications such as\ninformation retrieval and recommender systems. The few existing estimators for\nsuch data either favor high-frequency elements or rely on a partial\nindependence assumption, which limits their practical applicability. We propose\nACE, an Attention-based Cardinality Estimator for estimating the cardinality of\nqueries over set-valued data. We first design a distillation-based data encoder\nto condense the dataset into a compact matrix. We then design an\nattention-based query analyzer to capture correlations among query elements. To\nhandle variable-sized queries, a pooling module is introduced, followed by a\nregression model (MLP) to generate final cardinality estimates. We evaluate ACE\non three datasets with varying query element distributions, demonstrating that\nACE outperforms the state-of-the-art competitors in terms of both accuracy and\nefficiency.\n","authors":["Yufan Sheng","Xin Cao","Kaiqi Zhao","Yixiang Fang","Jianzhong Qi","Wenjie Zhang","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2503.14929v1.pdf","comment":"This paper has been accepted by PVLDB Vol 18"},{"id":"http://arxiv.org/abs/2503.09985v2","updated":"2025-03-19T06:27:43Z","published":"2025-03-13T02:50:19Z","title":"ES-Parkour: Advanced Robot Parkour with Bio-inspired Event Camera and\n  Spiking Neural Network","summary":"  In recent years, quadruped robotics has advanced significantly, particularly\nin perception and motion control via reinforcement learning, enabling complex\nmotions in challenging environments. Visual sensors like depth cameras enhance\nstability and robustness but face limitations, such as low operating\nfrequencies relative to joint control and sensitivity to lighting, which hinder\noutdoor deployment. Additionally, deep neural networks in sensor and control\nsystems increase computational demands. To address these issues, we introduce\nspiking neural networks (SNNs) and event cameras to perform a challenging\nquadruped parkour task. Event cameras capture dynamic visual data, while SNNs\nefficiently process spike sequences, mimicking biological perception.\nExperimental results demonstrate that this approach significantly outperforms\ntraditional models, achieving excellent parkour performance with just 11.7% of\nthe energy consumption of an artificial neural network (ANN)-based model,\nyielding an 88.3% energy reduction. By integrating event cameras with SNNs, our\nwork advances robotic reinforcement learning and opens new possibilities for\napplications in demanding environments.\n","authors":["Qiang Zhang","Jiahang Cao","Jingkai Sun","Yecheng Shao","Gang Han","Wen Zhao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2503.09985v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14927v1","updated":"2025-03-19T06:27:12Z","published":"2025-03-19T06:27:12Z","title":"Semi-Gradient SARSA Routing with Theoretical Guarantee on Traffic\n  Stability and Weight Convergence","summary":"  We consider the traffic control problem of dynamic routing over parallel\nservers, which arises in a variety of engineering systems such as\ntransportation and data transmission. We propose a semi-gradient, on-policy\nalgorithm that learns an approximate optimal routing policy. The algorithm uses\ngeneric basis functions with flexible weights to approximate the value function\nacross the unbounded state space. Consequently, the training process lacks\nLipschitz continuity of the gradient, boundedness of the temporal-difference\nerror, and a prior guarantee on ergodicity, which are the standard\nprerequisites in existing literature on reinforcement learning theory. To\naddress this, we combine a Lyapunov approach and an ordinary differential\nequation-based method to jointly characterize the behavior of traffic state and\napproximation weights. Our theoretical analysis proves that the training scheme\nguarantees traffic state stability and ensures almost surely convergence of the\nweights to the approximate optimum. We also demonstrate via simulations that\nour algorithm attains significantly faster convergence than neural\nnetwork-based methods with an insignificant approximation error.\n","authors":["Yidan Wu","Yu Yu","Jianan Zhang","Li Jin"],"pdf_url":"https://arxiv.org/pdf/2503.14927v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2404.09188"},{"id":"http://arxiv.org/abs/2410.17579v4","updated":"2025-03-19T06:20:44Z","published":"2024-10-23T06:08:45Z","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","summary":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","authors":["Mridul Gupta","Samyak Jain","Vansh Ramani","Hariprasad Kodamana","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2410.17579v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14925v1","updated":"2025-03-19T06:15:31Z","published":"2025-03-19T06:15:31Z","title":"pFedFair: Towards Optimal Group Fairness-Accuracy Trade-off in\n  Heterogeneous Federated Learning","summary":"  Federated learning (FL) algorithms commonly aim to maximize clients' accuracy\nby training a model on their collective data. However, in several FL\napplications, the model's decisions should meet a group fairness constraint to\nbe independent of sensitive attributes such as gender or race. While such group\nfairness constraints can be incorporated into the objective function of the FL\noptimization problem, in this work, we show that such an approach would lead to\nsuboptimal classification accuracy in an FL setting with heterogeneous client\ndistributions. To achieve an optimal accuracy-group fairness trade-off, we\npropose the Personalized Federated Learning for Client-Level Group Fairness\n(pFedFair) framework, where clients locally impose their fairness constraints\nover the distributed training process. Leveraging the image embedding models,\nwe extend the application of pFedFair to computer vision settings, where we\nnumerically show that pFedFair achieves an optimal group fairness-accuracy\ntrade-off in heterogeneous FL settings. We present the results of several\nnumerical experiments on benchmark and synthetic datasets, which highlight the\nsuboptimality of non-personalized FL algorithms and the improvements made by\nthe pFedFair method.\n","authors":["Haoyu Lei","Shizhan Gong","Qi Dou","Farzan Farnia"],"pdf_url":"https://arxiv.org/pdf/2503.14925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09049v2","updated":"2025-03-19T06:14:04Z","published":"2024-12-12T08:19:01Z","title":"Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for\n  Customer Service Dialogues","summary":"  Discovering customer intentions in dialogue conversations is crucial for\nautomated service agents. Yet, existing intent clustering methods often fail to\nalign with human perceptions due to the heavy reliance on embedding distance\nmetrics and sentence embeddings. To address these limitations, we propose\nintegrating the semantic understanding capabilities of LLMs into an\n$\\textbf{LLM-in-the-loop (LLM-ITL)}$ intent clustering framework. Specifically,\nthis paper (1) investigates the effectiveness of fine-tuned LLMs in semantic\ncoherence evaluation and intent cluster naming, achieving over 95% accuracy;\n(2) designs an LLM-ITL clustering algorithm that facilitates the iterative\ndiscovery of coherent intent clusters; and (3) proposes task-specific\ntechniques tailored for customer service dialogue intent clustering. Since\nexisting English benchmarks pose limited semantic diversity and intent labels,\nwe introduced a comprehensive Chinese dialogue intent dataset, comprising over\n100,000 real customer service calls and 1,507 human-annotated intent clusters.\nThe proposed approaches significantly outperformed LLM-guided baselines,\nachieving notable improvements in clustering quality and a 12% boost in the\ndownstream intent classification task. Combined with several best practices,\nour findings highlight the potential of LLM-in-the-loop techniques for scalable\nand human-aligned problem-solving. Sample code and datasets are available at:\nhttps://anonymous.4open.science/r/Dial-in-LLM-0410.\n","authors":["Mengze Hong","Di Jiang","Yuanfeng Song","Lu Wang","Wailing Ng","Yanjie Sun","Chen Jason Zhang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2412.09049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14922v1","updated":"2025-03-19T06:04:55Z","published":"2025-03-19T06:04:55Z","title":"A Semantic and Clean-label Backdoor Attack against Graph Convolutional\n  Networks","summary":"  Graph Convolutional Networks (GCNs) have shown excellent performance in\ngraph-structured tasks such as node classification and graph classification.\nHowever, recent research has shown that GCNs are vulnerable to a new type of\nthreat called the backdoor attack, where the adversary can inject a hidden\nbackdoor into the GCNs so that the backdoored model performs well on benign\nsamples, whereas its prediction will be maliciously changed to the\nattacker-specified target label if the hidden backdoor is activated by the\nattacker-defined trigger. Clean-label backdoor attack and semantic backdoor\nattack are two new backdoor attacks to Deep Neural Networks (DNNs), they are\nmore imperceptible and have posed new and serious threats. The semantic and\nclean-label backdoor attack is not fully explored in GCNs. In this paper, we\npropose a semantic and clean-label backdoor attack against GCNs under the\ncontext of graph classification to reveal the existence of this security\nvulnerability in GCNs. Specifically, SCLBA conducts an importance analysis on\ngraph samples to select one type of node as semantic trigger, which is then\ninserted into the graph samples to create poisoning samples without changing\nthe labels of the poisoning samples to the attacker-specified target label. We\nevaluate SCLBA on multiple datasets and the results show that SCLBA can achieve\nattack success rates close to 99% with poisoning rates of less than 3%, and\nwith almost no impact on the performance of model on benign samples.\n","authors":["Jiazhu Dai","Haoyu Sun"],"pdf_url":"https://arxiv.org/pdf/2503.14922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12370v5","updated":"2025-03-19T05:38:58Z","published":"2024-12-16T21:56:01Z","title":"Scam Detection for Ethereum Smart Contracts: Leveraging Graph\n  Representation Learning for Secure Blockchain","summary":"  As more and more attacks have been detected on Ethereum smart contracts, it\nhas seriously affected finance and credibility. Current anti-fraud detection\ntechniques, including code parsing or manual feature extraction, still have\nsome shortcomings, although some generalization or adaptability can be\nobtained. In the face of this situation, this paper proposes to use graphical\nrepresentation learning technology to find transaction patterns and distinguish\nmalicious transaction contracts, that is, to represent Ethereum transaction\ndata as graphs, and then use advanced ML technology to obtain reliable and\naccurate results. Taking into account the sample imbalance, we treated with\nSMOTE-ENN and tested several models, in which MLP performed better than GCN,\nbut the exact effect depends on its field trials. Our research opens up more\npossibilities for trust and security in the Ethereum ecosystem.\n","authors":["Yihong Jin","Ze Yang","Xinhe Xu"],"pdf_url":"https://arxiv.org/pdf/2412.12370v5.pdf","comment":"Accepted to ISCAIT 2025"},{"id":"http://arxiv.org/abs/2408.00315v4","updated":"2025-03-19T05:26:47Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v4.pdf","comment":"ICLR 2025, fix typos in the proof"},{"id":"http://arxiv.org/abs/2503.13954v2","updated":"2025-03-19T05:21:06Z","published":"2025-03-18T06:46:53Z","title":"Enhanced High-Dimensional Data Visualization through Adaptive\n  Multi-Scale Manifold Embedding","summary":"  To address the dual challenges of the curse of dimensionality and the\ndifficulty in separating intra-cluster and inter-cluster structures in\nhigh-dimensional manifold embedding, we proposes an Adaptive Multi-Scale\nManifold Embedding (AMSME) algorithm. By introducing ordinal distance to\nreplace traditional Euclidean distances, we theoretically demonstrate that\nordinal distance overcomes the constraints of the curse of dimensionality in\nhigh-dimensional spaces, effectively distinguishing heterogeneous samples. We\ndesign an adaptive neighborhood adjustment method to construct similarity\ngraphs that simultaneously balance intra-cluster compactness and inter-cluster\nseparability. Furthermore, we develop a two-stage embedding framework: the\nfirst stage achieves preliminary cluster separation while preserving\nconnectivity between structurally similar clusters via the similarity graph,\nand the second stage enhances inter-cluster separation through a label-driven\ndistance reweighting. Experimental results demonstrate that AMSME significantly\npreserves intra-cluster topological structures and improves inter-cluster\nseparation on real-world datasets. Additionally, leveraging its\nmulti-resolution analysis capability, AMSME discovers novel neuronal subtypes\nin the mouse lumbar dorsal root ganglion scRNA-seq dataset, with marker gene\nanalysis revealing their distinct biological roles.\n","authors":["Tianhao Ni","Bingjie Li","Zhigang Yao"],"pdf_url":"https://arxiv.org/pdf/2503.13954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18263v6","updated":"2025-03-19T05:07:51Z","published":"2024-12-24T08:25:38Z","title":"High-Rank Irreducible Cartesian Tensor Decomposition and Bases of\n  Equivariant Spaces","summary":"  Irreducible Cartesian tensors (ICTs) play a crucial role in the design of\nequivariant graph neural networks, as well as in theoretical chemistry and\nchemical physics. Meanwhile, the design space of available linear operations on\ntensors that preserve symmetry presents a significant challenge. The ICT\ndecomposition and a basis of this equivariant space are difficult to obtain for\nhigh-rank tensors. After decades of research, Bonvicini (2024) recently\nachieves an explicit ICT decomposition for $n=5$ with factorial time/space\ncomplexity. In this work we, for the first time, obtains decomposition matrices\nfor ICTs up to rank $n=9$ with reduced and affordable complexity, by\nconstructing what we call path matrices. The path matrices are obtained via\nperforming chain-like contractions with Clebsch-Gordan matrices following the\nparentage scheme. We prove and leverage that the concatenation of path matrices\nis an orthonormal change-of-basis matrix between the Cartesian tensor product\nspace and the spherical direct sum spaces. Furthermore, we identify a complete\northogonal basis for the equivariant space, rather than a spanning set\n(Pearce-Crump, 2023), through this path matrices technique. To the best of our\nknowledge, this is also the first analytic, rather than numerical, method for\ntheoretically obtaining arbitrary rank orthogonal ICT decomposition matrices\nand orthogonal equivariant bases. We further extend our result to the arbitrary\ntensor product and direct sum spaces, enabling free design between different\nspaces while keeping symmetry. The Python code is available at\nhttps://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases, where\nthe $n=6,\\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and\n4m32s on 28-cores Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz, respectively.\n","authors":["Shihao Shao","Yikang Li","Zhouchen Lin","Qinghua Cui"],"pdf_url":"https://arxiv.org/pdf/2412.18263v6.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2502.02430v2","updated":"2025-03-19T04:49:08Z","published":"2025-02-04T15:55:10Z","title":"A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals","summary":"  Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.\n","authors":["Róbert Busa-Fekete","Julian Zimmert","András György","Linhai Qiu","Tzu-Wei Sung","Hao Shen","Hyomin Choi","Sharmila Subramaniam","Li Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.02430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14887v1","updated":"2025-03-19T04:30:20Z","published":"2025-03-19T04:30:20Z","title":"Pseudo-Relevance Feedback Can Improve Zero-Shot LLM-Based Dense\n  Retrieval","summary":"  Pseudo-relevance feedback (PRF) refines queries by leveraging initially\nretrieved documents to improve retrieval effectiveness. In this paper, we\ninvestigate how large language models (LLMs) can facilitate PRF for zero-shot\nLLM-based dense retrieval, extending the recently proposed PromptReps method.\nSpecifically, our approach uses LLMs to extract salient passage features-such\nas keywords and summaries-from top-ranked documents, which are then integrated\ninto PromptReps to produce enhanced query representations. Experiments on\npassage retrieval benchmarks demonstrate that incorporating PRF significantly\nboosts retrieval performance. Notably, smaller rankers with PRF can match the\neffectiveness of larger rankers without PRF, highlighting PRF's potential to\nimprove LLM-driven search while maintaining an efficient balance between\neffectiveness and resource usage.\n","authors":["Hang Li","Xiao Wang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2503.14887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05686v3","updated":"2025-03-19T04:28:43Z","published":"2024-08-11T03:39:46Z","title":"The Bandit Whisperer: Communication Learning for Restless Bandits","summary":"  Applying Reinforcement Learning (RL) to Restless Multi-Arm Bandits (RMABs)\noffers a promising avenue for addressing allocation problems with resource\nconstraints and temporal dynamics. However, classic RMAB models largely\noverlook the challenges of (systematic) data errors - a common occurrence in\nreal-world scenarios due to factors like varying data collection protocols and\nintentional noise for differential privacy. We demonstrate that conventional RL\nalgorithms used to train RMABs can struggle to perform well in such settings.\nTo solve this problem, we propose the first communication learning approach in\nRMABs, where we study which arms, when involved in communication, are most\neffective in mitigating the influence of such systematic data errors. In our\nsetup, the arms receive Q-function parameters from similar arms as messages to\nguide behavioral policies, steering Q-function updates. We learn communication\nstrategies by considering the joint utility of messages across all pairs of\narms and using a Q-network architecture that decomposes the joint utility. Both\ntheoretical and empirical evidence validate the effectiveness of our method in\nsignificantly improving RMAB performance across diverse problems.\n","authors":["Yunfan Zhao","Tonghan Wang","Dheeraj Nagaraj","Aparna Taneja","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2408.05686v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.06052v2","updated":"2025-03-19T04:23:11Z","published":"2025-03-08T04:37:28Z","title":"Interpretable High-order Knowledge Graph Neural Network for Predicting\n  Synthetic Lethality in Human Cancers","summary":"  Synthetic lethality (SL) is a promising gene interaction for cancer therapy.\nRecent SL prediction methods integrate knowledge graphs (KGs) into graph neural\nnetworks (GNNs) and employ attention mechanisms to extract local subgraphs as\nexplanations for target gene pairs. However, attention mechanisms often lack\nfidelity, typically generate a single explanation per gene pair, and fail to\nensure trustworthy high-order structures in their explanations. To overcome\nthese limitations, we propose Diverse Graph Information Bottleneck for\nSynthetic Lethality (DGIB4SL), a KG-based GNN that generates multiple faithful\nexplanations for the same gene pair and effectively encodes high-order\nstructures. Specifically, we introduce a novel DGIB objective, integrating a\nDeterminant Point Process (DPP) constraint into the standard IB objective, and\nemploy 13 motif-based adjacency matrices to capture high-order structures in\ngene representations. Experimental results show that DGIB4SL outperforms\nstate-of-the-art baselines and provides multiple explanations for SL\nprediction, revealing diverse biological mechanisms underlying SL inference.\n","authors":["Xuexin Chen","Ruichu Cai","Zhengting Huang","Zijian Li","Jie Zheng","Min Wu"],"pdf_url":"https://arxiv.org/pdf/2503.06052v2.pdf","comment":"15 pages. Accepted by Briefings in Bioinformatics"},{"id":"http://arxiv.org/abs/2503.14881v1","updated":"2025-03-19T04:18:57Z","published":"2025-03-19T04:18:57Z","title":"Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers","summary":"  A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.\n","authors":["Bo Chen","Xiaoyu Li","Yekun Ke","Yingyu Liang","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2503.14881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13837v2","updated":"2025-03-19T04:09:17Z","published":"2025-03-18T02:21:07Z","title":"Self-Vocabularizing Training for Neural Machine Translation","summary":"  Past vocabulary learning techniques identify relevant vocabulary before\ntraining, relying on statistical and entropy-based assumptions that largely\nneglect the role of model training. Empirically, we observe that trained\ntranslation models are induced to use a byte-pair encoding (BPE) vocabulary\nsubset distinct from the original BPE vocabulary, leading to performance\nimprovements when retrained with the induced vocabulary. In this paper, we\nanalyze this discrepancy in neural machine translation by examining vocabulary\nand entropy shifts during self-training--where each iteration generates a\nlabeled dataset by pairing source sentences with the model's predictions to\ndefine a new vocabulary. Building on these insights, we propose\nself-vocabularizing training, an iterative method that self-selects a smaller,\nmore optimal vocabulary, yielding up to a 1.49 BLEU improvement. Moreover, we\nfind that deeper model architectures lead to both an increase in unique token\nusage and a 6-8% reduction in vocabulary size.\n","authors":["Pin-Jie Lin","Ernie Chang"],"pdf_url":"https://arxiv.org/pdf/2503.13837v2.pdf","comment":"Accepted to NAACL SRW 2025"},{"id":"http://arxiv.org/abs/2503.10695v2","updated":"2025-03-19T04:07:06Z","published":"2025-03-12T05:11:11Z","title":"Introducing Verification Task of Set Consistency with Set-Consistency\n  Energy Networks","summary":"  Examining logical inconsistencies among multiple statements (such as\ncollections of sentences or question-answer pairs) is a crucial challenge in\nmachine learning, particularly for ensuring the safety and reliability of\nmodels. Traditional methods that rely on pairwise comparisons often fail to\ncapture inconsistencies that only emerge when more than two statements are\nevaluated collectively. To address this gap, we introduce the task of\nset-consistency verification, an extension of natural language inference (NLI)\nthat assesses the logical coherence of entire sets rather than isolated pairs.\nBuilding on this task, we present the Set-Consistency Energy Network\n(SC-Energy), a novel model that employs a contrastive loss framework to learn\nthe compatibility among a collection of statements. Our approach not only\nefficiently verifies inconsistencies and pinpoints the specific statements\nresponsible for logical contradictions, but also significantly outperforms\nexisting methods including prompting-based LLM models. Furthermore, we release\ntwo new datasets: Set-LConVQA and Set-SNLI for set-consistency verification\ntask.\n","authors":["Mooho Song","Hyeryung Son","Jay-Yoon Lee"],"pdf_url":"https://arxiv.org/pdf/2503.10695v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14873v1","updated":"2025-03-19T04:03:39Z","published":"2025-03-19T04:03:39Z","title":"Robust Support Vector Machines for Imbalanced and Noisy Data via Benders\n  Decomposition","summary":"  This study introduces a novel formulation to enhance Support Vector Machines\n(SVMs) in handling class imbalance and noise. Unlike the conventional Soft\nMargin SVM, which penalizes the magnitude of constraint violations, the\nproposed model quantifies the number of violations and aims to minimize their\nfrequency. To achieve this, a binary variable is incorporated into the\nobjective function of the primal SVM formulation, replacing the traditional\nslack variable. Furthermore, each misclassified sample is assigned a priority\nand an associated constraint. The resulting formulation is a mixed-integer\nprogramming model, efficiently solved using Benders decomposition. The proposed\nmodel's performance was benchmarked against existing models, including Soft\nMargin SVM, weighted SVM, and NuSVC. Two primary hypotheses were examined: 1)\nThe proposed model improves the F1-score for the minority class in imbalanced\nclassification tasks. 2) The proposed model enhances classification accuracy in\nnoisy datasets. These hypotheses were evaluated using a Wilcoxon test across\nmultiple publicly available datasets from the OpenML repository. The results\nsupported both hypotheses (\\( p < 0.05 \\)). In addition, the proposed model\nexhibited several interesting properties, such as improved robustness to noise,\na decision boundary shift favoring the minority class, a reduced number of\nsupport vectors, and decreased prediction time. The open-source Python\nimplementation of the proposed SVM model is available.\n","authors":["Seyed Mojtaba Mohasel","Hamidreza Koosha"],"pdf_url":"https://arxiv.org/pdf/2503.14873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14869v1","updated":"2025-03-19T03:48:56Z","published":"2025-03-19T03:48:56Z","title":"Evaluating Time Series Models with Knowledge Discovery","summary":"  Time series data is one of the most ubiquitous data modalities existing in a\ndiverse critical domains such as healthcare, seismology, manufacturing and\nenergy. Recent years, there are increasing interest of the data mining\ncommunity to develop time series deep learning models to pursue better\nperformance. The models performance often evaluate by certain evaluation\nmetrics such as RMSE, Accuracy, and F1-score. Yet time series data are often\nhard to interpret and are collected with unknown environmental factors, sensor\nconfiguration, latent physic mechanisms, and non-stationary evolving behavior.\nAs a result, a model that is better on standard metric-based evaluation may not\nalways perform better in real-world tasks. In this blue sky paper, we aim to\nexplore the challenge that exists in the metric-based evaluation framework for\ntime series data mining and propose a potential blue-sky idea -- developing a\nknowledge-discovery-based evaluation framework, which aims to effectively\nutilize domain-expertise knowledge to evaluate a model. We demonstrate that an\nevidence-seeking explanation can potentially have stronger persuasive power\nthan metric-based evaluation and obtain better generalization ability for time\nseries data mining tasks.\n","authors":["Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.14869v1.pdf","comment":"accepted in SIAM SDM 2025 - Blue Sky Track (to appear)"},{"id":"http://arxiv.org/abs/2503.14860v1","updated":"2025-03-19T03:38:43Z","published":"2025-03-19T03:38:43Z","title":"Global Renewables Watch: A Temporal Dataset of Solar and Wind Energy\n  Derived from Satellite Imagery","summary":"  We present a comprehensive global temporal dataset of commercial solar\nphotovoltaic (PV) farms and onshore wind turbines, derived from high-resolution\nsatellite imagery analyzed quarterly from the fourth quarter of 2017 to the\nsecond quarter of 2024. We create this dataset by training deep learning-based\nsegmentation models to identify these renewable energy installations from\nsatellite imagery, then deploy them on over 13 trillion pixels covering the\nworld. For each detected feature, we estimate the construction date and the\npreceding land use type. This dataset offers crucial insights into progress\ntoward sustainable development goals and serves as a valuable resource for\npolicymakers, researchers, and stakeholders aiming to assess and promote\neffective strategies for renewable energy deployment. Our final spatial dataset\nincludes 375,197 individual wind turbines and 86,410 solar PV installations. We\naggregate our predictions to the country level -- estimating total power\ncapacity based on construction date, solar PV area, and number of windmills --\nand find an $r^2$ value of $0.96$ and $0.93$ for solar PV and onshore wind\nrespectively compared to IRENA's most recent 2023 country-level capacity\nestimates.\n","authors":["Caleb Robinson","Anthony Ortiz","Allen Kim","Rahul Dodhia","Andrew Zolli","Shivaprakash K Nagaraju","James Oakleaf","Joe Kiesecker","Juan M. Lavista Ferres"],"pdf_url":"https://arxiv.org/pdf/2503.14860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18137v3","updated":"2025-03-19T03:35:59Z","published":"2025-01-30T04:59:21Z","title":"Tensor Completion for Surrogate Modeling of Material Property Prediction","summary":"  When designing materials to optimize certain properties, there are often many\npossible configurations of designs that need to be explored. For example, the\nmaterials' composition of elements will affect properties such as strength or\nconductivity, which are necessary to know when developing new materials.\nExploring all combinations of elements to find optimal materials becomes very\ntime consuming, especially when there are more design variables. For this\nreason, there is growing interest in using machine learning (ML) to predict a\nmaterial's properties. In this work, we model the optimization of certain\nmaterial properties as a tensor completion problem, to leverage the structure\nof our datasets and navigate the vast number of combinations of material\nconfigurations. Across a variety of material property prediction tasks, our\nexperiments show tensor completion methods achieving 10-20% decreased error\ncompared with baseline ML models such as GradientBoosting and Multilayer\nPerceptron (MLP), while maintaining similar training speed.\n","authors":["Shaan Pakala","Dawon Ahn","Evangelos Papalexakis"],"pdf_url":"https://arxiv.org/pdf/2501.18137v3.pdf","comment":"2 page paper presented at the AAAI 2025 Bridge on Knowledge-Guided\n  Machine Learning"},{"id":"http://arxiv.org/abs/2503.14858v1","updated":"2025-03-19T03:33:57Z","published":"2025-03-19T03:33:57Z","title":"1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New\n  Goal-Reaching Capabilities","summary":"  Scaling up self-supervised learning has driven breakthroughs in language and\nvision, yet comparable progress has remained elusive in reinforcement learning\n(RL). In this paper, we study building blocks for self-supervised RL that\nunlock substantial improvements in scalability, with network depth serving as a\ncritical factor. Whereas most RL papers in recent years have relied on shallow\narchitectures (around 2 - 5 layers), we demonstrate that increasing the depth\nup to 1024 layers can significantly boost performance. Our experiments are\nconducted in an unsupervised goal-conditioned setting, where no demonstrations\nor rewards are provided, so an agent must explore (from scratch) and learn how\nto maximize the likelihood of reaching commanded goals. Evaluated on simulated\nlocomotion and manipulation tasks, our approach increases performance by\n$2\\times$ - $50\\times$. Increasing the model depth not only increases success\nrates but also qualitatively changes the behaviors learned.\n","authors":["Kevin Wang","Ishaan Javali","Michał Bortkiewicz","Tomasz Trzciński","Benjamin Eysenbach"],"pdf_url":"https://arxiv.org/pdf/2503.14858v1.pdf","comment":"Link to project website:\n  https://wang-kevin3290.github.io/scaling-crl/"},{"id":"http://arxiv.org/abs/2410.11843v5","updated":"2025-03-19T03:17:47Z","published":"2024-09-23T08:39:16Z","title":"From Commands to Prompts: LLM-based Semantic File System for AIOS","summary":"  Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.\n","authors":["Zeru Shi","Kai Mei","Mingyu Jin","Yongye Su","Chaoji Zuo","Wenyue Hua","Wujiang Xu","Yujie Ren","Zirui Liu","Mengnan Du","Dong Deng","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11843v5.pdf","comment":"Accepted by International Conference on Learning Representations\n  2025(ICLR2025)"},{"id":"http://arxiv.org/abs/2503.14849v1","updated":"2025-03-19T03:13:37Z","published":"2025-03-19T03:13:37Z","title":"LogLLaMA: Transformer-based log anomaly detection with LLaMA","summary":"  Log anomaly detection refers to the task that distinguishes the anomalous log\nmessages from normal log messages. Transformer-based large language models\n(LLMs) are becoming popular for log anomaly detection because of their superb\nability to understand complex and long language patterns. In this paper, we\npropose LogLLaMA, a novel framework that leverages LLaMA2. LogLLaMA is first\nfinetuned on normal log messages from three large-scale datasets to learn their\npatterns. After finetuning, the model is capable of generating successive log\nmessages given previous log messages. Our generative model is further trained\nto identify anomalous log messages using reinforcement learning (RL). The\nexperimental results show that LogLLaMA outperforms the state-of-the-art\napproaches for anomaly detection on BGL, Thunderbird, and HDFS datasets.\n","authors":["Zhuoyi Yang","Ian G. Harris"],"pdf_url":"https://arxiv.org/pdf/2503.14849v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.14628v2","updated":"2025-03-19T03:10:27Z","published":"2024-11-21T23:06:15Z","title":"HotSpot: Signed Distance Function Optimization with an Asymptotically\n  Sufficient Condition","summary":"  We propose a method, HotSpot, for optimizing neural signed distance\nfunctions. Existing losses, such as the eikonal loss, act as necessary but\ninsufficient constraints and cannot guarantee that the recovered implicit\nfunction represents a true distance function, even if the output minimizes\nthese losses almost everywhere. Furthermore, the eikonal loss suffers from\nstability issues in optimization. Finally, in conventional methods,\nregularization losses that penalize surface area distort the reconstructed\nsigned distance function. We address these challenges by designing a loss\nfunction using the solution of a screened Poisson equation. Our loss, when\nminimized, provides an asymptotically sufficient condition to ensure the output\nconverges to a true distance function. Our loss also leads to stable\noptimization and naturally penalizes large surface areas. We present\ntheoretical analysis and experiments on both challenging 2D and 3D datasets and\nshow that our method provides better surface reconstruction and a more accurate\ndistance approximation.\n","authors":["Zimo Wang","Cheng Wang","Taiki Yoshino","Sirui Tao","Ziyang Fu","Tzu-Mao Li"],"pdf_url":"https://arxiv.org/pdf/2411.14628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14836v1","updated":"2025-03-19T02:35:01Z","published":"2025-03-19T02:35:01Z","title":"On the Robustness Tradeoff in Fine-Tuning","summary":"  Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments.\n","authors":["Kunyang Li","Jean-Charles Noirot Ferrand","Ryan Sheatsley","Blaine Hoak","Yohan Beugin","Eric Pauley","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2503.14836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11835v2","updated":"2025-03-19T02:28:56Z","published":"2025-03-14T19:56:57Z","title":"How Can Time Series Analysis Benefit From Multiple Modalities? A Survey\n  and Outlook","summary":"  Time series analysis (TSA) is a longstanding research topic in the data\nmining community and has wide real-world significance. Compared to \"richer\"\nmodalities such as language and vision, which have recently experienced\nexplosive development and are densely connected, the time-series modality\nremains relatively underexplored and isolated. We notice that many recent TSA\nworks have formed a new research field, i.e., Multiple Modalities for TSA\n(MM4TSA). In general, these MM4TSA works follow a common motivation: how TSA\ncan benefit from multiple modalities. This survey is the first to offer a\ncomprehensive review and a detailed outlook for this emerging field.\nSpecifically, we systematically discuss three benefits: (1) reusing foundation\nmodels of other modalities for efficient TSA, (2) multimodal extension for\nenhanced TSA, and (3) cross-modality interaction for advanced TSA. We further\ngroup the works by the introduced modality type, including text, images, audio,\ntables, and others, within each perspective. Finally, we identify the gaps with\nfuture opportunities, including the reused modalities selections, heterogeneous\nmodality combinations, and unseen tasks generalizations, corresponding to the\nthree benefits. We release an up-to-date GitHub repository that includes key\npapers and resources.\n","authors":["Haoxin Liu","Harshavardhan Kamarthi","Zhiyuan Zhao","Shangqing Xu","Shiyu Wang","Qingsong Wen","Tom Hartvigsen","Fei Wang","B. Aditya Prakash"],"pdf_url":"https://arxiv.org/pdf/2503.11835v2.pdf","comment":"Github Repo: https://github.com/AdityaLab/MM4TSA"},{"id":"http://arxiv.org/abs/2503.14833v1","updated":"2025-03-19T02:25:36Z","published":"2025-03-19T02:25:36Z","title":"Curiosity-Diffuser: Curiosity Guide Diffusion Models for Reliability","summary":"  One of the bottlenecks in robotic intelligence is the instability of neural\nnetwork models, which, unlike control models, lack a well-defined convergence\ndomain and stability. This leads to risks when applying intelligence in the\nphysical world. Specifically, imitation policy based on neural network may\ngenerate hallucinations, leading to inaccurate behaviors that impact the safety\nof real-world applications. To address this issue, this paper proposes the\nCuriosity-Diffuser, aimed at guiding the conditional diffusion model to\ngenerate trajectories with lower curiosity, thereby improving the reliability\nof policy. The core idea is to use a Random Network Distillation (RND)\ncuriosity module to assess whether the model's behavior aligns with the\ntraining data, and then minimize curiosity by classifier guidance diffusion to\nreduce overgeneralization during inference. Additionally, we propose a\ncomputationally efficient metric for evaluating the reliability of the policy,\nmeasuring the similarity between the generated behaviors and the training\ndataset, to facilitate research about reliability learning. Finally, simulation\nverify the effectiveness and applicability of the proposed method to a variety\nof scenarios, showing that Curiosity-Diffuser significantly improves task\nperformance and produces behaviors that are more similar to the training data.\nThe code for this work is available at: github.com/CarlDegio/Curiosity-Diffuser\n","authors":["Zihao Liu","Xing Liu","Yizhai Zhang","Zhengxiong Liu","Panfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2503.14833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14832v1","updated":"2025-03-19T02:24:43Z","published":"2025-03-19T02:24:43Z","title":"H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution\n  Detection","summary":"  Task Incremental Learning (TIL) is a specialized form of Continual Learning\n(CL) in which a model incrementally learns from non-stationary data streams.\nExisting TIL methodologies operate under the closed-world assumption, presuming\nthat incoming data remains in-distribution (ID). However, in an open-world\nsetting, incoming samples may originate from out-of-distribution (OOD) sources,\nwith their task identities inherently unknown. Continually detecting OOD\nsamples presents several challenges for current OOD detection methods: reliance\non model outputs leads to excessive dependence on model performance, selecting\nsuitable thresholds is difficult, hindering real-world deployment, and binary\nID/OOD classification fails to provide task-level identification. To address\nthese issues, we propose a novel continual OOD detection method called the\nHierarchical Two-sample Tests (H2ST). H2ST eliminates the need for threshold\nselection through hypothesis testing and utilizes feature maps to better\nexploit model capabilities without excessive dependence on model performance.\nThe proposed hierarchical architecture enables task-level detection with\nsuperior performance and lower overhead compared to non-hierarchical classifier\ntwo-sample tests. Extensive experiments and analysis validate the effectiveness\nof H2ST in open-world TIL scenarios and its superiority to the existing\nmethods. Code is available at\n\\href{https://github.com/YuhangLiuu/H2ST}{https://github.com/YuhangLiuu/H2ST}.\n","authors":["Yuhang Liu","Wenjie Zhao","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2503.14832v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.14831v1","updated":"2025-03-19T02:16:08Z","published":"2025-03-19T02:16:08Z","title":"Robust Transmission of Punctured Text with Large Language Model-based\n  Recovery","summary":"  With the recent advancements in deep learning, semantic communication which\ntransmits only task-oriented features, has rapidly emerged. However, since\nfeature extraction relies on learning-based models, its performance\nfundamentally depends on the training dataset or tasks. For practical\nscenarios, it is essential to design a model that demonstrates robust\nperformance regardless of dataset or tasks. In this correspondence, we propose\na novel text transmission model that selects and transmits only a few\ncharacters and recovers the missing characters at the receiver using a large\nlanguage model (LLM). Additionally, we propose a novel importance character\nextractor (ICE), which selects transmitted characters to enhance LLM recovery\nperformance. Simulations demonstrate that the proposed filter selection by ICE\noutperforms random filter selection, which selects transmitted characters\nrandomly. Moreover, the proposed model exhibits robust performance across\ndifferent datasets and tasks and outperforms traditional bit-based\ncommunication in low signal-to-noise ratio conditions.\n","authors":["Sojeong Park","Hyeonho Noh","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2503.14831v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.17497v2","updated":"2025-03-19T02:03:59Z","published":"2025-02-21T07:54:01Z","title":"Hard constraint learning approaches with trainable influence functions\n  for evolutionary equations","summary":"  This paper develops a novel deep learning approach for solving evolutionary\nequations, which integrates sequential learning strategies with an enhanced\nhard constraint strategy featuring trainable parameters, addressing the low\ncomputational accuracy of standard Physics-Informed Neural Networks (PINNs) in\nlarge temporal domains.Sequential learning strategies divide a large temporal\ndomain into multiple subintervals and solve them one by one in a chronological\norder, which naturally respects the principle of causality and improves the\nstability of the PINN solution. The improved hard constraint strategy strictly\nensures the continuity and smoothness of the PINN solution at time interval\nnodes, and at the same time passes the information from the previous interval\nto the next interval, which avoids the incorrect/trivial solution at the\nposition far from the initial time. Furthermore, by investigating the\nrequirements of different types of equations on hard constraints, we design a\nnovel influence function with trainable parameters for hard constraints, which\nprovides theoretical and technical support for the effective implementations of\nhard constraint strategies, and significantly improves the universality and\ncomputational accuracy of our method. In addition, an adaptive time-domain\npartitioning algorithm is proposed, which plays an important role in the\napplication of the proposed method as well as in the improvement of\ncomputational efficiency and accuracy. Numerical experiments verify the\nperformance of the method. The data and code accompanying this paper are\navailable at https://github.com/zhizhi4452/HCS.\n","authors":["Yushi Zhang","Shuai Su","Yong Wang","Yanzhong Yao"],"pdf_url":"https://arxiv.org/pdf/2502.17497v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04398v3","updated":"2025-03-19T02:03:39Z","published":"2025-03-06T12:52:22Z","title":"Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling","summary":"  MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.\n","authors":["Yan Li","Pengfei Zheng","Shuang Chen","Zewei Xu","Yuanhao Lai","Yunfei Du","Zhengang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04398v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14506v2","updated":"2025-03-19T01:46:48Z","published":"2024-08-24T15:36:36Z","title":"Distilling Long-tailed Datasets","summary":"  Dataset distillation aims to synthesize a small, information-rich dataset\nfrom a large one for efficient model training. However, existing dataset\ndistillation methods struggle with long-tailed datasets, which are prevalent in\nreal-world scenarios. By investigating the reasons behind this unexpected\nresult, we identified two main causes: 1) The distillation process on\nimbalanced datasets develops biased gradients, leading to the synthesis of\nsimilarly imbalanced distilled datasets. 2) The experts trained on such\ndatasets perform suboptimally on tail classes, resulting in misguided\ndistillation supervision and poor-quality soft-label initialization. To address\nthese issues, we first propose Distribution-agnostic Matching to avoid directly\nmatching the biased expert trajectories. It reduces the distance between the\nstudent and the biased expert trajectories and prevents the tail class bias\nfrom being distilled to the synthetic dataset. Moreover, we improve the\ndistillation guidance with Expert Decoupling, which jointly matches the\ndecoupled backbone and classifier to improve the tail class performance and\ninitialize reliable soft labels. This work pioneers the field of long-tailed\ndataset distillation, marking the first effective effort to distill long-tailed\ndatasets.\n","authors":["Zhenghao Zhao","Haoxuan Wang","Yuzhang Shang","Kai Wang","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2408.14506v2.pdf","comment":"CVPR 2025. Code is available at https://github.com/ichbill/LTDD"},{"id":"http://arxiv.org/abs/2503.09659v2","updated":"2025-03-19T01:40:55Z","published":"2025-03-12T12:31:07Z","title":"Edge AI for Real-time Fetal Assessment in Rural Guatemala","summary":"  Perinatal complications, defined as conditions that arise during pregnancy,\nchildbirth, and the immediate postpartum period, represent a significant burden\non maternal and neonatal health worldwide. Factors contributing to these\ndisparities include limited access to quality healthcare, socioeconomic\ninequalities, and variations in healthcare infrastructure. Addressing these\nissues is crucial for improving health outcomes for mothers and newborns,\nparticularly in underserved communities. To mitigate these challenges, we have\ndeveloped an AI-enabled smartphone application designed to provide decision\nsupport at the point-of-care. This tool aims to enhance health monitoring\nduring pregnancy by leveraging machine learning (ML) techniques. The intended\nuse of this application is to assist midwives during routine home visits by\noffering real-time analysis and providing feedback based on collected data. The\napplication integrates TensorFlow Lite (TFLite) and other Python-based\nalgorithms within a Kotlin framework to process data in real-time. It is\ndesigned for use in low-resource settings, where traditional healthcare\ninfrastructure may be lacking. The intended patient population includes\npregnant women and new mothers in underserved areas and the developed system\nwas piloted in rural Guatemala. This ML-based solution addresses the critical\nneed for accessible and quality perinatal care by empowering healthcare\nproviders with decision support tools to improve maternal and neonatal health\noutcomes.\n","authors":["Nasim Katebi","Mohammad Ahmad","Mohsen Motie-Shirazi","Daniel Phan","Ellen Kolesnikova","Sepideh Nikookar","Alireza Rafiei","Murali K. Korikana","Rachel Hall-Clifford","Esteban Castro","Rosibely Sut","Enma Coyote","Anahi Venzor Strader","Edlyn Ramos","Peter Rohloff","Reza Sameni","Gari D. Clifford"],"pdf_url":"https://arxiv.org/pdf/2503.09659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09956v2","updated":"2025-03-19T01:32:33Z","published":"2025-03-13T01:59:11Z","title":"DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless\n  Networks: A Survey","summary":"  Reinforcement learning (RL)-based large language models (LLMs), such as\nChatGPT, DeepSeek, and Grok-3, have gained significant attention for their\nexceptional capabilities in natural language processing and multimodal data\nunderstanding. Meanwhile, the rapid expansion of information services has\ndriven the growing need for intelligence, efficient, and adaptable wireless\nnetworks. Wireless networks require the empowerment of RL-based LLMs while\nthese models also benefit from wireless networks to broaden their application\nscenarios. Specifically, RL-based LLMs can enhance wireless communication\nsystems through intelligent resource allocation, adaptive network optimization,\nand real-time decision-making. Conversely, wireless networks provide a vital\ninfrastructure for the efficient training, deployment, and distributed\ninference of RL-based LLMs, especially in decentralized and edge computing\nenvironments. This mutual empowerment highlights the need for a deeper\nexploration of the interplay between these two domains. We first review recent\nadvancements in wireless communications, highlighting the associated challenges\nand potential solutions. We then discuss the progress of RL-based LLMs,\nfocusing on key technologies for LLM training, challenges, and potential\nsolutions. Subsequently, we explore the mutual empowerment between these two\nfields, highlighting key motivations, open challenges, and potential solutions.\nFinally, we provide insights into future directions, applications, and their\nsocietal impact to further explore this intersection, paving the way for\nnext-generation intelligent communication systems. Overall, this survey\nprovides a comprehensive overview of the relationship between RL-based LLMs and\nwireless networks, offering a vision where these domains empower each other to\ndrive innovations.\n","authors":["Yu Qiao","Phuong-Nam Tran","Ji Su Yoon","Loc X. Nguyen","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2503.09956v2.pdf","comment":"25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2503.14813v1","updated":"2025-03-19T01:01:28Z","published":"2025-03-19T01:01:28Z","title":"Scaled Supervision is an Implicit Lipschitz Regularizer","summary":"  In modern social media, recommender systems (RecSys) rely on the\nclick-through rate (CTR) as the standard metric to evaluate user engagement.\nCTR prediction is traditionally framed as a binary classification task to\npredict whether a user will interact with a given item. However, this approach\noverlooks the complexity of real-world social modeling, where the user, item,\nand their interactive features change dynamically in fast-paced online\nenvironments. This dynamic nature often leads to model instability, reflected\nin overfitting short-term fluctuations rather than higher-level interactive\npatterns. While overfitting calls for more scaled and refined supervisions,\ncurrent solutions often rely on binary labels that overly simplify fine-grained\nuser preferences through the thresholding process, which significantly reduces\nthe richness of the supervision. Therefore, we aim to alleviate the overfitting\nproblem by increasing the supervision bandwidth in CTR training. Specifically,\n(i) theoretically, we formulate the impact of fine-grained preferences on model\nstability as a Lipschitz constrain; (ii) empirically, we discover that scaling\nthe supervision bandwidth can act as an implicit Lipschitz regularizer, stably\noptimizing existing CTR models to achieve better generalizability. Extensive\nexperiments show that this scaled supervision significantly and consistently\nimproves the optimization process and the performance of existing CTR models,\neven without the need for additional hyperparameter tuning.\n","authors":["Zhongyu Ouyang","Chunhui Zhang","Yaning Jia","Soroush Vosoughi"],"pdf_url":"https://arxiv.org/pdf/2503.14813v1.pdf","comment":"Accepted to the International AAAI Conference on Web and Social Media\n  (ICWSM 2025)"},{"id":"http://arxiv.org/abs/2412.10782v2","updated":"2025-03-19T00:47:29Z","published":"2024-12-14T10:38:09Z","title":"ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient\n  PINNs learning","summary":"  In the recent years, Physics Informed Neural Networks (PINNs) have received\nstrong interest as a method to solve PDE driven systems, in particular for data\nassimilation purpose. This method is still in its infancy, with many\nshortcomings and failures that remain not properly understood. In this paper we\npropose a natural gradient approach to PINNs which contributes to speed-up and\nimprove the accuracy of the training. Based on an in depth analysis of the\ndifferential geometric structures of the problem, we come up with two distinct\ncontributions: (i) a new natural gradient algorithm that scales as $\\min(P^2S,\nS^2P)$, where $P$ is the number of parameters, and $S$ the batch size; (ii) a\nmathematically principled reformulation of the PINNs problem that allows the\nextension of natural gradient to it, with proved connections to Green's\nfunction theory.\n","authors":["Nilo Schwencke","Cyril Furtlehner"],"pdf_url":"https://arxiv.org/pdf/2412.10782v2.pdf","comment":"accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2503.14809v1","updated":"2025-03-19T00:44:23Z","published":"2025-03-19T00:44:23Z","title":"Learning with Expert Abstractions for Efficient Multi-Task Continuous\n  Control","summary":"  Decision-making in complex, continuous multi-task environments is often\nhindered by the difficulty of obtaining accurate models for planning and the\ninefficiency of learning purely from trial and error. While precise environment\ndynamics may be hard to specify, human experts can often provide high-fidelity\nabstractions that capture the essential high-level structure of a task and user\npreferences in the target environment. Existing hierarchical approaches often\ntarget discrete settings and do not generalize across tasks. We propose a\nhierarchical reinforcement learning approach that addresses these limitations\nby dynamically planning over the expert-specified abstraction to generate\nsubgoals to learn a goal-conditioned policy. To overcome the challenges of\nlearning under sparse rewards, we shape the reward based on the optimal state\nvalue in the abstract model. This structured decision-making process enhances\nsample efficiency and facilitates zero-shot generalization. Our empirical\nevaluation on a suite of procedurally generated continuous control environments\ndemonstrates that our approach outperforms existing hierarchical reinforcement\nlearning methods in terms of sample efficiency, task completion rate,\nscalability to complex tasks, and generalization to novel scenarios.\n","authors":["Jeff Jewett","Sandhya Saisubramanian"],"pdf_url":"https://arxiv.org/pdf/2503.14809v1.pdf","comment":"12 pages, 6 figures. Submitted to RLC 2025. Code and experiments at\n  https://github.com/Intelligent-Reliable-Autonomous-Systems/gcrs-expert-abstractions"},{"id":"http://arxiv.org/abs/2310.10948v2","updated":"2025-03-19T00:34:29Z","published":"2023-10-17T02:46:04Z","title":"Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL\n  for Coordinated Platooning and Traffic Signal Control","summary":"  Over the years, reinforcement learning has emerged as a popular approach to\ndevelop signal control and vehicle platooning strategies either independently\nor in a hierarchical way. However, jointly controlling both in real-time to\nalleviate traffic congestion presents new challenges, such as the inherent\nphysical and behavioral heterogeneity between signal control and platooning, as\nwell as coordination between them. This paper proposes an innovative solution\nto tackle these challenges based on heterogeneous graph multi-agent\nreinforcement learning and traffic theories. Our approach involves: 1)\ndesigning platoon and signal control as distinct reinforcement learning agents\nwith their own set of observations, actions, and reward functions to optimize\ntraffic flow; 2) designing coordination by incorporating graph neural networks\nwithin multi-agent reinforcement learning to facilitate seamless information\nexchange among agents on a regional scale; 3) applying alternating optimization\nfor training, allowing agents to update their own policies and adapt to other\nagents' policies. We evaluate our approach through SUMO simulations, which show\nconvergent results in terms of both travel time and fuel consumption, and\nsuperior performance compared to other adaptive signal control methods.\n","authors":["Xianyue Peng","Hang Gao","Shenyang Chen","Hao Wang","H. Michael Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.10948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14800v1","updated":"2025-03-19T00:24:01Z","published":"2025-03-19T00:24:01Z","title":"Long Context Modeling with Ranked Memory-Augmented Retrieval","summary":"  Effective long-term memory management is crucial for language models handling\nextended contexts. We introduce a novel framework that dynamically ranks memory\nentries based on relevance. Unlike previous works, our model introduces a novel\nrelevance scoring and a pointwise re-ranking model for key-value embeddings,\ninspired by learning-to-rank techniques in information retrieval. Enhanced\nRanked Memory Augmented Retrieval ERMAR achieves state-of-the-art results on\nstandard benchmarks.\n","authors":["Ghadir Alselwi","Hao Xue","Shoaib Jameel","Basem Suleiman","Flora D. Salim","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2503.14800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14799v1","updated":"2025-03-19T00:18:37Z","published":"2025-03-19T00:18:37Z","title":"Pruning-Based TinyML Optimization of Machine Learning Models for Anomaly\n  Detection in Electric Vehicle Charging Infrastructure","summary":"  With the growing need for real-time processing on IoT devices, optimizing\nmachine learning (ML) models' size, latency, and computational efficiency is\nessential. This paper investigates a pruning method for anomaly detection in\nresource-constrained environments, specifically targeting Electric Vehicle\nCharging Infrastructure (EVCI). Using the CICEVSE2024 dataset, we trained and\noptimized three models-Multi-Layer Perceptron (MLP), Long Short-Term Memory\n(LSTM), and XGBoost-through hyperparameter tuning with Optuna, further refining\nthem using SHapley Additive exPlanations (SHAP)-based feature selection (FS)\nand unstructured pruning techniques. The optimized models achieved significant\nreductions in model size and inference times, with only a marginal impact on\ntheir performance. Notably, our findings indicate that, in the context of EVCI,\npruning and FS can enhance computational efficiency while retaining critical\nanomaly detection capabilities.\n","authors":["Fatemeh Dehrouyeh","Ibrahim Shaer","Soodeh Nikan","Firouz Badrkhani Ajaei","Abdallah Shami"],"pdf_url":"https://arxiv.org/pdf/2503.14799v1.pdf","comment":"This paper has been accepted for presentation at IEEE ICC 2025. The\n  final published version will be available in the conference proceedings. The\n  implementation and code are available at:\n  https://github.com/Western-OC2-Lab/EVCI-Pruning"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.15237v1","updated":"2025-03-19T14:14:57Z","published":"2025-03-19T14:14:57Z","title":"QuMATL: Query-based Multi-annotator Tendency Learning","summary":"  Different annotators often assign different labels to the same sample due to\nbackgrounds or preferences, and such labeling patterns are referred to as\ntendency. In multi-annotator scenarios, we introduce a novel task called\nMulti-annotator Tendency Learning (MATL), which aims to capture each annotator\ntendency. Unlike traditional tasks that prioritize consensus-oriented learning,\nwhich averages out annotator differences and leads to tendency information\nloss, MATL emphasizes learning each annotator tendency, better preserves\ntendency information. To this end, we propose an efficient baseline method,\nQuery-based Multi-annotator Tendency Learning (QuMATL), which uses lightweight\nquery to represent each annotator for tendency modeling. It saves the costs of\nbuilding separate conventional models for each annotator, leverages shared\nlearnable queries to capture inter-annotator correlations as an additional\nhidden supervisory signal to enhance modeling performance. Meanwhile, we\nprovide a new metric, Difference of Inter-annotator Consistency (DIC), to\nevaluate how effectively models preserve annotators tendency information.\nAdditionally, we contribute two large-scale datasets, STREET and AMER,\nproviding averages of 4300 and 3118 per-annotator labels, respectively.\nExtensive experiments verified the effectiveness of our QuMATL.\n","authors":["Liyun Zhang","Zheng Lian","Hong Liu","Takanori Takebe","Yuta Nakashima"],"pdf_url":"https://arxiv.org/pdf/2503.15237v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2503.15166v1","updated":"2025-03-19T12:47:37Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.15001v1","updated":"2025-03-19T08:52:04Z","published":"2025-03-19T08:52:04Z","title":"Low-Complexity Patch-based No-Reference Point Cloud Quality Metric\n  exploiting Weighted Structure and Texture Features","summary":"  During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA.\n","authors":["Michael Neri","Federica Battisti"],"pdf_url":"https://arxiv.org/pdf/2503.15001v1.pdf","comment":"Accepted for publication in IEEE Transactions on Broadcasting. Code\n  at https://github.com/michaelneri/PST-PCQA"},{"id":"http://arxiv.org/abs/2503.00374v2","updated":"2025-03-19T02:50:30Z","published":"2025-03-01T07:02:30Z","title":"MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning\n  via Modality Alignment and Retention","summary":"  Histopathology and transcriptomics are fundamental modalities in oncology,\nencapsulating the morphological and molecular aspects of the disease.\nMulti-modal self-supervised learning has demonstrated remarkable potential in\nlearning pathological representations by integrating diverse data sources.\nConventional multi-modal integration methods primarily emphasize modality\nalignment, while paying insufficient attention to retaining the\nmodality-specific structures. However, unlike conventional scenarios where\nmulti-modal inputs share highly overlapping features, histopathology and\ntranscriptomics exhibit pronounced heterogeneity, offering orthogonal yet\ncomplementary insights. Histopathology provides morphological and spatial\ncontext, elucidating tissue architecture and cellular topology, whereas\ntranscriptomics delineates molecular signatures through gene expression\npatterns. This inherent disparity introduces a major challenge in aligning them\nwhile maintaining modality-specific fidelity. To address these challenges, we\npresent MIRROR, a novel multi-modal representation learning method designed to\nfoster both modality alignment and retention. MIRROR employs dedicated encoders\nto extract comprehensive features for each modality, which is further\ncomplemented by a modality alignment module to achieve seamless integration\nbetween phenotype patterns and molecular profiles. Furthermore, a modality\nretention module safeguards unique attributes from each modality, while a style\nclustering module mitigates redundancy and enhances disease-relevant\ninformation by modeling and aligning consistent pathological signatures within\na clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping\nand survival analysis highlight MIRROR's superior performance, demonstrating\nits effectiveness in constructing comprehensive oncological feature\nrepresentations and benefiting the cancer diagnosis.\n","authors":["Tianyi Wang","Jianan Fan","Dingxin Zhang","Dongnan Liu","Yong Xia","Heng Huang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2503.00374v2.pdf","comment":"10 pages, 5 figures, 4 tables. Code available at\n  https://github.com/TianyiFranklinWang/MIRROR. Project page:\n  https://tianyifranklinwang.github.io/MIRROR"},{"id":"http://arxiv.org/abs/2503.15621v1","updated":"2025-03-19T18:10:12Z","published":"2025-03-19T18:10:12Z","title":"LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for\n  Enhanced Visual Instruction Tuning","summary":"  Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.\n","authors":["Federico Cocchi","Nicholas Moratelli","Davide Caffagni","Sara Sarto","Lorenzo Baraldi","Marcella Cornia","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.15621v1.pdf","comment":null}]},"2025-03-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.16428v1","updated":"2025-03-20T17:59:58Z","published":"2025-03-20T17:59:58Z","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring","summary":"  Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.\n","authors":["Ruyi Xu","Guangxuan Xiao","Haofeng Huang","Junxian Guo","Song Han"],"pdf_url":"https://arxiv.org/pdf/2503.16428v1.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2503.16419v1","updated":"2025-03-20T17:59:38Z","published":"2025-03-20T17:59:38Z","title":"Stop Overthinking: A Survey on Efficient Reasoning for Large Language\n  Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking.\n","authors":["Yang Sui","Yu-Neng Chuang","Guanchu Wang","Jiamu Zhang","Tianyi Zhang","Jiayi Yuan","Hongyi Liu","Andrew Wen"," Shaochen"," Zhong","Hanjie Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2503.16419v1.pdf","comment":"Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs"},{"id":"http://arxiv.org/abs/2503.16416v1","updated":"2025-03-20T17:59:23Z","published":"2025-03-20T17:59:23Z","title":"Survey on Evaluation of LLM-based Agents","summary":"  The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.\n","authors":["Asaf Yehudai","Lilach Eden","Alan Li","Guy Uziel","Yilun Zhao","Roy Bar-Haim","Arman Cohan","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2503.16416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15242v2","updated":"2025-03-20T17:58:17Z","published":"2025-03-19T14:19:57Z","title":"BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space\n  Complexity?","summary":"  We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time.\n","authors":["Pierre Chambon","Baptiste Roziere","Benoit Sagot","Gabriel Synnaeve"],"pdf_url":"https://arxiv.org/pdf/2503.15242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18908v2","updated":"2025-03-20T17:56:05Z","published":"2024-07-26T17:59:09Z","title":"Wolf: Dense Video Captioning with a World Summarization Framework","summary":"  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/.\n","authors":["Boyi Li","Ligeng Zhu","Ran Tian","Shuhan Tan","Yuxiao Chen","Yao Lu","Yin Cui","Sushant Veer","Max Ehrlich","Jonah Philion","Xinshuo Weng","Fuzhao Xue","Linxi Fan","Yuke Zhu","Jan Kautz","Andrew Tao","Ming-Yu Liu","Sanja Fidler","Boris Ivanovic","Trevor Darrell","Jitendra Malik","Song Han","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2407.18908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16402v1","updated":"2025-03-20T17:55:04Z","published":"2025-03-20T17:55:04Z","title":"The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination","summary":"  Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.\n","authors":["Yifan Sun","Han Wang","Dongbai Li","Gang Wang","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16402v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2503.16394v1","updated":"2025-03-20T17:53:12Z","published":"2025-03-20T17:53:12Z","title":"Do Visual Imaginations Improve Vision-and-Language Navigation Agents?","summary":"  Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/.\n","authors":["Akhil Perincherry","Jacob Krantz","Stefan Lee"],"pdf_url":"https://arxiv.org/pdf/2503.16394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01482v4","updated":"2025-03-20T17:39:10Z","published":"2024-09-02T22:17:18Z","title":"Masked Mixers for Language Generation and Retrieval","summary":"  Attention mechanisms that confer selective focus on a strict subset of input\nelements are nearly ubiquitous in language models today. We posit there to be\ndownside to the use of attention: most input information is lost. In support of\nthis idea we observe poor input representation accuracy in transformers and\nmore accurate representation in what we term masked mixers, which replace\nself-attention with masked convolutions. The masked mixer learns causal\nlanguage modeling more efficiently than early transformer implementations and\neven outperforms optimized, current transformers when training on small\n($n_{ctx}<512$) but not larger context windows. Evidence is presented for the\nhypothesis that differences in transformer and masked mixer training\nefficiencies for various tasks are best predicted by input representation\naccuracy, or equivalently global invertibility. We hypothesize that the\ninformation loss exhibited by transformers would be more detrimental to\nretrieval than generation, as the former is more closely approximated by a\nbijective and thus invertible function. We find that masked mixers are more\neffective retrieval models both when the pretrained embedding model is\nunchanged as well as when the embedding model is modified via cosine\nsimilarity-based InfoNCE loss minimization. A small masked mixer is shown to\noutperform a large and near state-of-the-art transformer-based retrieval model,\ndespite the latter being trained with many orders of magnitude more data and\ncompute.\n","authors":["Benjamin L. Badger"],"pdf_url":"https://arxiv.org/pdf/2409.01482v4.pdf","comment":"31 pages, 9 figures, 4 tables, 14 supplementary figures, 10\n  supplementary tables"},{"id":"http://arxiv.org/abs/2411.02344v2","updated":"2025-03-20T17:37:44Z","published":"2024-11-04T18:14:07Z","title":"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations\n  for Enhanced Reasoning","summary":"  Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.\n","authors":["Md Rifat Arefin","Gopeshh Subbaraj","Nicolas Gontier","Yann LeCun","Irina Rish","Ravid Shwartz-Ziv","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2411.02344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11190v2","updated":"2025-03-20T17:20:55Z","published":"2025-02-16T16:31:00Z","title":"ReLearn: Unlearning via Learning for Large Language Models","summary":"  Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.\n","authors":["Haoming Xu","Ningyuan Zhao","Liming Yang","Sendong Zhao","Shumin Deng","Mengru Wang","Bryan Hooi","Nay Oo","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11190v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.16356v1","updated":"2025-03-20T17:14:34Z","published":"2025-03-20T17:14:34Z","title":"CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners","summary":"  Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.\n","authors":["Yunzhi Yao","Jizhan Fang","Jia-Chen Gu","Ningyu Zhang","Shumin Deng","Huajun Chen","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2503.16356v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.16334v1","updated":"2025-03-20T16:55:26Z","published":"2025-03-20T16:55:26Z","title":"LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates","summary":"  Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications.\n","authors":["Ying Shen","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2503.16334v1.pdf","comment":"16 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.12509v4","updated":"2025-03-20T16:45:57Z","published":"2025-02-18T03:47:53Z","title":"LegalCore: A Dataset for Event Coreference Resolution in Legal Documents","summary":"  Recognizing events and their coreferential mentions in a document is\nessential for understanding semantic meanings of text. The existing research on\nevent coreference resolution is mostly limited to news articles. In this paper,\nwe present the first dataset for the legal domain, LegalCore, which has been\nannotated with comprehensive event and event coreference information. The legal\ncontract documents we annotated in this dataset are several times longer than\nnews articles, with an average length of around 25k tokens per document. The\nannotations show that legal documents have dense event mentions and feature\nboth short-distance and super long-distance coreference links between event\nmentions. We further benchmark mainstream Large Language Models (LLMs) on this\ndataset for both event detection and event coreference resolution tasks, and\nfind that this dataset poses significant challenges for state-of-the-art\nopen-source and proprietary LLMs, which perform significantly worse than a\nsupervised baseline. We will publish the dataset as well as the code.\n","authors":["Kangda Wei","Xi Shi","Jonathan Tong","Sai Ramana Reddy","Anandhavelu Natarajan","Rajiv Jain","Aparna Garimella","Ruihong Huang"],"pdf_url":"https://arxiv.org/pdf/2502.12509v4.pdf","comment":"Need company internal approval before public release"},{"id":"http://arxiv.org/abs/2404.18400v3","updated":"2025-03-20T16:37:17Z","published":"2024-04-29T03:30:06Z","title":"LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models","summary":"  Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR\n","authors":["Parshin Shojaee","Kazem Meidani","Shashank Gupta","Amir Barati Farimani","Chandan K Reddy"],"pdf_url":"https://arxiv.org/pdf/2404.18400v3.pdf","comment":"ICLR 2025 Oral"},{"id":"http://arxiv.org/abs/2412.09165v3","updated":"2025-03-20T16:15:29Z","published":"2024-12-12T10:50:26Z","title":"When Text Embedding Meets Large Language Model: A Comprehensive Survey","summary":"  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.\n","authors":["Zhijie Nie","Zhangchi Feng","Mingxin Li","Cunwang Zhang","Yanzhao Zhang","Dingkun Long","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09165v3.pdf","comment":"Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders"},{"id":"http://arxiv.org/abs/2503.16252v1","updated":"2025-03-20T15:46:18Z","published":"2025-03-20T15:46:18Z","title":"Fin-R1: A Large Language Model for Financial Reasoning through\n  Reinforcement Learning","summary":"  Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1.\n","authors":["Zhaowei Liu","Xin Guo","Fangqi Lou","Lingfeng Zeng","Jinyi Niu","Zixuan Wang","Jiajie Xu","Weige Cai","Ziwei Yang","Xueqian Zhao","Chao Li","Sheng Xu","Dezhi Chen","Yun Chen","Zuo Bai","Liwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20089v2","updated":"2025-03-20T15:28:18Z","published":"2024-09-30T08:41:39Z","title":"Robust LLM safeguarding via refusal feature adversarial training","summary":"  Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.\n","authors":["Lei Yu","Virginie Do","Karen Hambardzumyan","Nicola Cancedda"],"pdf_url":"https://arxiv.org/pdf/2409.20089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12318v3","updated":"2025-03-20T15:13:26Z","published":"2024-12-16T19:35:55Z","title":"Graph-Guided Textual Explanation Generation Framework","summary":"  Natural language explanations (NLEs) are commonly used to provide plausible\nfree-text explanations of a model's reasoning about its predictions. However,\nrecent work has questioned their faithfulness, as they may not accurately\nreflect the model's internal reasoning process regarding its predicted answer.\nIn contrast, highlight explanations--input fragments critical for the model's\npredicted answers--exhibit measurable faithfulness. Building on this\nfoundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation\nframework designed to enhance the faithfulness of NLEs. Specifically, highlight\nexplanations are first extracted as faithful cues reflecting the model's\nreasoning logic toward answer prediction. They are subsequently encoded through\na graph neural network layer to guide the NLE generation, which aligns the\ngenerated explanations with the model's underlying reasoning toward the\npredicted answer. Experiments on T5 and BART using three reasoning datasets\nshow that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline\nmethods. Additionally, G-Tex generates NLEs with greater semantic and lexical\nsimilarity to human-written ones. Human evaluations show that G-Tex can\ndecrease redundant content and enhance the overall quality of NLEs. Our work\npresents a novel method for explicitly guiding NLE generation to enhance\nfaithfulness, serving as a foundation for addressing broader criteria in NLE\nand generated text.\n","authors":["Shuzhou Yuan","Jingyi Sun","Ran Zhang","Michael Färber","Steffen Eger","Pepa Atanasova","Isabelle Augenstein"],"pdf_url":"https://arxiv.org/pdf/2412.12318v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16219v1","updated":"2025-03-20T15:13:23Z","published":"2025-03-20T15:13:23Z","title":"Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't","summary":"  Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.\n","authors":["Quy-Anh Dang","Chris Ngo"],"pdf_url":"https://arxiv.org/pdf/2503.16219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14258v2","updated":"2025-03-20T15:09:51Z","published":"2025-03-18T13:48:18Z","title":"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System","summary":"  This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.\n","authors":["Weihang Su","Baoqing Yue","Qingyao Ai","Yiran Hu","Jiaqi Li","Changyue Wang","Kaiyuan Zhang","Yueyue Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07058v3","updated":"2025-03-20T15:01:11Z","published":"2025-02-10T21:49:35Z","title":"Using Contextually Aligned Online Reviews to Measure LLMs' Performance\n  Disparities Across Language Varieties","summary":"  A language can have different varieties. These varieties can affect the\nperformance of natural language processing (NLP) models, including large\nlanguage models (LLMs), which are often trained on data from widely spoken\nvarieties. This paper introduces a novel and cost-effective approach to\nbenchmark model performance across language varieties. We argue that\ninternational online review platforms, such as Booking.com, can serve as\neffective data sources for constructing datasets that capture comments in\ndifferent language varieties from similar real-world scenarios, like reviews\nfor the same hotel with the same rating using the same language (e.g., Mandarin\nChinese) but different language varieties (e.g., Taiwan Mandarin, Mainland\nMandarin). To prove this concept, we constructed a contextually aligned dataset\ncomprising reviews in Taiwan Mandarin and Mainland Mandarin and tested six LLMs\nin a sentiment analysis task. Our results show that LLMs consistently\nunderperform in Taiwan Mandarin.\n","authors":["Zixin Tang","Chieh-Yang Huang","Tsung-Che Li","Ho Yin Sam Ng","Hen-Hsen Huang","Ting-Hao 'Kenneth' Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07058v3.pdf","comment":"Accepted by 2025 Annual Conference of the Nations of the Americas\n  Chapter of the Association for Computational Linguistics (NAACL), theme track"},{"id":"http://arxiv.org/abs/2503.16212v1","updated":"2025-03-20T15:00:41Z","published":"2025-03-20T15:00:41Z","title":"MathFusion: Enhancing Mathematic Problem-solving of LLM through\n  Instruction Fusion","summary":"  Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion.\n","authors":["Qizhi Pei","Lijun Wu","Zhuoshi Pan","Yu Li","Honglin Lin","Chenlin Ming","Xin Gao","Conghui He","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2503.16212v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2411.16730v4","updated":"2025-03-20T14:48:10Z","published":"2024-11-23T09:32:44Z","title":"\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks","summary":"  As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.\n","authors":["Libo Wang"],"pdf_url":"https://arxiv.org/pdf/2411.16730v4.pdf","comment":"This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2503.16184v1","updated":"2025-03-20T14:35:46Z","published":"2025-03-20T14:35:46Z","title":"Accurate Scene Text Recognition with Efficient Model Scaling and Cloze\n  Self-Distillation","summary":"  Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs.\n","authors":["Andrea Maracani","Savas Ozkan","Sijun Cho","Hyowon Kim","Eunchung Noh","Jeongwon Min","Cho Jung Min","Dookun Park","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.16184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15451v2","updated":"2025-03-20T14:10:27Z","published":"2025-02-21T13:25:00Z","title":"Binary-Integer-Programming Based Algorithm for Expert Load Balancing in\n  Mixture-of-Experts Models","summary":"  For pre-training of MoE (Mixture-of-Experts) models, one of the main issues\nis unbalanced expert loads, which may cause routing collapse or increased\ncomputational overhead. Existing methods contain the Loss-Controlled method and\nthe Loss-Free method, where both the unbalanced degrees at first several\ntraining steps are still high and decrease slowly. In this work, we propose\nBIP-Based Balancing, an expert load balancing algorithm based on binary integer\nprogramming (BIP). The algorithm maintains an additional vector q on each MoE\nlayer that can help change the top-K order of s by solving a binary integer\nprogramming with very small time costs. We implement the algorithm on two MoE\nlanguage models: 16-expert (0.3B) and 64-expert (1.1B). The experimental\nresults show that on both models comparing with the Loss-Controlled method and\nthe Loss-Free method, our algorithm trains models with the lowest perplexities,\nwhile saves at least 13% of pre-training time compared with the Loss-Controlled\nmethod. Within our current knowledge, this is the first routing algorithm that\nachieves maintaining load balance status on every expert in every MoE layer\nfrom the first step to the last step during the whole pre-training process,\nwhile the trained MoE models also perform well. The code material of this work\nis available at https://github.com/sunyuanLLM/bip_routing_algorithm.\n","authors":["Yuan Sun"],"pdf_url":"https://arxiv.org/pdf/2502.15451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16167v1","updated":"2025-03-20T14:07:31Z","published":"2025-03-20T14:07:31Z","title":"CodeReviewQA: The Code Review Comprehension Assessment for Large\n  Language Models","summary":"  State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results.\n","authors":["Hong Yi Lin","Chunhua Liu","Haoyu Gao","Patanamon Thongtanunam","Christoph Treude"],"pdf_url":"https://arxiv.org/pdf/2503.16167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16163v1","updated":"2025-03-20T14:01:56Z","published":"2025-03-20T14:01:56Z","title":"SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs","summary":"  Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.\n","authors":["Shibo Jie","Yehui Tang","Kai Han","Zhi-Hong Deng","Jing Han"],"pdf_url":"https://arxiv.org/pdf/2503.16163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16161v1","updated":"2025-03-20T13:58:32Z","published":"2025-03-20T13:58:32Z","title":"Towards Lighter and Robust Evaluation for Retrieval Augmented Generation","summary":"  Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment.\n","authors":["Alex-Razvan Ispas","Charles-Elie Simon","Fabien Caspani","Vincent Guigue"],"pdf_url":"https://arxiv.org/pdf/2503.16161v1.pdf","comment":"17 pages, 5 figures, published at 1st workshop of Quantify\n  Uncertainty and Hallucination in Foundation Models: The Next Frontier in\n  Reliable AI at ICLR 25"},{"id":"http://arxiv.org/abs/2503.16158v1","updated":"2025-03-20T13:56:15Z","published":"2025-03-20T13:56:15Z","title":"Automatically Generating Chinese Homophone Words to Probe Machine\n  Translation Estimation Systems","summary":"  Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research.\n","authors":["Shenbin Qian","Constantin Orăsan","Diptesh Kanojia","Félix do Carmo"],"pdf_url":"https://arxiv.org/pdf/2503.16158v1.pdf","comment":"Accepted to the 10th Workshop on Noisy and User-generated Text at\n  NAACL 2025"},{"id":"http://arxiv.org/abs/2503.16148v1","updated":"2025-03-20T13:51:06Z","published":"2025-03-20T13:51:06Z","title":"Only a Little to the Left: A Theory-grounded Measure of Political Bias\n  in Large Language Models","summary":"  Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings and most research\nrelies on constrained-answer settings to extract model responses. Moreover, the\nPolitical Compass Test is not a scientifically valid survey instrument. In this\nwork, we contribute a political bias measured informed by political science\ntheory, building on survey design principles to test a wide variety of input\nprompts, while taking into account prompt sensitivity. We then prompt 11\ndifferent open and commercial models, differentiating between instruction-tuned\nand non-instruction-tuned models, and automatically classify their political\nstances from 88,110 responses. Leveraging this dataset, we compute political\nbias profiles across different prompt variations and find that while PCT\nexaggerates bias in certain models like GPT3.5, measures of political bias are\noften unstable, but generally more left-leaning for instruction-tuned models.\n","authors":["Mats Faulborn","Indira Sen","Max Pellert","Andreas Spitz","David Garcia"],"pdf_url":"https://arxiv.org/pdf/2503.16148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06759v4","updated":"2025-03-20T13:46:48Z","published":"2025-02-10T18:38:57Z","title":"Rationalization Models for Text-to-SQL","summary":"  We introduce a framework for generating Chain-of-Thought (CoT) rationales to\nenhance text-to-SQL model fine-tuning. These rationales consist of intermediate\nSQL statements and explanations, serving as incremental steps toward\nconstructing the final SQL query. The process begins with manually annotating a\nsmall set of examples, which are then used to prompt a large language model in\nan iterative, dynamic few-shot knowledge distillation procedure from a teacher\nmodel. A rationalization model is subsequently trained on the validated\ndecomposed queries, enabling extensive synthetic CoT annotations for\ntext-to-SQL datasets. To evaluate the approach, we fine-tune small language\nmodels with and without these rationales on the BIRD dataset. Results indicate\nthat step-by-step query generation improves execution accuracy, especially for\nmoderately and highly complex queries, while also enhancing explainability.\n","authors":["Gaetano Rossiello","Nhan Pham","Michael Glass","Junkyu Lee","Dharmashankar Subramanian"],"pdf_url":"https://arxiv.org/pdf/2502.06759v4.pdf","comment":"Published at ICLR 2025 Workshop on Reasoning and Planning for LLMs"},{"id":"http://arxiv.org/abs/2403.03029v2","updated":"2025-03-20T13:43:29Z","published":"2024-03-05T15:05:06Z","title":"Socratic Reasoning Improves Positive Text Rewriting","summary":"  Reframing a negative into a positive thought is at the crux of several\ncognitive approaches to mental health and psychotherapy that could be made more\naccessible by large language model-based solutions. Such reframing is typically\nnon-trivial and requires multiple rationalization steps to uncover the\nunderlying issue of a negative thought and transform it to be more positive.\nHowever, this rationalization process is currently neglected by both datasets\nand models which reframe thoughts in one step. In this work, we address this\ngap by augmenting open-source datasets for positive text rewriting with\nsynthetically-generated Socratic rationales using a novel framework called\n\\textsc{SocraticReframe}. SocraticReframe uses a sequence of question-answer\npairs to rationalize the thought rewriting process. We show that such Socratic\nrationales significantly improve positive text rewriting for different\nopen-source LLMs according to both automatic and human evaluations guided by\ncriteria from psychotherapy research. We validate our framework and the\nsynthetic rationalizations with expert judgements from domain experts and\npsychology students in an IRB-approved annotation study. Our findings highlight\nthe potential of utilizing the synergy between LLM reasoning and established\npsychotherapy techniques to build assistive solutions for reframing negative\nthoughts.\n","authors":["Anmol Goel","Nico Daheim","Christian Montag","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2403.03029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16131v1","updated":"2025-03-20T13:25:03Z","published":"2025-03-20T13:25:03Z","title":"MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering","summary":"  Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.\n","authors":["Feiyang Li","Yingjian Chen","Haoran Liu","Rui Yang","Han Yuan","Yuang Jiang","Tianxiao Li","Edison Marrese Taylor","Hossein Rouhizadeh","Yusuke Iwasawa","Douglas Teodoro","Yutaka Matsuo","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2503.16131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16094v1","updated":"2025-03-20T12:34:01Z","published":"2025-03-20T12:34:01Z","title":"Cultural Alignment in Large Language Models Using Soft Prompt Tuning","summary":"  Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances.\n","authors":["Reem I. Masoud","Martin Ferianc","Philip Treleaven","Miguel Rodrigues"],"pdf_url":"https://arxiv.org/pdf/2503.16094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16072v1","updated":"2025-03-20T12:09:01Z","published":"2025-03-20T12:09:01Z","title":"Redefining Toxicity: An Objective and Context-Aware Approach for\n  Stress-Level-Based Detection","summary":"  The fundamental problem of toxicity detection lies in the fact that the term\n\"toxicity\" is ill-defined. Such uncertainty causes researchers to rely on\nsubjective and vague data during model training, which leads to non-robust and\ninaccurate results, following the 'garbage in - garbage out' paradigm. This\nstudy introduces a novel, objective, and context-aware framework for toxicity\ndetection, leveraging stress levels as a key determinant of toxicity. We\npropose new definition, metric and training approach as a parts of our\nframework and demonstrate it's effectiveness using a dataset we collected.\n","authors":["Sergey Berezin","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2503.16072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11624v4","updated":"2025-03-20T12:06:17Z","published":"2024-06-17T15:07:55Z","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","summary":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","authors":["Omer Sahin Tas","Royden Wagner"],"pdf_url":"https://arxiv.org/pdf/2406.11624v4.pdf","comment":"ICLR 2025 camera-ready. Our implementation is available at\n  \\href{https://github.com/kit-mrt/future-motion}{this https URL}"},{"id":"http://arxiv.org/abs/2503.16071v1","updated":"2025-03-20T12:04:40Z","published":"2025-03-20T12:04:40Z","title":"Tuning LLMs by RAG Principles: Towards LLM-native Memory","summary":"  Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.\n","authors":["Jiale Wei","Shuchi Wu","Ruochen Liu","Xiang Ying","Jingbo Shang","Fangbo Tao"],"pdf_url":"https://arxiv.org/pdf/2503.16071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16063v1","updated":"2025-03-20T11:56:14Z","published":"2025-03-20T11:56:14Z","title":"Two-stage Incomplete Utterance Rewriting on Editing Operation","summary":"  Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly.\n","authors":["Zhiyu Cao","Peifeng Li","Qiaoming Zhu","Yaxin Fan"],"pdf_url":"https://arxiv.org/pdf/2503.16063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19772v3","updated":"2025-03-20T11:55:30Z","published":"2024-11-29T15:18:06Z","title":"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos","summary":"  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n","authors":["Tiantian Geng","Jinrui Zhang","Qingni Wang","Teng Wang","Jinming Duan","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.19772v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.16048v1","updated":"2025-03-20T11:33:59Z","published":"2025-03-20T11:33:59Z","title":"Meta-Learning Neural Mechanisms rather than Bayesian Priors","summary":"  Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms.\n","authors":["Michael Goodale","Salvador Mascarenhas","Yair Lakretz"],"pdf_url":"https://arxiv.org/pdf/2503.16048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15220v2","updated":"2025-03-20T11:33:29Z","published":"2025-03-19T14:00:55Z","title":"Entity-aware Cross-lingual Claim Detection for Automated Fact-checking","summary":"  Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite significant progress in the task, there remain open\nchallenges such as dealing with multilingual and multimodal data prevalent in\nonline discourse. Addressing the multilingual challenge, recent efforts have\nfocused on fine-tuning pre-trained multilingual language models. While these\nmodels can handle multiple languages, their ability to effectively transfer\ncross-lingual knowledge for detecting claims spreading on social media remains\nunder-explored. In this paper, we introduce EX-Claim, an entity-aware\ncross-lingual claim detection model that generalizes well to handle claims\nwritten in any language. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model significantly outperforms the\nbaselines, across 27 languages, and achieves the highest rate of knowledge\ntransfer, even with limited training data.\n","authors":["Rrubaa Panchendrarajan","Arkaitz Zubiaga"],"pdf_url":"https://arxiv.org/pdf/2503.15220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01478v5","updated":"2025-03-20T11:28:41Z","published":"2025-03-03T12:37:34Z","title":"SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction","summary":"  Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.\n","authors":["Lu Dai","Yijie Xu","Jinhui Ye","Hao Liu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.01478v5.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2503.16043v1","updated":"2025-03-20T11:26:46Z","published":"2025-03-20T11:26:46Z","title":"Incomplete Utterance Rewriting with Editing Operation Guidance and\n  Utterance Augmentation","summary":"  Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR.\n","authors":["Zhiyu Cao","Peifeng Li","Yaxin Fan","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.16043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16040v1","updated":"2025-03-20T11:14:39Z","published":"2025-03-20T11:14:39Z","title":"Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1,\n  DeepSeek-R1, and Beyond","summary":"  Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped.\n","authors":["Yaoyao Yu","Leilei Gan","Yinghao Hu","Bin Wei","Kun Kuang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16040v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16036v1","updated":"2025-03-20T11:09:18Z","published":"2025-03-20T11:09:18Z","title":"Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models","summary":"  Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.\n","authors":["Zhihang Liu","Chen-Wei Xie","Pandeng Li","Liming Zhao","Longxiang Tang","Yun Zheng","Chuanbin Liu","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2503.16036v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.16031v1","updated":"2025-03-20T10:58:02Z","published":"2025-03-20T10:58:02Z","title":"Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging\n  Fabricated Claims with Humorous Content","summary":"  This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models.\n","authors":["Sai Kartheek Reddy Kasu","Shankar Biradar","Sunil Saumya"],"pdf_url":"https://arxiv.org/pdf/2503.16031v1.pdf","comment":"15 Pages, 4 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.16182v2","updated":"2025-03-20T10:52:45Z","published":"2025-02-22T10:59:11Z","title":"IPO: Your Language Model is Secretly a Preference Classifier","summary":"  Reinforcement learning from human feedback (RLHF) has emerged as the primary\nmethod for aligning large language models (LLMs) with human preferences. While\nit enables LLMs to achieve human-level alignment, it often incurs significant\ncomputational and financial costs due to its reliance on training external\nreward models or human-labeled preferences. In this work, we propose Implicit\nPreference Optimization (IPO), an alternative approach that leverages\ngenerative LLMs as preference classifiers, thereby reducing the dependence on\nexternal human feedback or reward models to obtain preferences. We conduct a\ncomprehensive evaluation on the preference classification ability of LLMs using\nRewardBench, assessing models across different sizes, architectures, and\ntraining levels to validate our hypothesis. Furthermore, we investigate the\nself-improvement capabilities of LLMs by generating multiple responses for a\ngiven instruction and employing the model itself as a preference classifier for\nDirect Preference Optimization (DPO)-based training. Our findings demonstrate\nthat models trained through IPO achieve performance comparable to those\nutilizing state-of-the-art reward models for obtaining preferences.\n","authors":["Shivank Garg","Ayush Singh","Shweta Singh","Paras Chopra"],"pdf_url":"https://arxiv.org/pdf/2502.16182v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16024v1","updated":"2025-03-20T10:42:33Z","published":"2025-03-20T10:42:33Z","title":"The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided\n  Improvement","summary":"  Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents.\n","authors":["Ruihan Yang","Fanghua Ye","Jian Li","Siyu Yuan","Yikai Zhang","Zhaopeng Tu","Xiaolong Li","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2503.16024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13262v3","updated":"2025-03-20T10:42:08Z","published":"2025-03-17T15:16:59Z","title":"TablePilot: Recommending Human-Preferred Tabular Data Analysis with\n  Large Language Models","summary":"  Tabular data analysis is crucial in many scenarios, yet efficiently\nidentifying the most relevant data analysis queries and results for a new table\nremains a significant challenge. The complexity of tabular data, diverse\nanalytical operations, and the demand for high-quality analysis make the\nprocess tedious. To address these challenges, we aim to recommend\nquery-code-result triplets tailored for new tables in tabular data analysis\nworkflows. In this paper, we present TablePilot, a pioneering tabular data\nanalysis framework leveraging large language models to autonomously generate\ncomprehensive and superior analytical results without relying on user profiles\nor prior interactions. The framework incorporates key designs in analysis\npreparation and analysis optimization to enhance accuracy. Additionally, we\npropose Rec-Align, a novel method to further improve recommendation quality and\nbetter align with human preferences. Experiments on DART, a dataset\nspecifically designed for comprehensive tabular data analysis recommendation,\ndemonstrate the effectiveness of our framework. Based on GPT-4o, the tuned\nTablePilot achieves 77.0% top-5 recommendation recall. Human evaluations\nfurther highlight its effectiveness in optimizing tabular data analysis\nworkflows.\n","authors":["Deyin Yi","Yihao Liu","Lang Cao","Mengyu Zhou","Haoyu Dong","Shi Han","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.13262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16022v1","updated":"2025-03-20T10:39:39Z","published":"2025-03-20T10:39:39Z","title":"Corrective In-Context Learning: Evaluating Self-Correction in Large\n  Language Models","summary":"  In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research.\n","authors":["Mario Sanz-Guerrero","Katharina von der Wense"],"pdf_url":"https://arxiv.org/pdf/2503.16022v1.pdf","comment":"Accepted to the 6th Workshop on Insights from Negative Results in NLP\n  at NAACL 2025"},{"id":"http://arxiv.org/abs/2503.16021v1","updated":"2025-03-20T10:37:29Z","published":"2025-03-20T10:37:29Z","title":"Autonomous AI imitators increase diversity in homogeneous information\n  ecosystems","summary":"  Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's potential\nimpact on the diversity and democratic value of information ecosystems. Here,\nwe introduce a large-scale simulation framework to examine AI-based imitation\nin news, a context critically influential for public discourse. By\nsystematically testing two distinct imitation strategies across a range of\ninformation environments varying in initial diversity, we demonstrate that\nAI-generated articles do not uniformly homogenize content. Instead, AI's\ninfluence is strongly context-dependent: AI-generated articles can introduce\nvaluable diversity in originally homogeneous news environments, while\npotentially diminishing diversity in contexts that initially display high\nheterogeneity. These results illustrate that the baseline diversity of an\ninformation space critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens information diversity. Instead, when\ninformation is initially homogeneous, AI-driven imitation can expand\nperspectives, styles, and topics. This is especially important in news\ncontexts, where information diversity fosters richer public debate by exposing\ncitizens to alternative viewpoints, challenging biases, and preventing\nnarrative monopolies, which is essential for a resilient democracy.\n","authors":["Emil Bakkensen Johansen","Oliver Baumann"],"pdf_url":"https://arxiv.org/pdf/2503.16021v1.pdf","comment":"35 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.15469v2","updated":"2025-03-20T10:09:43Z","published":"2025-03-19T17:45:13Z","title":"Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional\n  Context-Aware Representation Learning for Enhanced Text Classification","summary":"  Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency.\n","authors":["ZhengLin Lai","MengYao Liao","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2503.15469v2.pdf","comment":"11 pages,1 figure"},{"id":"http://arxiv.org/abs/2501.18532v2","updated":"2025-03-20T09:58:49Z","published":"2025-01-30T17:58:36Z","title":"Differentially Private Steering for Large Language Model Alignment","summary":"  Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe Private Steering for LLM Alignment (PSA) algorithm to edit LLM activations\nwith differential privacy (DP) guarantees. We conduct extensive experiments on\nseven different benchmarks with open-source LLMs of different sizes (0.5B to\n7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that\nPSA achieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our experiments support the theoretical\nguarantees by showing improved guarantees for our PSA algorithm compared to\nseveral existing non-private techniques.\n","authors":["Anmol Goel","Yaxi Hu","Iryna Gurevych","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2501.18532v2.pdf","comment":"ICLR 2025 Camera Ready; Code: https://github.com/UKPLab/iclr2025-psa"},{"id":"http://arxiv.org/abs/2503.15990v1","updated":"2025-03-20T09:49:15Z","published":"2025-03-20T09:49:15Z","title":"ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging\n  Knowledge Graph","summary":"  Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce.\n","authors":["Langming Liu","Haibin Chen","Yuhao Wang","Yujin Yuan","Shilei Liu","Wenbo Su","Xiangyu Zhao","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.15990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03884v3","updated":"2025-03-20T09:46:11Z","published":"2024-11-06T13:00:34Z","title":"Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models","summary":"  Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.\n","authors":["Zhijian Zhuo","Ya Wang","Yutao Zeng","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2411.03884v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2407.01082v4","updated":"2025-03-20T09:39:39Z","published":"2024-07-01T08:37:25Z","title":"Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs","summary":"  Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its significant impact on improving text generation quality.\n","authors":["Minh Nguyen","Andrew Baker","Clement Neo","Allen Roush","Andreas Kirsch","Ravid Shwartz-Ziv"],"pdf_url":"https://arxiv.org/pdf/2407.01082v4.pdf","comment":"Added acknowledgements and minor rewordings to make the\n  intro/abstract more readable. No major change in length or content"},{"id":"http://arxiv.org/abs/2503.15983v1","updated":"2025-03-20T09:30:35Z","published":"2025-03-20T09:30:35Z","title":"InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based\n  Transformer","summary":"  This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks.\n","authors":["Tony Zhang","Rickard Brännvall"],"pdf_url":"https://arxiv.org/pdf/2503.15983v1.pdf","comment":"7 pages, 2 tables"},{"id":"http://arxiv.org/abs/2503.15979v1","updated":"2025-03-20T09:23:35Z","published":"2025-03-20T09:23:35Z","title":"Exploratory Study into Relations between Cognitive Distortions and\n  Emotional Appraisals","summary":"  In recent years, there has been growing interest in studying cognitive\ndistortions and emotional appraisals from both computational and psychological\nperspectives. Despite considerable similarities between emotional reappraisal\nand cognitive reframing as emotion regulation techniques, these concepts have\nlargely been examined in isolation. This research explores the relationship\nbetween cognitive distortions and emotional appraisal dimensions, examining\ntheir potential connections and relevance for future interdisciplinary studies.\nUnder this pretext, we conduct an exploratory computational study, aimed at\ninvestigating the relationship between cognitive distortion and emotional\nappraisals. We show that the patterns of statistically significant\nrelationships between cognitive distortions and appraisal dimensions vary\nacross different distortion categories, giving rise to distinct appraisal\nprofiles for individual distortion classes. Additionally, we analyze the impact\nof cognitive restructuring on appraisal dimensions, exemplifying the emotion\nregulation aspect of cognitive restructuring.\n","authors":["Navneet Agarwal","Kairit Sirts"],"pdf_url":"https://arxiv.org/pdf/2503.15979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15952v1","updated":"2025-03-20T08:48:57Z","published":"2025-03-20T08:48:57Z","title":"Adaptive Group Policy Optimization: Towards Stable Training and\n  Token-Efficient Reasoning","summary":"  Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps.\n","authors":["Chen Li","Nazhou Liu","Kai Yang"],"pdf_url":"https://arxiv.org/pdf/2503.15952v1.pdf","comment":"This is an unfinished version and will be updated. We aim to share\n  some findings"},{"id":"http://arxiv.org/abs/2409.18042v4","updated":"2025-03-20T08:47:39Z","published":"2024-09-26T16:44:02Z","title":"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions","summary":"  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n","authors":["Kai Chen","Yunhao Gou","Runhui Huang","Zhili Liu","Daxin Tan","Jing Xu","Chunwei Wang","Yi Zhu","Yihan Zeng","Kuo Yang","Dingdong Wang","Kun Xiang","Haoyuan Li","Haoli Bai","Jianhua Han","Xiaohui Li","Weike Jin","Nian Xie","Yu Zhang","James T. Kwok","Hengshuang Zhao","Xiaodan Liang","Dit-Yan Yeung","Xiao Chen","Zhenguo Li","Wei Zhang","Qun Liu","Jun Yao","Lanqing Hong","Lu Hou","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2409.18042v4.pdf","comment":"Accepted by CVPR 2025. Project Page: https://emova-ollm.github.io/"},{"id":"http://arxiv.org/abs/2503.15948v1","updated":"2025-03-20T08:44:10Z","published":"2025-03-20T08:44:10Z","title":"Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI\n  over Atomic Facts","summary":"  Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.\n","authors":["Elisei Rykov","Kseniia Petrushina","Kseniia Titova","Alexander Panchenko","Vasily Konovalov"],"pdf_url":"https://arxiv.org/pdf/2503.15948v1.pdf","comment":"Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025"},{"id":"http://arxiv.org/abs/2503.15944v1","updated":"2025-03-20T08:34:53Z","published":"2025-03-20T08:34:53Z","title":"From Chaos to Order: The Atomic Reasoner Framework for Fine-grained\n  Reasoning in Large Language Models","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation.\n","authors":["Jinyi Liu","Yan Zheng","Rong Cheng","Qiyu Wu","Wei Guo","Fei Ni","Hebin Liang","Yifu Yuan","Hangyu Mao","Fuzheng Zhang","Jianye Hao"],"pdf_url":"https://arxiv.org/pdf/2503.15944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15924v1","updated":"2025-03-20T08:00:41Z","published":"2025-03-20T08:00:41Z","title":"Towards Automatic Continual Learning: A Self-Adaptive Framework for\n  Continual Instruction Tuning","summary":"  Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.\n","authors":["Peiyi Lin","Fukai Zhang","Kai Niu","Hao Fu"],"pdf_url":"https://arxiv.org/pdf/2503.15924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15904v1","updated":"2025-03-20T07:15:45Z","published":"2025-03-20T07:15:45Z","title":"From Structured Prompts to Open Narratives: Measuring Gender Bias in\n  LLMs Through Open-Ended Storytelling","summary":"  Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.\n","authors":["Evan Chen","Run-Jun Zhan","Yan-Bai Lin","Hung-Hsuan Chen"],"pdf_url":"https://arxiv.org/pdf/2503.15904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09449v3","updated":"2025-03-20T06:51:27Z","published":"2024-10-12T09:06:09Z","title":"Continuous Risk Prediction","summary":"  Lifelong learning (LL) capabilities are essential for QA models to excel in\nreal-world applications, and architecture-based LL approaches have proven to be\na promising direction for achieving this goal. However, adapting existing\nmethods to QA tasks is far from straightforward. Many prior approaches either\nrely on access to task identities during testing or fail to adequately model\nsamples from unseen tasks, which limits their practical applicability. To\novercome these limitations, we introduce Diana , a novel\n\\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based\nlifelo\\underline{n}g Q\\underline{A} framework designed to learn a sequence of\nQA tasks using a prompt-enhanced language model.Diana leverages four\nhierarchically structured types of prompts to capture QA knowledge at multiple\nlevels of granularity. Task-level prompts are specifically designed to encode\ntask-specific knowledge, ensuring strong lifelong learning performance.\nMeanwhile, instance-level prompts are utilized to capture shared knowledge\nacross diverse input samples, enhancing the model's generalization\ncapabilities. Additionally, Diana incorporates dedicated prompts to explicitly\nhandle unseen tasks and introduces a set of prompt key vectors that facilitate\nefficient knowledge transfer and sharing between tasks. Through extensive\nexperimentation, we demonstrate that Diana achieves state-of-the-art\nperformance among lifelong QA models, with particularly notable improvements in\nits ability to handle previously unseen tasks. This makes Diana a significant\nadvancement in the field of lifelong learning for question-answering systems.\n","authors":["Yi Dai"],"pdf_url":"https://arxiv.org/pdf/2410.09449v3.pdf","comment":"In revision"},{"id":"http://arxiv.org/abs/2411.19930v2","updated":"2025-03-20T06:35:22Z","published":"2024-11-29T18:42:28Z","title":"On Domain-Specific Post-Training for Multimodal Large Language Models","summary":"  Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs through post-training, focusing on data synthesis,\ntraining pipelines, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models (e.g., GPT-4V) in enhancing domain-specific performance.\n(2) Training Pipeline: While the two-stage training--initially on image-caption\npairs followed by visual instruction tasks--is commonly adopted for developing\ngeneral MLLMs, we apply a single-stage training pipeline to enhance task\ndiversity for domain-specific post-training. (3) Task Evaluation: We conduct\nextensive experiments in high-impact domains such as biomedicine, food, and\nremote sensing, by post-training a variety of MLLMs and then evaluating MLLM\nperformance on various domain-specific tasks. Furthermore, we fully open-source\nour models, code, and data to encourage future research in this area.\n","authors":["Daixuan Cheng","Shaohan Huang","Ziyu Zhu","Xintong Zhang","Wayne Xin Zhao","Zhongzhi Luan","Bo Dai","Zhenliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.19930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15888v1","updated":"2025-03-20T06:26:28Z","published":"2025-03-20T06:26:28Z","title":"Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in\n  Language Models","summary":"  Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https://github.com/byronBBL/CK-PLUG}{\\text{this https URL}}$.\n","authors":["Baolong Bi","Shenghua Liu","Yiwei Wang","Yilong Xu","Junfeng Fang","Lingrui Mei","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.15888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15880v1","updated":"2025-03-20T06:05:36Z","published":"2025-03-20T06:05:36Z","title":"InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced\n  Preference Optimization","summary":"  Direct Preference Optimization (DPO) optimizes language models to align with\nhuman preferences. Utilizing on-policy samples, generated directly by the\npolicy model, typically results in better performance due to its distribution\nconsistency with the model compared to off-policy samples. This paper\nidentifies the quality of candidate preference samples as another critical\nfactor. While the quality of on-policy data is inherently constrained by the\ncapabilities of the policy model, off-policy data, which can be derived from\ndiverse sources, offers greater potential for quality despite experiencing\ndistribution shifts. However, current research mostly relies on on-policy data\nand neglects the value of off-policy data in terms of data quality, due to the\nchallenge posed by distribution shift. In this paper, we propose InCo-DPO, an\nefficient method for synthesizing preference data by integrating on-policy and\noff-policy data, allowing dynamic adjustments to balance distribution shifts\nand data quality, thus finding an optimal trade-off. Consequently, InCo-DPO\novercomes the limitations of distribution shifts in off-policy data and the\nquality constraints of on-policy data. We evaluated InCo-DPO with the\nAlpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate\nthat our approach not only outperforms both on-policy and off-policy data but\nalso achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the\nvanilla DPO using Gemma-2 model.\n","authors":["Yunan Wang","Jijie Li","Bo-Wen Zhang","Liangdong Wang","Guang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15879v1","updated":"2025-03-20T06:04:12Z","published":"2025-03-20T06:04:12Z","title":"Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering","summary":"  Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\n\\href{https://github.com/TeamNLP/Typed-RAG}{https://github.com/TeamNLP/Typed-RAG}.\n","authors":["DongGeon Lee","Ahjeong Park","Hyeri Lee","Hyeonseo Nam","Yunho Maeng"],"pdf_url":"https://arxiv.org/pdf/2503.15879v1.pdf","comment":"Accepted to NAACL 2025 SRW"},{"id":"http://arxiv.org/abs/2503.05244v3","updated":"2025-03-20T05:13:53Z","published":"2025-03-07T08:56:20Z","title":"WritingBench: A Comprehensive Benchmark for Generative Writing","summary":"  Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.\n","authors":["Yuning Wu","Jiahao Mei","Ming Yan","Chenliang Li","Shaopeng Lai","Yuran Ren","Zijia Wang","Ji Zhang","Mengyue Wu","Qin Jin","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2503.05244v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07627v2","updated":"2025-03-20T05:08:24Z","published":"2024-10-10T05:43:07Z","title":"Automatic Curriculum Expert Iteration for Reliable LLM Reasoning","summary":"  Hallucinations (i.e., generating plausible but inaccurate content) and\nlaziness (i.e. excessive refusals or defaulting to \"I don't know\") persist as\nmajor challenges in LLM reasoning. Current efforts to reduce hallucinations\nprimarily focus on factual errors in knowledge-grounded tasks, often neglecting\nhallucinations related to faulty reasoning. Meanwhile, some approaches render\nLLMs overly conservative, limiting their problem-solving capabilities. To\nmitigate hallucination and laziness in reasoning tasks, we propose Automatic\nCurriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align\nresponses to the model's capabilities--assertively answering within its limits\nand declining when tasks exceed them. In our method, Expert Iteration explores\nthe reasoning trajectories near the LLM policy, guiding incorrect paths back on\ntrack to reduce compounding errors and improve robustness; it also promotes\nappropriate \"I don't know\" responses after sufficient reasoning attempts. The\ncurriculum automatically adjusts rewards, incentivizing extended reasoning\nbefore acknowledging incapability, thereby pushing the limits of LLM reasoning\nand aligning its behaviour with these limits. We compare Auto-CEI with various\nSOTA baselines across logical reasoning, mathematics, and planning tasks, where\nAuto-CEI achieves superior alignment by effectively balancing assertiveness and\nconservativeness. The code is available at\nhttps://github.com/SalesforceAIResearch/Auto-CEI .\n","authors":["Zirui Zhao","Hanze Dong","Amrita Saha","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.07627v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2503.15850v1","updated":"2025-03-20T05:04:29Z","published":"2025-03-20T05:04:29Z","title":"Uncertainty Quantification and Confidence Calibration in Large Language\n  Models: A Survey","summary":"  Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability.\n","authors":["Xiaoou Liu","Tiejin Chen","Longchao Da","Chacha Chen","Zhen Lin","Hua Wei"],"pdf_url":"https://arxiv.org/pdf/2503.15850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15848v1","updated":"2025-03-20T05:03:26Z","published":"2025-03-20T05:03:26Z","title":"Entropy-based Exploration Conduction for Multi-step Reasoning","summary":"  In large language model (LLM) reasoning, multi-step processes have proven\neffective for solving complex tasks. However, the depth of exploration can\nsignificantly affect the reasoning performance. Existing methods to\nautomatically decide the depth often bring high costs and lack flexibility, and\nthus undermine the model's reasoning accuracy. To address these issues, we\npropose Entropy-based Exploration Depth Conduction (Entro-duction), a novel\nmethod that dynamically adjusts the exploration depth during multi-step\nreasoning by monitoring LLM's output entropy and variance entropy. We employ\nthese two metrics to capture the model's current uncertainty and the\nfluctuation of uncertainty across consecutive reasoning steps. Based on the\nobserved changes, the LLM selects whether to deepen, expand or stop exploration\naccording to the probability. In this way, we balance the reasoning accuracy\nand exploration effectiveness. Experimental results across four benchmark\ndatasets demonstrate the efficacy of Entro-duction. We further conduct\nexperiments and analysis on the components of Entro-duction to discuss their\ncontributions to reasoning performance.\n","authors":["Jinghan Zhang","Xiting Wang","Fengran Mo","Yeyang Zhou","Wanfu Gao","Kunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15837v1","updated":"2025-03-20T04:26:40Z","published":"2025-03-20T04:26:40Z","title":"Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese\n  Text Understanding and Generation","summary":"  Ancient Chinese text processing presents unique challenges for large language\nmodels (LLMs) due to its distinct linguistic features, complex structural\nconstraints, and rich cultural context. While existing benchmarks have\nprimarily focused on evaluating comprehension through multiple-choice\nquestions, there remains a critical gap in assessing models' generative\ncapabilities in classical Chinese. We introduce F\\`ux\\`i, a comprehensive\nbenchmark that evaluates both understanding and generation capabilities across\n21 diverse tasks. Our benchmark distinguishes itself through three key\ncontributions: (1) balanced coverage of both comprehension and generation\ntasks, including novel tasks like poetry composition and couplet completion,\n(2) specialized evaluation metrics designed specifically for classical Chinese\ntext generation, combining rule-based verification with fine-tuned LLM\nevaluators, and (3) a systematic assessment framework that considers both\nlinguistic accuracy and cultural authenticity. Through extensive evaluation of\nstate-of-the-art LLMs, we reveal significant performance gaps between\nunderstanding and generation tasks, with models achieving promising results in\ncomprehension but struggling considerably in generation tasks, particularly\nthose requiring deep cultural knowledge and adherence to classical formats. Our\nfindings highlight the current limitations in ancient Chinese text processing\nand provide insights for future model development. The benchmark, evaluation\ntoolkit, and baseline results are publicly available to facilitate research in\nthis domain.\n","authors":["Shangqing Zhao","Yuhao Zhou","Yupei Ren","Zhe Chen","Chenghao Jia","Fang Zhe","Zhaogaung Long","Shu Liu","Man Lan"],"pdf_url":"https://arxiv.org/pdf/2503.15837v1.pdf","comment":"working in progress"},{"id":"http://arxiv.org/abs/2401.13218v2","updated":"2025-03-20T03:34:29Z","published":"2024-01-24T04:13:28Z","title":"ULTRA: Unleash LLMs' Potential for Event Argument Extraction through\n  Hierarchical Modeling and Pair-wise Self-Refinement","summary":"  Structural extraction of events within discourse is critical since it avails\na deeper understanding of communication patterns and behavior trends. Event\nargument extraction (EAE), at the core of event-centric understanding, is the\ntask of identifying role-specific text spans (i.e., arguments) for a given\nevent. Document-level EAE (DocEAE) focuses on arguments that are scattered\nacross an entire document. In this work, we explore open-source Large Language\nModels (LLMs) for DocEAE, and propose ULTRA, a hierarchical framework that\nextracts event arguments more cost-effectively. Further, it alleviates the\npositional bias issue intrinsic to LLMs. ULTRA sequentially reads text chunks\nof a document to generate a candidate argument set, upon which non-pertinent\ncandidates are dropped through self-refinement. We introduce LEAFER to address\nthe challenge LLMs face in locating the exact boundary of an argument. ULTRA\noutperforms strong baselines, including strong supervised models and ChatGPT,\nby 9.8% when evaluated by Exact Match (EM).\n","authors":["Xinliang Frederick Zhang","Carter Blum","Temma Choji","Shalin Shah","Alakananda Vempala"],"pdf_url":"https://arxiv.org/pdf/2401.13218v2.pdf","comment":"ACL'24 Findings"},{"id":"http://arxiv.org/abs/2403.16952v2","updated":"2025-03-20T03:31:27Z","published":"2024-03-25T17:14:00Z","title":"Data Mixing Laws: Optimizing Data Mixtures by Predicting Language\n  Modeling Performance","summary":"  Pretraining data of large language models composes multiple domains (e.g.,\nweb texts, academic papers, codes), whose mixture proportions crucially impact\nthe competence of outcome models. While existing endeavors rely on heuristics\nor qualitative strategies to tune the proportions, we discover the quantitative\npredictability of model performance regarding the mixture proportions in\nfunction forms, which we refer to as the data mixing laws. Fitting such\nfunctions on sample mixtures unveils model performance on unseen mixtures\nbefore actual runs, thus guiding the selection of an ideal data mixture.\nFurthermore, we propose nested use of the scaling laws of training steps, model\nsizes, and our data mixing law to enable predicting the performance of large\nmodels trained on massive data under various mixtures with only small-scale\ntraining. Moreover, experimental results verify that our method effectively\noptimizes the training mixture of a 1B model trained for 100B tokens in\nRedPajama, reaching a performance comparable to the one trained for 48% more\nsteps on the default mixture. Extending the application of data mixing laws to\ncontinual training accurately predicts the critical mixture proportion that\navoids catastrophic forgetting and outlooks the potential for dynamic data\nschedules\n","authors":["Jiasheng Ye","Peiju Liu","Tianxiang Sun","Jun Zhan","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2403.16952v2.pdf","comment":"accepted by ICLR2025, camera ready version"},{"id":"http://arxiv.org/abs/2411.04905v3","updated":"2025-03-20T03:28:56Z","published":"2024-11-07T17:47:25Z","title":"OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models","summary":"  Large language models (LLMs) for code have become indispensable in various\ndomains, including code generation, reasoning tasks and agent systems. While\nopen-access code LLMs are increasingly approaching the performance levels of\nproprietary models, high-quality code LLMs suitable for rigorous scientific\ninvestigation, particularly those with reproducible data processing pipelines\nand transparent training protocols, remain limited. The scarcity is due to\nvarious challenges, including resource constraints, ethical considerations, and\nthe competitive advantages of keeping models advanced. To address the gap, we\nintroduce OpenCoder, a top-tier code LLM that not only achieves performance\ncomparable to leading models but also serves as an \"open cookbook\" for the\nresearch community. Unlike most prior efforts, we release not only model\nweights and inference code, but also the reproducible training data, complete\ndata processing pipeline, rigorous experimental ablation results, and detailed\ntraining protocols for open scientific research. Through this comprehensive\nrelease, we identify the key ingredients for building a top-tier code LLM: (1)\ncode optimized heuristic rules for data cleaning and methods for data\ndeduplication, (2) recall of text corpus related to code and (3) high-quality\nsynthetic data in both annealing and supervised fine-tuning stages. By offering\nthis level of openness, we aim to broaden access to all aspects of a top-tier\ncode LLM, with OpenCoder serving as both a powerful model and an open\nfoundation to accelerate research, and enable reproducible advancements in code\nAI.\n","authors":["Siming Huang","Tianhao Cheng","J. K. Liu","Jiaran Hao","Liuyihan Song","Yang Xu","J. Yang","Jiaheng Liu","Chenchen Zhang","Linzheng Chai","Ruifeng Yuan","Zhaoxiang Zhang","Jie Fu","Qian Liu","Ge Zhang","Zili Wang","Yuan Qi","Yinghui Xu","Wei Chu"],"pdf_url":"https://arxiv.org/pdf/2411.04905v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18435v2","updated":"2025-03-20T03:25:21Z","published":"2025-02-25T18:30:25Z","title":"Reversal Blessing: Thinking Backward May Outpace Thinking Forward in\n  Multi-choice Questions","summary":"  Language models usually use left-to-right (L2R) autoregressive factorization.\nHowever, L2R factorization may not always be the best inductive bias.\nTherefore, we investigate whether alternative factorizations of the text\ndistribution could be beneficial in some tasks. We investigate right-to-left\n(R2L) training as a compelling alternative, focusing on multiple-choice\nquestions (MCQs) as a test bed for knowledge extraction and reasoning. Through\nextensive experiments across various model sizes (2B-8B parameters) and\ntraining datasets, we find that R2L models can significantly outperform L2R\nmodels on several MCQ benchmarks, including logical reasoning, commonsense\nunderstanding, and truthfulness assessment tasks. Our analysis reveals that\nthis performance difference may be fundamentally linked to multiple factors\nincluding calibration, computability and directional conditional entropy. We\nablate the impact of these factors through controlled simulation studies using\narithmetic tasks, where the impacting factors can be better disentangled. Our\nwork demonstrates that exploring alternative factorizations of the text\ndistribution can lead to improvements in LLM capabilities and provides\ntheoretical insights into optimal factorization towards approximating human\nlanguage distribution, and when each reasoning order might be more\nadvantageous.\n","authors":["Yizhe Zhang","Richard Bai","Zijin Gu","Ruixiang Zhang","Jiatao Gu","Emmanuel Abbe","Samy Bengio","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2502.18435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04784v3","updated":"2025-03-20T03:04:01Z","published":"2025-02-27T01:56:09Z","title":"KunlunBaize: LLM with Multi-Scale Convolution and Multi-Token Prediction\n  Under TransformerX Framework","summary":"  Large language models have demonstrated remarkable performance across various\ntasks, yet they face challenges such as low computational efficiency, gradient\nvanishing, and difficulties in capturing complex feature interactions. To\naddress these limitations, a novel framework has been proposed. This framework\nincorporates a learnable dense residual skip connection mechanism, a\nTransformerX module a transformer based component integrating multiscale\nconvolution and adaptive activation functions and a multitoken prediction\ninteraction module. The learnable dense residual connections enhance\ninformation flow and feature capture across layers. Within the TransformerX\nmodule, large convolutional kernels aggregate semantic information from\nextensive text segments, while smaller convolutions focus on local word order\nand syntactic structures. The adaptive activation function dynamically adjusts\nits parameters based on the semantic features of the input text, improving the\nmodel's ability to handle diverse semantic expressions and complex\nrelationships. The multitoken prediction module boosts data utilization and\naccelerates inference by predicting multiple future tokens. These components\nsignificantly enhance the performance and efficiency of large language models.\n","authors":["Cheng Li","Jiexiong Liu","Yixuan Chen","Yanqin Jia","Zhepeng Li"],"pdf_url":"https://arxiv.org/pdf/2503.04784v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2411.04425v3","updated":"2025-03-20T02:52:47Z","published":"2024-11-07T04:38:29Z","title":"DELIFT: Data Efficient Language model Instruction Fine Tuning","summary":"  Fine-tuning large language models (LLMs) is essential for enhancing their\nperformance on specific tasks but is often resource-intensive due to redundant\nor uninformative data. To address this inefficiency, we introduce DELIFT (Data\nEfficient Language model Instruction Fine-Tuning), a novel algorithm that\nsystematically optimizes data selection across the three key stages of\nfine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,\nreasoning, question-answering), and (3) continual fine-tuning (e.g.,\nincorporating new data versions). Unlike existing methods that focus on\nsingle-stage optimization or rely on computationally intensive gradient\ncalculations, DELIFT operates efficiently across all stages. Central to our\napproach is a pairwise utility metric that quantifies how beneficial a data\nsample is for improving the model's responses to other samples, effectively\nmeasuring the informational value relative to the model's current capabilities.\nBy leveraging different submodular functions applied to this metric, DELIFT\nselects diverse and optimal subsets that are useful across all stages of\nfine-tuning. Experiments across various tasks and model scales demonstrate that\nDELIFT can reduce the fine-tuning data size by up to 70% without compromising\nperformance, offering significant computational savings and outperforming\nexisting methods in both efficiency and efficacy.\n","authors":["Ishika Agarwal","Krishnateja Killamsetty","Lucian Popa","Marina Danilevksy"],"pdf_url":"https://arxiv.org/pdf/2411.04425v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04675v2","updated":"2025-03-20T02:52:15Z","published":"2025-02-07T05:41:23Z","title":"Scalable Oversight for Superhuman AI via Recursive Self-Critiquing","summary":"  As AI capabilities increasingly surpass human proficiency in complex tasks,\ncurrent alignment techniques including SFT and RLHF face fundamental challenges\nin ensuring reliable oversight. These methods rely on direct human assessment\nand become untenable when AI outputs exceed human cognitive thresholds. In\nresponse to this challenge, we explore two hypotheses: (1) critique of critique\ncan be easier than critique itself, extending the widely-accepted observation\nthat verification is easier than generation to the critique domain, as critique\nitself is a specialized form of generation; (2) this difficulty relationship is\nrecursively held, suggesting that when direct evaluation is infeasible,\nperforming high-order critiques (e.g., critique of critique of critique) offers\na more tractable supervision pathway. To examine these hypotheses, we perform\nHuman-Human, Human-AI, and AI-AI experiments across multiple tasks. Our results\ndemonstrate encouraging evidence supporting these hypotheses and suggest that\nrecursive self-critiquing is a promising direction for scalable oversight.\n","authors":["Xueru Wen","Jie Lou","Xinyu Lu","Junjie Yang","Yanjiang Liu","Yaojie Lu","Debing Zhang","Xing Yu"],"pdf_url":"https://arxiv.org/pdf/2502.04675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15808v1","updated":"2025-03-20T02:51:11Z","published":"2025-03-20T02:51:11Z","title":"ChatGPT and U(X): A Rapid Review on Measuring the User Experience","summary":"  ChatGPT, powered by a large language model (LLM), has revolutionized everyday\nhuman-computer interaction (HCI) since its 2022 release. While now used by\nmillions around the world, a coherent pathway for evaluating the user\nexperience (UX) ChatGPT offers remains missing. In this rapid review (N = 58),\nI explored how ChatGPT UX has been approached quantitatively so far. I focused\non the independent variables (IVs) manipulated, the dependent variables (DVs)\nmeasured, and the methods used for measurement. Findings reveal trends, gaps,\nand emerging consensus in UX assessments. This work offers a first step towards\nsynthesizing existing approaches to measuring ChatGPT UX, urgent trajectories\nto advance standardization and breadth, and two preliminary frameworks aimed at\nguiding future research and tool development. I seek to elevate the field of\nChatGPT UX by empowering researchers and practitioners in optimizing user\ninteractions with ChatGPT and similar LLM-based systems.\n","authors":["Katie Seaborn"],"pdf_url":"https://arxiv.org/pdf/2503.15808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04445v2","updated":"2025-03-20T02:50:55Z","published":"2024-12-05T18:57:04Z","title":"Moto: Latent Motion Token as the Bridging Language for Learning Robot\n  Manipulation from Videos","summary":"  Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.\n","authors":["Yi Chen","Yuying Ge","Weiliang Tang","Yizhuo Li","Yixiao Ge","Mingyu Ding","Ying Shan","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.04445v2.pdf","comment":"Project released at: https://chenyi99.github.io/moto/ Update: Added\n  content related to real-world robot experiments and learning from human\n  videos"},{"id":"http://arxiv.org/abs/2407.01509v5","updated":"2025-03-20T02:49:09Z","published":"2024-07-01T17:53:35Z","title":"MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal\n  LLMs","summary":"  We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex\ninstructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,\neach crafted to challenge the models' compliance with layered instructions in\ngenerating accurate responses that satisfy specific requested patterns.\nEvaluation results from a wide array of state-of-the-art MLLMs reveal\nsignificant variations in performance, highlighting areas for improvement in\ninstruction fidelity. Additionally, we create extra training data and explore\nsupervised fine-tuning to enhance the models' ability to strictly follow\ninstructions without compromising performance on other tasks. We hope this\nbenchmark not only serves as a tool for measuring MLLM adherence to\ninstructions, but also guides future developments in MLLM training methods.\n","authors":["Yusu Qian","Hanrong Ye","Jean-Philippe Fauconnier","Peter Grasch","Yinfei Yang","Zhe Gan"],"pdf_url":"https://arxiv.org/pdf/2407.01509v5.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.15798v1","updated":"2025-03-20T02:31:57Z","published":"2025-03-20T02:31:57Z","title":"Mixture of Lookup Experts","summary":"  Mixture-of-Experts (MoE) activates only a subset of experts during inference,\nallowing the model to maintain low inference FLOPs and latency even as the\nparameter count scales up. However, since MoE dynamically selects the experts,\nall the experts need to be loaded into VRAM. Their large parameter size still\nlimits deployment, and offloading, which load experts into VRAM only when\nneeded, significantly increase inference latency. To address this, we propose\nMixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in\nboth communication and VRAM usage. In MoLE, the experts are Feed-Forward\nNetworks (FFNs) during training, taking the output of the embedding layer as\ninput. Before inference, these experts can be re-parameterized as lookup tables\n(LUTs) that retrieves expert outputs based on input ids, and offloaded to\nstorage devices. Therefore, we do not need to perform expert computations\nduring inference. Instead, we directly retrieve the expert's computation\nresults based on input ids and load them into VRAM, and thus the resulting\ncommunication overhead is negligible. Experiments show that, with the same\nFLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models\nand significantly faster than MoE with experts offloading, while maintaining\nperformance on par with MoE.\n","authors":["Shibo Jie","Yehui Tang","Kai Han","Yitong Li","Duyu Tang","Zhi-Hong Deng","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2503.15798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05889v4","updated":"2025-03-20T02:27:50Z","published":"2024-02-08T18:27:22Z","title":"CREMA: Generalizable and Efficient Video-Language Reasoning via\n  Multimodal Modular Fusion","summary":"  Despite impressive advancements in recent multimodal reasoning approaches,\nthey are still limited in flexibility and efficiency, as these models typically\nprocess only a few fixed modality inputs and require updates to numerous\nparameters. This paper tackles these critical challenges and proposes CREMA, a\ngeneralizable, highly efficient, and modular modality-fusion framework that can\nincorporate any new modality to enhance video reasoning. We first augment\nmultiple informative modalities (such as optical flow, 3D point cloud, audio,\nthermal heatmap, and touch map) from given videos without extra human\nannotation by leveraging sensors or existing pre-trained models. Next, we\nintroduce a query transformer with multiple parameter-efficient modules\nassociated with each accessible modality. It projects diverse modality features\nto the LLM token embedding space, allowing the model to integrate different\ndata types for response generation. Furthermore, we propose a novel progressive\nmultimodal fusion design supported by a lightweight fusion module and\nmodality-sequential training strategy. It helps compress information across\nvarious assisting modalities, maintaining computational efficiency in the LLM\nwhile improving performance. We validate our method on 7 video-language\nreasoning tasks assisted by diverse modalities, including conventional VideoQA\nand Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance\nagainst strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while\nreducing over 90% trainable parameters. We provide extensive analyses of CREMA,\nincluding the impact of each modality on reasoning domains, the design of the\nfusion module, and example visualizations.\n","authors":["Shoubin Yu","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2402.05889v4.pdf","comment":"ICLR 2025; first two authors contributed equally. Project page:\n  https://CREMA-VideoLLM.github.io/"},{"id":"http://arxiv.org/abs/2503.11989v2","updated":"2025-03-20T02:18:33Z","published":"2025-03-15T04:18:01Z","title":"Applications of Large Language Model Reasoning in Feature Generation","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nthrough their state of art reasoning capabilities. This paper explores the\nconvergence of LLM reasoning techniques and feature generation for machine\nlearning tasks. We examine four key reasoning approaches: Chain of Thought,\nTree of Thoughts, Retrieval-Augmented Generation, and Thought Space\nExploration. Our analysis reveals how these approaches can be used to identify\neffective feature generation rules without having to manually specify search\nspaces. The paper categorizes LLM-based feature generation methods across\nvarious domains including finance, healthcare, and text analytics. LLMs can\nextract key information from clinical notes and radiology reports in\nhealthcare, by enabling more efficient data utilization. In finance, LLMs\nfacilitate text generation, summarization, and entity extraction from complex\ndocuments. We analyze evaluation methodologies for assessing feature quality\nand downstream performance, with particular attention to OCTree's decision tree\nreasoning approach that provides language-based feedback for iterative\nimprovements. Current challenges include hallucination, computational\nefficiency, and domain adaptation. As of March 2025, emerging approaches\ninclude inference-time compute scaling, reinforcement learning, and supervised\nfine-tuning with model distillation. Future directions point toward multimodal\nfeature generation, self-improving systems, and neuro-symbolic approaches. This\npaper provides a detailed overview of an emerging field that promises to\nautomate and enhance feature engineering through language model reasoning.\n","authors":["Dharani Chandra"],"pdf_url":"https://arxiv.org/pdf/2503.11989v2.pdf","comment":"I just updated the format of the references in the paper"},{"id":"http://arxiv.org/abs/2503.15783v1","updated":"2025-03-20T01:47:33Z","published":"2025-03-20T01:47:33Z","title":"Grammar and Gameplay-aligned RL for Game Description Generation with\n  LLMs","summary":"  Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.\n","authors":["Tsunehiko Tanaka","Edgar Simo-Serra"],"pdf_url":"https://arxiv.org/pdf/2503.15783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.07459v2","updated":"2025-03-20T01:30:56Z","published":"2025-03-10T15:38:44Z","title":"MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for\n  Complex Medical Reasoning","summary":"  Large Language Models (LLMs) have shown impressive performance on existing\nmedical question-answering benchmarks. This high performance makes it\nincreasingly difficult to meaningfully evaluate and differentiate advanced\nmethods. We present MedAgentsBench, a benchmark that focuses on challenging\nmedical questions requiring multi-step clinical reasoning, diagnosis\nformulation, and treatment planning-scenarios where current models still\nstruggle despite their strong performance on standard tests. Drawing from seven\nestablished medical datasets, our benchmark addresses three key limitations in\nexisting evaluations: (1) the prevalence of straightforward questions where\neven base models achieve high performance, (2) inconsistent sampling and\nevaluation protocols across studies, and (3) lack of systematic analysis of the\ninterplay between performance, cost, and inference time. Through experiments\nwith various base models and reasoning methods, we demonstrate that the latest\nthinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in\ncomplex medical reasoning tasks. Additionally, advanced search-based agent\nmethods offer promising performance-to-cost ratios compared to traditional\napproaches. Our analysis reveals substantial performance gaps between model\nfamilies on complex questions and identifies optimal model selections for\ndifferent computational constraints. Our benchmark and evaluation framework are\npublicly available at https://github.com/gersteinlab/medagents-benchmark.\n","authors":["Xiangru Tang","Daniel Shao","Jiwoong Sohn","Jiapeng Chen","Jiayi Zhang","Jinyu Xiang","Fang Wu","Yilun Zhao","Chenglin Wu","Wenqi Shi","Arman Cohan","Mark Gerstein"],"pdf_url":"https://arxiv.org/pdf/2503.07459v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15768v1","updated":"2025-03-20T00:57:38Z","published":"2025-03-20T00:57:38Z","title":"Can one size fit all?: Measuring Failure in Multi-Document Summarization\n  Domain Transfer","summary":"  Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.\n","authors":["Alexandra DeLucia","Mark Dredze"],"pdf_url":"https://arxiv.org/pdf/2503.15768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11624v4","updated":"2025-03-20T12:06:17Z","published":"2024-06-17T15:07:55Z","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","summary":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","authors":["Omer Sahin Tas","Royden Wagner"],"pdf_url":"https://arxiv.org/pdf/2406.11624v4.pdf","comment":"ICLR 2025 camera-ready. Our implementation is available at\n  github.com/kit-mrt/future-motion"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.16429v1","updated":"2025-03-20T17:59:59Z","published":"2025-03-20T17:59:59Z","title":"Sonata: Self-Supervised Learning of Reliable Point Representations","summary":"  In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks.\n","authors":["Xiaoyang Wu","Daniel DeTone","Duncan Frost","Tianwei Shen","Chris Xie","Nan Yang","Jakob Engel","Richard Newcombe","Hengshuang Zhao","Julian Straub"],"pdf_url":"https://arxiv.org/pdf/2503.16429v1.pdf","comment":"CVPR 2025, produced by Pointcept x Meta, project page:\n  https://xywu.me/sonata/"},{"id":"http://arxiv.org/abs/2503.16430v1","updated":"2025-03-20T17:59:59Z","published":"2025-03-20T17:59:59Z","title":"Bridging Continuous and Discrete Tokens for Autoregressive Visual\n  Generation","summary":"  Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge.\n","authors":["Yuqing Wang","Zhijie Lin","Yao Teng","Yuanzhi Zhu","Shuhuai Ren","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2503.16430v1.pdf","comment":"Project page: https://yuqingwang1029.github.io/TokenBridge"},{"id":"http://arxiv.org/abs/2503.16428v1","updated":"2025-03-20T17:59:58Z","published":"2025-03-20T17:59:58Z","title":"XAttention: Block Sparse Attention with Antidiagonal Scoring","summary":"  Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention.\n","authors":["Ruyi Xu","Guangxuan Xiao","Haofeng Huang","Junxian Guo","Song Han"],"pdf_url":"https://arxiv.org/pdf/2503.16428v1.pdf","comment":"The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2501.06187v2","updated":"2025-03-20T17:59:56Z","published":"2025-01-10T18:59:54Z","title":"Multi-subject Open-set Personalization in Video Generation","summary":"  Video personalization methods allow us to synthesize videos with specific\nconcepts such as people, pets, and places. However, existing methods often\nfocus on limited domains, require time-consuming optimization per subject, or\nsupport only a single subject. We present Video Alchemist $-$ a video model\nwith built-in multi-subject, open-set personalization capabilities for both\nforeground objects and background, eliminating the need for time-consuming\ntest-time optimization. Our model is built on a new Diffusion Transformer\nmodule that fuses each conditional reference image and its corresponding\nsubject-level text prompt with cross-attention layers. Developing such a large\nmodel presents two main challenges: dataset and evaluation. First, as paired\ndatasets of reference images and videos are extremely hard to collect, we\nsample selected video frames as reference images and synthesize a clip of the\ntarget video. However, while models can easily denoise training videos given\nreference frames, they fail to generalize to new contexts. To mitigate this\nissue, we design a new automatic data construction pipeline with extensive\nimage augmentations. Second, evaluating open-set video personalization is a\nchallenge in itself. To address this, we introduce a personalization benchmark\nthat focuses on accurate subject fidelity and supports diverse personalization\nscenarios. Finally, our extensive experiments show that our method\nsignificantly outperforms existing personalization methods in both quantitative\nand qualitative evaluations.\n","authors":["Tsai-Shien Chen","Aliaksandr Siarohin","Willi Menapace","Yuwei Fang","Kwot Sin Lee","Ivan Skorokhodov","Kfir Aberman","Jun-Yan Zhu","Ming-Hsuan Yang","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2501.06187v2.pdf","comment":"CVPR 2025. Project page:\n  https://snap-research.github.io/open-set-video-personalization/"},{"id":"http://arxiv.org/abs/2503.16426v1","updated":"2025-03-20T17:59:54Z","published":"2025-03-20T17:59:54Z","title":"DynamicVis: An Efficient and General Visual Foundation Model for Remote\n  Sensing Image Understanding","summary":"  The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's).\n","authors":["Keyan Chen","Chenyang Liu","Bowen Chen","Wenyuan Li","Zhengxia Zou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2503.16426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16425v1","updated":"2025-03-20T17:59:51Z","published":"2025-03-20T17:59:51Z","title":"Tokenize Image as a Set","summary":"  This paper proposes a fundamentally new paradigm for image generation through\nset-based tokenization and distribution modeling. Unlike conventional methods\nthat serialize images into fixed-position latent codes with a uniform\ncompression ratio, we introduce an unordered token set representation to\ndynamically allocate coding capacity based on regional semantic complexity.\nThis TokenSet enhances global context aggregation and improves robustness\nagainst local perturbations. To address the critical challenge of modeling\ndiscrete sets, we devise a dual transformation mechanism that bijectively\nconverts sets into fixed-length integer sequences with summation constraints.\nFurther, we propose Fixed-Sum Discrete Diffusion--the first framework to\nsimultaneously handle discrete values, fixed sequence length, and summation\ninvariance--enabling effective set distribution modeling. Experiments\ndemonstrate our method's superiority in semantic-aware representation and\ngeneration quality. Our innovations, spanning novel representation and modeling\nstrategies, advance visual generation beyond traditional sequential token\nparadigms. Our code and models are publicly available at\nhttps://github.com/Gengzigang/TokenSet.\n","authors":["Zigang Geng","Mengde Xu","Han Hu","Shuyang Gu"],"pdf_url":"https://arxiv.org/pdf/2503.16425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16424v1","updated":"2025-03-20T17:59:50Z","published":"2025-03-20T17:59:50Z","title":"Bézier Splatting for Fast and Differentiable Vector Graphics","summary":"  Differentiable vector graphics (VGs) are widely used in image vectorization\nand vector synthesis, while existing representations are costly to optimize and\nstruggle to achieve high-quality rendering results for high-resolution images.\nThis work introduces a new differentiable VG representation, dubbed B\\'ezier\nsplatting, that enables fast yet high-fidelity VG rasterization. B\\'ezier\nsplatting samples 2D Gaussians along B\\'ezier curves, which naturally provide\npositional gradients at object boundaries. Thanks to the efficient\nsplatting-based differentiable rasterizer, B\\'ezier splatting achieves over 20x\nand 150x faster per forward and backward rasterization step for open curves\ncompared to DiffVG. Additionally, we introduce an adaptive pruning and\ndensification strategy that dynamically adjusts the spatial distribution of\ncurves to escape local minima, further improving VG quality. Experimental\nresults show that B\\'ezier splatting significantly outperforms existing methods\nwith better visual fidelity and 10x faster optimization speed.\n","authors":["Xi Liu","Chaoyi Zhou","Nanxuan Zhao","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2503.16424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16423v1","updated":"2025-03-20T17:59:47Z","published":"2025-03-20T17:59:47Z","title":"GAEA: A Geolocation Aware Conversational Model","summary":"  Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs)\nproprietary and open-source researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose a comprehensive dataset GAEA\nwith 800K images and around 1.6M question answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark comprising 4K\nimage-text pairs to evaluate conversational capabilities equipped with diverse\nquestion types. We consider 11 state-of-the-art open-source and proprietary\nLMMs and demonstrate that GAEA significantly outperforms the best open-source\nmodel, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by\n8.28%. Our dataset, model and codes are available\n","authors":["Ron Campos","Ashmal Vayani","Parth Parag Kulkarni","Rohit Gupta","Aritra Dutta","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2503.16423v1.pdf","comment":"The dataset and code used in this submission is available at:\n  https://ucf-crcv.github.io/GAEA/"},{"id":"http://arxiv.org/abs/2503.16422v1","updated":"2025-03-20T17:59:44Z","published":"2025-03-20T17:59:44Z","title":"1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering","summary":"  4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n\\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io.\n","authors":["Yuheng Yuan","Qiuhong Shen","Xingyi Yang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16421v1","updated":"2025-03-20T17:59:42Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16420v1","updated":"2025-03-20T17:59:40Z","published":"2025-03-20T17:59:40Z","title":"SynCity: Training-Free Generation of 3D Worlds","summary":"  We address the challenge of generating 3D worlds from textual descriptions.\nWe propose SynCity, a training- and optimization-free approach, which leverages\nthe geometric precision of pre-trained 3D generative models and the artistic\nversatility of 2D image generators to create large, high-quality 3D spaces.\nWhile most 3D generative models are object-centric and cannot generate\nlarge-scale worlds, we show how 3D and 2D generators can be combined to\ngenerate ever-expanding scenes. Through a tile-based approach, we allow\nfine-grained control over the layout and the appearance of scenes. The world is\ngenerated tile-by-tile, and each new tile is generated within its world-context\nand then fused with the scene. SynCity generates compelling and immersive\nscenes that are rich in detail and diversity.\n","authors":["Paul Engstler","Aleksandar Shtedritski","Iro Laina","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2503.16420v1.pdf","comment":"Project page: https://research.paulengstler.com/syncity/"},{"id":"http://arxiv.org/abs/2503.16418v1","updated":"2025-03-20T17:59:34Z","published":"2025-03-20T17:59:34Z","title":"InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity","summary":"  Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.\n","authors":["Liming Jiang","Qing Yan","Yumin Jia","Zichuan Liu","Hao Kang","Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.16418v1.pdf","comment":"Project page: https://bytedance.github.io/InfiniteYou/ Code and\n  model: https://github.com/bytedance/InfiniteYou"},{"id":"http://arxiv.org/abs/2503.16412v1","updated":"2025-03-20T17:59:12Z","published":"2025-03-20T17:59:12Z","title":"DreamTexture: Shape from Virtual Texture with Analysis by Augmentation","summary":"  DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation.\n","authors":["Ananta R. Bhattarai","Xingzhe He","Alla Sheffer","Helge Rhodin"],"pdf_url":"https://arxiv.org/pdf/2503.16412v1.pdf","comment":"Project page: https://anantarb.github.io/dreamtexture/"},{"id":"http://arxiv.org/abs/2503.16413v1","updated":"2025-03-20T17:59:12Z","published":"2025-03-20T17:59:12Z","title":"M3: 3D-Spatial MultiModal Memory","summary":"  We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation.\n","authors":["Xueyan Zou","Yuchen Song","Ri-Zhao Qiu","Xuanbin Peng","Jianglong Ye","Sifei Liu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16413v1.pdf","comment":"ICLR2025 homepage: https://m3-spatial-memory.github.io code:\n  https://github.com/MaureenZOU/m3-spatial"},{"id":"http://arxiv.org/abs/2503.16408v1","updated":"2025-03-20T17:58:38Z","published":"2025-03-20T17:58:38Z","title":"RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints","summary":"  Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.\n","authors":["Yiran Qin","Li Kang","Xiufeng Song","Zhenfei Yin","Xiaohong Liu","Xihui Liu","Ruimao Zhang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2503.16408v1.pdf","comment":"Project page: https://iranqin.github.io/robofactory/"},{"id":"http://arxiv.org/abs/2503.16406v1","updated":"2025-03-20T17:56:20Z","published":"2025-03-20T17:56:20Z","title":"VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness","summary":"  Recent large-scale text-to-image diffusion models generate photorealistic\nimages but often struggle to accurately depict interactions between humans and\nobjects due to their limited ability to differentiate various interaction\nwords. In this work, we propose VerbDiff to address the challenge of capturing\nnuanced interactions within text-to-image diffusion models. VerbDiff is a novel\ntext-to-image generation model that weakens the bias between interaction words\nand objects, enhancing the understanding of interactions. Specifically, we\ndisentangle various interaction words from frequency-based anchor words and\nleverage localized interaction regions from generated images to help the model\nbetter capture semantics in distinctive words without extra conditions. Our\napproach enables the model to accurately understand the intended interaction\nbetween humans and objects, producing high-quality images with accurate\ninteractions aligned with specified verbs. Extensive experiments on the\nHICO-DET dataset demonstrate the effectiveness of our method compared to\nprevious approaches.\n","authors":["SeungJu Cha","Kwanyoung Lee","Ye-Chan Kim","Hyunwoo Oh","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2503.16406v1.pdf","comment":"Accepted at CVPR 2025, code :\n  https://github.com/SeungJuCha/VerbDiff.git"},{"id":"http://arxiv.org/abs/2407.18908v2","updated":"2025-03-20T17:56:05Z","published":"2024-07-26T17:59:09Z","title":"Wolf: Dense Video Captioning with a World Summarization Framework","summary":"  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/.\n","authors":["Boyi Li","Ligeng Zhu","Ran Tian","Shuhan Tan","Yuxiao Chen","Yao Lu","Yin Cui","Sushant Veer","Max Ehrlich","Jonah Philion","Xinshuo Weng","Fuzhao Xue","Linxi Fan","Yuke Zhu","Jan Kautz","Andrew Tao","Ming-Yu Liu","Sanja Fidler","Boris Ivanovic","Trevor Darrell","Jitendra Malik","Song Han","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2407.18908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16399v1","updated":"2025-03-20T17:54:29Z","published":"2025-03-20T17:54:29Z","title":"SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World","summary":"  Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ.\n","authors":["Chen Chen","Zhirui Wang","Taowei Sheng","Yi Jiang","Yundu Li","Peirui Cheng","Luning Zhang","Kaiqiang Chen","Yanfeng Hu","Xue Yang","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2503.16399v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.16397v1","updated":"2025-03-20T17:54:02Z","published":"2025-03-20T17:54:02Z","title":"Scale-wise Distillation of Diffusion Models","summary":"  We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies.\n","authors":["Nikita Starodubcev","Denis Kuznedelev","Artem Babenko","Dmitry Baranchuk"],"pdf_url":"https://arxiv.org/pdf/2503.16397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16396v1","updated":"2025-03-20T17:53:38Z","published":"2025-03-20T17:53:38Z","title":"SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video\n  Diffusion for High-Quality 4D Generation","summary":"  We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model\nfor dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is\nmore robust to occlusions and large motion, generalizes better to real-world\nvideos, and produces higher-quality outputs in terms of detail sharpness and\nspatio-temporal consistency. We achieve this by introducing key improvements in\nmultiple aspects: 1) network architecture: eliminating the dependency of\nreference multi-views and designing blending mechanism for 3D and frame\nattention, 2) data: enhancing quality and quantity of training data, 3)\ntraining strategy: adopting progressive 3D-4D training for better\ngeneralization, and 4) 4D optimization: handling 3D inconsistency and large\nmotion via 2-stage refinement and progressive frame sampling. Extensive\nexperiments demonstrate significant performance gain by SV4D 2.0 both visually\nand quantitatively, achieving better detail (-14\\% LPIPS) and 4D consistency\n(-44\\% FV4D) in novel-view video synthesis and 4D optimization (-12\\% LPIPS and\n-24\\% FV4D) compared to SV4D. Project page: https://sv4d2.0.github.io.\n","authors":["Chun-Han Yao","Yiming Xie","Vikram Voleti","Huaizu Jiang","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2503.16396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16394v1","updated":"2025-03-20T17:53:12Z","published":"2025-03-20T17:53:12Z","title":"Do Visual Imaginations Improve Vision-and-Language Navigation Agents?","summary":"  Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/.\n","authors":["Akhil Perincherry","Jacob Krantz","Stefan Lee"],"pdf_url":"https://arxiv.org/pdf/2503.16394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16389v1","updated":"2025-03-20T17:49:01Z","published":"2025-03-20T17:49:01Z","title":"Attentional Triple-Encoder Network in Spatiospectral Domains for Medical\n  Image Segmentation","summary":"  Retinal Optical Coherence Tomography (OCT) segmentation is essential for\ndiagnosing pathology. Traditional methods focus on either spatial or spectral\ndomains, overlooking their combined dependencies. We propose a triple-encoder\nnetwork that integrates CNNs for spatial features, Fast Fourier Convolution\n(FFC) for spectral features, and attention mechanisms to capture global\nrelationships across both domains. Attention fusion modules integrate\nconvolution and cross-attention to further enhance features. Our method\nachieves an average Dice score improvement from 0.855 to 0.864, outperforming\nprior work.\n","authors":["Kristin Qi","Xinhan Di"],"pdf_url":"https://arxiv.org/pdf/2503.16389v1.pdf","comment":"IEEE Conference on Artificial Intelligence (IEEE CAI)"},{"id":"http://arxiv.org/abs/2503.16378v1","updated":"2025-03-20T17:41:16Z","published":"2025-03-20T17:41:16Z","title":"Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in\n  Rainy Conditions","summary":"  Existing autonomous driving datasets are predominantly oriented towards\nwell-structured urban settings and favorable weather conditions, leaving the\ncomplexities of rural environments and adverse weather conditions largely\nunaddressed. Although some datasets encompass variations in weather and\nlighting, bad weather scenarios do not appear often. Rainfall can significantly\nimpair sensor functionality, introducing noise and reflections in LiDAR and\ncamera data and reducing the system's capabilities for reliable environmental\nperception and safe navigation. We introduce the Panoptic-CUDAL dataset, a\nnovel dataset purpose-built for panoptic segmentation in rural areas subject to\nrain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL\noffers a diverse, information-rich dataset in a challenging scenario. We\npresent analysis of the recorded data and provide baseline results for panoptic\nand semantic segmentation methods on LiDAR point clouds. The dataset can be\nfound here:\nhttps://robotics.sydney.edu.au/our-research/intelligent-transportation-systems/\n","authors":["Tzu-Yun Tseng","Alexey Nekrasov","Malcolm Burdorf","Bastian Leibe","Julie Stephany Berrio","Mao Shan","Stewart Worrall"],"pdf_url":"https://arxiv.org/pdf/2503.16378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16376v1","updated":"2025-03-20T17:39:06Z","published":"2025-03-20T17:39:06Z","title":"LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial\n  Images","summary":"  The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG.\n","authors":["Leyang Wang","Joice Lin"],"pdf_url":"https://arxiv.org/pdf/2503.16376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16375v1","updated":"2025-03-20T17:37:43Z","published":"2025-03-20T17:37:43Z","title":"NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes","summary":"  In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training.\n","authors":["Han-Hung Lee","Qinghong Han","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2503.16375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16365v1","updated":"2025-03-20T17:21:58Z","published":"2025-03-20T17:21:58Z","title":"JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play\n  Visual Games with Keyboards and Mouse","summary":"  Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA.\n","authors":["Muyao Li","Zihao Wang","Kaichen He","Xiaojian Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2503.16365v1.pdf","comment":"22 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.11190v2","updated":"2025-03-20T17:20:55Z","published":"2025-02-16T16:31:00Z","title":"ReLearn: Unlearning via Learning for Large Language Models","summary":"  Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.\n","authors":["Haoming Xu","Ningyuan Zhao","Liming Yang","Sendong Zhao","Shumin Deng","Mengru Wang","Bryan Hooi","Nay Oo","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11190v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2211.14312v4","updated":"2025-03-20T17:19:33Z","published":"2022-11-20T04:59:23Z","title":"Karyotype AI for Precision Oncology","summary":"  We present a machine learning method capable of accurately detecting\nchromosome abnormalities that cause blood cancers directly from microscope\nimages of the metaphase stage of cell division. The pipeline is built on a\nseries of fine-tuned Vision Transformers. Current state of the art (and\nstandard clinical practice) requires expensive, manual expert analysis, whereas\nour pipeline takes only 15 seconds per metaphase image. Using a novel\npretraining-finetuning strategy to mitigate the challenge of data scarcity, we\nachieve a high precision-recall score of 94% AUC for the clinically significant\ndel(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of\nrare aberrations based on model latent embeddings. The ability to quickly,\naccurately, and scalably diagnose genetic abnormalities directly from metaphase\nimages could transform karyotyping practice and improve patient outcomes. We\nwill make code publicly available.\n","authors":["Zahra Shamsi","Drew Bryant","Jacob Wilson","Xiaoyu Qu","Avinava Dubey","Konik Kothari","Mostafa Dehghani","Mariya Chavarha","Valerii Likhosherstov","Brian Williams","Michael Frumkin","Fred Appelbaum","Krzysztof Choromanski","Ali Bashir","Min Fang"],"pdf_url":"https://arxiv.org/pdf/2211.14312v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16357v1","updated":"2025-03-20T17:16:03Z","published":"2025-03-20T17:16:03Z","title":"UniSync: A Unified Framework for Audio-Visual Synchronization","summary":"  Precise audio-visual synchronization in speech videos is crucial for content\nquality and viewer comprehension. Existing methods have made significant\nstrides in addressing this challenge through rule-based approaches and\nend-to-end learning techniques. However, these methods often rely on limited\naudio-visual representations and suboptimal learning strategies, potentially\nconstraining their effectiveness in more complex scenarios. To address these\nlimitations, we present UniSync, a novel approach for evaluating audio-visual\nsynchronization using embedding similarities. UniSync offers broad\ncompatibility with various audio representations (e.g., Mel spectrograms,\nHuBERT) and visual representations (e.g., RGB images, face parsing maps, facial\nlandmarks, 3DMM), effectively handling their significant dimensional\ndifferences. We enhance the contrastive learning framework with a margin-based\nloss component and cross-speaker unsynchronized pairs, improving discriminative\ncapabilities. UniSync outperforms existing methods on standard datasets and\ndemonstrates versatility across diverse audio-visual representations. Its\nintegration into talking face generation frameworks enhances synchronization\nquality in both natural and AI-generated content.\n","authors":["Tao Feng","Yifan Xie","Xun Guan","Jiyuan Song","Zhou Liu","Fei Ma","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2503.16357v1.pdf","comment":"7 pages, 3 figures, accepted by ICME 2025"},{"id":"http://arxiv.org/abs/2503.16356v1","updated":"2025-03-20T17:14:34Z","published":"2025-03-20T17:14:34Z","title":"CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners","summary":"  Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.\n","authors":["Yunzhi Yao","Jizhan Fang","Jia-Chen Gu","Ningyu Zhang","Shumin Deng","Huajun Chen","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2503.16356v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.16338v1","updated":"2025-03-20T16:56:13Z","published":"2025-03-20T16:56:13Z","title":"Gaussian Graph Network: Learning Efficient and Generalizable Gaussian\n  Representations from Multi-view Images","summary":"  3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis\nperformance. While conventional methods require per-scene optimization, more\nrecently several feed-forward methods have been proposed to generate\npixel-aligned Gaussian representations with a learnable network, which are\ngeneralizable to different scenes. However, these methods simply combine\npixel-aligned Gaussians from multiple views as scene representations, thereby\nleading to artifacts and extra memory cost without fully capturing the\nrelations of Gaussians from different images. In this paper, we propose\nGaussian Graph Network (GGN) to generate efficient and generalizable Gaussian\nrepresentations. Specifically, we construct Gaussian Graphs to model the\nrelations of Gaussian groups from different views. To support message passing\nat Gaussian level, we reformulate the basic graph operations over Gaussian\nrepresentations, enabling each Gaussian to benefit from its connected Gaussian\ngroups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling\nlayer to aggregate various Gaussian groups for efficient representations. We\nconduct experiments on the large-scale RealEstate10K and ACID datasets to\ndemonstrate the efficiency and generalization of our method. Compared to the\nstate-of-the-art methods, our model uses fewer Gaussians and achieves better\nimage quality with higher rendering speed.\n","authors":["Shengjun Zhang","Xin Fei","Fangfu Liu","Haixu Song","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2503.16338v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2503.16322v1","updated":"2025-03-20T16:44:43Z","published":"2025-03-20T16:44:43Z","title":"Ultra-Resolution Adaptation with Ease","summary":"  Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed \\emph{URAE}. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\n\\href{https://github.com/Huage001/URAE}{here}.\n","authors":["Ruonan Yu","Songhua Liu","Zhenxiong Tan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16322v1.pdf","comment":"Technical Report. Codes are available\n  \\href{https://github.com/Huage001/URAE}{here}"},{"id":"http://arxiv.org/abs/2503.09091v2","updated":"2025-03-20T16:43:54Z","published":"2025-03-12T06:03:33Z","title":"Multi-Modal Foundation Models for Computational Pathology: A Survey","summary":"  Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.\n","authors":["Dong Li","Guihong Wan","Xintao Wu","Xinyu Wu","Xiaohui Chen","Yi He","Christine G. Lian","Peter K. Sorger","Yevgeniy R. Semenov","Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.09091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16318v1","updated":"2025-03-20T16:41:50Z","published":"2025-03-20T16:41:50Z","title":"Dynamic Point Maps: A Versatile Representation for Dynamic 3D\n  Reconstruction","summary":"  DUSt3R has recently shown that one can reduce many tasks in multi-view\ngeometry, including estimating camera intrinsics and extrinsics, reconstructing\nthe scene in 3D, and establishing image correspondences, to the prediction of a\npair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds\ndefined in a common reference frame. This formulation is elegant and powerful,\nbut unable to tackle dynamic scenes. To address this challenge, we introduce\nthe concept of Dynamic Point Maps (DPM), extending standard point maps to\nsupport 4D tasks such as motion segmentation, scene flow estimation, 3D object\ntracking, and 2D correspondence. Our key intuition is that, when time is\nintroduced, there are several possible spatial and time references that can be\nused to define the point maps. We identify a minimal subset of such\ncombinations that can be regressed by a network to solve the sub tasks\nmentioned above. We train a DPM predictor on a mixture of synthetic and real\ndata and evaluate it across diverse benchmarks for video depth prediction,\ndynamic point cloud reconstruction, 3D scene flow and object pose tracking,\nachieving state-of-the-art performance. Code, models and additional results are\navailable at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/.\n","authors":["Edgar Sucar","Zihang Lai","Eldar Insafutdinov","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2503.16318v1.pdf","comment":"Web page:\n  https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/"},{"id":"http://arxiv.org/abs/2503.16309v1","updated":"2025-03-20T16:33:45Z","published":"2025-03-20T16:33:45Z","title":"Rapid patient-specific neural networks for intraoperative X-ray to\n  volume registration","summary":"  The integration of artificial intelligence in image-guided interventions\nholds transformative potential, promising to extract 3D geometric and\nquantitative information from conventional 2D imaging modalities during complex\nprocedures. Achieving this requires the rapid and precise alignment of 2D\nintraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,\nMRI). However, current 2D/3D registration methods fail across the broad\nspectrum of procedures dependent on X-ray guidance: traditional optimization\ntechniques require custom parameter tuning for each subject, whereas neural\nnetworks trained on small datasets do not generalize to new patients or require\nlabor-intensive manual annotations, increasing clinical burden and precluding\napplication to new anatomical targets. To address these challenges, we present\nxvr, a fully automated framework for training patient-specific neural networks\nfor 2D/3D registration. xvr uses physics-based simulation to generate abundant\nhigh-quality training data from a patient's own preoperative volumetric\nimaging, thereby overcoming the inherently limited ability of supervised models\nto generalize to new patients and procedures. Furthermore, xvr requires only 5\nminutes of training per patient, making it suitable for emergency interventions\nas well as planned procedures. We perform the largest evaluation of a 2D/3D\nregistration algorithm on real X-ray data to date and find that xvr robustly\ngeneralizes across a diverse dataset comprising multiple anatomical structures,\nimaging modalities, and hospitals. Across surgical tasks, xvr achieves\nsubmillimeter-accurate registration at intraoperative speeds, improving upon\nexisting methods by an order of magnitude. xvr is released as open-source\nsoftware freely available at https://github.com/eigenvivek/xvr.\n","authors":["Vivek Gopalakrishnan","Neel Dey","David-Dimitris Chlorogiannis","Andrew Abumoussa","Anna M. Larson","Darren B. Orbach","Sarah Frisken","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2503.16309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.10745v2","updated":"2025-03-20T16:24:10Z","published":"2025-03-13T17:56:22Z","title":"Unifying 2D and 3D Vision-Language Understanding","summary":"  Progress in 3D vision-language learning has been hindered by the scarcity of\nlarge-scale 3D datasets. We introduce UniVLG, a unified architecture for 2D and\n3D vision-language understanding that bridges the gap between existing\n2D-centric models and the rich 3D sensory data available in embodied systems.\nOur approach initializes most model weights from pre-trained 2D models and\ntrains on both 2D and 3D vision-language data. We propose a novel\nlanguage-conditioned mask decoder shared across 2D and 3D modalities to ground\nobjects effectively in both RGB and RGB-D images, outperforming box-based\napproaches. To further reduce the domain gap between 2D and 3D, we incorporate\n2D-to-3D lifting strategies, enabling UniVLG to utilize 2D data to enhance 3D\nperformance. With these innovations, our model achieves state-of-the-art\nperformance across multiple 3D vision-language grounding tasks, demonstrating\nthe potential of transferring advances from 2D vision-language learning to the\ndata-constrained 3D domain. Furthermore, co-training on both 2D and 3D data\nenhances performance across modalities without sacrificing 2D capabilities. By\nremoving the reliance on 3D mesh reconstruction and ground-truth object\nproposals, UniVLG sets a new standard for realistic, embodied-aligned\nevaluation. Code and additional visualizations are available at\nhttps://univlg.github.io .\n","authors":["Ayush Jain","Alexander Swerdlow","Yuzhou Wang","Sergio Arnaud","Ada Martin","Alexander Sax","Franziska Meier","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2503.10745v2.pdf","comment":"The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2503.16302v1","updated":"2025-03-20T16:23:44Z","published":"2025-03-20T16:23:44Z","title":"Unleashing Vecset Diffusion Model for Fast Shape Generation","summary":"  3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.\n","authors":["Zeqiang Lai","Yunfei Zhao","Zibo Zhao","Haolin Liu","Fuyun Wang","Huiwen Shi","Xianghui Yang","Qinxiang Lin","Jinwei Huang","Yuhong Liu","Jie Jiang","Chunchao Guo","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2503.16302v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2503.16289v1","updated":"2025-03-20T16:15:16Z","published":"2025-03-20T16:15:16Z","title":"SceneMI: Motion In-betweening for Modeling Human-Scene Interactions","summary":"  Modeling human-scene interactions (HSI) is essential for understanding and\nsimulating everyday human behaviors. Recent approaches utilizing generative\nmodeling have made progress in this domain; however, they are limited in\ncontrollability and flexibility for real-world applications. To address these\nchallenges, we propose reformulating the HSI modeling problem as Scene-aware\nMotion In-betweening -- a more tractable and practical task. We introduce\nSceneMI, a framework that supports several practical applications, including\nkeyframe-guided character animation in 3D scenes and enhancing the motion\nquality of imperfect HSI data. SceneMI employs dual scene descriptors to\ncomprehensively encode global and local scene context. Furthermore, our\nframework leverages the inherent denoising nature of diffusion models to\ngeneralize on noisy keyframes. Experimental results demonstrate SceneMI's\neffectiveness in scene-aware keyframe in-betweening and generalization to the\nreal-world GIMO dataset, where motions and scenes are acquired by noisy IMU\nsensors and smartphones. We further showcase SceneMI's applicability in HSI\nreconstruction from monocular videos.\n","authors":["Inwoo Hwang","Bing Zhou","Young Min Kim","Jian Wang","Chuan Guo"],"pdf_url":"https://arxiv.org/pdf/2503.16289v1.pdf","comment":"15 pages, Project page: http://inwoohwang.me/SceneMI"},{"id":"http://arxiv.org/abs/2411.10411v2","updated":"2025-03-20T16:15:14Z","published":"2024-11-15T18:29:59Z","title":"Repurposing Stable Diffusion Attention for Training-Free Unsupervised\n  Interactive Segmentation","summary":"  Recent progress in interactive point prompt based Image Segmentation allows\nto significantly reduce the manual effort to obtain high quality semantic\nlabels. State-of-the-art unsupervised methods use self-supervised pre-trained\nmodels to obtain pseudo-labels which are used in training a prompt-based\nsegmentation model. In this paper, we propose a novel unsupervised and\ntraining-free approach based solely on the self-attention of Stable Diffusion.\nWe interpret the self-attention tensor as a Markov transition operator, which\nenables us to iteratively construct a Markov chain. Pixel-wise counting of the\nrequired number of iterations along the Markov chain to reach a relative\nprobability threshold yields a Markov-iteration-map, which we simply call a\nMarkov-map. Compared to the raw attention maps, we show that our proposed\nMarkov-map has less noise, sharper semantic boundaries and more uniform values\nwithin semantically similar regions. We integrate the Markov-map in a simple\nyet effective truncated nearest neighbor framework to obtain interactive point\nprompt based segmentation. Despite being training-free, we experimentally show\nthat our approach yields excellent results in terms of Number of Clicks (NoC),\neven outperforming state-of-the-art training based unsupervised methods in most\nof the datasets. Code is available at https://github.com/mkarmann/m2n2.\n","authors":["Markus Karmann","Onay Urfalioglu"],"pdf_url":"https://arxiv.org/pdf/2411.10411v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16284v1","updated":"2025-03-20T16:12:42Z","published":"2025-03-20T16:12:42Z","title":"PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance\n  Learning for Whole Slide Image Classification","summary":"  Whole Slide Images (WSIs) are high-resolution digital scans widely used in\nmedical diagnostics. WSI classification is typically approached using Multiple\nInstance Learning (MIL), where the slide is partitioned into tiles treated as\ninterconnected instances. While attention-based MIL methods aim to identify the\nmost informative tiles, they often fail to fully exploit the spatial\nrelationships among them, potentially overlooking intricate tissue structures\ncrucial for accurate diagnosis. To address this limitation, we propose\nProbabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL\nframework that integrates spatial context into the attention mechanism through\nlearnable distance-decayed priors, formulated within a probabilistic\ninterpretation of self-attention as a posterior distribution. This formulation\nenables a dynamic inference of spatial relationships during training,\neliminating the need for predefined assumptions often imposed by previous\napproaches. Additionally, we suggest a spatial pruning strategy for the\nposterior, effectively reducing self-attention's quadratic complexity. To\nfurther enhance spatial modeling, we introduce a diversity loss that encourages\nvariation among attention heads, ensuring each captures distinct spatial\nrepresentations. Together, PSA-MIL enables a more data-driven and adaptive\nintegration of spatial context, moving beyond predefined constraints. We\nachieve state-of-the-art performance across both contextual and non-contextual\nbaselines, while significantly reducing computational costs.\n","authors":["Sharon Peled","Yosef E. Maruvka","Moti Freiman"],"pdf_url":"https://arxiv.org/pdf/2503.16284v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.16282v1","updated":"2025-03-20T16:10:33Z","published":"2025-03-20T16:10:33Z","title":"Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language\n  Model","summary":"  Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL\n","authors":["Zhaochong An","Guolei Sun","Yun Liu","Runjia Li","Junlin Han","Ender Konukoglu","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2503.16282v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16264v1","updated":"2025-03-20T15:57:25Z","published":"2025-03-20T15:57:25Z","title":"Do image and video quality metrics model low-level human vision?","summary":"  Image and video quality metrics, such as SSIM, LPIPS, and VMAF, are aimed to\npredict the perceived quality of the evaluated content and are often claimed to\nbe \"perceptual\". Yet, few metrics directly model human visual perception, and\nmost rely on hand-crafted formulas or training datasets to achieve alignment\nwith perceptual data. In this paper, we propose a set of tests for\nfull-reference quality metrics that examine their ability to model several\naspects of low-level human vision: contrast sensitivity, contrast masking, and\ncontrast matching. The tests are meant to provide additional scrutiny for newly\nproposed metrics. We use our tests to analyze 33 existing image and video\nquality metrics and find their strengths and weaknesses, such as the ability of\nLPIPS and MS-SSIM to predict contrast masking and poor performance of VMAF in\nthis task. We further find that the popular SSIM metric overemphasizes\ndifferences in high spatial frequencies, but its multi-scale counterpart,\nMS-SSIM, addresses this shortcoming. Such findings cannot be easily made using\nexisting evaluation protocols.\n","authors":["Dounia Hammou","Yancheng Cai","Pavan Madhusudanarao","Christos G. Bampis","Rafał K. Mantiuk"],"pdf_url":"https://arxiv.org/pdf/2503.16264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16263v1","updated":"2025-03-20T15:57:18Z","published":"2025-03-20T15:57:18Z","title":"From Monocular Vision to Autonomous Action: Guiding Tumor Resection via\n  3D Reconstruction","summary":"  Surgical automation requires precise guidance and understanding of the scene.\nCurrent methods in the literature rely on bulky depth cameras to create maps of\nthe anatomy, however this does not translate well to space-limited clinical\napplications. Monocular cameras are small and allow minimally invasive\nsurgeries in tight spaces but additional processing is required to generate 3D\nscene understanding. We propose a 3D mapping pipeline that uses only RGB images\nto create segmented point clouds of the target anatomy. To ensure the most\nprecise reconstruction, we compare different structure from motion algorithms'\nperformance on mapping the central airway obstructions, and test the pipeline\non a downstream task of tumor resection. In several metrics, including\npost-procedure tissue model evaluation, our pipeline performs comparably to\nRGB-D cameras and, in some cases, even surpasses their performance. These\npromising results demonstrate that automation guidance can be achieved in\nminimally invasive procedures with monocular cameras. This study is a step\ntoward the complete autonomy of surgical robots.\n","authors":["Ayberk Acar","Mariana Smith","Lidia Al-Zogbi","Tanner Watts","Fangjie Li","Hao Li","Nural Yilmaz","Paul Maria Scheikl","Jesse F. d'Almeida","Susheela Sharma","Lauren Branscombe","Tayfun Efe Ertop","Robert J. Webster III","Ipek Oguz","Alan Kuntz","Axel Krieger","Jie Ying Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16263v1.pdf","comment":"7 Pages, 8 Figures, 1 Table. This work has been submitted IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS) for\n  possible publication"},{"id":"http://arxiv.org/abs/2411.18941v2","updated":"2025-03-20T15:57:02Z","published":"2024-11-28T06:18:31Z","title":"Revealing Key Details to See Differences: A Novel Prototypical\n  Perspective for Skeleton-based Action Recognition","summary":"  In skeleton-based action recognition, a key challenge is distinguishing\nbetween actions with similar trajectories of joints due to the lack of\nimage-level details in skeletal representations. Recognizing that the\ndifferentiation of similar actions relies on subtle motion details in specific\nbody parts, we direct our approach to focus on the fine-grained motion of local\nskeleton components. To this end, we introduce ProtoGCN, a Graph Convolutional\nNetwork (GCN)-based model that breaks down the dynamics of entire skeleton\nsequences into a combination of learnable prototypes representing core motion\npatterns of action units. By contrasting the reconstruction of prototypes,\nProtoGCN can effectively identify and enhance the discriminative representation\nof similar actions. Without bells and whistles, ProtoGCN achieves\nstate-of-the-art performance on multiple benchmark datasets, including NTU\nRGB+D, NTU RGB+D 120, Kinetics-Skeleton, and FineGYM, which demonstrates the\neffectiveness of the proposed method. The code is available at\nhttps://github.com/firework8/ProtoGCN.\n","authors":["Hongda Liu","Yunfan Liu","Min Ren","Hao Wang","Yunlong Wang","Zhenan Sun"],"pdf_url":"https://arxiv.org/pdf/2411.18941v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16260v1","updated":"2025-03-20T15:56:04Z","published":"2025-03-20T15:56:04Z","title":"Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart\n  Reasoning Data","summary":"  Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts.\n","authors":["Zijian Li","Jingjing Fu","Lei Song","Jiang Bian","Jun Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16260v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2503.16257v1","updated":"2025-03-20T15:52:43Z","published":"2025-03-20T15:52:43Z","title":"Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models","summary":"  Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.\n","authors":["Keda Tao","Haoxuan You","Yang Sui","Can Qin","Huan Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16257v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2412.09668v2","updated":"2025-03-20T15:50:45Z","published":"2024-12-12T18:53:49Z","title":"Vision-Language Models Generate More Homogeneous Stories for\n  Phenotypically Black Individuals","summary":"  Vision-Language Models (VLMs) extend Large Language Models' capabilities by\nintegrating image processing, but concerns persist about their potential to\nreproduce and amplify human biases. While research has documented how these\nmodels perpetuate stereotypes across demographic groups, most work has focused\non between-group biases rather than within-group differences. This study\ninvestigates homogeneity bias-the tendency to portray groups as more uniform\nthan they are-within Black Americans, examining how perceived racial\nphenotypicality influences VLMs' outputs. Using computer-generated images that\nsystematically vary in phenotypicality, we prompted VLMs to generate stories\nabout these individuals and measured text similarity to assess content\nhomogeneity. Our findings reveal three key patterns: First, VLMs generate\nsignificantly more homogeneous stories about Black individuals with higher\nphenotypicality compared to those with lower phenotypicality. Second, stories\nabout Black women consistently display greater homogeneity than those about\nBlack men across all models tested. Third, in two of three VLMs, this\nhomogeneity bias is primarily driven by a pronounced interaction where\nphenotypicality strongly influences content variation for Black women but has\nminimal impact for Black men. These results demonstrate how intersectionality\nshapes AI-generated representations and highlight the persistence of\nstereotyping that mirror documented biases in human perception, where increased\nracial phenotypicality leads to greater stereotyping and less individualized\nrepresentation.\n","authors":["Messi H. J. Lee","Soyeon Jeon"],"pdf_url":"https://arxiv.org/pdf/2412.09668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15195v2","updated":"2025-03-20T15:49:10Z","published":"2025-03-19T13:33:29Z","title":"Benchmarking Large Language Models for Handwritten Text Recognition","summary":"  Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.\n","authors":["Giorgia Crosilla","Lukas Klic","Giovanni Colavizza"],"pdf_url":"https://arxiv.org/pdf/2503.15195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16254v1","updated":"2025-03-20T15:47:14Z","published":"2025-03-20T15:47:14Z","title":"M2N2V2: Multi-Modal Unsupervised and Training-free Interactive\n  Segmentation","summary":"  We present Markov Map Nearest Neighbor V2 (M2N2V2), a novel and simple, yet\neffective approach which leverages depth guidance and attention maps for\nunsupervised and training-free point-prompt-based interactive segmentation.\nFollowing recent trends in supervised multimodal approaches, we carefully\nintegrate depth as an additional modality to create novel depth-guided\nMarkov-maps. Furthermore, we observe occasional segment size fluctuations in\nM2N2 during the interactive process, which can decrease the overall mIoU's. To\nmitigate this problem, we model the prompting as a sequential process and\npropose a novel adaptive score function which considers the previous\nsegmentation and the current prompt point in order to prevent unreasonable\nsegment size changes. Using Stable Diffusion 2 and Depth Anything V2 as\nbackbones, we empirically show that our proposed M2N2V2 significantly improves\nthe Number of Clicks (NoC) and mIoU compared to M2N2 in all datasets except\nthose from the medical domain. Interestingly, our unsupervised approach\nachieves competitive results compared to supervised methods like SAM and\nSimpleClick in the more challenging DAVIS and HQSeg44K datasets in the NoC\nmetric, reducing the gap between supervised and unsupervised methods.\n","authors":["Markus Karmann","Peng-Tao Jiang","Bo Li","Onay Urfalioglu"],"pdf_url":"https://arxiv.org/pdf/2503.16254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16251v1","updated":"2025-03-20T15:46:03Z","published":"2025-03-20T15:46:03Z","title":"RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning\n  by Balancing Privacy, Fairness and Utility in Autonomous Vehicles","summary":"  Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches.\n","authors":["Dawood Wasif","Terrence J. Moore","Jin-Hee Cho"],"pdf_url":"https://arxiv.org/pdf/2503.16251v1.pdf","comment":"Submitted to PETS 2025 (under review)"},{"id":"http://arxiv.org/abs/2503.16247v1","updated":"2025-03-20T15:43:14Z","published":"2025-03-20T15:43:14Z","title":"OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution\n  Detection","summary":"  The growing reliance on Artificial Intelligence (AI) in critical domains such\nas healthcare demands robust mechanisms to ensure the trustworthiness of these\nsystems, especially when faced with unexpected or anomalous inputs. This paper\nintroduces the Open Medical Imaging Benchmarks for Out-Of-Distribution\nDetection (OpenMIBOOD), a comprehensive framework for evaluating\nout-of-distribution (OOD) detection methods specifically in medical imaging\ncontexts. OpenMIBOOD includes three benchmarks from diverse medical domains,\nencompassing 14 datasets divided into covariate-shifted in-distribution,\nnear-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these\nbenchmarks, providing a standardized reference to advance the development and\nfair comparison of OOD detection methods. Results reveal that findings from\nbroad-scale OOD benchmarks in natural image domains do not translate to medical\napplications, underscoring the critical need for such benchmarks in the medical\nfield. By mitigating the risk of exposing AI models to inputs outside their\ntraining distribution, OpenMIBOOD aims to support the advancement of reliable\nand trustworthy AI systems in healthcare. The repository is available at\nhttps://github.com/remic-othr/OpenMIBOOD.\n","authors":["Max Gutbrod","David Rauber","Danilo Weber Nunes","Christoph Palm"],"pdf_url":"https://arxiv.org/pdf/2503.16247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10491v2","updated":"2025-03-20T15:32:47Z","published":"2024-10-14T13:35:47Z","title":"TWIST & SCOUT: Grounding Multimodal LLM-Experts by Forget-Free Tuning","summary":"  Spatial awareness is key to enable embodied multimodal AI systems. Yet,\nwithout vast amounts of spatial supervision, current Multimodal Large Language\nModels (MLLMs) struggle at this task. In this paper, we introduce TWIST &\nSCOUT, a framework that equips pre-trained MLLMs with visual grounding ability\nwithout forgetting their existing image and language understanding skills. To\nthis end, we propose TWIST, a twin-expert stepwise tuning module that modifies\nthe decoder of the language model using one frozen module pre-trained on image\nunderstanding tasks and another learnable one for visual grounding tasks. This\nallows the MLLM to retain previously learned knowledge and skills, while\nacquiring what is missing. To fine-tune the model effectively, we generate a\nhigh-quality synthetic dataset we call SCOUT, which mimics human reasoning in\nvisual grounding. This dataset provides rich supervision signals, describing a\nstep-by-step multimodal reasoning process, thereby simplifying the task of\nvisual grounding. We evaluate our approach on several standard benchmark\ndatasets, encompassing grounded image captioning, zero-shot localization, and\nvisual grounding tasks. Our method consistently delivers strong performance\nacross all tasks, while retaining the pre-trained image understanding\ncapabilities.\n","authors":["Aritra Bhowmik","Mohammad Mahdi Derakhshani","Dennis Koelma","Yuki M. Asano","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.10491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05421v2","updated":"2025-03-20T15:21:00Z","published":"2024-08-10T03:15:24Z","title":"EPAM-Net: An Efficient Pose-driven Attention-guided Multimodal Network\n  for Video Action Recognition","summary":"  Existing multimodal-based human action recognition approaches are\ncomputationally intensive, limiting their deployment in real-time applications.\nIn this work, we present a novel and efficient pose-driven attention-guided\nmultimodal network (EPAM-Net) for action recognition in videos. Specifically,\nwe propose eXpand temporal Shift (X-ShiftNet) convolutional architectures for\nRGB and pose streams to capture spatio-temporal features from RGB videos and\ntheir skeleton sequences. The X-ShiftNet tackles the high computational cost of\nthe 3D CNNs by integrating the Temporal Shift Module (TSM) into an efficient 2D\nCNN, enabling efficient spatiotemporal learning. Then skeleton features are\nutilized to guide the visual network stream, focusing on keyframes and their\nsalient spatial regions using the proposed spatial-temporal attention block.\nFinally, the predictions of the two streams are fused for final classification.\nThe experimental results show that our method, with a significant reduction in\nfloating-point operations (FLOPs), outperforms and competes with the\nstate-of-the-art methods on NTU RGB-D 60, NTU RGB-D 120, PKU-MMD, and Toyota\nSmartHome datasets. The proposed EPAM-Net provides up to a 72.8x reduction in\nFLOPs and up to a 48.6x reduction in the number of network parameters. The code\nwill be available at\nhttps://github.com/ahmed-nady/Multimodal-Action-Recognition.\n","authors":["Ahmed Abdelkawy","Asem Ali","Aly Farag"],"pdf_url":"https://arxiv.org/pdf/2408.05421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16222v1","updated":"2025-03-20T15:17:05Z","published":"2025-03-20T15:17:05Z","title":"Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson\n  Inverse Problems","summary":"  This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods.\n","authors":["Teresa Klatzer","Savvas Melidonis","Marcelo Pereyra","Konstantinos C. Zygalakis"],"pdf_url":"https://arxiv.org/pdf/2503.16222v1.pdf","comment":"31 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.16218v1","updated":"2025-03-20T15:11:56Z","published":"2025-03-20T15:11:56Z","title":"Temporal Score Analysis for Understanding and Correcting Diffusion\n  Artifacts","summary":"  Visual artifacts remain a persistent challenge in diffusion models, even with\ntraining on massive datasets. Current solutions primarily rely on supervised\ndetectors, yet lack understanding of why these artifacts occur in the first\nplace. In our analysis, we identify three distinct phases in the diffusion\ngenerative process: Profiling, Mutation, and Refinement. Artifacts typically\nemerge during the Mutation phase, where certain regions exhibit anomalous score\ndynamics over time, causing abrupt disruptions in the normal evolution pattern.\nThis temporal nature explains why existing methods focusing only on spatial\nuncertainty of the final output fail at effective artifact localization. Based\non these insights, we propose ASCED (Abnormal Score Correction for Enhancing\nDiffusion), that detects artifacts by monitoring abnormal score dynamics during\nthe diffusion process, with a trajectory-aware on-the-fly mitigation strategy\nthat appropriate generation of noise in the detected areas. Unlike most\nexisting methods that apply post hoc corrections, \\eg, by applying a\nnoising-denoising scheme after generation, our mitigation strategy operates\nseamlessly within the existing diffusion process. Extensive experiments\ndemonstrate that our proposed approach effectively reduces artifacts across\ndiverse domains, matching or surpassing existing supervised methods without\nadditional training.\n","authors":["Yu Cao","Zengqun Zhao","Ioannis Patras","Shaogang Gong"],"pdf_url":"https://arxiv.org/pdf/2503.16218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15060v2","updated":"2025-03-20T15:09:59Z","published":"2025-03-19T09:53:11Z","title":"Conjuring Positive Pairs for Efficient Unification of Representation\n  Learning and Image Synthesis","summary":"  While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models.\n","authors":["Imanol G. Estepa","Jesús M. Rodríguez-de-Vera","Ignacio Sarasúa","Bhalaji Nagarajan","Petia Radeva"],"pdf_url":"https://arxiv.org/pdf/2503.15060v2.pdf","comment":"The source code is available in https://github.com/ImaGonEs/Sorcen"},{"id":"http://arxiv.org/abs/2409.04982v2","updated":"2025-03-20T15:03:39Z","published":"2024-09-08T05:35:05Z","title":"2DSig-Detect: a semi-supervised framework for anomaly detection on image\n  data using 2D-signatures","summary":"  The rapid advancement of machine learning technologies raises questions about\nthe security of machine learning models, with respect to both training-time\n(poisoning) and test-time (evasion, impersonation, and inversion) attacks.\nModels performing image-related tasks, e.g. detection, and classification, are\nvulnerable to adversarial attacks that can degrade their performance and\nproduce undesirable outcomes. This paper introduces a novel technique for\nanomaly detection in images called 2DSig-Detect, which uses a\n2D-signature-embedded semi-supervised framework rooted in rough path theory. We\ndemonstrate our method in adversarial settings for training-time and test-time\nattacks, and benchmark our framework against other state of the art methods.\nUsing 2DSig-Detect for anomaly detection, we show both superior performance and\na reduction in the computation time to detect the presence of adversarial\nperturbations in images.\n","authors":["Xinheng Xie","Kureha Yamaguchi","Margaux Leblanc","Simon Malzard","Varun Chhabra","Victoria Nockles","Yue Wu"],"pdf_url":"https://arxiv.org/pdf/2409.04982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09487v2","updated":"2025-03-20T14:58:40Z","published":"2025-03-12T15:46:12Z","title":"Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness","summary":"  While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.\n","authors":["Beier Zhu","Jiequan Cui","Hanwang Zhang","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.09487v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16195v1","updated":"2025-03-20T14:42:11Z","published":"2025-03-20T14:42:11Z","title":"VP-NTK: Exploring the Benefits of Visual Prompting in Differentially\n  Private Data Synthesis","summary":"  Differentially private (DP) synthetic data has become the de facto standard\nfor releasing sensitive data. However, many DP generative models suffer from\nthe low utility of synthetic data, especially for high-resolution images. On\nthe other hand, one of the emerging techniques in parameter efficient\nfine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing\nmodels to be reused for the purpose of adapting to subsequent downstream tasks.\nIn this work, we explore such a phenomenon in constructing captivating\ngenerative models with DP constraints. We show that VP in conjunction with\nDP-NTK, a DP generator that exploits the power of the neural tangent kernel\n(NTK) in training DP generative models, achieves a significant performance\nboost, particularly for high-resolution image datasets, with accuracy improving\nfrom 0.644$\\pm$0.044 to 0.769. Lastly, we perform ablation studies on the\neffect of different parameters that influence the overall performance of\nVP-NTK. Our work demonstrates a promising step forward in improving the utility\nof DP synthetic data, particularly for high-resolution images.\n","authors":["Chia-Yi Hsu","Jia-You Chen","Yu-Lin Tsai","Chih-Hsun Lin","Pin-Yu Chen","Chia-Mu Yu","Chun-Ying Huang"],"pdf_url":"https://arxiv.org/pdf/2503.16195v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2503.16194v1","updated":"2025-03-20T14:41:29Z","published":"2025-03-20T14:41:29Z","title":"Improving Autoregressive Image Generation through Coarse-to-Fine Token\n  Prediction","summary":"  Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds.\n","authors":["Ziyao Guo","Kaipeng Zhang","Michael Qizhe Shieh"],"pdf_url":"https://arxiv.org/pdf/2503.16194v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.16188v1","updated":"2025-03-20T14:37:45Z","published":"2025-03-20T14:37:45Z","title":"CLS-RL: Image Classification with Rule-Based Reinforcement Learning","summary":"  Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL.\n","authors":["Ming Li","Shitian Zhao","Jike Zhong","Yuxiang Lai","Kaipeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16188v1.pdf","comment":"Preprint, work in progress"},{"id":"http://arxiv.org/abs/2503.16185v1","updated":"2025-03-20T14:36:16Z","published":"2025-03-20T14:36:16Z","title":"MapGlue: Multimodal Remote Sensing Image Matching","summary":"  Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal\nfusion, localization, and object detection, but it faces severe challenges due\nto geometric, radiometric, and viewpoint discrepancies across imaging\nmodalities. Existing unimodal datasets lack scale and diversity, limiting deep\nlearning solutions. This paper proposes MapGlue, a universal MRSI matching\nframework, and MapData, a large-scale multimodal dataset addressing these gaps.\nOur contributions are twofold. MapData, a globally diverse dataset spanning 233\nsampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels).\nAfter rigorous cleaning, it provides 121,781 aligned electronic map-visible\nimage pairs (512x512 pixels) with hybrid manual-automated ground truth,\naddressing the scarcity of scalable multimodal benchmarks. MapGlue integrates\nsemantic context with a dual graph-guided mechanism to extract cross-modal\ninvariant features. This structure enables global-to-local interaction,\nenhancing descriptor robustness against modality-specific distortions.\nExtensive evaluations on MapData and five public datasets demonstrate MapGlue's\nsuperiority in matching accuracy under complex conditions, outperforming\nstate-of-the-art methods. Notably, MapGlue generalizes effectively to unseen\nmodalities without retraining, highlighting its adaptability. This work\naddresses longstanding challenges in MRSI matching by combining scalable\ndataset construction with a robust, semantics-driven framework. Furthermore,\nMapGlue shows strong generalization capabilities on other modality matching\ntasks for which it was not specifically trained. The dataset and code are\navailable at https://github.com/PeihaoWu/MapGlue.\n","authors":["Peihao Wu","Yongxiang Yao","Wenfei Zhang","Dong Wei","Yi Wan","Yansheng Li","Yongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16185v1.pdf","comment":"The dataset and code are available at\n  https://github.com/PeihaoWu/MapGlue"},{"id":"http://arxiv.org/abs/2503.16184v1","updated":"2025-03-20T14:35:46Z","published":"2025-03-20T14:35:46Z","title":"Accurate Scene Text Recognition with Efficient Model Scaling and Cloze\n  Self-Distillation","summary":"  Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs.\n","authors":["Andrea Maracani","Savas Ozkan","Sijun Cho","Hyowon Kim","Eunchung Noh","Jeongwon Min","Cho Jung Min","Dookun Park","Mete Ozay"],"pdf_url":"https://arxiv.org/pdf/2503.16184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17993v4","updated":"2025-03-20T14:31:10Z","published":"2024-09-26T16:04:31Z","title":"SSHNet: Unsupervised Cross-modal Homography Estimation via Problem\n  Reformulation and Split Optimization","summary":"  We propose a novel unsupervised cross-modal homography estimation learning\nframework, named Split Supervised Homography estimation Network (SSHNet).\nSSHNet reformulates the unsupervised cross-modal homography estimation into two\nsupervised sub-problems, each addressed by its specialized network: a\nhomography estimation network and a modality transfer network. To realize\nstable training, we introduce an effective split optimization strategy to train\neach network separately within its respective sub-problem. We also formulate an\nextra homography feature space supervision to enhance feature consistency,\nfurther boosting the estimation accuracy. Moreover, we employ a simple yet\neffective distillation training technique to reduce model parameters and\nimprove cross-domain generalization ability while maintaining comparable\nperformance. The training stability of SSHNet enables its cooperation with\nvarious homography estimation architectures. Experiments reveal that the SSHNet\nusing IHN as homography estimation network, namely SSHNet-IHN, outperforms\nprevious unsupervised approaches by a significant margin. Even compared to\nsupervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4% and 85.8%\nmean average corner errors (MACEs) reduction on the challenging OPT-SAR\ndataset.\n","authors":["Junchen Yu","Si-Yuan Cao","Runmin Zhang","Chenghao Zhang","Zhu Yu","Shujie Chen","Bailin Yang","Hui-liang Shen"],"pdf_url":"https://arxiv.org/pdf/2409.17993v4.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16179v1","updated":"2025-03-20T14:24:01Z","published":"2025-03-20T14:24:01Z","title":"Narrowing Class-Wise Robustness Gaps in Adversarial Training","summary":"  Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training.\n","authors":["Fatemeh Amerehi","Patrick Healy"],"pdf_url":"https://arxiv.org/pdf/2503.16179v1.pdf","comment":"4 figures, ICLR 2025 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2503.16177v1","updated":"2025-03-20T14:18:52Z","published":"2025-03-20T14:18:52Z","title":"OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene\n  Reconstruction and Rendering","summary":"  In large-scale scene reconstruction using 3D Gaussian splatting, it is common\nto partition the scene into multiple smaller regions and reconstruct them\nindividually. However, existing division methods are occlusion-agnostic,\nmeaning that each region may contain areas with severe occlusions. As a result,\nthe cameras within those regions are less correlated, leading to a low average\ncontribution to the overall reconstruction. In this paper, we propose an\nocclusion-aware scene division strategy that clusters training cameras based on\ntheir positions and co-visibilities to acquire multiple regions. Cameras in\nsuch regions exhibit stronger correlations and a higher average contribution,\nfacilitating high-quality scene reconstruction. We further propose a\nregion-based rendering technique to accelerate large scene rendering, which\nculls Gaussians invisible to the region where the viewpoint is located. Such a\ntechnique significantly speeds up the rendering without compromising quality.\nExtensive experiments on multiple large scenes show that our method achieves\nsuperior reconstruction results with faster rendering speed compared to\nexisting state-of-the-art approaches. Project page:\nhttps://occlugaussian.github.io.\n","authors":["Shiyong Liu","Xiao Tang","Zhihao Li","Yingfan He","Chongjie Ye","Jianzhuang Liu","Binxiao Huang","Shunbo Zhou","Xiaofei Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16177v1.pdf","comment":"Project website: https://occlugaussian.github.io"},{"id":"http://arxiv.org/abs/2503.16165v1","updated":"2025-03-20T14:06:53Z","published":"2025-03-20T14:06:53Z","title":"Iterative Optimal Attention and Local Model for Single Image Rain Streak\n  Removal","summary":"  High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks.\n","authors":["Xiangyu Li","Wanshu Fan","Yue Shen","Cong Wang","Wei Wang","Xin Yang","Qiang Zhang","Dongsheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.16165v1.pdf","comment":"14 pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.16153v1","updated":"2025-03-20T13:55:12Z","published":"2025-03-20T13:55:12Z","title":"FreeFlux: Understanding and Exploiting Layer-Specific Roles in\n  RoPE-Based MMDiT for Versatile Image Editing","summary":"  The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion\nTransformer (MMDiT) has significantly enhanced text-to-image generation\nquality. However, the fundamental reliance of self-attention layers on\npositional embedding versus query-key similarity during generation remains an\nintriguing question. We present the first mechanistic analysis of RoPE-based\nMMDiT models (e.g., FLUX), introducing an automated probing strategy that\ndisentangles positional information versus content dependencies by\nstrategically manipulating RoPE during generation. Our analysis reveals\ndistinct dependency patterns that do not straightforwardly correlate with\ndepth, offering new insights into the layer-specific roles in RoPE-based MMDiT.\nBased on these findings, we propose a training-free, task-specific image\nediting framework that categorizes editing tasks into three types:\nposition-dependent editing (e.g., object addition), content\nsimilarity-dependent editing (e.g., non-rigid editing), and region-preserved\nediting (e.g., background replacement). For each type, we design tailored\nkey-value injection strategies based on the characteristics of the editing\ntask. Extensive qualitative and quantitative evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches, particularly in preserving\noriginal semantic content and achieving seamless modifications.\n","authors":["Tianyi Wei","Yifan Zhou","Dongdong Chen","Xingang Pan"],"pdf_url":"https://arxiv.org/pdf/2503.16153v1.pdf","comment":"Project page: https://wtybest.github.io/projects/FreeFlux/"},{"id":"http://arxiv.org/abs/2502.02257v2","updated":"2025-03-20T13:55:08Z","published":"2025-02-04T12:08:20Z","title":"UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic\n  Segmentation","summary":"  Pre-training techniques significantly enhance the performance of semantic\nsegmentation tasks with limited training data. However, the efficacy under a\nlarge domain gap between pre-training (e.g. RGB) and fine-tuning (e.g.\ninfrared) remains underexplored. In this study, we first benchmark the infrared\nsemantic segmentation performance of various pre-training methods and reveal\nseveral phenomena distinct from the RGB domain. Next, our layerwise analysis of\npre-trained attention maps uncovers that: (1) There are three typical attention\npatterns (local, hybrid, and global); (2) Pre-training tasks notably influence\nthe pattern distribution across layers; (3) The hybrid pattern is crucial for\nsemantic segmentation as it attends to both nearby and foreground elements; (4)\nThe texture bias impedes model generalization in infrared tasks. Building on\nthese insights, we propose UNIP, a UNified Infrared Pre-training framework, to\nenhance the pre-trained model performance. This framework uses the\nhybrid-attention distillation NMI-HAD as the pre-training target, a large-scale\nmixed dataset InfMix for pre-training, and a last-layer feature pyramid network\nLL-FPN for fine-tuning. Experimental results show that UNIP outperforms various\npre-training methods by up to 13.5\\% in average mIoU on three infrared\nsegmentation tasks, evaluated using fine-tuning and linear probing metrics.\nUNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the\ncomputational cost. Furthermore, UNIP significantly surpasses state-of-the-art\n(SOTA) infrared or RGB segmentation methods and demonstrates broad potential\nfor application in other modalities, such as RGB and depth. Our code is\navailable at https://github.com/casiatao/UNIP.\n","authors":["Tao Zhang","Jinyong Wen","Zhen Chen","Kun Ding","Shiming Xiang","Chunhong Pan"],"pdf_url":"https://arxiv.org/pdf/2502.02257v2.pdf","comment":"ICLR 2025. 27 pages, 13 figures, 21 tables"},{"id":"http://arxiv.org/abs/2501.04004v2","updated":"2025-03-20T13:53:48Z","published":"2025-01-07T18:59:58Z","title":"LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes","summary":"  LiDAR data pretraining offers a promising approach to leveraging large-scale,\nreadily available datasets for enhanced data utilization. However, existing\nmethods predominantly focus on sparse voxel representation, overlooking the\ncomplementary attributes provided by other LiDAR representations. In this work,\nwe propose LiMoE, a framework that integrates the Mixture of Experts (MoE)\nparadigm into LiDAR data representation learning to synergistically combine\nmultiple representations, such as range images, sparse voxels, and raw points.\nOur approach consists of three stages: i) Image-to-LiDAR Pretraining, which\ntransfers prior knowledge from images to point clouds across different\nrepresentations; ii) Contrastive Mixture Learning (CML), which uses MoE to\nadaptively activate relevant attributes from each representation and distills\nthese mixed features into a unified 3D network; iii) Semantic Mixture\nSupervision (SMS), which combines semantic logits from multiple representations\nto boost downstream segmentation performance. Extensive experiments across\neleven large-scale LiDAR datasets demonstrate our effectiveness and\nsuperiority. The code has been made publicly accessible.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Liang Pan","Ziwei Liu","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2501.04004v2.pdf","comment":"CVPR 2025; 27 pages, 17 figures, 10 tables; Project Page at\n  https://ldkong.com/LiMoE"},{"id":"http://arxiv.org/abs/2412.18406v2","updated":"2025-03-20T13:53:20Z","published":"2024-12-24T12:52:16Z","title":"How accurate is mechanobiology? A statistical test of cell force","summary":"  Mechanobiology is gaining more and more traction as the fundamental role of\nphysical forces in biological function becomes clearer. Forces at the\nmicroscale are often measured indirectly using inverse problems such as\nTraction Force Microscopy because biological experiments are hard to access\nwith physical probes. In contrast with the experimental nature of biology and\nphysics, these measurements do not come with error bars, confidence regions, or\np-values. The aim of this manuscript is to publicize this issue and to propose\na first step towards a remedy therefor in the form of a general reconstruction\nframework. We also show that this opens the door to hypothesis testing of\nseemingly abstract experimental questions.\n","authors":["Aleix Boquet-Pujadas"],"pdf_url":"https://arxiv.org/pdf/2412.18406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16149v1","updated":"2025-03-20T13:52:51Z","published":"2025-03-20T13:52:51Z","title":"Selective Complementary Feature Fusion and Modal Feature Compression\n  Interaction for Brain Tumor Segmentation","summary":"  Efficient modal feature fusion strategy is the key to achieve accurate\nsegmentation of brain glioma. However, due to the specificity of different MRI\nmodes, it is difficult to carry out cross-modal fusion with large differences\nin modal features, resulting in the model ignoring rich feature information. On\nthe other hand, the problem of multi-modal feature redundancy interaction\noccurs in parallel networks due to the proliferation of feature dimensions,\nfurther increase the difficulty of multi-modal feature fusion at the bottom\nend. In order to solve the above problems, we propose a noval complementary\nfeature compression interaction network (CFCI-Net), which realizes the\ncomplementary fusion and compression interaction of multi-modal feature\ninformation with an efficient mode fusion strategy. Firstly, we propose a\nselective complementary feature fusion (SCFF) module, which adaptively fuses\nrich cross-modal feature information by complementary soft selection weights.\nSecondly, a modal feature compression interaction (MFCI) transformer is\nproposed to deal with the multi-mode fusion redundancy problem when the feature\ndimension surges. The MFCI transformer is composed of modal feature compression\n(MFC) and modal feature interaction (MFI) to realize redundancy feature\ncompression and multi-mode feature interactive learning. %In MFI, we propose a\nhierarchical interactive attention mechanism based on multi-head attention.\nEvaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net\nachieves superior results compared to state-of-the-art models. Code:\nhttps://github.com/CDmm0/CFCI-Net\n","authors":["Dong Chen","Boyue Zhao","Yi Zhang","Meng Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.16149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16134v1","updated":"2025-03-20T13:32:27Z","published":"2025-03-20T13:32:27Z","title":"Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS\n  Demosaicing","summary":"  Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps://github.com/Clausy9/BMTNet.\n","authors":["Shiyang Zhou","Haijin Zeng","Yunfan Lu","Tong Shao","Ke Tang","Yongyong Chen","Jie Liu","Jingyong Su"],"pdf_url":"https://arxiv.org/pdf/2503.16134v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2412.20392v2","updated":"2025-03-20T13:29:43Z","published":"2024-12-29T08:09:20Z","title":"Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning","summary":"  Multimodal contrastive learning models (e.g., CLIP) can learn high-quality\nrepresentations from large-scale image-text datasets, yet they exhibit\nsignificant vulnerabilities to backdoor attacks, raising serious safety\nconcerns. In this paper, we disclose that CLIP's vulnerabilities primarily stem\nfrom its excessive encoding of class-irrelevant features, which can compromise\nthe model's visual feature resistivity to input perturbations, making it more\nsusceptible to capturing the trigger patterns inserted by backdoor attacks.\nInspired by this finding, we propose Repulsive Visual Prompt Tuning (RVPT), a\nnovel defense approach that employs specially designed deep visual prompt\ntuning and feature-repelling loss to eliminate excessive class-irrelevant\nfeatures while simultaneously optimizing cross-entropy loss to maintain clean\naccuracy. Unlike existing multimodal backdoor defense methods that typically\nrequire the availability of poisoned data or involve fine-tuning the entire\nmodel, RVPT leverages few-shot downstream clean samples and only tunes a small\nnumber of parameters. Empirical results demonstrate that RVPT tunes only 0.27\\%\nof the parameters relative to CLIP, yet it significantly outperforms\nstate-of-the-art baselines, reducing the attack success rate from 67.53\\% to\n2.76\\% against SoTA attacks and effectively generalizing its defensive\ncapabilities across multiple datasets.\n","authors":["Zhifang Zhang","Shuo He","Haobo Wang","Bingquan Shen","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2412.20392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16128v1","updated":"2025-03-20T13:24:02Z","published":"2025-03-20T13:24:02Z","title":"Coupling deep and handcrafted features to assess smile genuineness","summary":"  Assessing smile genuineness from video sequences is a vital topic concerned\nwith recognizing facial expression and linking them with the underlying\nemotional states. There have been a number of techniques proposed underpinned\nwith handcrafted features, as well as those that rely on deep learning to\nelaborate the useful features. As both of these approaches have certain\nbenefits and limitations, in this work we propose to combine the features\nlearned by a long short-term memory network with the features handcrafted to\ncapture the dynamics of facial action units. The results of our experiments\nindicate that the proposed solution is more effective than the baseline\ntechniques and it allows for assessing the smile genuineness from video\nsequences in real-time.\n","authors":["Benedykt Pawlus","Bogdan Smolka","Jolanta Kawulok","Michal Kawulok"],"pdf_url":"https://arxiv.org/pdf/2503.16128v1.pdf","comment":"Submitted to SPIE Defense + Commercial Sensing 2024"},{"id":"http://arxiv.org/abs/2501.07305v2","updated":"2025-03-20T13:22:27Z","published":"2025-01-13T13:13:06Z","title":"The Devil is in the Spurious Correlations: Boosting Moment Retrieval\n  with Dynamic Learning","summary":"  Given a textual query along with a corresponding video, the objective of\nmoment retrieval aims to localize the moments relevant to the query within the\nvideo. While commendable results have been demonstrated by existing\ntransformer-based approaches, predicting the accurate temporal span of the\ntarget moment is still a major challenge. This paper reveals that a crucial\nreason stems from the spurious correlation between the text query and the\nmoment context. Namely, the model makes predictions by overly associating\nqueries with background frames rather than distinguishing target moments. To\naddress this issue, we propose a dynamic learning approach for moment\nretrieval, where two strategies are designed to mitigate the spurious\ncorrelation. First, we introduce a novel video synthesis approach to construct\na dynamic context for the queried moment, enabling the model to attend to the\ntarget moment of the corresponding query across dynamic backgrounds. Second, to\nalleviate the over-association with backgrounds, we enhance representations\ntemporally by incorporating text-dynamics interaction, which encourages the\nmodel to align text with target moments through complementary dynamic\nrepresentations. With the proposed method, our model significantly alleviates\nthe spurious correlation issue in moment retrieval and establishes new\nstate-of-the-art performance on two popular benchmarks, \\ie, QVHighlights and\nCharades-STA. In addition, detailed ablation studies and evaluations across\ndifferent architectures demonstrate the generalization and effectiveness of the\nproposed strategies. Our code will be publicly available.\n","authors":["Xinyang Zhou","Fanyue Wei","Lixin Duan","Angela Yao","Wen Li"],"pdf_url":"https://arxiv.org/pdf/2501.07305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08144v2","updated":"2025-03-20T13:21:00Z","published":"2025-03-11T08:02:54Z","title":"Bring Remote Sensing Object Detect Into Nature Language Model: Using SFT\n  Method","summary":"  Recently, large language models (LLMs) and vision-language models (VLMs) have\nachieved significant success, demonstrating remarkable capabilities in\nunderstanding various images and videos, particularly in classification and\ndetection tasks. However, due to the substantial differences between remote\nsensing images and conventional optical images, these models face considerable\nchallenges in comprehension, especially in detection tasks. Directly prompting\nVLMs with detection instructions often leads to unsatisfactory results. To\naddress this issue, this letter explores the application of VLMs for object\ndetection in remote sensing images. Specifically, we constructed supervised\nfine-tuning (SFT) datasets using publicly available remote sensing object\ndetection datasets, including SSDD, HRSID, and NWPU-VHR-10. In these new\ndatasets, we converted annotation information into JSON-compliant natural\nlanguage descriptions, facilitating more effective understanding and training\nfor the VLM. We then evaluate the detection performance of various fine-tuning\nstrategies for VLMs and derive optimized model weights for object detection in\nremote sensing images. Finally, we evaluate the model's prior knowledge\ncapabilities using natural language queries. Experimental results demonstrate\nthat, without modifying the model architecture, remote sensing object detection\ncan be effectively achieved using natural language alone. Additionally, the\nmodel exhibits the ability to perform certain vision question answering (VQA)\ntasks. Our datasets and related code will be released soon.\n","authors":["Fei Wang","Chengcheng Chen","Hongyu Chen","Yugang Chang","Weiming Zeng"],"pdf_url":"https://arxiv.org/pdf/2503.08144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00371v3","updated":"2025-03-20T13:20:09Z","published":"2025-03-01T06:56:58Z","title":"Jointly Understand Your Command and Intention:Reciprocal Co-Evolution\n  between Scene-Aware 3D Human Motion Synthesis and Analysis","summary":"  As two intimate reciprocal tasks, scene-aware human motion synthesis and\nanalysis require a joint understanding between multiple modalities, including\n3D body motions, 3D scenes, and textual descriptions. In this paper, we\nintegrate these two paired processes into a Co-Evolving Synthesis-Analysis\n(CESA) pipeline and mutually benefit their learning. Specifically, scene-aware\ntext-to-human synthesis generates diverse indoor motion samples from the same\ntextual description to enrich human-scene interaction intra-class diversity,\nthus significantly benefiting training a robust human motion analysis system.\nReciprocally, human motion analysis would enforce semantic scrutiny on each\nsynthesized motion sample to ensure its semantic consistency with the given\ntextual description, thus improving realistic motion synthesis. Considering\nthat real-world indoor human motions are goal-oriented and path-guided, we\npropose a cascaded generation strategy that factorizes text-driven\nscene-specific human motion generation into three stages: goal inferring, path\nplanning, and pose synthesizing. Coupling CESA with this powerful cascaded\nmotion synthesis model, we jointly improve realistic human motion synthesis and\nrobust human motion analysis in 3D scenes.\n","authors":["Xuehao Gao","Yang Yang","Shaoyi Du","Guo-Jun Qi","Junwei Han"],"pdf_url":"https://arxiv.org/pdf/2503.00371v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16125v1","updated":"2025-03-20T13:12:39Z","published":"2025-03-20T13:12:39Z","title":"Uncertainty Meets Diversity: A Comprehensive Active Learning Framework\n  for Indoor 3D Object Detection","summary":"  Active learning has emerged as a promising approach to reduce the substantial\nannotation burden in 3D object detection tasks, spurring several initiatives in\noutdoor environments. However, its application in indoor environments remains\nunexplored. Compared to outdoor 3D datasets, indoor datasets face significant\nchallenges, including fewer training samples per class, a greater number of\nclasses, more severe class imbalance, and more diverse scene types and\nintra-class variances. This paper presents the first study on active learning\nfor indoor 3D object detection, where we propose a novel framework tailored for\nthis task. Our method incorporates two key criteria - uncertainty and diversity\n- to actively select the most ambiguous and informative unlabeled samples for\nannotation. The uncertainty criterion accounts for both inaccurate detections\nand undetected objects, ensuring that the most ambiguous samples are\nprioritized. Meanwhile, the diversity criterion is formulated as a joint\noptimization problem that maximizes the diversity of both object class\ndistributions and scene types, using a new Class-aware Adaptive Prototype (CAP)\nbank. The CAP bank dynamically allocates representative prototypes to each\nclass, helping to capture varying intra-class diversity across different\ncategories. We evaluate our method on SUN RGB-D and ScanNetV2, where it\noutperforms baselines by a significant margin, achieving over 85% of\nfully-supervised performance with just 10% of the annotation budget.\n","authors":["Jiangyi Wang","Na Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.16125v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16120v1","updated":"2025-03-20T13:06:26Z","published":"2025-03-20T13:06:26Z","title":"Probabilistic Prompt Distribution Learning for Animal Pose Estimation","summary":"  Multi-species animal pose estimation has emerged as a challenging yet\ncritical task, hindered by substantial visual diversity and uncertainty. This\npaper challenges the problem by efficient prompt learning for Vision-Language\nPretrained (VLP) models, \\textit{e.g.} CLIP, aiming to resolve the\ncross-species generalization problem. At the core of the solution lies in the\nprompt designing, probabilistic prompt modeling and cross-modal adaptation,\nthereby enabling prompts to compensate for cross-modal information and\neffectively overcome large data variances under unbalanced data distribution.\nTo this end, we propose a novel probabilistic prompting approach to fully\nexplore textual descriptions, which could alleviate the diversity issues caused\nby long-tail property and increase the adaptability of prompts on unseen\ncategory instance. Specifically, we first introduce a set of learnable prompts\nand propose a diversity loss to maintain distinctiveness among prompts, thus\nrepresenting diverse image attributes. Diverse textual probabilistic\nrepresentations are sampled and used as the guidance for the pose estimation.\nSubsequently, we explore three different cross-modal fusion strategies at\nspatial level to alleviate the adverse impacts of visual uncertainty. Extensive\nexperiments on multi-species animal pose benchmarks show that our method\nachieves the state-of-the-art performance under both supervised and zero-shot\nsettings. The code is available at https://github.com/Raojiyong/PPAP.\n","authors":["Jiyong Rao","Brian Nlong Zhao","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16120v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2501.00895v2","updated":"2025-03-20T13:03:26Z","published":"2025-01-01T16:56:43Z","title":"Text2Earth: Unlocking Text-driven Remote Sensing Image Generation with a\n  Global-Scale Dataset and a Foundation Model","summary":"  Generative foundation models have advanced large-scale text-driven natural\nimage generation, becoming a prominent research trend across various vertical\ndomains. However, in the remote sensing field, there is still a lack of\nresearch on large-scale text-to-image (text2image) generation technology.\nExisting remote sensing image-text datasets are small in scale and confined to\nspecific geographic areas and scene types. Besides, existing text2image methods\nhave struggled to achieve global-scale, multi-resolution controllable, and\nunbounded image generation. To address these challenges, this paper presents\ntwo key contributions: the Git-10M dataset and the Text2Earth foundation model.\nGit-10M is a global-scale image-text dataset comprising 10.5 million image-text\npairs, 5 times larger than the previous largest one. The dataset covers a wide\nrange of geographic scenes and contains resolution information, significantly\nsurpassing existing datasets in both size and diversity. Building on Git-10M,\nwe propose Text2Earth, a 1.3 billion parameter generative foundation model\nbased on the diffusion framework to model global-scale remote sensing scenes.\nText2Earth integrates a resolution guidance mechanism, enabling users to\nspecify image resolutions. A dynamic condition adaptation strategy is proposed\nfor training and inference to improve image quality. Text2Earth excels in\nzero-shot text2image generation and demonstrates robust generalization and\nflexibility across multiple tasks, including unbounded scene construction,\nimage editing, and cross-modal image generation. This robust capability\nsurpasses previous models restricted to the basic fixed size and limited scene\ntypes. On the previous benchmark dataset, Text2Earth outperforms previous\nmodels with an improvement of +26.23 FID and +20.95% Zero-shot Cls-OA\nmetric.Our project page is https://chen-yang-liu.github.io/Text2Earth\n","authors":["Chenyang Liu","Keyan Chen","Rui Zhao","Zhengxia Zou","Zhenwei Shi"],"pdf_url":"https://arxiv.org/pdf/2501.00895v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15138v2","updated":"2025-03-20T13:00:45Z","published":"2025-03-19T11:59:14Z","title":"VideoGen-of-Thought: Step-by-step generating multi-shot video with\n  minimal manual intervention","summary":"  Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives.\n","authors":["Mingzhe Zheng","Yongqi Xu","Haojian Huang","Xuran Ma","Yexin Liu","Wenjie Shu","Yatian Pang","Feilong Tang","Qifeng Chen","Harry Yang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2503.15138v2.pdf","comment":"This paper should be a refined version of arXiv:2412.02259,\n  \"VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video\n  Generation\", but I mistakenly submit it as a new paper"},{"id":"http://arxiv.org/abs/2412.01819v4","updated":"2025-03-20T12:59:49Z","published":"2024-12-02T18:57:41Z","title":"Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis","summary":"  This work presents Switti, a scale-wise transformer for text-to-image\ngeneration. We start by adapting an existing next-scale prediction\nautoregressive (AR) architecture to T2I generation, investigating and\nmitigating training stability issues in the process. Next, we argue that\nscale-wise transformers do not require causality and propose a non-causal\ncounterpart facilitating ~21% faster sampling and lower memory usage while also\nachieving slightly better generation quality. Furthermore, we reveal that\nclassifier-free guidance at high-resolution scales is often unnecessary and can\neven degrade performance. By disabling guidance at these scales, we achieve an\nadditional sampling acceleration of ~32% and improve the generation of\nfine-grained details. Extensive human preference studies and automated\nevaluations show that Switti outperforms existing T2I AR models and competes\nwith state-of-the-art T2I diffusion models while being up to 7x faster.\n","authors":["Anton Voronov","Denis Kuznedelev","Mikhail Khoroshikh","Valentin Khrulkov","Dmitry Baranchuk"],"pdf_url":"https://arxiv.org/pdf/2412.01819v4.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2409.16502v3","updated":"2025-03-20T12:57:03Z","published":"2024-09-24T23:18:32Z","title":"GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for\n  Improved Visual Localization","summary":"  Although various visual localization approaches exist, such as scene\ncoordinate regression and camera pose regression, these methods often struggle\nwith optimization complexity or limited accuracy. To address these challenges,\nwe explore the use of novel view synthesis techniques, particularly 3D Gaussian\nSplatting (3DGS), which enables the compact encoding of both 3D geometry and\nscene appearance. We propose a two-stage procedure that integrates dense and\nrobust keypoint descriptors from the lightweight XFeat feature extractor into\n3DGS, enhancing performance in both indoor and outdoor environments. The coarse\npose estimates are directly obtained via 2D-3D correspondences between the 3DGS\nrepresentation and query image descriptors. In the second stage, the initial\npose estimate is refined by minimizing the rendering-based photometric warp\nloss. Benchmarking on widely used indoor and outdoor datasets demonstrates\nimprovements over recent neural rendering-based localization methods, such as\nNeRFMatch and PNeRFLoc.\n","authors":["Gennady Sidorov","Malik Mohrat","Denis Gridusov","Ruslan Rakhimov","Sergey Kolyubin"],"pdf_url":"https://arxiv.org/pdf/2409.16502v3.pdf","comment":"Project website at https://gsplatloc.github.io/"},{"id":"http://arxiv.org/abs/2503.16106v1","updated":"2025-03-20T12:51:19Z","published":"2025-03-20T12:51:19Z","title":"OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain\n  Generalization in CLIP","summary":"  We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel\nparadigm unifying low-shot learning with open-set domain generalization (ODG).\nWhile prompt-based methods using models like CLIP have advanced DG, they falter\nin low-data regimes (e.g., 1-shot) and lack precision in detecting open-set\nsamples with fine-grained semantics related to training classes. To address\nthese challenges, we propose OSLOPROMPT, an advanced prompt-learning framework\nfor CLIP with two core innovations. First, to manage limited supervision across\nsource domains and improve DG, we introduce a domain-agnostic prompt-learning\nmechanism that integrates adaptable domain-specific cues and visually guided\nsemantic attributes through a novel cross-attention module, besides being\nsupported by learnable domain- and class-generic visual prompts to enhance\ncross-modal adaptability. Second, to improve outlier rejection during\ninference, we classify unfamiliar samples as \"unknown\" and train specialized\nprompts with systematically synthesized pseudo-open samples that maintain\nfine-grained relationships to known classes, generated through a targeted query\nstrategy with off-the-shelf foundation models. This strategy enhances feature\nlearning, enabling our model to detect open samples with varied granularity\nmore effectively. Extensive evaluations across five benchmarks demonstrate that\nOSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly\noutperforming existing methods.\n","authors":["Mohamad Hassan N C","Divyam Gupta","Mainak Singha","Sai Bhargav Rongali","Ankit Jha","Muhammad Haris Khan","Biplab Banerjee"],"pdf_url":"https://arxiv.org/pdf/2503.16106v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2501.19140v2","updated":"2025-03-20T12:43:48Z","published":"2025-01-31T13:49:16Z","title":"Transformation trees -- documentation of multimodal image registration","summary":"  Multimodal image registration plays a key role in creating digital patient\nmodels by combining data from different imaging techniques into a single\ncoordinate system. This process often involves multiple sequential and\ninterconnected transformations, which must be well-documented to ensure\ntransparency and reproducibility. In this paper, we propose the use of\ntransformation trees as a method for structured recording and management of\nthese transformations. This approach has been implemented in the dpVision\nsoftware and uses a dedicated .dpw file format to store hierarchical\nrelationships between images, transformations, and motion data. Transformation\ntrees allow precise tracking of all image processing steps, reduce the need to\nstore multiple copies of the same data, and enable the indirect registration of\nimages that do not share common reference points. This improves the\nreproducibility of the analyses and facilitates later processing and\nintegration of images from different sources. The practical application of this\nmethod is demonstrated with examples from orthodontics, including the\nintegration of 3D face scans, intraoral scans, and CBCT images, as well as the\ndocumentation of mandibular motion. Beyond orthodontics, this method can be\napplied in other fields that require systematic management of image\nregistration processes, such as maxillofacial surgery, oncology, and\nbiomechanical analysis. Maintaining long-term data consistency is essential for\nboth scientific research and clinical practice. It enables easier comparison of\nresults in longitudinal studies, improves retrospective analysis, and supports\nthe development of artificial intelligence algorithms by providing standardized\nand well-documented datasets. The proposed approach enhances data organization,\nallows for efficient analysis, and facilitates the reuse of information in\nfuture studies and diagnostic procedures.\n","authors":["Agnieszka Anna Tomaka","Dariusz Pojda","Michał Tarnawski","Leszek Luchowski"],"pdf_url":"https://arxiv.org/pdf/2501.19140v2.pdf","comment":"28 pages, 15 figures"},{"id":"http://arxiv.org/abs/2503.16096v1","updated":"2025-03-20T12:40:38Z","published":"2025-03-20T12:40:38Z","title":"MarkushGrapher: Joint Visual and Textual Recognition of Markush\n  Structures","summary":"  The automated analysis of chemical literature holds promise to accelerate\ndiscovery in fields such as material science and drug development. In\nparticular, search capabilities for chemical structures and Markush structures\n(chemical structure templates) within patent documents are valuable, e.g., for\nprior-art search. Advancements have been made in the automatic extraction of\nchemical structures from text and images, yet the Markush structures remain\nlargely unexplored due to their complex multi-modal nature. In this work, we\npresent MarkushGrapher, a multi-modal approach for recognizing Markush\nstructures in documents. Our method jointly encodes text, image, and layout\ninformation through a Vision-Text-Layout encoder and an Optical Chemical\nStructure Recognition vision encoder. These representations are merged and used\nto auto-regressively generate a sequential graph representation of the Markush\nstructure along with a table defining its variable groups. To overcome the lack\nof real-world training data, we propose a synthetic data generation pipeline\nthat produces a wide range of realistic Markush structures. Additionally, we\npresent M2S, the first annotated benchmark of real-world Markush structures, to\nadvance research on this challenging task. Extensive experiments demonstrate\nthat our approach outperforms state-of-the-art chemistry-specific and\ngeneral-purpose vision-language models in most evaluation settings. Code,\nmodels, and datasets will be available.\n","authors":["Lucas Morin","Valéry Weber","Ahmed Nassar","Gerhard Ingmar Meijer","Luc Van Gool","Yawei Li","Peter Staar"],"pdf_url":"https://arxiv.org/pdf/2503.16096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16086v1","updated":"2025-03-20T12:28:31Z","published":"2025-03-20T12:28:31Z","title":"Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly","summary":"  Ensuring food safety and quality is critical in the food processing industry,\nwhere the detection of contaminants remains a persistent challenge. This study\npresents an automated solution for detecting foreign objects on pork belly meat\nusing hyperspectral imaging (HSI). A hyperspectral camera was used to capture\ndata across various bands in the near-infrared (NIR) spectrum (900-1700 nm),\nenabling accurate identification of contaminants that are often undetectable\nthrough traditional visual inspection methods. The proposed solution combines\npre-processing techniques with a segmentation approach based on a lightweight\nVision Transformer (ViT) to distinguish contaminants from meat, fat, and\nconveyor belt materials. The adopted strategy demonstrates high detection\naccuracy and training efficiency, while also addressing key industrial\nchallenges such as inherent noise, temperature variations, and spectral\nsimilarity between contaminants and pork belly. Experimental results validate\nthe effectiveness of hyperspectral imaging in enhancing food safety,\nhighlighting its potential for broad real-time applications in automated\nquality control processes.\n","authors":["Gabriela Ghimpeteanu","Hayat Rajani","Josep Quintana","Rafael Garcia"],"pdf_url":"https://arxiv.org/pdf/2503.16086v1.pdf","comment":"Article under review by Computers in Industry, Elsevier"},{"id":"http://arxiv.org/abs/2411.14743v2","updated":"2025-03-20T12:16:47Z","published":"2024-11-22T05:36:38Z","title":"FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole\n  Slide Image Classification","summary":"  Few-shot learning presents a critical solution for cancer diagnosis in\ncomputational pathology (CPath), addressing fundamental limitations in data\navailability, particularly the scarcity of expert annotations and patient\nprivacy constraints. A key challenge in this paradigm stems from the inherent\ndisparity between the limited training set of whole slide images (WSIs) and the\nenormous number of contained patches, where a significant portion of these\npatches lacks diagnostically relevant information, potentially diluting the\nmodel's ability to learn and focus on critical diagnostic features. While\nrecent works attempt to address this by incorporating additional knowledge,\nseveral crucial gaps hinder further progress: (1) despite the emergence of\npowerful pathology foundation models (FMs), their potential remains largely\nuntapped, with most approaches limiting their use to basic feature extraction;\n(2) current language guidance mechanisms attempt to align text prompts with\nvast numbers of WSI patches all at once, struggling to leverage rich\npathological semantic information. To this end, we introduce the\nknowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which\nuniquely combines pathology FMs with language prior knowledge to enable a\nfocused analysis of diagnostically relevant regions by prioritizing\ndiscriminative WSI patches. Our approach implements a progressive three-stage\ncompression strategy: we first leverage FMs for global visual redundancy\nelimination, and integrate compressed features with language prompts for\nsemantic relevance assessment, then perform neighbor-aware visual token\nfiltering while preserving spatial coherence. Extensive experiments on\npathological datasets spanning breast, lung, and ovarian cancers demonstrate\nits superior performance in few-shot pathology diagnosis. Codes are available\nat https://github.com/dddavid4real/FOCUS.\n","authors":["Zhengrui Guo","Conghao Xiong","Jiabo Ma","Qichen Sun","Lishuang Feng","Jinzhuo Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2411.14743v2.pdf","comment":"Accepted by CVPR'2025"},{"id":"http://arxiv.org/abs/2503.03644v3","updated":"2025-03-20T12:16:23Z","published":"2025-03-05T16:20:53Z","title":"DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms","summary":"  Dongba pictographs are the only pictographs still in use in the world. They\nhave pictorial ideographic features, and their symbols carry rich cultural and\ncontextual information. Due to the lack of relevant datasets, existing research\nhas difficulty in advancing the study of semantic understanding of Dongba\npictographs. To this end, we propose \\textbf{DongbaMIE}, the first multimodal\ndataset for semantic understanding and extraction of Dongba pictographs,\nconsisting of Dongba pictograph images and corresponding Chinese semantic\nannotations. DongbaMIE contains 23,530 sentence-level and 2,539 paragraph-level\nimages, covering four semantic dimensions: objects, actions, relations, and\nattributes. We systematically evaluate multimodal large language models\n(MLLMs), such as GPT-4o, Gemini-2.0, and Qwen2-VL. Experimental results show\nthat best F1 scores of proprietary models, GPT-4o and Gemini, for object\nextraction task are only 3.16 and 3.11 respectively. For the open-source model\nQwen2-VL, it achieves only 11.49 after supervised fine-tuning. These suggest\nthat current MLLMs still face significant challenges in accurately recognizing\ndiverse semantic information in Dongba pictographs.\n","authors":["Xiaojun Bi","Shuo Li","Ziyue Wang","Fuwen Luo","Weizheng Qiao","Lu Han","Ziwei Sun","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.03644v3.pdf","comment":"Our dataset can be obtained from:\n  https://github.com/thinklis/DongbaMIE"},{"id":"http://arxiv.org/abs/2503.16075v1","updated":"2025-03-20T12:12:01Z","published":"2025-03-20T12:12:01Z","title":"3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step\n  Adversarial Network: Contribution to the FuseMyCells Challenge","summary":"  Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively.\n","authors":["Marek Wodzinski","Henning Müller"],"pdf_url":"https://arxiv.org/pdf/2503.16075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13028v2","updated":"2025-03-20T12:08:07Z","published":"2025-03-17T10:30:26Z","title":"Beyond Role-Based Surgical Domain Modeling: Generalizable\n  Re-Identification in the Operating Room","summary":"  Surgical domain models improve workflow optimization through automated\npredictions of each staff member's surgical role. However, mounting evidence\nindicates that team familiarity and individuality impact surgical outcomes. We\npresent a novel staff-centric modeling approach that characterizes individual\nteam members through their distinctive movement patterns and physical\ncharacteristics, enabling long-term tracking and analysis of surgical personnel\nacross multiple procedures. To address the challenge of inter-clinic\nvariability, we develop a generalizable re-identification framework that\nencodes sequences of 3D point clouds to capture shape and articulated motion\npatterns unique to each individual. Our method achieves 86.19% accuracy on\nrealistic clinical data while maintaining 75.27% accuracy when transferring\nbetween different environments - a 12% improvement over existing methods. When\nused to augment markerless personnel tracking, our approach improves accuracy\nby over 50%. Through extensive validation across three datasets and the\nintroduction of a novel workflow visualization technique, we demonstrate how\nour framework can reveal novel insights into surgical team dynamics and space\nutilization patterns, advancing methods to analyze surgical workflows and team\ncoordination.\n","authors":["Tony Danjun Wang","Lennart Bastian","Tobias Czempiel","Christian Heiliger","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2503.13028v2.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2501.15890v3","updated":"2025-03-20T12:06:51Z","published":"2025-01-27T09:32:56Z","title":"Complexity in Complexity: Understanding Visual Complexity Through\n  Structure, Color, and Surprise","summary":"  Understanding how humans perceive visual complexity is a key area of study in\nvisual cognition. Previous approaches to modeling visual complexity assessments\nhave often resulted in intricate, difficult-to-interpret algorithms that employ\nnumerous features or sophisticated deep learning architectures. While these\ncomplex models achieve high performance on specific datasets, they often\nsacrifice interpretability, making it challenging to understand the factors\ndriving human perception of complexity. Recently (Shen, et al. 2024) proposed\nan interpretable segmentation-based model that accurately predicted complexity\nacross various datasets, supporting the idea that complexity can be explained\nsimply. In this work, we investigate the failure of their model to capture\nstructural, color and surprisal contributions to complexity. To this end, we\npropose Multi-Scale Sobel Gradient (MSG) which measures spatial intensity\nvariations, Multi-Scale Unique Color (MUC) which quantifies colorfulness across\nmultiple scales, and surprise scores generated using a Large Language Model. We\ntest our features on existing benchmarks and a novel dataset (Surprising Visual\nGenome) containing surprising images from Visual Genome. Our experiments\ndemonstrate that modeling complexity accurately is not as simple as previously\nthought, requiring additional perceptual and semantic factors to address\ndataset biases. Our model improves predictive performance while maintaining\ninterpretability, offering deeper insights into how visual complexity is\nperceived and assessed. Our code, analysis and data are available at\nhttps://github.com/Complexity-Project/Complexity-in-Complexity.\n","authors":["Karahan Sarıtaş","Peter Dayan","Tingke Shen","Surabhi S Nath"],"pdf_url":"https://arxiv.org/pdf/2501.15890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11624v4","updated":"2025-03-20T12:06:17Z","published":"2024-06-17T15:07:55Z","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","summary":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","authors":["Omer Sahin Tas","Royden Wagner"],"pdf_url":"https://arxiv.org/pdf/2406.11624v4.pdf","comment":"ICLR 2025 camera-ready. Our implementation is available at\n  \\href{https://github.com/kit-mrt/future-motion}{this https URL}"},{"id":"http://arxiv.org/abs/2406.03146v2","updated":"2025-03-20T12:04:41Z","published":"2024-06-05T11:01:42Z","title":"Tiny models from tiny data: Textual and null-text inversion for few-shot\n  distillation","summary":"  Few-shot learning deals with problems such as image classification using very\nfew training examples. Recent vision foundation models show excellent few-shot\ntransfer abilities, but are large and slow at inference. Using knowledge\ndistillation, the capabilities of high-performing but slow models can be\ntransferred to tiny, efficient models. However, common distillation methods\nrequire a large set of unlabeled data, which is not available in the few-shot\nsetting. To overcome this lack of data, there has been a recent interest in\nusing synthetic data. We expand on this line of research by presenting a novel\ndiffusion model inversion technique (TINT) combining the diversity of textual\ninversion with the specificity of null-text inversion. Using this method in a\nfew-shot distillation pipeline leads to state-of-the-art accuracy among small\nstudent models on popular benchmarks, while being significantly faster than\nprior work. Popular few-shot benchmarks involve evaluation over a large number\nof episodes, which is computationally cumbersome for methods involving\nsynthetic data generation. We also present a theoretical analysis on how the\naccuracy estimator variance depends on the number of episodes and query\nexamples, and use these results to lower the computational effort required for\nmethod evaluation. Finally, to further motivate the use of generative models in\nfew-shot distillation, we demonstrate that our method outperforms training on\nreal data mined from the dataset used in the original diffusion model training.\nSource code is available at https://github.com/pixwse/tiny2.\n","authors":["Erik Landolsi","Fredrik Kahl"],"pdf_url":"https://arxiv.org/pdf/2406.03146v2.pdf","comment":"24 pages (13 main pages + references and appendix)"},{"id":"http://arxiv.org/abs/2503.16069v1","updated":"2025-03-20T12:02:10Z","published":"2025-03-20T12:02:10Z","title":"Disentangled and Interpretable Multimodal Attention Fusion for Cancer\n  Survival Prediction","summary":"  To improve the prediction of cancer survival using whole-slide images and\ntranscriptomics data, it is crucial to capture both modality-shared and\nmodality-specific information. However, multimodal frameworks often entangle\nthese representations, limiting interpretability and potentially suppressing\ndiscriminative features. To address this, we propose Disentangled and\nInterpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that\nseparates the intra- and inter-modal interactions within an attention-based\nfusion mechanism to learn distinct modality-specific and modality-shared\nrepresentations. We introduce a loss based on Distance Correlation to promote\ndisentanglement between these representations and integrate Shapley additive\nexplanations to assess their relative contributions to survival prediction. We\nevaluate DIMAF on four public cancer survival datasets, achieving a relative\naverage improvement of 1.85% in performance and 23.7% in disentanglement\ncompared to current state-of-the-art multimodal models. Beyond improved\nperformance, our interpretable framework enables a deeper exploration of the\nunderlying interactions between and within modalities in cancer biology.\n","authors":["Aniek Eijpe","Soufyan Lakbir","Melis Erdal Cesur","Sara P. Oliveira","Sanne Abeln","Wilson Silva"],"pdf_url":"https://arxiv.org/pdf/2503.16069v1.pdf","comment":"11 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2503.16068v1","updated":"2025-03-20T12:01:43Z","published":"2025-03-20T12:01:43Z","title":"PoseTraj: Pose-Aware Trajectory Control in Video Diffusion","summary":"  Recent advancements in trajectory-guided video generation have achieved\nnotable progress. However, existing models still face challenges in generating\nobject motions with potentially changing 6D poses under wide-range rotations,\ndue to limited 3D understanding. To address this problem, we introduce\nPoseTraj, a pose-aware video dragging model for generating 3D-aligned motion\nfrom 2D trajectories. Our method adopts a novel two-stage pose-aware\npretraining framework, improving 3D understanding across diverse trajectories.\nSpecifically, we propose a large-scale synthetic dataset PoseTraj-10K,\ncontaining 10k videos of objects following rotational trajectories, and enhance\nthe model perception of object pose changes by incorporating 3D bounding boxes\nas intermediate supervision signals. Following this, we fine-tune the\ntrajectory-controlling module on real-world videos, applying an additional\ncamera-disentanglement module to further refine motion accuracy. Experiments on\nvarious benchmark datasets demonstrate that our method not only excels in 3D\npose-aligned dragging for rotational trajectories but also outperforms existing\nbaselines in trajectory accuracy and video quality.\n","authors":["Longbin Ji","Lei Zhong","Pengfei Wei","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2503.16068v1.pdf","comment":"Code, data and project page: https://robingg1.github.io/Pose-Traj/"},{"id":"http://arxiv.org/abs/2503.16067v1","updated":"2025-03-20T12:00:45Z","published":"2025-03-20T12:00:45Z","title":"Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures","summary":"  Bokeh rendering methods play a key role in creating the visually appealing,\nsoftly blurred backgrounds seen in professional photography. While recent\nlearning-based approaches show promising results, generating realistic Bokeh\nwith variable strength remains challenging. Existing methods require additional\ninputs and suffer from unrealistic Bokeh reproduction due to reliance on\nsynthetic data. In this work, we propose Bokehlicious, a highly efficient\nnetwork that provides intuitive control over Bokeh strength through an\nAperture-Aware Attention mechanism, mimicking the physical lens aperture. To\nfurther address the lack of high-quality real-world data, we present RealBokeh,\na novel dataset featuring 23,000 high-resolution (24-MP) images captured by\nprofessional photographers, covering diverse scenes with varied aperture and\nfocal length settings. Evaluations on both our new RealBokeh and established\nBokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA\nmethods while significantly reducing computational cost and exhibiting strong\nzero-shot generalization. Our method and dataset further extend to defocus\ndeblurring, achieving competitive results on the RealDOF benchmark. Our code\nand data can be found at https://github.com/TimSeizinger/Bokehlicious\n","authors":["Tim Seizinger","Florin-Alexandru Vasluianu","Marcos V. Conde","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2503.16067v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2503.16065v1","updated":"2025-03-20T11:57:32Z","published":"2025-03-20T11:57:32Z","title":"Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion\n  Model","summary":"  While virtual try-on for clothes and shoes with diffusion models has gained\nattraction, virtual try-on for ornaments, such as bracelets, rings, earrings,\nand necklaces, remains largely unexplored. Due to the intricate tiny patterns\nand repeated geometric sub-structures in most ornaments, it is much more\ndifficult to guarantee identity and appearance consistency under large pose and\nscale variances between ornaments and models. This paper proposes the task of\nvirtual try-on for ornaments and presents a method to improve the geometric and\nappearance preservation of ornament virtual try-ons. Specifically, we estimate\nan accurate wearing mask to improve the alignments between ornaments and models\nin an iterative scheme alongside the denoising process. To preserve structure\ndetails, we further regularize attention layers to map the reference ornament\nmask to the wearing mask in an implicit way. Experimental results demonstrate\nthat our method successfully wears ornaments from reference images onto target\nmodels, handling substantial differences in scale and pose while preserving\nidentity and achieving realistic visual effects.\n","authors":["Yingmao Miao","Zhanpeng Huang","Rui Han","Zibin Wang","Chenhao Lin","Chao Shen"],"pdf_url":"https://arxiv.org/pdf/2503.16065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16064v1","updated":"2025-03-20T11:56:27Z","published":"2025-03-20T11:56:27Z","title":"PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for\n  Adaptive Hashing Retrieval","summary":"  Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.\n","authors":["Qiang Zou","Shuli Cheng","Jiayi Chen"],"pdf_url":"https://arxiv.org/pdf/2503.16064v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2410.17856v3","updated":"2025-03-20T11:55:54Z","published":"2024-10-23T13:26:59Z","title":"ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context\n  Prompting","summary":"  Vision-language models (VLMs) have excelled in multimodal tasks, but adapting\nthem to embodied decision-making in open-world environments presents\nchallenges. One critical issue is bridging the gap between discrete entities in\nlow-level observations and the abstract concepts required for effective\nplanning. A common solution is building hierarchical agents, where VLMs serve\nas high-level reasoners that break down tasks into executable sub-tasks,\ntypically specified using language. However, language suffers from the\ninability to communicate detailed spatial information. We propose\nvisual-temporal context prompting, a novel communication protocol between VLMs\nand policy models. This protocol leverages object segmentation from past\nobservations to guide policy-environment interactions. Using this approach, we\ntrain ROCKET-1, a low-level policy that predicts actions based on concatenated\nvisual observations and segmentation masks, supported by real-time object\ntracking from SAM-2. Our method unlocks the potential of VLMs, enabling them to\ntackle complex tasks that demand spatial reasoning. Experiments in Minecraft\nshow that our approach enables agents to achieve previously unattainable tasks,\nwith a $\\mathbf{76}\\%$ absolute improvement in open-world interaction\nperformance. Codes and demos are now available on the project page:\nhttps://craftjarvis.github.io/ROCKET-1.\n","authors":["Shaofei Cai","Zihao Wang","Kewei Lian","Zhancun Mu","Xiaojian Ma","Anji Liu","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2410.17856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19772v3","updated":"2025-03-20T11:55:30Z","published":"2024-11-29T15:18:06Z","title":"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos","summary":"  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n","authors":["Tiantian Geng","Jinrui Zhang","Qingni Wang","Teng Wang","Jinming Duan","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.19772v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2403.05906v2","updated":"2025-03-20T11:49:18Z","published":"2024-03-09T13:11:59Z","title":"Segmentation Guided Sparse Transformer for Under-Display Camera Image\n  Restoration","summary":"  Under-Display Camera (UDC) is an emerging technology that achieves\nfull-screen display via hiding the camera under the display panel. However, the\ncurrent implementation of UDC causes serious degradation. The incident light\nrequired for camera imaging undergoes attenuation and diffraction when passing\nthrough the display panel, leading to various artifacts in UDC imaging.\nPresently, the prevailing UDC image restoration methods predominantly utilize\nconvolutional neural network architectures, whereas Transformer-based methods\nhave exhibited superior performance in the majority of image restoration tasks.\nThis is attributed to the Transformer's capability to sample global features\nfor the local reconstruction of images, thereby achieving high-quality image\nrestoration. In this paper, we observe that when using the Vision Transformer\nfor UDC degraded image restoration, the global attention samples a large amount\nof redundant information and noise. Furthermore, compared to the ordinary\nTransformer employing dense attention, the Transformer utilizing sparse\nattention can alleviate the adverse impact of redundant information and noise.\nBuilding upon this discovery, we propose a Segmentation Guided Sparse\nTransformer method (SGSFormer) for the task of restoring high-quality images\nfrom UDC degraded images. Specifically, we utilize sparse self-attention to\nfilter out redundant information and noise, directing the model's attention to\nfocus on the features more relevant to the degraded regions in need of\nreconstruction. Moreover, we integrate the instance segmentation map as prior\ninformation to guide the sparse self-attention in filtering and focusing on the\ncorrect regions.\n","authors":["Jingyun Xue","Tao Wang","Pengwen Dai","Kaihao Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.05906v2.pdf","comment":"13 pages, 10 figures, conference or other essential info"},{"id":"http://arxiv.org/abs/2503.16058v1","updated":"2025-03-20T11:46:29Z","published":"2025-03-20T11:46:29Z","title":"Landmarks Are Alike Yet Distinct: Harnessing Similarity and\n  Individuality for One-Shot Medical Landmark Detection","summary":"  Landmark detection plays a crucial role in medical imaging applications such\nas disease diagnosis, bone age estimation, and therapy planning. However,\ntraining models for detecting multiple landmarks simultaneously often\nencounters the \"seesaw phenomenon\", where improvements in detecting certain\nlandmarks lead to declines in detecting others. Yet, training a separate model\nfor each landmark increases memory usage and computational overhead. To address\nthese challenges, we propose a novel approach based on the belief that\n\"landmarks are distinct\" by training models with pseudo-labels and template\ndata updated continuously during the training process, where each model is\ndedicated to detecting a single landmark to achieve high accuracy. Furthermore,\ngrounded on the belief that \"landmarks are also alike\", we introduce an\nadapter-based fusion model, combining shared weights with landmark-specific\nweights, to efficiently share model parameters while allowing flexible\nadaptation to individual landmarks. This approach not only significantly\nreduces memory and computational resource requirements but also effectively\nmitigates the seesaw phenomenon in multi-landmark training. Experimental\nresults on publicly available medical image datasets demonstrate that the\nsingle-landmark models significantly outperform traditional multi-point joint\ntraining models in detecting individual landmarks. Although our adapter-based\nfusion model shows slightly lower performance compared to the combined results\nof all single-landmark models, it still surpasses the current state-of-the-art\nmethods while achieving a notable improvement in resource efficiency.\n","authors":["Xu He","Zhen Huang","Qingsong Yao","Xiaoqian Zhou","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.16058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16057v1","updated":"2025-03-20T11:45:08Z","published":"2025-03-20T11:45:08Z","title":"Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts","summary":"  Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.\n","authors":["Yike Yuan","Ziyu Wang","Zihao Huang","Defa Zhu","Xun Zhou","Jingyi Yu","Qiyang Min"],"pdf_url":"https://arxiv.org/pdf/2503.16057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16056v1","updated":"2025-03-20T11:43:55Z","published":"2025-03-20T11:43:55Z","title":"Semantic-Guided Global-Local Collaborative Networks for Lightweight\n  Image Super-Resolution","summary":"  Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the\naccuracy and reliability of measurement systems, which are integral to various\nvision-based instrumentation and measurement applications. These systems often\nrequire clear and detailed images for precise object detection and recognition.\nHowever, images captured by visual measurement tools frequently suffer from\ndegradation, including blurring and loss of detail, which can impede\nmeasurement accuracy.As a potential remedy, we in this paper propose a\nSemantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight\nSISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained\nmodel to guide the super-resolution process, enhancing image detail quality\neffectively. Specifically,we propose a Semantic Guidance Module that seamlessly\nintegrates the semantic priors into the super-resolution network, enabling the\nnetwork to more adeptly capture and utilize semantic priors, thereby enhancing\nimage details. To further explore both local and non-local interactions for\nimproved detail rendition,we propose a Global-Local Collaborative Module, which\nfeatures three Global and Local Detail Enhancement Modules, as well as a Hybrid\nAttention Mechanism to work together to efficiently learn more useful features.\nOur extensive experiments show that SGGLC-Net achieves competitive PSNR and\nSSIM values across multiple benchmark datasets, demonstrating higher\nperformance with the multi-adds reduction of 12.81G compared to\nstate-of-the-art lightweight super-resolution approaches. These improvements\nunderscore the potential of our approach to enhance the precision and\neffectiveness of visual measurement systems. Codes are at\nhttps://github.com/fanamber831/SGGLC-Net.\n","authors":["Wanshu Fan","Yue Wang","Cong Wang","Yunzhe Zhang","Wei Wang","Dongsheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.16056v1.pdf","comment":"14 pages,13 figures, 9 tables"},{"id":"http://arxiv.org/abs/2503.16055v1","updated":"2025-03-20T11:42:41Z","published":"2025-03-20T11:42:41Z","title":"SALT: Singular Value Adaptation with Low-Rank Transformation","summary":"  The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT\n","authors":["Abdelrahman Elsayed","Sarim Hashmi","Mohammed Elseiagy","Hu Wang","Mohammad Yaqub","Ibrahim Almakky"],"pdf_url":"https://arxiv.org/pdf/2503.16055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16051v1","updated":"2025-03-20T11:34:45Z","published":"2025-03-20T11:34:45Z","title":"Closer to Ground Truth: Realistic Shape and Appearance Labeled Data\n  Generation for Unsupervised Underwater Image Segmentation","summary":"  Solving fish segmentation in underwater videos, a real-world problem of great\npractical value in marine and aquaculture industry, is a challenging task due\nto the difficulty of the filming environment, poor visibility and limited\nexisting annotated underwater fish data. In order to overcome these obstacles,\nwe introduce a novel two stage unsupervised segmentation approach that requires\nno human annotations and combines artificially created and real images. Our\nmethod generates challenging synthetic training data, by placing virtual fish\nin real-world underwater habitats, after performing fish transformations such\nas Thin Plate Spline shape warping and color Histogram Matching, which\nrealistically integrate synthetic fish into the backgrounds, making the\ngenerated images increasingly closer to the real world data with every stage of\nour approach. While we validate our unsupervised method on the popular DeepFish\ndataset, obtaining a performance close to a fully-supervised SoTA model, we\nfurther show its effectiveness on the specific case of salmon segmentation in\nunderwater videos, for which we introduce DeepSalmon, the largest dataset of\nits kind in the literature (30 GB). Moreover, on both datasets we prove the\ncapability of our approach to boost the performance of the fully-supervised\nSoTA model.\n","authors":["Andrei Jelea","Ahmed Nabil Belbachir","Marius Leordeanu"],"pdf_url":"https://arxiv.org/pdf/2503.16051v1.pdf","comment":"Proceedings of ECCVW 2024"},{"id":"http://arxiv.org/abs/2503.16036v1","updated":"2025-03-20T11:09:18Z","published":"2025-03-20T11:09:18Z","title":"Hybrid-Level Instruction Injection for Video Token Compression in\n  Multi-modal Large Language Models","summary":"  Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom.\n","authors":["Zhihang Liu","Chen-Wei Xie","Pandeng Li","Liming Zhao","Longxiang Tang","Yun Zheng","Chuanbin Liu","Hongtao Xie"],"pdf_url":"https://arxiv.org/pdf/2503.16036v1.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2503.16032v1","updated":"2025-03-20T10:58:12Z","published":"2025-03-20T10:58:12Z","title":"Agentic Keyframe Search for Video Question Answering","summary":"  Video question answering (VideoQA) enables machines to extract and comprehend\nkey information from videos through natural language interaction, which is a\ncritical step towards achieving intelligence. However, the demand for a\nthorough understanding of videos and high computational costs still limit the\nwidespread applications of VideoQA. To address it, we propose Agentic Keyframe\nSearch (AKeyS), a simple yet powerful algorithm for identifying keyframes in\nthe VideoQA task. It can effectively distinguish key information from\nredundant, irrelevant content by leveraging modern language agents to direct\nclassical search algorithms. Specifically, we first segment the video and\norganize it as a tree structure. Then, AKeyS uses a language agent to estimate\nheuristics and movement costs while dynamically expanding nodes. Finally, the\nagent determines if sufficient keyframes have been collected based on\ntermination conditions and provides answers. Extensive experiments on the\nEgoSchema and NExT-QA datasets show that AKeyS outperforms all previous methods\nwith the highest keyframe searching efficiency, which means it can accurately\nidentify key information and conduct effective visual reasoning with minimal\ncomputational overhead. For example, on the EgoSchema subset, it achieves 1.8%\nhigher accuracy while processing only 43.5% of the frames compared to\nVideoTree. We believe that AKeyS represents a significant step towards building\nintelligent agents for video understanding. The code is publicly available at\nhttps://github.com/fansunqi/AKeyS.\n","authors":["Sunqi Fan","Meng-Hao Guo","Shuojin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.16032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21001v2","updated":"2025-03-20T10:50:43Z","published":"2025-02-28T12:43:46Z","title":"Towards Lossless Implicit Neural Representation via Bit Plane\n  Decomposition","summary":"  We quantify the upper bound on the size of the implicit neural representation\n(INR) model from a digital perspective. The upper bound of the model size\nincreases exponentially as the required bit-precision increases. To this end,\nwe present a bit-plane decomposition method that makes INR predict bit-planes,\nproducing the same effect as reducing the upper bound of the model size. We\nvalidate our hypothesis that reducing the upper bound leads to faster\nconvergence with constant model size. Our method achieves lossless\nrepresentation in 2D image and audio fitting, even for high bit-depth signals,\nsuch as 16-bit, which was previously unachievable. We pioneered the presence of\nbit bias, which INR prioritizes as the most significant bit (MSB). We expand\nthe application of the INR task to bit depth expansion, lossless image\ncompression, and extreme network quantization. Our source code is available at\nhttps://github.com/WooKyoungHan/LosslessINR\n","authors":["Woo Kyoung Han","Byeonghun Lee","Hyunmin Cho","Sunghoon Im","Kyong Hwan Jin"],"pdf_url":"https://arxiv.org/pdf/2502.21001v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16025v1","updated":"2025-03-20T10:45:04Z","published":"2025-03-20T10:45:04Z","title":"Single Image Iterative Subject-driven Generation and Editing","summary":"  Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation.\n","authors":["Yair Shpitzer","Gal Chechik","Idan Schwartz"],"pdf_url":"https://arxiv.org/pdf/2503.16025v1.pdf","comment":"Project page is at https://siso-paper.github.io/"},{"id":"http://arxiv.org/abs/2503.16013v1","updated":"2025-03-20T10:32:38Z","published":"2025-03-20T10:32:38Z","title":"GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping\n  under Flexible Language Instructions","summary":"  Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. Codes and data will be released.\n","authors":["Xiaomeng Chu","Jiajun Deng","Guoliang You","Wei Liu","Xingchen Li","Jianmin Ji","Yanyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16012v1","updated":"2025-03-20T10:32:15Z","published":"2025-03-20T10:32:15Z","title":"GazeSCRNN: Event-based Near-eye Gaze Tracking using a Spiking Neural\n  Network","summary":"  This work introduces GazeSCRNN, a novel spiking convolutional recurrent\nneural network designed for event-based near-eye gaze tracking. Leveraging the\nhigh temporal resolution, energy efficiency, and compatibility of Dynamic\nVision Sensor (DVS) cameras with event-based systems, GazeSCRNN uses a spiking\nneural network (SNN) to address the limitations of traditional gaze-tracking\nsystems in capturing dynamic movements. The proposed model processes event\nstreams from DVS cameras using Adaptive Leaky-Integrate-and-Fire (ALIF) neurons\nand a hybrid architecture optimized for spatio-temporal data. Extensive\nevaluations on the EV-Eye dataset demonstrate the model's accuracy in\npredicting gaze vectors. In addition, we conducted ablation studies to reveal\nthe importance of the ALIF neurons, dynamic event framing, and training\ntechniques, such as Forward-Propagation-Through-Time, in enhancing overall\nsystem performance. The most accurate model achieved a Mean Angle Error (MAE)\nof 6.034{\\deg} and a Mean Pupil Error (MPE) of 2.094 mm. Consequently, this\nwork is pioneering in demonstrating the feasibility of using SNNs for\nevent-based gaze tracking, while shedding light on critical challenges and\nopportunities for further improvement.\n","authors":["Stijn Groenen","Marzieh Hassanshahi Varposhti","Mahyar Shahsavari"],"pdf_url":"https://arxiv.org/pdf/2503.16012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14295v2","updated":"2025-03-20T10:27:54Z","published":"2025-03-18T14:35:48Z","title":"PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face\n  Generation","summary":"  Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments.\n","authors":["Baiqin Wang","Xiangyu Zhu","Fan Shen","Hao Xu","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2503.14295v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20099v3","updated":"2025-03-20T10:26:35Z","published":"2024-06-28T17:59:51Z","title":"Odd-One-Out: Anomaly Detection by Comparing with Neighbors","summary":"  This paper introduces a novel anomaly detection (AD) problem aimed at\nidentifying `odd-looking' objects within a scene by comparing them to other\nobjects present. Unlike traditional AD benchmarks with fixed anomaly criteria,\nour task detects anomalies specific to each scene by inferring a reference\ngroup of regular objects. To address occlusions, we use multiple views of each\nscene as input, construct 3D object-centric models for each instance from 2D\nviews, enhancing these models with geometrically consistent part-aware\nrepresentations. Anomalous objects are then detected through cross-instance\ncomparison. We also introduce two new benchmarks, ToysAD-8K and PartsAD-15K as\ntestbeds for future research in this task. We provide a comprehensive analysis\nof our method quantitatively and qualitatively on these benchmarks.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.20099v3.pdf","comment":"Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD"},{"id":"http://arxiv.org/abs/2503.14538v2","updated":"2025-03-20T10:20:22Z","published":"2025-03-17T14:08:35Z","title":"Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal\n  Approach Combining Imaging and Clinical Data","summary":"  Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings.\n","authors":["Ananya Ganapthy","Praveen Shastry","Naveen Kumarasami","Anandakumar D","Keerthana R","Mounigasri M","Varshinipriya M","Kishore Prasath Venkatesh","Bargava Subramanian","Kalyan Sivasailam"],"pdf_url":"https://arxiv.org/pdf/2503.14538v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.06903v2","updated":"2025-03-20T10:18:44Z","published":"2025-01-12T19:01:05Z","title":"Synthetic Prior for Few-Shot Drivable Head Avatar Inversion","summary":"  We present SynShot, a novel method for the few-shot inversion of a drivable\nhead avatar based on a synthetic prior. We tackle three major challenges.\nFirst, training a controllable 3D generative network requires a large number of\ndiverse sequences, for which pairs of images and high-quality tracked meshes\nare not always available. Second, the use of real data is strictly regulated\n(e.g., under the General Data Protection Regulation, which mandates frequent\ndeletion of models and data to accommodate a situation when a participant's\nconsent is withdrawn). Synthetic data, free from these constraints, is an\nappealing alternative. Third, state-of-the-art monocular avatar models struggle\nto generalize to new views and expressions, lacking a strong prior and often\noverfitting to a specific viewpoint distribution. Inspired by machine learning\nmodels trained solely on synthetic data, we propose a method that learns a\nprior model from a large dataset of synthetic heads with diverse identities,\nexpressions, and viewpoints. With few input images, SynShot fine-tunes the\npretrained synthetic prior to bridge the domain gap, modeling a photorealistic\nhead avatar that generalizes to novel expressions and viewpoints. We model the\nhead avatar using 3D Gaussian splatting and a convolutional encoder-decoder\nthat outputs Gaussian parameters in UV texture space. To account for the\ndifferent modeling complexities over parts of the head (e.g., skin vs hair), we\nembed the prior with explicit control for upsampling the number of per-part\nprimitives. Compared to SOTA monocular and GAN-based methods, SynShot\nsignificantly improves novel view and expression synthesis.\n","authors":["Wojciech Zielonka","Stephan J. Garbin","Alexandros Lattas","George Kopanas","Paulo Gotardo","Thabo Beeler","Justus Thies","Timo Bolkart"],"pdf_url":"https://arxiv.org/pdf/2501.06903v2.pdf","comment":"Accepted to CVPR25 Website: https://zielon.github.io/synshot/"},{"id":"http://arxiv.org/abs/2410.13924v2","updated":"2025-03-20T10:16:27Z","published":"2024-10-17T14:44:35Z","title":"ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding","summary":"  Neural network performance scales with both model size and data volume, as\nshown in both language and image processing. This requires scaling-friendly\narchitectures and large datasets. While transformers have been adapted for 3D\nvision, a `GPT-moment' remains elusive due to limited training data. We\nintroduce ARKit LabelMaker, a large-scale real-world 3D dataset with dense\nsemantic annotation that is more than three times larger than prior largest\ndataset. Specifically, we extend ARKitScenes with automatically generated dense\n3D labels using an extended LabelMaker pipeline, tailored for large-scale\npre-training. Training on our dataset improves accuracy across architectures,\nachieving state-of-the-art 3D semantic segmentation scores on ScanNet and\nScanNet200, with notable gains on tail classes. Our code is available at\nhttps://labelmaker.org and our dataset at\nhttps://huggingface.co/datasets/labelmaker/arkit_labelmaker.\n","authors":["Guangda Ji","Silvan Weder","Francis Engelmann","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2410.13924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15110v2","updated":"2025-03-20T10:15:48Z","published":"2025-03-19T11:07:01Z","title":"GIVEPose: Gradual Intra-class Variation Elimination for RGB-based\n  Category-Level Object Pose Estimation","summary":"  Recent advances in RGBD-based category-level object pose estimation have been\nlimited by their reliance on precise depth information, restricting their\nbroader applicability. In response, RGB-based methods have been developed.\nAmong these methods, geometry-guided pose regression that originated from\ninstance-level tasks has demonstrated strong performance. However, we argue\nthat the NOCS map is an inadequate intermediate representation for\ngeometry-guided pose regression method, as its many-to-one correspondence with\ncategory-level pose introduces redundant instance-specific information,\nresulting in suboptimal results. This paper identifies the intra-class\nvariation problem inherent in pose regression based solely on the NOCS map and\nproposes the Intra-class Variation-Free Consensus (IVFC) map, a novel\ncoordinate representation generated from the category-level consensus model. By\nleveraging the complementary strengths of the NOCS map and the IVFC map, we\nintroduce GIVEPose, a framework that implements Gradual Intra-class Variation\nElimination for category-level object pose estimation. Extensive evaluations on\nboth synthetic and real-world datasets demonstrate that GIVEPose significantly\noutperforms existing state-of-the-art RGB-based approaches, achieving\nsubstantial improvements in category-level object pose estimation. Our code is\navailable at https://github.com/ziqin-h/GIVEPose.\n","authors":["Zinqin Huang","Gu Wang","Chenyangguang Zhang","Ruida Zhang","Xiu Li","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2503.15110v2.pdf","comment":"CVPR2025"},{"id":"http://arxiv.org/abs/2503.13344v2","updated":"2025-03-20T10:11:27Z","published":"2025-03-17T16:22:00Z","title":"STEP: Simultaneous Tracking and Estimation of Pose for Animals and\n  Humans","summary":"  We introduce STEP, a novel framework utilizing Transformer-based\ndiscriminative model prediction for simultaneous tracking and estimation of\npose across diverse animal species and humans. We are inspired by the fact that\nthe human brain exploits spatiotemporal continuity and performs concurrent\nlocalization and pose estimation despite the specialization of brain areas for\nform and motion processing. Traditional discriminative models typically require\npredefined target states for determining model weights, a challenge we address\nthrough Gaussian Map Soft Prediction (GMSP) and Offset Map Regression Adapter\n(OMRA) Modules. These modules remove the necessity of keypoint target states as\ninput, streamlining the process. Our method starts with a known target state in\nthe initial frame of a given video sequence. It then seamlessly tracks the\ntarget and estimates keypoints of anatomical importance as output for\nsubsequent frames. Unlike prevalent top-down pose estimation methods, our\napproach doesn't rely on per-frame target detections due to its tracking\ncapability. This facilitates a significant advancement in inference efficiency\nand potential applications. We train and validate our approach on datasets\nencompassing diverse species. Our experiments demonstrate superior results\ncompared to existing methods, opening doors to various applications, including\nbut not limited to action recognition and behavioral analysis.\n","authors":["Shashikant Verma","Harish Katti","Soumyaratna Debnath","Yamuna Swamy","Shanmuganathan Raman"],"pdf_url":"https://arxiv.org/pdf/2503.13344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16000v1","updated":"2025-03-20T10:07:51Z","published":"2025-03-20T10:07:51Z","title":"SenseExpo: Efficient Autonomous Exploration with Prediction Information\n  from Lightweight Neural Networks","summary":"  This paper proposes SenseExpo, an efficient autonomous exploration framework\nbased on a lightweight prediction network, which addresses the limitations of\ntraditional methods in computational overhead and environmental generalization.\nBy integrating Generative Adversarial Networks (GANs), Transformer, and Fast\nFourier Convolution (FFC), we designed a lightweight prediction model with\nmerely 709k parameters. Our smallest model achieves better performance on the\nKTH dataset than U-net (24.5M) and LaMa (51M), delivering PSNR 9.026 and SSIM\n0.718, particularly representing a 38.7% PSNR improvement over the\n51M-parameter LaMa model. Cross-domain testing demonstrates its strong\ngeneralization capability, with an FID score of 161.55 on the HouseExpo\ndataset, significantly outperforming comparable methods. Regarding exploration\nefficiency, on the KTH dataset,SenseExpo demonstrates approximately a 67.9%\ntime reduction in exploration time compared to MapEx. On the MRPB 1.0 dataset,\nSenseExpo achieves 77.1% time reduction roughly compared to MapEx. Deployed as\na plug-and-play ROS node, the framework seamlessly integrates with existing\nnavigation systems, providing an efficient solution for resource-constrained\ndevices.\n","authors":["Haojia Gao","Haohua Que","Hoiian Au","Weihao Shan","Mingkai Liu","Yusen Qin","Lei Mu","Rong Zhao","Xinghua Yang","Qi Wei","Fei Qiao"],"pdf_url":"https://arxiv.org/pdf/2503.16000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04545v3","updated":"2025-03-20T10:07:15Z","published":"2024-07-05T14:30:24Z","title":"Gaussian Eigen Models for Human Heads","summary":"  Current personalized neural head avatars face a trade-off: lightweight models\nlack detail and realism, while high-quality, animatable avatars require\nsignificant computational resources, making them unsuitable for commodity\ndevices. To address this gap, we introduce Gaussian Eigen Models (GEM), which\nprovide high-quality, lightweight, and easily controllable head avatars. GEM\nutilizes 3D Gaussian primitives for representing the appearance combined with\nGaussian splatting for rendering. Building on the success of mesh-based 3D\nmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbases\nfor representing the head appearance of a specific subject. In particular, we\nconstruct linear bases to represent the position, scale, rotation, and opacity\nof the 3D Gaussians. This allows us to efficiently generate Gaussian primitives\nof a specific head shape by a linear combination of the basis vectors, only\nrequiring a low-dimensional parameter vector that contains the respective\ncoefficients. We propose to construct these linear bases (GEM) by distilling\nhigh-quality compute-intense CNN-based Gaussian avatar models that can generate\nexpression-dependent appearance changes like wrinkles. These high-quality\nmodels are trained on multi-view videos of a subject and are distilled using a\nseries of principal component analyses. Once we have obtained the bases that\nrepresent the animatable appearance space of a specific human, we learn a\nregressor that takes a single RGB image as input and predicts the\nlow-dimensional parameter vector that corresponds to the shown facial\nexpression. In a series of experiments, we compare GEM's self-reenactment and\ncross-person reenactment results to state-of-the-art 3D avatar methods,\ndemonstrating GEM's higher visual quality and better generalization to new\nexpressions.\n","authors":["Wojciech Zielonka","Timo Bolkart","Thabo Beeler","Justus Thies"],"pdf_url":"https://arxiv.org/pdf/2407.04545v3.pdf","comment":"Accepted to CVPR25 Website: https://zielon.github.io/gem/"},{"id":"http://arxiv.org/abs/2503.15997v1","updated":"2025-03-20T10:01:32Z","published":"2025-03-20T10:01:32Z","title":"Automating 3D Dataset Generation with Neural Radiance Fields","summary":"  3D detection is a critical task to understand spatial characteristics of the\nenvironment and is used in a variety of applications including robotics,\naugmented reality, and image retrieval. Training performant detection models\nrequire diverse, precisely annotated, and large scale datasets that involve\ncomplex and expensive creation processes. Hence, there are only few public 3D\ndatasets that are additionally limited in their range of classes. In this work,\nwe propose a pipeline for automatic generation of 3D datasets for arbitrary\nobjects. By utilizing the universal 3D representation and rendering\ncapabilities of Radiance Fields, our pipeline generates high quality 3D models\nfor arbitrary objects. These 3D models serve as input for a synthetic dataset\ngenerator. Our pipeline is fast, easy to use and has a high degree of\nautomation. Our experiments demonstrate, that 3D pose estimation networks,\ntrained with our generated datasets, archive strong performance in typical\napplication scenarios.\n","authors":["P. Schulz","T. Hempel","A. Al-Hamadi"],"pdf_url":"https://arxiv.org/pdf/2503.15997v1.pdf","comment":"Accepted and presented at ROBOVIS 2025 (5th International Conference\n  on Robotics, Computer Vision and Intelligent Systems)"},{"id":"http://arxiv.org/abs/2503.15996v1","updated":"2025-03-20T10:00:22Z","published":"2025-03-20T10:00:22Z","title":"Animating the Uncaptured: Humanoid Mesh Animation with Video Diffusion\n  Models","summary":"  Animation of humanoid characters is essential in various graphics\napplications, but requires significant time and cost to create realistic\nanimations. We propose an approach to synthesize 4D animated sequences of input\nstatic 3D humanoid meshes, leveraging strong generalized motion priors from\ngenerative video models -- as such video models contain powerful motion\ninformation covering a wide variety of human motions. From an input static 3D\nhumanoid mesh and a text prompt describing the desired animation, we synthesize\na corresponding video conditioned on a rendered image of the 3D mesh. We then\nemploy an underlying SMPL representation to animate the corresponding 3D mesh\naccording to the video-generated motion, based on our motion optimization. This\nenables a cost-effective and accessible solution to enable the synthesis of\ndiverse and realistic 4D animations.\n","authors":["Marc Benedí San Millán","Angela Dai","Matthias Nießner"],"pdf_url":"https://arxiv.org/pdf/2503.15996v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.13214v2","updated":"2025-03-20T09:38:17Z","published":"2025-03-17T14:24:00Z","title":"A General Adaptive Dual-level Weighting Mechanism for Remote Sensing\n  Pansharpening","summary":"  Currently, deep learning-based methods for remote sensing pansharpening have\nadvanced rapidly. However, many existing methods struggle to fully leverage\nfeature heterogeneity and redundancy, thereby limiting their effectiveness. We\nuse the covariance matrix to model the feature heterogeneity and redundancy and\npropose Correlation-Aware Covariance Weighting (CACW) to adjust them. CACW\ncaptures these correlations through the covariance matrix, which is then\nprocessed by a nonlinear function to generate weights for adjustment. Building\nupon CACW, we introduce a general adaptive dual-level weighting mechanism\n(ADWM) to address these challenges from two key perspectives, enhancing a wide\nrange of existing deep-learning methods. First, Intra-Feature Weighting (IFW)\nevaluates correlations among channels within each feature to reduce redundancy\nand enhance unique information. Second, Cross-Feature Weighting (CFW) adjusts\ncontributions across layers based on inter-layer correlations, refining the\nfinal output. Extensive experiments demonstrate the superior performance of\nADWM compared to recent state-of-the-art (SOTA) methods. Furthermore, we\nvalidate the effectiveness of our approach through generality experiments,\nredundancy visualization, comparison experiments, key variables and complexity\nanalysis, and ablation studies. Our code is available at\nhttps://github.com/Jie-1203/ADWM.\n","authors":["Jie Huang","Haorui Chen","Jiaxuan Ren","Siran Peng","Liangjian Deng"],"pdf_url":"https://arxiv.org/pdf/2503.13214v2.pdf","comment":"This paper is accepted at the CVPR Conference on Computer Vision and\n  Pattern Recognition 2025"},{"id":"http://arxiv.org/abs/2503.15986v1","updated":"2025-03-20T09:36:31Z","published":"2025-03-20T09:36:31Z","title":"SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition","summary":"  Spiking Neural Networks (SNNs) based on Transformers have garnered\nsignificant attention due to their superior performance and high energy\nefficiency. However, the spiking attention modules of most existing\nTransformer-based SNNs are adapted from those of analog Transformers, failing\nto fully address the issue of over-allocating attention to irrelevant contexts.\nTo fix this fundamental yet overlooked issue, we propose a Lateral\nInhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's\nlateral inhibition mechanism, guiding the model to enhance attention to\nrelevant tokens while suppressing attention to irrelevant ones. Our model\nachieves state-of-the-art (SOTA) performance across multiple datasets,\nincluding CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),\nN-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K\ndataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)\noutperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a\nSOTA spiking Transformer, by 0.46% using only 39% of the parameters and half\nthe time steps. Our code and training checkpoints will be released upon\nacceptance.\n","authors":["Zeqi Zheng","Yanchen Huang","Yingchao Yu","Zizheng Zhu","Junfeng Tang","Zhaofei Yu","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2503.15986v1.pdf","comment":"16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.03911v2","updated":"2025-03-20T09:35:49Z","published":"2024-12-05T06:28:54Z","title":"Multi-View Pose-Agnostic Change Localization with Zero Labels","summary":"  Autonomous agents often require accurate methods for detecting and localizing\nchanges in their environment, particularly when observations are captured from\nunconstrained and inconsistent viewpoints. We propose a novel label-free,\npose-agnostic change detection method that integrates information from multiple\nviewpoints to construct a change-aware 3D Gaussian Splatting (3DGS)\nrepresentation of the scene. With as few as 5 images of the post-change scene,\nour approach can learn an additional change channel in a 3DGS and produce\nchange masks that outperform single-view techniques. Our change-aware 3D scene\nrepresentation additionally enables the generation of accurate change masks for\nunseen viewpoints. Experimental results demonstrate state-of-the-art\nperformance in complex multi-object scenes, achieving a 1.7x and 1.5x\nimprovement in Mean Intersection Over Union and F1 score respectively over\nother baselines. We also contribute a new real-world dataset to benchmark\nchange detection in diverse challenging scenes in the presence of lighting\nvariations.\n","authors":["Chamuditha Jayanga Galappaththige","Jason Lai","Lloyd Windrim","Donald Dansereau","Niko Suenderhauf","Dimity Miller"],"pdf_url":"https://arxiv.org/pdf/2412.03911v2.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2212.05216v2","updated":"2025-03-20T09:33:45Z","published":"2022-12-10T05:53:00Z","title":"Information-Preserved Blending Method for Forward-Looking Sonar\n  Mosaicing in Non-Ideal System Configuration","summary":"  Forward-Looking Sonar (FLS) has started to gain attention in the field of\nnear-bottom close-range underwater inspection because of its high resolution\nand high framerate features. Although Automatic Target Recognition (ATR)\nalgorithms have been applied tentatively for object-searching tasks, human\nsupervision is still indispensable, especially when involving critical areas. A\nclear FLS mosaic containing all suspicious information is in demand to help\nexperts deal with tremendous perception data. However, previous work only\nconsidered that FLS is working in an ideal system configuration, which assumes\nan appropriate sonar imaging setup and the availability of accurate positioning\ndata. Without those promises, the intra-frame and inter-frame artifacts will\nappear and degrade the quality of the final mosaic by making the information of\ninterest invisible. In this paper, we propose a novel blending method for FLS\nmosaicing which can preserve interested information. A Long-Short Time Sliding\nWindow (LST-SW) is designed to rectify the local statistics of raw sonar\nimages. The statistics are then utilized to construct a Global Variance Map\n(GVM). The GVM helps to emphasize the useful information contained in images in\nthe blending phase by classifying the informative and featureless pixels,\nthereby enhancing the quality of final mosaic. The method is verified using\ndata collected in the real environment. The results show that our method can\npreserve more details in FLS mosaics for human inspection purposes in practice.\n","authors":["Jiayi Su","Xingbin Tu","Fengzhong Qu","Yan Wei"],"pdf_url":"https://arxiv.org/pdf/2212.05216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15984v1","updated":"2025-03-20T09:33:16Z","published":"2025-03-20T09:33:16Z","title":"DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image\n  Restoration","summary":"  Contemporary image restoration and super-resolution techniques effectively\nharness deep neural networks, markedly outperforming traditional methods.\nHowever, astrophotography presents unique challenges for deep learning due to\nlimited training data. This work explores hybrid strategies, such as the Deep\nImage Prior (DIP) model, which facilitates blind training but is susceptible to\noverfitting, artifact generation, and instability when handling noisy images.\nWe propose enhancements to the DIP model's baseline performance through several\nadvanced techniques. First, we refine the model to process multiple frames\nconcurrently, employing the Back Projection method and the TVNet model. Next,\nwe adopt a Markov approach incorporating Monte Carlo estimation, Langevin\ndynamics, and a variational input technique to achieve unbiased estimates with\nminimal variance and counteract overfitting effectively. Collectively, these\nmodifications reduce the likelihood of noise learning and mitigate loss\nfunction fluctuations during training, enhancing result stability. We validated\nour algorithm across multiple image sets of astronomical and celestial objects,\nachieving performance that not only mitigates limitations of Lucky Imaging, a\nclassical computer vision technique that remains a standard in astronomical\nimage reconstruction but surpasses the original DIP model, state of the art\ntransformer- and diffusion-based models, underscoring the significance of our\nimprovements.\n","authors":["Suraj Singh","Anastasia Batsheva","Oleg Y. Rogov","Ahmed Bouridane"],"pdf_url":"https://arxiv.org/pdf/2503.15984v1.pdf","comment":"10 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.15978v1","updated":"2025-03-20T09:23:07Z","published":"2025-03-20T09:23:07Z","title":"A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal\n  Stimuli","summary":"  In daily life, we encounter diverse external stimuli, such as images, sounds,\nand videos. As research in multimodal stimuli and neuroscience advances,\nfMRI-based brain decoding has become a key tool for understanding brain\nperception and its complex cognitive processes. Decoding brain signals to\nreconstruct stimuli not only reveals intricate neural mechanisms but also\ndrives progress in AI, disease treatment, and brain-computer interfaces. Recent\nadvancements in neuroimaging and image generation models have significantly\nimproved fMRI-based decoding. While fMRI offers high spatial resolution for\nprecise brain activity mapping, its low temporal resolution and signal noise\npose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models\nhave enhanced reconstructed image quality, and multimodal pre-trained models\nhave boosted cross-modal decoding tasks. This survey systematically reviews\nrecent progress in fMRI-based brain decoding, focusing on stimulus\nreconstruction from passive brain signals. It summarizes datasets, relevant\nbrain regions, and categorizes existing methods by model structure.\nAdditionally, it evaluates model performance and discusses their effectiveness.\nFinally, it identifies key challenges and proposes future research directions,\noffering valuable insights for the field. For more information and resources\nrelated to this survey, visit https://github.com/LpyNow/BrainDecodingImage.\n","authors":["Pengyu Liu","Guohua Dong","Dan Guo","Kun Li","Fengling Li","Xun Yang","Meng Wang","Xiaomin Ying"],"pdf_url":"https://arxiv.org/pdf/2503.15978v1.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.15975v1","updated":"2025-03-20T09:18:10Z","published":"2025-03-20T09:18:10Z","title":"Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge\n  Consistency Guided Score Distillation","summary":"  We present Acc3D to tackle the challenge of accelerating the diffusion\nprocess to generate 3D models from single images. To derive high-quality\nreconstructions through few-step inferences, we emphasize the critical issue of\nregularizing the learning of score function in states of random noise. To this\nend, we propose edge consistency, i.e., consistent predictions across the high\nsignal-to-noise ratio region, to enhance a pre-trained diffusion model,\nenabling a distillation-based refinement of the endpoint score function.\nBuilding on those distilled diffusion models, we propose an adversarial\naugmentation strategy to further enrich the generation detail and boost overall\ngeneration quality. The two modules complement each other, mutually reinforcing\nto elevate generative performance. Extensive experiments demonstrate that our\nAcc3D not only achieves over a $20\\times$ increase in computational efficiency\nbut also yields notable quality improvements, compared to the\nstate-of-the-arts.\n","authors":["Kendong Liu","Zhiyu Zhu","Hui Liu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2503.15975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.11032v2","updated":"2025-03-20T09:17:56Z","published":"2025-03-14T03:01:10Z","title":"Weakly Supervised Contrastive Adversarial Training for Learning Robust\n  Features from Semi-supervised Data","summary":"  Existing adversarial training (AT) methods often suffer from incomplete\nperturbation, meaning that not all non-robust features are perturbed when\ngenerating adversarial examples (AEs). This results in residual correlations\nbetween non-robust features and labels, leading to suboptimal learning of\nrobust features. However, achieving complete perturbation, i.e., perturbing as\nmany non-robust features as possible, is challenging due to the difficulty in\ndistinguishing robust and non-robust features and the sparsity of labeled data.\nTo address these challenges, we propose a novel approach called Weakly\nSupervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete\nperturbation for improved learning of robust features by disrupting\ncorrelations between non-robust features and labels through complete AE\ngeneration over partially labeled data, grounded in information theory.\nExtensive theoretical analysis and comprehensive experiments on widely adopted\nbenchmarks validate the superiority of WSCAT. Our code is available at\nhttps://github.com/zhang-lilin/WSCAT.\n","authors":["Lilin Zhang","Chengpei Wu","Ning Yang"],"pdf_url":"https://arxiv.org/pdf/2503.11032v2.pdf","comment":"This paper has been accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15973v1","updated":"2025-03-20T09:16:20Z","published":"2025-03-20T09:16:20Z","title":"STOP: Integrated Spatial-Temporal Dynamic Prompting for Video\n  Understanding","summary":"  Pre-trained on tremendous image-text pairs, vision-language models like CLIP\nhave demonstrated promising zero-shot generalization across numerous\nimage-based tasks. However, extending these capabilities to video tasks remains\nchallenging due to limited labeled video data and high training costs. Recent\nvideo prompting methods attempt to adapt CLIP for video tasks by introducing\nlearnable prompts, but they typically rely on a single static prompt for all\nvideo sequences, overlooking the diverse temporal dynamics and spatial\nvariations that exist across frames. This limitation significantly hinders the\nmodel's ability to capture essential temporal information for effective video\nunderstanding. To address this, we propose an integrated Spatial-TempOral\ndynamic Prompting (STOP) model which consists of two complementary modules, the\nintra-frame spatial prompting and inter-frame temporal prompting. Our\nintra-frame spatial prompts are designed to adaptively highlight discriminative\nregions within each frame by leveraging intra-frame attention and temporal\nvariation, allowing the model to focus on areas with substantial temporal\ndynamics and capture fine-grained spatial details. Additionally, to highlight\nthe varying importance of frames for video understanding, we further introduce\ninter-frame temporal prompts, dynamically inserting prompts between frames with\nhigh temporal variance as measured by frame similarity. This enables the model\nto prioritize key frames and enhances its capacity to understand temporal\ndependencies across sequences. Extensive experiments on various video\nbenchmarks demonstrate that STOP consistently achieves superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-STOP.\n","authors":["Zichen Liu","Kunlun Xu","Bing Su","Xu Zou","Yuxin Peng","Jiahuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.15973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15970v1","updated":"2025-03-20T09:13:34Z","published":"2025-03-20T09:13:34Z","title":"V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression\n  Recognition","summary":"  Facial Expression Recognition (FER) plays a crucial role in human affective\nanalysis and has been widely applied in computer vision tasks such as\nhuman-computer interaction and psychological assessment. The 8th Affective\nBehavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions\nusing the video-based Aff-Wild2 dataset. This challenge includes various tasks,\nincluding the video-based EXPR recognition track, which is our primary focus.\nIn this paper, we demonstrate that addressing label ambiguity and class\nimbalance, which are known to cause performance degradation, can lead to\nmeaningful performance improvements. Specifically, we propose Video-based\nNoise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to\neach frame in a clip to address label ambiguity and effectively capture\ntemporal variations in facial expressions. Furthermore, we introduce a simple\nand effective augmentation strategy to reduce redundancy between consecutive\nframes, which is a primary cause of overfitting. Through extensive experiments,\nwe validate the effectiveness of our approach, demonstrating significant\nimprovements in video-based FER performance.\n","authors":["JunGyu Lee","Kunyoung Lee","Haesol Park","Ig-Jae Kim","Gi Pyo Nam"],"pdf_url":"https://arxiv.org/pdf/2503.15970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15969v1","updated":"2025-03-20T09:13:31Z","published":"2025-03-20T09:13:31Z","title":"Beyond the Visible: Multispectral Vision-Language Learning for Earth\n  Observation","summary":"  Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, in this paper, we introduce Llama3-MS-CLIP, the first\nvision-language model pre-trained with contrastive learning on a large-scale\nmultispectral dataset and report on the performance gains due to the extended\nspectral range. Furthermore, we present the largest-to-date image-caption\ndataset for multispectral data, consisting of one million Sentinel-2 samples\nand corresponding textual descriptions generated with Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n6.77% on average and retrieval performance by 4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. We release the image-caption dataset, code, and model\nweights under an open-source license.\n","authors":["Clive Tinashe Marimo","Benedikt Blumenstiel","Maximilian Nitsche","Johannes Jakubik","Thomas Brunschwiler"],"pdf_url":"https://arxiv.org/pdf/2503.15969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01448v2","updated":"2025-03-20T08:48:44Z","published":"2025-03-03T11:55:19Z","title":"Generative Human Geometry Distribution","summary":"  Realistic human geometry generation is an important yet challenging task,\nrequiring both the preservation of fine clothing details and the accurate\nmodeling of clothing-pose interactions. Geometry distributions, which can model\nthe geometry of a single human as a distribution, provide a promising\nrepresentation for high-fidelity synthesis. However, applying geometry\ndistributions for human generation requires learning a dataset-level\ndistribution over numerous individual geometry distributions. To address the\nresulting challenges, we propose a novel 3D human generative framework that,\nfor the first time, models the distribution of human geometry distributions.\nOur framework operates in two stages: first, generating the human geometry\ndistribution, and second, synthesizing high-fidelity humans by sampling from\nthis distribution. We validate our method on two tasks: pose-conditioned 3D\nhuman generation and single-view-based novel pose generation. Experimental\nresults demonstrate that our approach achieves the best quantitative results in\nterms of realism and geometric fidelity, outperforming state-of-the-art\ngenerative methods.\n","authors":["Xiangjun Tang","Biao Zhang","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2503.01448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18042v4","updated":"2025-03-20T08:47:39Z","published":"2024-09-26T16:44:02Z","title":"EMOVA: Empowering Language Models to See, Hear and Speak with Vivid\n  Emotions","summary":"  GPT-4o, an omni-modal model that enables vocal conversations with diverse\nemotions and tones, marks a milestone for omni-modal foundation models.\nHowever, empowering Large Language Models to perceive and generate images,\ntexts, and speeches end-to-end with publicly available data remains challenging\nfor the open-source community. Existing vision-language models rely on external\ntools for speech processing, while speech-language models still suffer from\nlimited or totally without vision-understanding capabilities. To address this\ngap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable\nLarge Language Models with end-to-end speech abilities while maintaining the\nleading vision-language performance. With a semantic-acoustic disentangled\nspeech tokenizer, we surprisingly notice that omni-modal alignment can further\nenhance vision-language and speech abilities compared with the bi-modal aligned\ncounterparts. Moreover, a lightweight style module is introduced for the\nflexible speech style controls including emotions and pitches. For the first\ntime, EMOVA achieves state-of-the-art performance on both the vision-language\nand speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue\nwith vivid emotions.\n","authors":["Kai Chen","Yunhao Gou","Runhui Huang","Zhili Liu","Daxin Tan","Jing Xu","Chunwei Wang","Yi Zhu","Yihan Zeng","Kuo Yang","Dingdong Wang","Kun Xiang","Haoyuan Li","Haoli Bai","Jianhua Han","Xiaohui Li","Weike Jin","Nian Xie","Yu Zhang","James T. Kwok","Hengshuang Zhao","Xiaodan Liang","Dit-Yan Yeung","Xiao Chen","Zhenguo Li","Wei Zhang","Qun Liu","Jun Yao","Lanqing Hong","Lu Hou","Hang Xu"],"pdf_url":"https://arxiv.org/pdf/2409.18042v4.pdf","comment":"Accepted by CVPR 2025. Project Page: https://emova-ollm.github.io/"},{"id":"http://arxiv.org/abs/2503.15949v1","updated":"2025-03-20T08:46:24Z","published":"2025-03-20T08:46:24Z","title":"CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image\n  Segmentation with Causal Intervention","summary":"  Referring medical image segmentation targets delineating lesions indicated by\ntextual descriptions. Aligning visual and textual cues is challenging due to\ntheir distinct data properties. Inspired by large-scale pre-trained\nvision-language models, we propose CausalCLIPSeg, an end-to-end framework for\nreferring medical image segmentation that leverages CLIP. Despite not being\ntrained on medical data, we enforce CLIP's rich semantic space onto the medical\ndomain by a tailored cross-modal decoding method to achieve text-to-pixel\nalignment. Furthermore, to mitigate confounding bias that may cause the model\nto learn spurious correlations instead of meaningful causal relationships,\nCausalCLIPSeg introduces a causal intervention module which self-annotates\nconfounders and excavates causal features from inputs for segmentation\njudgments. We also devise an adversarial min-max game to optimize causal\nfeatures while penalizing confounding ones. Extensive experiments demonstrate\nthe state-of-the-art performance of our proposed method. Code is available at\nhttps://github.com/WUTCM-Lab/CausalCLIPSeg.\n","authors":["Yaxiong Chen","Minghong Wei","Zixuan Zheng","Jingliang Hu","Yilei Shi","Shengwu Xiong","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2503.15949v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2503.15948v1","updated":"2025-03-20T08:44:10Z","published":"2025-03-20T08:44:10Z","title":"Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI\n  over Atomic Facts","summary":"  Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset.\n","authors":["Elisei Rykov","Kseniia Petrushina","Kseniia Titova","Alexander Panchenko","Vasily Konovalov"],"pdf_url":"https://arxiv.org/pdf/2503.15948v1.pdf","comment":"Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025"},{"id":"http://arxiv.org/abs/2503.04997v2","updated":"2025-03-20T08:40:35Z","published":"2025-03-06T21:56:31Z","title":"ISP-AD: A Large-Scale Real-World Dataset for Advancing Industrial\n  Anomaly Detection with Synthetic and Real Defects","summary":"  Automatic visual inspection using machine learning-based methods plays a key\nrole in achieving zero-defect policies in industry. Research on anomaly\ndetection approaches is constrained by the availability of datasets that\nrepresent complex defect appearances and imperfect imaging conditions, which\nare typical to industrial processes. Recent benchmarks indicate that most\npublicly available datasets are biased towards optimal imaging conditions,\nleading to an overestimation of the methods' applicability to real-world\nindustrial scenarios. To address this gap, we introduce the Industrial Screen\nPrinting Anomaly Detection dataset (ISP-AD). It presents challenging small and\nweakly contrasted surface defects embedded within structured patterns\nexhibiting high permitted design variability. To the best of our knowledge, it\nis the largest publicly available industrial dataset to date, including both\nsynthetic and real defects collected directly from the factory floor. In\naddition to the evaluation of defect detection performance of recent\nunsupervised anomaly detection methods, experiments on a mixed supervised\ntraining approach, incorporating both synthesized and real defects, were\nconducted. Even small amounts of injected real defects prove beneficial for\nmodel generalization. Furthermore, starting from training on purely synthetic\ndefects, emerging real defective samples can be efficiently integrated into\nsubsequent scalable training. Research findings indicate that supervision by\nmeans of both synthetic and accumulated real defects can complement each other,\nmeeting demanded industrial inspection requirements such as low false positive\nrates and high recall. The presented unsupervised and supervised dataset splits\nare designed to emphasize research on unsupervised, self-supervised, and\nsupervised approaches, enhancing their applicability to industrial settings.\n","authors":["Paul J. Krassnig","Dieter P. Gruber"],"pdf_url":"https://arxiv.org/pdf/2503.04997v2.pdf","comment":"26 pages, 6 figures, this preprint has been submitted to the Journal\n  of Intelligent Manufacturing, the dataset is available at\n  https://doi.org/10.5281/zenodo.14911043"},{"id":"http://arxiv.org/abs/2503.15940v1","updated":"2025-03-20T08:28:53Z","published":"2025-03-20T08:28:53Z","title":"UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report\n  Generation","summary":"  Automated radiology report generation aims to expedite the tedious and\nerror-prone reporting process for radiologists. While recent works have made\nprogress, learning to align medical images and textual findings remains\nchallenging due to the relative scarcity of labeled medical data. For example,\ndatasets for this task are much smaller than those used for image captioning in\ncomputer vision. In this work, we propose to transfer representations from\nCLIP, a large-scale pre-trained vision-language model, to better capture\ncross-modal semantics between images and texts. However, directly applying CLIP\nis suboptimal due to the domain gap between natural images and radiology. To\nenable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter\nmodules that are incorporated into CLIP and fine-tuned on the target task while\nkeeping base parameters fixed. The adapters are distributed across modalities\nand their interaction to enhance vision-language alignment. Experiments on two\npublic datasets demonstrate the effectiveness of our approach, advancing\nstate-of-the-art in radiology report generation. The proposed transfer learning\nframework provides a means of harnessing semantic knowledge from large-scale\npre-trained models to tackle data-scarce medical vision-language tasks. Code is\navailable at https://github.com/chauncey-tow/MRG-CLIP.\n","authors":["Yaxiong Chen","Chuang Du","Chunlei Li","Jingliang Hu","Yilei Shi","Shengwu Xiong","Xiao Xiang Zhu","Lichao Mou"],"pdf_url":"https://arxiv.org/pdf/2503.15940v1.pdf","comment":"MICCAI 2024 Workshop"},{"id":"http://arxiv.org/abs/2503.15106v2","updated":"2025-03-20T08:27:13Z","published":"2025-03-19T11:04:37Z","title":"Distilling 3D distinctive local descriptors for 6D pose estimation","summary":"  Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. Can we retain GeDi's\neffectiveness while significantly improving its efficiency? In this paper, we\nexplore this question by introducing a knowledge distillation framework that\ntrains an efficient student model to regress local descriptors from a GeDi\nteacher. Our key contributions include: an efficient large-scale training\nprocedure that ensures robustness to occlusions and partial observations while\noperating under compute and storage constraints, and a novel loss formulation\nthat handles weak supervision from non-distinctive teacher descriptors. We\nvalidate our approach on five BOP Benchmark datasets and demonstrate a\nsignificant reduction in inference time while maintaining competitive\nperformance with existing methods, bringing zero-shot 6D pose estimation closer\nto real-time feasibility. Project Website: https://tev-fbk.github.io/dGeDi/\n","authors":["Amir Hamza","Andrea Caraffa","Davide Boscaini","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2503.15106v2.pdf","comment":"Project Website: https://tev-fbk.github.io/dGeDi/"},{"id":"http://arxiv.org/abs/2407.12331v2","updated":"2025-03-20T08:27:10Z","published":"2024-07-17T06:15:05Z","title":"I2AM: Interpreting Image-to-Image Latent Diffusion Models via\n  Bi-Attribution Maps","summary":"  Large-scale diffusion models have made significant advances in image\ngeneration, particularly through cross-attention mechanisms. While\ncross-attention has been well-studied in text-to-image tasks, their\ninterpretability in image-to-image (I2I) diffusion models remains\nunderexplored. This paper introduces Image-to-Image Attribution Maps (I2AM), a\nmethod that enhances the interpretability of I2I models by visualizing\nbidirectional attribution maps, from the reference image to the generated image\nand vice versa. I2AM aggregates cross-attention scores across time steps,\nattention heads, and layers, offering insights into how critical features are\ntransferred between images. We demonstrate the effectiveness of I2AM across\nobject detection, inpainting, and super-resolution tasks. Our results\ndemonstrate that I2AM successfully identifies key regions responsible for\ngenerating the output, even in complex scenes. Additionally, we introduce the\nInpainting Mask Attention Consistency Score (IMACS) as a novel evaluation\nmetric to assess the alignment between attribution maps and inpainting masks,\nwhich correlates strongly with existing performance metrics. Through extensive\nexperiments, we show that I2AM enables model debugging and refinement,\nproviding practical tools for improving I2I model's performance and\ninterpretability.\n","authors":["Junseo Park","Hyeryung Jang"],"pdf_url":"https://arxiv.org/pdf/2407.12331v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2503.15934v1","updated":"2025-03-20T08:18:27Z","published":"2025-03-20T08:18:27Z","title":"SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer","summary":"  Global effective receptive field plays a crucial role for image style\ntransfer (ST) to obtain high-quality stylized results. However, existing ST\nbackbones (e.g., CNNs and Transformers) suffer huge computational complexity to\nachieve global receptive fields. Recently, the State Space Model (SSM),\nespecially the improved variant Mamba, has shown great potential for long-range\ndependency modeling with linear complexity, which offers a approach to resolve\nthe above dilemma. In this paper, we develop a Mamba-based style transfer\nframework, termed SaMam. Specifically, a mamba encoder is designed to\nefficiently extract content and style information. In addition, a style-aware\nmamba decoder is developed to flexibly adapt to various styles. Moreover, to\naddress the problems of local pixel forgetting, channel redundancy and spatial\ndiscontinuity of existing SSMs, we introduce both local enhancement and zigzag\nscan. Qualitative and quantitative results demonstrate that our SaMam\noutperforms state-of-the-art methods in terms of both accuracy and efficiency.\n","authors":["Hongda Liu","Longguang Wang","Ye Zhang","Ziru Yu","Yulan Guo"],"pdf_url":"https://arxiv.org/pdf/2503.15934v1.pdf","comment":"11 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.15931v1","updated":"2025-03-20T08:15:29Z","published":"2025-03-20T08:15:29Z","title":"DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup\n  Tables","summary":"  While deep neural networks have revolutionized image denoising capabilities,\ntheir deployment on edge devices remains challenging due to substantial\ncomputational and memory requirements. To this end, we present DnLUT, an\nultra-efficient lookup table-based framework that achieves high-quality color\nimage denoising with minimal resource consumption. Our key innovation lies in\ntwo complementary components: a Pairwise Channel Mixer (PCM) that effectively\ncaptures inter-channel correlations and spatial dependencies in parallel, and a\nnovel L-shaped convolution design that maximizes receptive field coverage while\nminimizing storage overhead. By converting these components into optimized\nlookup tables post-training, DnLUT achieves remarkable efficiency - requiring\nonly 500KB storage and 0.1% energy consumption compared to its CNN contestant\nDnCNN, while delivering 20X faster inference. Extensive experiments demonstrate\nthat DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,\nestablishing a new state-of-the-art in resource-efficient color image\ndenoising. The project is available at https://github.com/Stephen0808/DnLUT.\n","authors":["Sidi Yang","Binxiao Huang","Yulun Zhang","Dahai Yu","Yujiu Yang","Ngai Wong"],"pdf_url":"https://arxiv.org/pdf/2503.15931v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2412.10116v3","updated":"2025-03-20T08:09:25Z","published":"2024-12-13T12:59:12Z","title":"HS-FPN: High Frequency and Spatial Perception FPN for Tiny Object\n  Detection","summary":"  The introduction of Feature Pyramid Network (FPN) has significantly improved\nobject detection performance. However, substantial challenges remain in\ndetecting tiny objects, as their features occupy only a very small proportion\nof the feature maps. Although FPN integrates multi-scale features, it does not\ndirectly enhance or enrich the features of tiny objects. Furthermore, FPN lacks\nspatial perception ability. To address these issues, we propose a novel High\nFrequency and Spatial Perception Feature Pyramid Network (HS-FPN) with two\ninnovative modules. First, we designed a high frequency perception module (HFP)\nthat generates high frequency responses through high pass filters. These high\nfrequency responses are used as mask weights from both spatial and channel\nperspectives to enrich and highlight the features of tiny objects in the\noriginal feature maps. Second, we developed a spatial dependency perception\nmodule (SDP) to capture the spatial dependencies that FPN lacks. Our\nexperiments demonstrate that detectors based on HS-FPN exhibit competitive\nadvantages over state-of-the-art models on the AI-TOD dataset for tiny object\ndetection.\n","authors":["Zican Shi","Jing Hu","Jie Ren","Hengkang Ye","Xuyang Yuan","Yan Ouyang","Jia He","Bo Ji","Junyu Guo"],"pdf_url":"https://arxiv.org/pdf/2412.10116v3.pdf","comment":"13 pages,12 figures,7 tables"},{"id":"http://arxiv.org/abs/2503.12552v2","updated":"2025-03-20T08:09:23Z","published":"2025-03-16T15:46:12Z","title":"MTGS: Multi-Traversal Gaussian Splatting","summary":"  Multi-traversal data, commonly collected through daily commutes or by\nself-driving fleets, provides multiple viewpoints for scene reconstruction\nwithin a road block. This data offers significant potential for high-quality\nnovel view synthesis, which is crucial for applications such as autonomous\nvehicle simulators. However, inherent challenges in multi-traversal data often\nresult in suboptimal reconstruction quality, including variations in appearance\nand the presence of dynamic objects. To address these issues, we propose\nMulti-Traversal Gaussian Splatting (MTGS), a novel approach that reconstructs\nhigh-quality driving scenes from arbitrarily collected multi-traversal data by\nmodeling a shared static geometry while separately handling dynamic elements\nand appearance variations. Our method employs a multi-traversal dynamic scene\ngraph with a shared static node and traversal-specific dynamic nodes,\ncomplemented by color correction nodes with learnable spherical harmonics\ncoefficient residuals. This approach enables high-fidelity novel view synthesis\nand provides flexibility to navigate any viewpoint. We conduct extensive\nexperiments on a large-scale driving dataset, nuPlan, with multi-traversal\ndata. Our results demonstrate that MTGS improves LPIPS by 23.5% and geometry\naccuracy by 46.3% compared to single-traversal baselines. The code and data\nwould be available to the public.\n","authors":["Tianyu Li","Yihang Qiu","Zhenhua Wu","Carl Lindström","Peng Su","Matthias Nießner","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2503.12552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15927v1","updated":"2025-03-20T08:07:31Z","published":"2025-03-20T08:07:31Z","title":"BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers","summary":"  Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.\n","authors":["Hui Zhang","Tingwei Gao","Jie Shao","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.15927v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.15917v1","updated":"2025-03-20T07:49:04Z","published":"2025-03-20T07:49:04Z","title":"Learning to Efficiently Adapt Foundation Models for Self-Supervised\n  Endoscopic 3D Scene Reconstruction from Any Cameras","summary":"  Accurate 3D scene reconstruction is essential for numerous medical tasks.\nGiven the challenges in obtaining ground truth data, there has been an\nincreasing focus on self-supervised learning (SSL) for endoscopic depth\nestimation as a basis for scene reconstruction. While foundation models have\nshown remarkable progress in visual tasks, their direct application to the\nmedical domain often leads to suboptimal results. However, the visual features\nfrom these models can still enhance endoscopic tasks, emphasizing the need for\nefficient adaptation strategies, which still lack exploration currently. In\nthis paper, we introduce Endo3DAC, a unified framework for endoscopic scene\nreconstruction that efficiently adapts foundation models. We design an\nintegrated network capable of simultaneously estimating depth maps, relative\nposes, and camera intrinsic parameters. By freezing the backbone foundation\nmodel and training only the specially designed Gated Dynamic Vector-Based\nLow-Rank Adaptation (GDV-LoRA) with separate decoder heads, Endo3DAC achieves\nsuperior depth and pose estimation while maintaining training efficiency.\nAdditionally, we propose a 3D scene reconstruction pipeline that optimizes\ndepth maps' scales, shifts, and a few parameters based on our integrated\nnetwork. Extensive experiments across four endoscopic datasets demonstrate that\nEndo3DAC significantly outperforms other state-of-the-art methods while\nrequiring fewer trainable parameters. To our knowledge, we are the first to\nutilize a single network that only requires surgical videos to perform both SSL\ndepth estimation and scene reconstruction tasks. The code will be released upon\nacceptance.\n","authors":["Beilei Cui","Long Bai","Mobarakol Islam","An Wang","Zhiqi Ma","Yiming Huang","Feng Li","Zhen Chen","Zhongliang Jiang","Nassir Navab","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2503.15917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11624v4","updated":"2025-03-20T12:06:17Z","published":"2024-06-17T15:07:55Z","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","summary":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","authors":["Omer Sahin Tas","Royden Wagner"],"pdf_url":"https://arxiv.org/pdf/2406.11624v4.pdf","comment":"ICLR 2025 camera-ready. Our implementation is available at\n  github.com/kit-mrt/future-motion"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.16356v1","updated":"2025-03-20T17:14:34Z","published":"2025-03-20T17:14:34Z","title":"CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners","summary":"  Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.\n","authors":["Yunzhi Yao","Jizhan Fang","Jia-Chen Gu","Ningyu Zhang","Shumin Deng","Huajun Chen","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2503.16356v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.09165v3","updated":"2025-03-20T16:15:29Z","published":"2024-12-12T10:50:26Z","title":"When Text Embedding Meets Large Language Model: A Comprehensive Survey","summary":"  Text embedding has become a foundational technology in natural language\nprocessing (NLP) during the deep learning era, driving advancements across a\nwide array of downstream tasks. While many natural language understanding\nchallenges can now be modeled using generative paradigms and leverage the\nrobust generative and comprehension capabilities of large language models\n(LLMs), numerous practical applications - such as semantic matching,\nclustering, and information retrieval - continue to rely on text embeddings for\ntheir efficiency and effectiveness. Therefore, integrating LLMs with text\nembeddings has become a major research focus in recent years. In this survey,\nwe categorize the interplay between LLMs and text embeddings into three\noverarching themes: (1) LLM-augmented text embedding, enhancing traditional\nembedding methods with LLMs; (2) LLMs as text embedders, adapting their innate\ncapabilities for high-quality embedding; and (3) Text embedding understanding\nwith LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing\nrecent works based on interaction patterns rather than specific downstream\napplications, we offer a novel and systematic overview of contributions from\nvarious research and application domains in the era of LLMs. Furthermore, we\nhighlight the unresolved challenges that persisted in the pre-LLM era with\npre-trained language models (PLMs) and explore the emerging obstacles brought\nforth by LLMs. Building on this analysis, we outline prospective directions for\nthe evolution of text embedding, addressing both theoretical and practical\nopportunities in the rapidly advancing landscape of NLP.\n","authors":["Zhijie Nie","Zhangchi Feng","Mingxin Li","Cunwang Zhang","Yanzhao Zhang","Dingkun Long","Richong Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09165v3.pdf","comment":"Version 3: We added some latest works of LLM-based Embedders and\n  MLLM-based Embedders"},{"id":"http://arxiv.org/abs/2503.16290v1","updated":"2025-03-20T16:15:20Z","published":"2025-03-20T16:15:20Z","title":"Diffusion-augmented Graph Contrastive Learning for Collaborative Filter","summary":"  Graph-based collaborative filtering has been established as a prominent\napproach in recommendation systems, leveraging the inherent graph topology of\nuser-item interactions to model high-order connectivity patterns and enhance\nrecommendation performance. Recent advances in Graph Contrastive Learning (GCL)\nhave demonstrated promising potential to alleviate data sparsity issues by\nimproving representation learning through contrastive view generation and\nmutual information maximization. However, existing approaches lack effective\ndata augmentation strategies. Structural augmentation risks distorting\nfundamental graph topology, while feature-level perturbation techniques\npredominantly employ uniform noise scales that fail to account for\nnode-specific characteristics. To solve these challenges, we propose\nDiffusion-augmented Contrastive Learning (DGCL), an innovative framework that\nintegrates diffusion models with contrastive learning for enhanced\ncollaborative filtering. Our approach employs a diffusion process that learns\nnode-specific Gaussian distributions of representations, thereby generating\nsemantically consistent yet diversified contrastive views through reverse\ndiffusion sampling. DGCL facilitates adaptive data augmentation based on\nreconstructed representations, considering both semantic coherence and\nnode-specific features. In addition, it explores unrepresented regions of the\nlatent sparse feature space, thereby enriching the diversity of contrastive\nviews. Extensive experimental results demonstrate the effectiveness of DGCL on\nthree public datasets.\n","authors":["Fan Huang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14258v2","updated":"2025-03-20T15:09:51Z","published":"2025-03-18T13:48:18Z","title":"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal\n  System","summary":"  This paper introduces JuDGE (Judgment Document Generation Evaluation), a\nnovel benchmark for evaluating the performance of judgment document generation\nin the Chinese legal system. We define the task as generating a complete legal\njudgment document from the given factual description of the case. To facilitate\nthis benchmark, we construct a comprehensive dataset consisting of factual\ndescriptions from real legal cases, paired with their corresponding full\njudgment documents, which serve as the ground truth for evaluating the quality\nof generated documents. This dataset is further augmented by two external legal\ncorpora that provide additional legal knowledge for the task: one comprising\nstatutes and regulations, and the other consisting of a large collection of\npast judgment documents. In collaboration with legal professionals, we\nestablish a comprehensive automated evaluation framework to assess the quality\nof generated judgment documents across various dimensions. We evaluate various\nbaseline approaches, including few-shot in-context learning, fine-tuning, and a\nmulti-source retrieval-augmented generation (RAG) approach, using both general\nand legal-domain LLMs. The experimental results demonstrate that, while RAG\napproaches can effectively improve performance in this task, there is still\nsubstantial room for further improvement. All the codes and datasets are\navailable at: https://github.com/oneal2000/JuDGE.\n","authors":["Weihang Su","Baoqing Yue","Qingyao Ai","Yiran Hu","Jiaqi Li","Changyue Wang","Kaiyuan Zhang","Yueyue Wu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2503.14258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17560v3","updated":"2025-03-20T14:41:55Z","published":"2023-12-29T11:07:37Z","title":"Uncertain research country rankings. Should we continue producing\n  uncertain rankings?","summary":"  Purpose: Citation-based assessments of countries' research capabilities often\nmisrepresent their ability to achieve breakthrough advancements. These\nassessments commonly classify Japan as a developing country, which contradicts\nits prominent scientific standing. The purpose of this study is to investigate\nthe underlying causes of such inaccurate assessments and to propose methods for\nconducting more reliable evaluations. Design/methodology/approach: The study\nevaluates the effectiveness of top-percentile citation metrics as indicators of\nbreakthrough research. Using case studies of selected countries and research\ntopics, the study examines how deviations from lognormal citation distributions\nimpact the accuracy of these percentile indicators. A similar analysis is\nconducted using university data from the Leiden Ranking to investigate citation\ndistribution deviations at the institutional level. Findings: The study finds\nthat inflated lower tails in citation distributions lead to undervaluation of\nresearch capabilities in advanced technological countries, as captured by some\npercentile indicators. Conversely, research-intensive universities exhibit the\nopposite trend: a reduced lower tail relative to the upper tail, which causes\npercentile indicators to overestimate their actual research capacity. Research\nlimitations: The descriptions are mathematical facts that are self-evident.\nPractical implications: Due to variations in citation patterns across countries\nand institutions, the Ptop 10%/P and Ptop 1%/P ratios are not universal\npredictors of breakthrough research. Evaluations should move away from these\nmetrics. Relying on inappropriate citation-based measures could lead to poor\ndecision-making in research policy, undermining the effectiveness of research\nstrategies and their outcomes.\n","authors":["Alonso Rodriguez-Navarro"],"pdf_url":"https://arxiv.org/pdf/2312.17560v3.pdf","comment":"29 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.16165v1","updated":"2025-03-20T14:06:53Z","published":"2025-03-20T14:06:53Z","title":"Iterative Optimal Attention and Local Model for Single Image Rain Streak\n  Removal","summary":"  High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks.\n","authors":["Xiangyu Li","Wanshu Fan","Yue Shen","Cong Wang","Wei Wang","Xin Yang","Qiang Zhang","Dongsheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.16165v1.pdf","comment":"14 pages, 14 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.16081v1","updated":"2025-03-20T12:22:18Z","published":"2025-03-20T12:22:18Z","title":"OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  through dynamic reinforcement learning","summary":"  Multimodal Language Models have gained significant traction for their ability\nto process diverse input data types and generate coherent, contextually\nrelevant outputs across various applications. While supervised fine-tuning\n(SFT) has been the predominant approach to enhance MLLM capabilities in\ntask-specific optimization, it often falls short in fostering crucial\ngeneralized reasoning abilities. Despite the potential of reinforcement\nlearning (RL) to address these limitations, it faces two issues: (1) its\ngeneralized capabilities in multimodal tasks remain underexplored. (2) its\ntraining constraints such as constant Kullback-Leibler or clamp strategy easily\nlead to suboptimal bottleneck. To adress these issues, we introduce OThink-MR1,\na framework that extends RL to MLLMs, enabling them to achieve deeper\nunderstanding and reasoning across multimodal tasks. We design a dynamic\nKullback-Leibler strategy that significantly enhances RL performance,\nsurpassing SFT in same-task evaluations. Also, we are the first to reveal that\nRL exhibits remarkable cross-task generalization capabilities, which shows that\nmodels post-trained with RL on one multimodal task can be effectively\ntransfered to another tasks. Finally, extensive experiments demonstrate the\ngreat reasoning ability of our proposed OThink-MR1.\n","authors":["Zhiyuan Liu","Yuting Zhang","Feng Liu","Changwang Zhang","Ying Sun","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16071v1","updated":"2025-03-20T12:04:40Z","published":"2025-03-20T12:04:40Z","title":"Tuning LLMs by RAG Principles: Towards LLM-native Memory","summary":"  Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types.\n","authors":["Jiale Wei","Shuchi Wu","Ruochen Liu","Xiang Ying","Jingbo Shang","Fangbo Tao"],"pdf_url":"https://arxiv.org/pdf/2503.16071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16064v1","updated":"2025-03-20T11:56:27Z","published":"2025-03-20T11:56:27Z","title":"PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for\n  Adaptive Hashing Retrieval","summary":"  Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.\n","authors":["Qiang Zou","Shuli Cheng","Jiayi Chen"],"pdf_url":"https://arxiv.org/pdf/2503.16064v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.15879v1","updated":"2025-03-20T06:04:12Z","published":"2025-03-20T06:04:12Z","title":"Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid\n  Question Answering","summary":"  Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\n\\href{https://github.com/TeamNLP/Typed-RAG}{https://github.com/TeamNLP/Typed-RAG}.\n","authors":["DongGeon Lee","Ahjeong Park","Hyeri Lee","Hyeonseo Nam","Yunho Maeng"],"pdf_url":"https://arxiv.org/pdf/2503.15879v1.pdf","comment":"Accepted to NAACL 2025 SRW"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.16423v1","updated":"2025-03-20T17:59:47Z","published":"2025-03-20T17:59:47Z","title":"GAEA: A Geolocation Aware Conversational Model","summary":"  Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs)\nproprietary and open-source researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose a comprehensive dataset GAEA\nwith 800K images and around 1.6M question answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark comprising 4K\nimage-text pairs to evaluate conversational capabilities equipped with diverse\nquestion types. We consider 11 state-of-the-art open-source and proprietary\nLMMs and demonstrate that GAEA significantly outperforms the best open-source\nmodel, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by\n8.28%. Our dataset, model and codes are available\n","authors":["Ron Campos","Ashmal Vayani","Parth Parag Kulkarni","Rohit Gupta","Aritra Dutta","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2503.16423v1.pdf","comment":"The dataset and code used in this submission is available at:\n  https://ucf-crcv.github.io/GAEA/"},{"id":"http://arxiv.org/abs/2503.16421v1","updated":"2025-03-20T17:59:42Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16418v1","updated":"2025-03-20T17:59:34Z","published":"2025-03-20T17:59:34Z","title":"InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity","summary":"  Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community.\n","authors":["Liming Jiang","Qing Yan","Yumin Jia","Zichuan Liu","Hao Kang","Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2503.16418v1.pdf","comment":"Project page: https://bytedance.github.io/InfiniteYou/ Code and\n  model: https://github.com/bytedance/InfiniteYou"},{"id":"http://arxiv.org/abs/2503.16416v1","updated":"2025-03-20T17:59:23Z","published":"2025-03-20T17:59:23Z","title":"Survey on Evaluation of LLM-based Agents","summary":"  The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch.\n","authors":["Asaf Yehudai","Lilach Eden","Alan Li","Guy Uziel","Yilun Zhao","Roy Bar-Haim","Arman Cohan","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2503.16416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16412v1","updated":"2025-03-20T17:59:12Z","published":"2025-03-20T17:59:12Z","title":"DreamTexture: Shape from Virtual Texture with Analysis by Augmentation","summary":"  DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation.\n","authors":["Ananta R. Bhattarai","Xingzhe He","Alla Sheffer","Helge Rhodin"],"pdf_url":"https://arxiv.org/pdf/2503.16412v1.pdf","comment":"Project page: https://anantarb.github.io/dreamtexture/"},{"id":"http://arxiv.org/abs/2503.16408v1","updated":"2025-03-20T17:58:38Z","published":"2025-03-20T17:58:38Z","title":"RoboFactory: Exploring Embodied Agent Collaboration with Compositional\n  Constraints","summary":"  Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems.\n","authors":["Yiran Qin","Li Kang","Xiufeng Song","Zhenfei Yin","Xiaohong Liu","Xihui Liu","Ruimao Zhang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2503.16408v1.pdf","comment":"Project page: https://iranqin.github.io/robofactory/"},{"id":"http://arxiv.org/abs/2407.18908v2","updated":"2025-03-20T17:56:05Z","published":"2024-07-26T17:59:09Z","title":"Wolf: Dense Video Captioning with a World Summarization Framework","summary":"  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Webpage: https://wolfv0.github.io/.\n","authors":["Boyi Li","Ligeng Zhu","Ran Tian","Shuhan Tan","Yuxiao Chen","Yao Lu","Yin Cui","Sushant Veer","Max Ehrlich","Jonah Philion","Xinshuo Weng","Fuzhao Xue","Linxi Fan","Yuke Zhu","Jan Kautz","Andrew Tao","Ming-Yu Liu","Sanja Fidler","Boris Ivanovic","Trevor Darrell","Jitendra Malik","Song Han","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2407.18908v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16402v1","updated":"2025-03-20T17:55:04Z","published":"2025-03-20T17:55:04Z","title":"The Emperor's New Clothes in Benchmarking? A Rigorous Examination of\n  Mitigation Strategies for LLM Benchmark Data Contamination","summary":"  Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment.\n","authors":["Yifan Sun","Han Wang","Dongbai Li","Gang Wang","Huan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16402v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2503.16401v1","updated":"2025-03-20T17:54:42Z","published":"2025-03-20T17:54:42Z","title":"Exploring the Hidden Reasoning Process of Large Language Models by\n  Misleading Them","summary":"  Large language models (LLMs) and Vision language models (VLMs) have been able\nto perform various forms of reasoning tasks in a wide range of scenarios, but\nare they truly engaging in task abstraction and rule-based reasoning beyond\nmere memorization and pattern matching? To answer this question, we propose a\nnovel experimental approach, Misleading Fine-Tuning (MisFT), to examine whether\nLLMs/VLMs perform abstract reasoning by altering their original understanding\nof fundamental rules. In particular, by constructing a dataset with math\nexpressions that contradict correct operation principles, we fine-tune the\nmodel to learn those contradictory rules and assess its generalization ability\non different test domains. Through a series of experiments, we find that\ncurrent LLMs/VLMs are capable of effectively applying contradictory rules to\nsolve practical math word problems and math expressions represented by images,\nimplying the presence of an internal mechanism that abstracts before reasoning.\n","authors":["Guanyu Chen","Peiyang Wang","Tianren Zhang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2503.16401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20881v2","updated":"2025-03-20T17:54:41Z","published":"2025-02-28T09:25:49Z","title":"Hamiltonian Neural Networks approach to fuzzball geodesics","summary":"  The recent increase in computational resources and data availability has led\nto a significant rise in the use of Machine Learning (ML) techniques for data\nanalysis in physics. However, the application of ML methods to solve\ndifferential equations capable of describing even complex physical systems is\nnot yet fully widespread in theoretical high-energy physics. Hamiltonian Neural\nNetworks (HNNs) are tools that minimize a loss function defined to solve\nHamilton equations of motion. In this work, we implement several HNNs trained\nto solve, with high accuracy, the Hamilton equations for a massless probe\nmoving inside a smooth and horizonless geometry known as D1-D5 circular\nfuzzball. We study both planar (equatorial) and non-planar geodesics in\ndifferent regimes according to the impact parameter, some of which are\nunstable. Our findings suggest that HNNs could eventually replace standard\nnumerical integrators, as they are equally accurate but more reliable in\ncritical situations.\n","authors":["Andrea Cipriani","Alessandro De Santis","Giorgio Di Russo","Alfredo Grillo","Luca Tabarroni"],"pdf_url":"https://arxiv.org/pdf/2502.20881v2.pdf","comment":"25 pages + Appendices, 39 figures"},{"id":"http://arxiv.org/abs/2503.16400v1","updated":"2025-03-20T17:54:37Z","published":"2025-03-20T17:54:37Z","title":"ScalingNoise: Scaling Inference-Time Search for Generating Infinite\n  Videos","summary":"  Video diffusion models (VDMs) facilitate the generation of high-quality\nvideos, with current research predominantly concentrated on scaling efforts\nduring training through improvements in data quality, computational resources,\nand model complexity. However, inference-time scaling has received less\nattention, with most approaches restricting models to a single generation\nattempt. Recent studies have uncovered the existence of \"golden noises\" that\ncan enhance video quality during generation. Building on this, we find that\nguiding the scaling inference-time search of VDMs to identify better noise\ncandidates not only evaluates the quality of the frames generated in the\ncurrent step but also preserves the high-level object features by referencing\nthe anchor frame from previous multi-chunks, thereby delivering long-term\nvalue. Our analysis reveals that diffusion models inherently possess flexible\nadjustments of computation by varying denoising steps, and even a one-step\ndenoising approach, when guided by a reward signal, yields significant\nlong-term benefits. Based on the observation, we proposeScalingNoise, a\nplug-and-play inference-time search strategy that identifies golden initial\nnoises for the diffusion sampling process to improve global content consistency\nand visual diversity. Specifically, we perform one-step denoising to convert\ninitial noises into a clip and subsequently evaluate its long-term value,\nleveraging a reward model anchored by previously generated content. Moreover,\nto preserve diversity, we sample candidates from a tilted noise distribution\nthat up-weights promising noises. In this way, ScalingNoise significantly\nreduces noise-induced errors, ensuring more coherent and spatiotemporally\nconsistent video generation. Extensive experiments on benchmark datasets\ndemonstrate that the proposed ScalingNoise effectively improves long video\ngeneration.\n","authors":["Haolin Yang","Feilong Tang","Ming Hu","Yulong Li","Junjie Guo","Yexin Liu","Zelin Peng","Junjun He","Zongyuan Ge","Imran Razzak"],"pdf_url":"https://arxiv.org/pdf/2503.16400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19227v2","updated":"2025-03-20T17:54:16Z","published":"2025-02-26T15:36:25Z","title":"Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians\n  for Molecular Systems","summary":"  Density Functional Theory (DFT) is a pivotal method within quantum chemistry\nand materials science, with its core involving the construction and solution of\nthe Kohn-Sham Hamiltonian. Despite its importance, the application of DFT is\nfrequently limited by the substantial computational resources required to\nconstruct the Kohn-Sham Hamiltonian. In response to these limitations, current\nresearch has employed deep-learning models to efficiently predict molecular and\nsolid Hamiltonians, with roto-translational symmetries encoded in their neural\nnetworks. However, the scalability of prior models may be problematic when\napplied to large molecules, resulting in non-physical predictions of\nground-state properties. In this study, we generate a substantially larger\ntraining set (PubChemQH) than used previously and use it to create a scalable\nmodel for DFT calculations with physical accuracy. For our model, we introduce\na loss function derived from physical principles, which we call Wavefunction\nAlignment Loss (WALoss). WALoss involves performing a basis change on the\npredicted Hamiltonian to align it with the observed one; thus, the resulting\ndifferences can serve as a surrogate for orbital energy differences, allowing\nmodels to make better predictions for molecular orbitals and total energies\nthan previously possible. WALoss also substantially accelerates\nself-consistent-field (SCF) DFT calculations. Here, we show it achieves a\nreduction in total energy prediction error by a factor of 1347 and an SCF\ncalculation speed-up by a factor of 18%. These substantial improvements set new\nbenchmarks for achieving accurate and applicable predictions in larger\nmolecular systems.\n","authors":["Yunyang Li","Zaishuo Xia","Lin Huang","Xinran Wei","Han Yang","Sam Harshe","Zun Wang","Chang Liu","Jia Zhang","Bin Shao","Mark B. Gerstein"],"pdf_url":"https://arxiv.org/pdf/2502.19227v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16398v1","updated":"2025-03-20T17:54:04Z","published":"2025-03-20T17:54:04Z","title":"The global convergence time of stochastic gradient descent in non-convex\n  landscapes: Sharp estimates via large deviations","summary":"  In this paper, we examine the time it takes for stochastic gradient descent\n(SGD) to reach the global minimum of a general, non-convex loss function. We\napproach this question through the lens of randomly perturbed dynamical systems\nand large deviations theory, and we provide a tight characterization of the\nglobal convergence time of SGD via matching upper and lower bounds. These\nbounds are dominated by the most \"costly\" set of obstacles that the algorithm\nmay need to overcome to reach a global minimizer from a given initialization,\ncoupling in this way the global geometry of the underlying loss landscape with\nthe statistics of the noise entering the process. Finally, motivated by\napplications to the training of deep neural networks, we also provide a series\nof refinements and extensions of our analysis for loss functions with shallow\nlocal minima.\n","authors":["Waïss Azizian","Franck Iutzeler","Jérôme Malick","Panayotis Mertikopoulos"],"pdf_url":"https://arxiv.org/pdf/2503.16398v1.pdf","comment":"62 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.16395v1","updated":"2025-03-20T17:53:35Z","published":"2025-03-20T17:53:35Z","title":"Truthful Elicitation of Imprecise Forecasts","summary":"  The quality of probabilistic forecasts is crucial for decision-making under\nuncertainty. While proper scoring rules incentivize truthful reporting of\nprecise forecasts, they fall short when forecasters face epistemic uncertainty\nabout their beliefs, limiting their use in safety-critical domains where\ndecision-makers (DMs) prioritize proper uncertainty management. To address\nthis, we propose a framework for scoring imprecise forecasts -- forecasts given\nas a set of beliefs. Despite existing impossibility results for deterministic\nscoring rules, we enable truthful elicitation by drawing connection to social\nchoice theory and introducing a two-way communication framework where DMs first\nshare their aggregation rules (e.g., averaging or min-max) used in downstream\ndecisions for resolving forecast ambiguity. This, in turn, helps forecasters\nresolve indecision during elicitation. We further show that truthful\nelicitation of imprecise forecasts is achievable using proper scoring rules\nrandomized over the aggregation procedure. Our approach allows DM to elicit and\nintegrate the forecaster's epistemic uncertainty into their decision-making\nprocess, thus improving credibility.\n","authors":["Anurag Singh","Siu Lun Chau","Krikamol Muandet"],"pdf_url":"https://arxiv.org/pdf/2503.16395v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.09701v2","updated":"2025-03-20T17:47:44Z","published":"2023-08-18T17:52:12Z","title":"Do you know what q-means?","summary":"  Clustering is one of the most important tools for analysis of large datasets,\nand perhaps the most popular clustering algorithm is Lloyd's iteration for\n$k$-means. This iteration takes $n$ vectors\n$V=[v_1,\\dots,v_n]\\in\\mathbb{R}^{n\\times d}$ and outputs $k$ centroids\n$c_1,\\dots,c_k\\in\\mathbb{R}^d$; these partition the vectors into clusters based\non which centroid is closest to a particular vector. We present an overall\nimproved version of the \"$q$-means\" algorithm, the quantum algorithm originally\nproposed by Kerenidis, Landman, Luongo, and Prakash (NeurIPS'19) which performs\n$\\varepsilon$-$k$-means, an approximate version of $k$-means clustering. Our\nalgorithm does not rely on quantum linear algebra primitives of prior work, but\ninstead only uses QRAM to prepare simple states based on the current\niteration's clusters and multivariate quantum amplitude estimation. The time\ncomplexity is\n$\\widetilde{O}\\big(\\frac{\\|V\\|_F}{\\sqrt{n}}\\frac{k^{5/2}d}{\\varepsilon}(\\sqrt{k}\n+ \\log{n})\\big)$ and maintains the logarithmic dependence on $n$ while\nimproving the dependence on most of the other parameters. We also present a\n\"dequantized\" algorithm for $\\varepsilon$-$k$-means which runs in\n$O\\big(\\frac{\\|V\\|_F^2}{n}\\frac{k^{2}}{\\varepsilon^2}(kd + \\log{n})\\big)$ time.\nNotably, this classical algorithm matches the logarithmic dependence on $n$\nattained by the quantum algorithm.\n","authors":["Joao F. Doriguello","Alessandro Luongo","Ewin Tang"],"pdf_url":"https://arxiv.org/pdf/2308.09701v2.pdf","comment":"14 pages. v2: improved the quantum complexity, added references"},{"id":"http://arxiv.org/abs/2503.16382v1","updated":"2025-03-20T17:44:56Z","published":"2025-03-20T17:44:56Z","title":"Sparse Nonparametric Contextual Bandits","summary":"  This paper studies the problem of simultaneously learning relevant features\nand minimising regret in contextual bandit problems. We introduce and analyse a\nnew class of contextual bandit problems, called sparse nonparametric contextual\nbandits, in which the expected reward function lies in the linear span of a\nsmall unknown set of features that belongs to a known infinite set of candidate\nfeatures. We consider two notions of sparsity, for which the set of candidate\nfeatures is either countable or uncountable. Our contribution is two-fold.\nFirst, we provide lower bounds on the minimax regret, which show that\npolynomial dependence on the number of actions is generally unavoidable in this\nsetting. Second, we show that a variant of the Feel-Good Thompson Sampling\nalgorithm enjoys regret bounds that match our lower bounds up to logarithmic\nfactors of the horizon, and have logarithmic dependence on the effective number\nof candidate features. When we apply our results to kernelised and neural\ncontextual bandits, we find that sparsity always enables better regret bounds,\nas long as the horizon is large enough relative to the sparsity and the number\nof actions.\n","authors":["Hamish Flynn","Julia Olkhovskaya","Paul Rognon-Vael"],"pdf_url":"https://arxiv.org/pdf/2503.16382v1.pdf","comment":"45 pages"},{"id":"http://arxiv.org/abs/2409.01482v4","updated":"2025-03-20T17:39:10Z","published":"2024-09-02T22:17:18Z","title":"Masked Mixers for Language Generation and Retrieval","summary":"  Attention mechanisms that confer selective focus on a strict subset of input\nelements are nearly ubiquitous in language models today. We posit there to be\ndownside to the use of attention: most input information is lost. In support of\nthis idea we observe poor input representation accuracy in transformers and\nmore accurate representation in what we term masked mixers, which replace\nself-attention with masked convolutions. The masked mixer learns causal\nlanguage modeling more efficiently than early transformer implementations and\neven outperforms optimized, current transformers when training on small\n($n_{ctx}<512$) but not larger context windows. Evidence is presented for the\nhypothesis that differences in transformer and masked mixer training\nefficiencies for various tasks are best predicted by input representation\naccuracy, or equivalently global invertibility. We hypothesize that the\ninformation loss exhibited by transformers would be more detrimental to\nretrieval than generation, as the former is more closely approximated by a\nbijective and thus invertible function. We find that masked mixers are more\neffective retrieval models both when the pretrained embedding model is\nunchanged as well as when the embedding model is modified via cosine\nsimilarity-based InfoNCE loss minimization. A small masked mixer is shown to\noutperform a large and near state-of-the-art transformer-based retrieval model,\ndespite the latter being trained with many orders of magnitude more data and\ncompute.\n","authors":["Benjamin L. Badger"],"pdf_url":"https://arxiv.org/pdf/2409.01482v4.pdf","comment":"31 pages, 9 figures, 4 tables, 14 supplementary figures, 10\n  supplementary tables"},{"id":"http://arxiv.org/abs/2411.02344v2","updated":"2025-03-20T17:37:44Z","published":"2024-11-04T18:14:07Z","title":"Seq-VCR: Preventing Collapse in Intermediate Transformer Representations\n  for Enhanced Reasoning","summary":"  Decoder-only Transformers often struggle with complex reasoning tasks,\nparticularly arithmetic reasoning requiring multiple sequential operations. In\nthis work, we identify representation collapse in the model's intermediate\nlayers as a key factor limiting their reasoning capabilities. To address this,\nwe propose Sequential Variance-Covariance Regularization (Seq-VCR), which\nenhances the entropy of intermediate representations and prevents collapse.\nCombined with dummy pause tokens as substitutes for chain-of-thought (CoT)\ntokens, our method significantly improves performance in arithmetic reasoning\nproblems. In the challenging $5 \\times 5$ integer multiplication task, our\napproach achieves $99.5\\%$ exact match accuracy, outperforming models of the\nsame size (which yield $0\\%$ accuracy) and GPT-4 with five-shot CoT prompting\n($44\\%$). We also demonstrate superior results on arithmetic expression and\nlongest increasing subsequence (LIS) datasets. Our findings highlight the\nimportance of preventing intermediate layer representation collapse to enhance\nthe reasoning capabilities of Transformers and show that Seq-VCR offers an\neffective solution without requiring explicit CoT supervision.\n","authors":["Md Rifat Arefin","Gopeshh Subbaraj","Nicolas Gontier","Yann LeCun","Irina Rish","Ravid Shwartz-Ziv","Christopher Pal"],"pdf_url":"https://arxiv.org/pdf/2411.02344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16371v1","updated":"2025-03-20T17:33:08Z","published":"2025-03-20T17:33:08Z","title":"Reinforcement Learning-based Heuristics to Guide Domain-Independent\n  Dynamic Programming","summary":"  Domain-Independent Dynamic Programming (DIDP) is a state-space search\nparadigm based on dynamic programming for combinatorial optimization. In its\ncurrent implementation, DIDP guides the search using user-defined dual bounds.\nReinforcement learning (RL) is increasingly being applied to combinatorial\noptimization problems and shares several key structures with DP, being\nrepresented by the Bellman equation and state-based transition systems. We\npropose using reinforcement learning to obtain a heuristic function to guide\nthe search in DIDP. We develop two RL-based guidance approaches: value-based\nguidance using Deep Q-Networks and policy-based guidance using Proximal Policy\nOptimization. Our experiments indicate that RL-based guidance significantly\noutperforms standard DIDP and problem-specific greedy heuristics with the same\nnumber of node expansions. Further, despite longer node evaluation times, RL\nguidance achieves better run-time performance than standard DIDP on three of\nfour benchmark domains.\n","authors":["Minori Narita","Ryo Kuroiwa","J. Christopher Beck"],"pdf_url":"https://arxiv.org/pdf/2503.16371v1.pdf","comment":"24 pages, 4 figures, to be published in CPAIOR 2025\n  (https://sites.google.com/view/cpaior2025)"},{"id":"http://arxiv.org/abs/2503.16364v1","updated":"2025-03-20T17:21:23Z","published":"2025-03-20T17:21:23Z","title":"Neural Networks: According to the Principles of Grassmann Algebra","summary":"  In this paper, we explore the algebra of quantum idempotents and the\nquantization of fermions which gives rise to a Hilbert space equal to the\nGrassmann algebra associated with the Lie algebra. Since idempotents carry\nrepresentations of the algebra under consideration, they form algebraic\nvarieties and smooth manifolds in the natural topology. In addition to the\nmotivation of linking up mathematical physics with machine learning, it is also\nshown that by using idempotents and invariant subspace of the corresponding\nalgebras, these representations encode and perhaps provide a probabilistic\ninterpretation of reasoning and relational paths in geometrical terms.\n","authors":["Z. Zarezadeh","N. Zarezadeh"],"pdf_url":"https://arxiv.org/pdf/2503.16364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11190v2","updated":"2025-03-20T17:20:55Z","published":"2025-02-16T16:31:00Z","title":"ReLearn: Unlearning via Learning for Large Language Models","summary":"  Current unlearning methods for large language models usually rely on reverse\noptimization to reduce target token probabilities. However, this paradigm\ndisrupts the subsequent tokens prediction, degrading model performance and\nlinguistic coherence. Moreover, existing evaluation metrics overemphasize\ncontextual forgetting while inadequately assessing response fluency and\nrelevance. To address these challenges, we propose ReLearn, a data augmentation\nand fine-tuning pipeline for effective unlearning, along with a comprehensive\nevaluation framework. This framework introduces Knowledge Forgetting Rate (KFR)\nand Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and\nLinguistic Score (LS) to evaluate generation quality. Our experiments show that\nReLearn successfully achieves targeted forgetting while preserving high-quality\noutput. Through mechanistic analysis, we further demonstrate how reverse\noptimization disrupts coherent text generation, while ReLearn preserves this\nessential capability. Code is available at https://github.com/zjunlp/unlearn.\n","authors":["Haoming Xu","Ningyuan Zhao","Liming Yang","Sendong Zhao","Shumin Deng","Mengru Wang","Bryan Hooi","Nay Oo","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.11190v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.16363v1","updated":"2025-03-20T17:20:26Z","published":"2025-03-20T17:20:26Z","title":"Probabilistic Quantum SVM Training on Ising Machine","summary":"  Quantum computing holds significant potential to accelerate machine learning\nalgorithms, especially in solving optimization problems like those encountered\nin Support Vector Machine (SVM) training. However, current QUBO-based Quantum\nSVM (QSVM) methods rely solely on binary optimal solutions, limiting their\nability to identify fuzzy boundaries in data. Additionally, the limited qubit\ncount in contemporary quantum devices constrains training on larger datasets.\nIn this paper, we propose a probabilistic quantum SVM training framework\nsuitable for Coherent Ising Machines (CIMs). By formulating the SVM training\nproblem as a QUBO model, we leverage CIMs' energy minimization capabilities and\nintroduce a Boltzmann distribution-based probabilistic approach to better\napproximate optimal SVM solutions, enhancing robustness. To address qubit\nlimitations, we employ batch processing and multi-batch ensemble strategies,\nenabling small-scale quantum devices to train SVMs on larger datasets and\nsupport multi-class classification tasks via a one-vs-one approach. Our method\nis validated through simulations and real-machine experiments on binary and\nmulti-class datasets. On the banknote binary classification dataset, our\nCIM-based QSVM, utilizing an energy-based probabilistic approach, achieved up\nto 20% higher accuracy compared to the original QSVM, while training up to\n$10^4$ times faster than simulated annealing methods. Compared with classical\nSVM, our approach either matched or reduced training time. On the IRIS\nthree-class dataset, our improved QSVM outperformed existing QSVM models in all\nkey metrics. As quantum technology advances, increased qubit counts are\nexpected to further enhance QSVM performance relative to classical SVM.\n","authors":["Haoqi He","Yan Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.16363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14312v4","updated":"2025-03-20T17:19:33Z","published":"2022-11-20T04:59:23Z","title":"Karyotype AI for Precision Oncology","summary":"  We present a machine learning method capable of accurately detecting\nchromosome abnormalities that cause blood cancers directly from microscope\nimages of the metaphase stage of cell division. The pipeline is built on a\nseries of fine-tuned Vision Transformers. Current state of the art (and\nstandard clinical practice) requires expensive, manual expert analysis, whereas\nour pipeline takes only 15 seconds per metaphase image. Using a novel\npretraining-finetuning strategy to mitigate the challenge of data scarcity, we\nachieve a high precision-recall score of 94% AUC for the clinically significant\ndel(5q) and t(9;22) anomalies. Our method also unlocks zero-shot detection of\nrare aberrations based on model latent embeddings. The ability to quickly,\naccurately, and scalably diagnose genetic abnormalities directly from metaphase\nimages could transform karyotyping practice and improve patient outcomes. We\nwill make code publicly available.\n","authors":["Zahra Shamsi","Drew Bryant","Jacob Wilson","Xiaoyu Qu","Avinava Dubey","Konik Kothari","Mostafa Dehghani","Mariya Chavarha","Valerii Likhosherstov","Brian Williams","Michael Frumkin","Fred Appelbaum","Krzysztof Choromanski","Ali Bashir","Min Fang"],"pdf_url":"https://arxiv.org/pdf/2211.14312v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16361v1","updated":"2025-03-20T17:17:58Z","published":"2025-03-20T17:17:58Z","title":"Enhancing variational quantum algorithms by balancing training on\n  classical and quantum hardware","summary":"  Quantum computers offer a promising route to tackling problems that are\nclassically intractable such as in prime-factorization, solving large-scale\nlinear algebra and simulating complex quantum systems, but require\nfault-tolerant quantum hardware. On the other hand, variational quantum\nalgorithms (VQAs) have the potential to provide a near-term route to quantum\nutility or advantage, and is usually constructed by using parametrized quantum\ncircuits (PQCs) in combination with a classical optimizer for training.\nAlthough VQAs have been proposed for a multitude of tasks such as ground-state\nestimation, combinatorial optimization and unitary compilation, there remain\nmajor challenges in its trainability and resource costs on quantum hardware.\nHere we address these challenges by adopting Hardware Efficient and dynamical\nLIe algebra Supported Ansatz (HELIA), and propose two training schemes that\ncombine an existing g-sim method (that uses the underlying group structure of\nthe operators) and the Parameter-Shift Rule (PSR). Our improvement comes from\ndistributing the resources required for gradient estimation and training to\nboth classical and quantum hardware. We numerically test our proposal for\nground-state estimation using Variational Quantum Eigensolver (VQE) and\nclassification of quantum phases using quantum neural networks. Our methods\nshow better accuracy and success of trials, and also need fewer calls to the\nquantum hardware on an average than using only PSR (upto 60% reduction), that\nruns exclusively on quantum hardware. We also numerically demonstrate the\ncapability of HELIA in mitigating barren plateaus, paving the way for training\nlarge-scale quantum models.\n","authors":["Rahul Bhowmick","Harsh Wadhwa","Avinash Singh","Tania Sidana","Quoc Hoan Tran","Krishna Kumar Sabapathy"],"pdf_url":"https://arxiv.org/pdf/2503.16361v1.pdf","comment":"28 pages, 13 figures, 5 tables, 4 algorithms"},{"id":"http://arxiv.org/abs/2503.16356v1","updated":"2025-03-20T17:14:34Z","published":"2025-03-20T17:14:34Z","title":"CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners","summary":"  Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.\n","authors":["Yunzhi Yao","Jizhan Fang","Jia-Chen Gu","Ningyu Zhang","Shumin Deng","Huajun Chen","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2503.16356v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.16351v1","updated":"2025-03-20T17:09:18Z","published":"2025-03-20T17:09:18Z","title":"Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling\n  Biological Sequences","summary":"  Deep learning architectures such as convolutional neural networks and\nTransformers have revolutionized biological sequence modeling, with recent\nadvances driven by scaling up foundation and task-specific models. The\ncomputational resources and large datasets required, however, limit their\napplicability in biological contexts. We introduce Lyra, a subquadratic\narchitecture for sequence modeling, grounded in the biological framework of\nepistasis for understanding sequence-to-function relationships. Mathematically,\nwe demonstrate that state space models efficiently capture global epistatic\ninteractions and combine them with projected gated convolutions for modeling\nlocal relationships. We demonstrate that Lyra is performant across over 100\nwide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in\nmany key areas, including protein fitness landscape prediction, biophysical\nproperty prediction (e.g. disordered protein region functions) peptide\nengineering applications (e.g. antibody binding, cell-penetrating peptide\nprediction), RNA structure analysis, RNA function prediction, and CRISPR guide\ndesign. It achieves this with orders-of-magnitude improvements in inference\nspeed and reduction in parameters (up to 120,000-fold in our tests) compared to\nrecent biology foundation models. Using Lyra, we were able to train and run\nevery task in this study on two or fewer GPUs in under two hours, democratizing\naccess to biological sequence modeling at SOTA performance, with potential\napplications to many fields.\n","authors":["Krithik Ramesh","Sameed M. Siddiqui","Albert Gu","Michael D. Mitzenmacher","Pardis C. Sabeti"],"pdf_url":"https://arxiv.org/pdf/2503.16351v1.pdf","comment":"53 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.14141v3","updated":"2025-03-20T17:05:31Z","published":"2023-12-21T18:57:54Z","title":"Quantum Algorithms for the Pathwise Lasso","summary":"  We present a novel quantum high-dimensional linear regression algorithm with\nan $\\ell_1$-penalty based on the classical LARS (Least Angle Regression)\npathwise algorithm. Similarly to available classical algorithms for Lasso, our\nquantum algorithm provides the full regularisation path as the penalty term\nvaries, but quadratically faster per iteration under specific conditions. A\nquadratic speedup on the number of features $d$ is possible by using the simple\nquantum minimum-finding subroutine from D\\\"urr and Hoyer (arXiv'96) in order to\nobtain the joining time at each iteration. We then improve upon this simple\nquantum algorithm and obtain a quadratic speedup both in the number of features\n$d$ and the number of observations $n$ by using the approximate quantum\nminimum-finding subroutine from Chen and de Wolf (ICALP'23). In order to do so,\nwe approximately compute the joining times to be searched over by the\napproximate quantum minimum-finding subroutine. As another main contribution,\nwe prove, via an approximate version of the KKT conditions and a duality gap,\nthat the LARS algorithm (and therefore our quantum algorithm) is robust to\nerrors. This means that it still outputs a path that minimises the Lasso cost\nfunction up to a small error if the joining times are only approximately\ncomputed. Furthermore, we show that, when the observations are sampled from a\nGaussian distribution, our quantum algorithm's complexity only depends\npolylogarithmically on $n$, exponentially better than the classical LARS\nalgorithm, while keeping the quadratic improvement on $d$. Moreover, we propose\na dequantised version of our quantum algorithm that also retains the\npolylogarithmic dependence on $n$, albeit presenting the linear scaling on $d$\nfrom the standard LARS algorithm. Finally, we prove query lower bounds for\nclassical and quantum Lasso algorithms.\n","authors":["Joao F. Doriguello","Debbie Lim","Chi Seng Pun","Patrick Rebentrost","Tushar Vaidya"],"pdf_url":"https://arxiv.org/pdf/2312.14141v3.pdf","comment":"54 pages. v2: several improvements, typos fixed, references added,\n  fixed a bug in Theorem 28, exponentially improved the complexity dependence\n  on the number of observations $n$ for a random Gaussian input matrix; v3: new\n  lower bounds added, published version at Quantum Journal"},{"id":"http://arxiv.org/abs/2503.16342v1","updated":"2025-03-20T16:58:40Z","published":"2025-03-20T16:58:40Z","title":"HiQ-Lip: The First Quantum-Classical Hierarchical Method for Global\n  Lipschitz Constant Estimation of ReLU Networks","summary":"  Estimating the global Lipschitz constant of neural networks is crucial for\nunderstanding and improving their robustness and generalization capabilities.\nHowever, precise calculations are NP-hard, and current semidefinite programming\n(SDP) methods face challenges such as high memory usage and slow processing\nspeeds. In this paper, we propose \\textbf{HiQ-Lip}, a hybrid quantum-classical\nhierarchical method that leverages Coherent Ising Machines (CIMs) to estimate\nthe global Lipschitz constant. We tackle the estimation by converting it into a\nQuadratic Unconstrained Binary Optimization (QUBO) problem and implement a\nmultilevel graph coarsening and refinement strategy to adapt to the constraints\nof contemporary quantum hardware. Our experimental evaluations on fully\nconnected neural networks demonstrate that HiQ-Lip not only provides estimates\ncomparable to state-of-the-art methods but also significantly accelerates the\ncomputation process. In specific tests involving two-layer neural networks with\n256 hidden neurons, HiQ-Lip doubles the solving speed and offers more accurate\nupper bounds than the existing best method, LiPopt. These findings highlight\nthe promising utility of small-scale quantum devices in advancing the\nestimation of neural network robustness.\n","authors":["Haoqi He","Yan Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.16342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16340v1","updated":"2025-03-20T16:57:15Z","published":"2025-03-20T16:57:15Z","title":"Nonlinear action prediction models reveal multi-timescale locomotor\n  control","summary":"  Modeling movement in real-world tasks is a fundamental scientific goal.\nHowever, it is unclear whether existing models and their assumptions,\noverwhelmingly tested in laboratory-constrained settings, generalize to the\nreal world. For example, data-driven models of foot placement control -- a\ncrucial action for stable locomotion -- assume linear and single timescale\nmappings. We develop nonlinear foot placement prediction models, finding that\nneural network architectures with flexible input history-dependence like GRU\nand Transformer perform best across multiple contexts (walking and running,\ntreadmill and overground, varying terrains) and input modalities (multiple body\nstates, gaze), outperforming traditional models. These models reveal context-\nand modality-dependent timescales: there is more reliance on fast-timescale\npredictions in complex terrain, gaze predictions precede body state\npredictions, and full-body state predictions precede center-of-mass-relevant\npredictions. Thus, nonlinear action prediction models provide quantifiable\ninsights into real-world motor control and can be extended to other actions,\ncontexts, and populations.\n","authors":["Wei-Chen Wang","Antoine De Comite","Monica Daley","Alexandra Voloshina","Nidhi Seethapathi"],"pdf_url":"https://arxiv.org/pdf/2503.16340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16337v1","updated":"2025-03-20T16:56:06Z","published":"2025-03-20T16:56:06Z","title":"Optimal Complexity in Byzantine-Robust Distributed Stochastic\n  Optimization with Data Heterogeneity","summary":"  In this paper, we establish tight lower bounds for Byzantine-robust\ndistributed first-order stochastic optimization methods in both strongly convex\nand non-convex stochastic optimization. We reveal that when the distributed\nnodes have heterogeneous data, the convergence error comprises two components:\na non-vanishing Byzantine error and a vanishing optimization error. We\nestablish the lower bounds on the Byzantine error and on the minimum number of\nqueries to a stochastic gradient oracle required to achieve an arbitrarily\nsmall optimization error. Nevertheless, we identify significant discrepancies\nbetween our established lower bounds and the existing upper bounds. To fill\nthis gap, we leverage the techniques of Nesterov's acceleration and variance\nreduction to develop novel Byzantine-robust distributed stochastic optimization\nmethods that provably match these lower bounds, up to logarithmic factors,\nimplying that our established lower bounds are tight.\n","authors":["Qiankun Shi","Jie Peng","Kun Yuan","Xiao Wang","Qing Ling"],"pdf_url":"https://arxiv.org/pdf/2503.16337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16328v1","updated":"2025-03-20T16:52:25Z","published":"2025-03-20T16:52:25Z","title":"Knowledge-guided machine learning model with soil moisture for corn\n  yield prediction under drought conditions","summary":"  Remote sensing (RS) techniques, by enabling non-contact acquisition of\nextensive ground observations, have become a valuable tool for corn yield\nprediction. Traditional process-based (PB) models are limited by fixed input\nfeatures and struggle to incorporate large volumes of RS data. In contrast,\nmachine learning (ML) models are often criticized for being ``black boxes''\nwith limited interpretability. To address these limitations, we used\nKnowledge-Guided Machine Learning (KGML), which combined the strengths of both\napproaches and fully used RS data. However, previous KGML methods overlooked\nthe crucial role of soil moisture in plant growth. To bridge this gap, we\nproposed the Knowledge-Guided Machine Learning with Soil Moisture (KGML-SM)\nframework, using soil moisture as an intermediate variable to emphasize its key\nrole in plant development. Additionally, based on the prior knowledge that the\nmodel may overestimate under drought conditions, we designed a drought-aware\nloss function that penalizes predicted yield in drought-affected areas. Our\nexperiments showed that the KGML-SM model outperformed other ML models.\nFinally, we explored the relationships between drought, soil moisture, and corn\nyield prediction, assessing the importance of various features and analyzing\nhow soil moisture impacts corn yield predictions across different regions and\ntime periods.\n","authors":["Xiaoyu Wang","Yijia Xu","Jingyi Huang","Zhengwei Yang","Zhou Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.16328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16323v1","updated":"2025-03-20T16:44:53Z","published":"2025-03-20T16:44:53Z","title":"NeuralFoil: An Airfoil Aerodynamics Analysis Tool Using Physics-Informed\n  Machine Learning","summary":"  NeuralFoil is an open-source Python-based tool for rapid aerodynamics\nanalysis of airfoils, similar in purpose to XFoil. Speedups ranging from 8x to\n1,000x over XFoil are demonstrated, after controlling for equivalent accuracy.\nNeuralFoil computes both global and local quantities (lift, drag, velocity\ndistribution, etc.) over a broad input space, including: an 18-dimensional\nspace of airfoil shapes, possibly including control deflections; a 360 degree\nrange of angles of attack; Reynolds numbers from $10^2$ to $10^{10}$; subsonic\nflows up to the transonic drag rise; and with varying turbulence parameters.\nResults match those of XFoil closely: the mean relative error of drag is 0.37%\non simple cases, and remains as low as 2.0% on a test dataset with numerous\npost-stall and transitional cases. NeuralFoil facilitates gradient-based design\noptimization, due to its $C^\\infty$-continuous solutions,\nautomatic-differentiation-compatibility, and bounded computational cost without\nnon-convergence issues.\n  NeuralFoil is a hybrid of physics-informed machine learning techniques and\nanalytical models. Here, physics information includes symmetries that are\nstructurally embedded into the model architecture, feature engineering using\ndomain knowledge, and guaranteed extrapolation to known limit cases. This work\nalso introduces a new approach for surrogate model uncertainty quantification\nthat enables robust design optimization.\n  This work discusses the methodology and performance of NeuralFoil with\nseveral case studies, including a practical airfoil design optimization study\nincluding both aerodynamic and non-aerodynamic constraints. Here, NeuralFoil\noptimization is able to produce airfoils nearly identical in performance and\nshape to expert-designed airfoils within seconds; these\ncomputationally-optimized airfoils provide a useful starting point for further\nexpert refinement.\n","authors":["Peter Sharpe","R. John Hansman"],"pdf_url":"https://arxiv.org/pdf/2503.16323v1.pdf","comment":"42 pages, 14 figures"},{"id":"http://arxiv.org/abs/2409.00553v2","updated":"2025-03-20T16:42:22Z","published":"2024-08-31T22:41:26Z","title":"Multi-Output Distributional Fairness via Post-Processing","summary":"  The post-processing approaches are becoming prominent techniques to enhance\nmachine learning models' fairness because of their intuitiveness, low\ncomputational cost, and excellent scalability. However, most existing\npost-processing methods are designed for task-specific fairness measures and\nare limited to single-output models. In this paper, we introduce a\npost-processing method for multi-output models, such as the ones used for\nmulti-task/multi-class classification and representation learning, to enhance a\nmodel's distributional parity, a task-agnostic fairness measure. Existing\nmethods for achieving distributional parity rely on the (inverse) cumulative\ndensity function of a model's output, restricting their applicability to\nsingle-output models. Extending previous works, we propose to employ optimal\ntransport mappings to move a model's outputs across different groups towards\ntheir empirical Wasserstein barycenter. An approximation technique is applied\nto reduce the complexity of computing the exact barycenter and a kernel\nregression method is proposed to extend this process to out-of-sample data. Our\nempirical studies evaluate the proposed approach against various baselines on\nmulti-task/multi-class classification and representation learning tasks,\ndemonstrating the effectiveness of the proposed approach.\n","authors":["Gang Li","Qihang Lin","Ayush Ghosh","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2409.00553v2.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.16316v1","updated":"2025-03-20T16:38:25Z","published":"2025-03-20T16:38:25Z","title":"On the Cone Effect in the Learning Dynamics","summary":"  Understanding the learning dynamics of neural networks is a central topic in\nthe deep learning community. In this paper, we take an empirical perspective to\nstudy the learning dynamics of neural networks in real-world settings.\nSpecifically, we investigate the evolution process of the empirical Neural\nTangent Kernel (eNTK) during training. Our key findings reveal a two-phase\nlearning process: i) in Phase I, the eNTK evolves significantly, signaling the\nrich regime, and ii) in Phase II, the eNTK keeps evolving but is constrained in\na narrow space, a phenomenon we term the cone effect. This two-phase framework\nbuilds on the hypothesis proposed by Fort et al. (2020), but we uniquely\nidentify the cone effect in Phase II, demonstrating its significant performance\nadvantages over fully linearized training.\n","authors":["Zhanpeng Zhou","Yongyi Yang","Jie Ren","Mahito Sugiyama","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2503.16316v1.pdf","comment":"Accepted by ICLR 2025 workshop DeLTa"},{"id":"http://arxiv.org/abs/2503.16315v1","updated":"2025-03-20T16:38:16Z","published":"2025-03-20T16:38:16Z","title":"Active Learning For Repairable Hardware Systems With Partial Coverage","summary":"  Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test.\n","authors":["Michael Potter","Beyza Kalkanlı","Deniz Erdoğmuş","Michael Everett"],"pdf_url":"https://arxiv.org/pdf/2503.16315v1.pdf","comment":"Submitted to IEEE Reliability and Maintainability Symposium - Europe\n  2025"},{"id":"http://arxiv.org/abs/2404.18400v3","updated":"2025-03-20T16:37:17Z","published":"2024-04-29T03:30:06Z","title":"LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models","summary":"  Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR\n","authors":["Parshin Shojaee","Kazem Meidani","Shashank Gupta","Amir Barati Farimani","Chandan K Reddy"],"pdf_url":"https://arxiv.org/pdf/2404.18400v3.pdf","comment":"ICLR 2025 Oral"},{"id":"http://arxiv.org/abs/2503.16311v1","updated":"2025-03-20T16:34:14Z","published":"2025-03-20T16:34:14Z","title":"Structured-Noise Masked Modeling for Video, Audio and Beyond","summary":"  Masked modeling has emerged as a powerful self-supervised learning framework,\nbut existing methods largely rely on random masking, disregarding the\nstructural properties of different modalities. In this work, we introduce\nstructured noise-based masking, a simple yet effective approach that naturally\naligns with the spatial, temporal, and spectral characteristics of video and\naudio data. By filtering white noise into distinct color noise distributions,\nwe generate structured masks that preserve modality-specific patterns without\nrequiring handcrafted heuristics or access to the data. Our approach improves\nthe performance of masked video and audio modeling frameworks without any\ncomputational overhead. Extensive experiments demonstrate that structured noise\nmasking achieves consistent improvement over random masking for standard and\nadvanced masked modeling methods, highlighting the importance of modality-aware\nmasking strategies for representation learning.\n","authors":["Aritra Bhowmik","Fida Mohammad Thoker","Carlos Hinojosa","Bernard Ghanem","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2503.16311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05916v2","updated":"2025-03-20T16:31:41Z","published":"2024-02-08T18:51:55Z","title":"GenEFT: Understanding Statics and Dynamics of Model Generalization via\n  Effective Theory","summary":"  We present GenEFT: an effective theory framework for shedding light on the\nstatics and dynamics of neural network generalization, and illustrate it with\ngraph learning examples. We first investigate the generalization phase\ntransition as data size increases, comparing experimental results with\ninformation-theory-based approximations. We find generalization in a Goldilocks\nzone where the decoder is neither too weak nor too powerful. We then introduce\nan effective theory for the dynamics of representation learning, where\nlatent-space representations are modeled as interacting particles (repons), and\nfind that it explains our experimentally observed phase transition between\ngeneralization and overfitting as encoder and decoder learning rates are\nscanned. This highlights the power of physics-inspired effective theories for\nbridging the gap between theoretical predictions and practice in machine\nlearning.\n","authors":["David D. Baek","Ziming Liu","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2402.05916v2.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.16286v1","updated":"2025-03-20T16:13:09Z","published":"2025-03-20T16:13:09Z","title":"Explainable Graph-theoretical Machine Learning: with Application to\n  Alzheimer's Disease Prediction","summary":"  Alzheimer's disease (AD) affects 50 million people worldwide and is projected\nto overwhelm 152 million by 2050. AD is characterized by cognitive decline due\npartly to disruptions in metabolic brain connectivity. Thus, early and accurate\ndetection of metabolic brain network impairments is crucial for AD management.\nChief to identifying such impairments is FDG-PET data. Despite advancements,\nmost graph-based studies using FDG-PET data rely on group-level analysis or\nthresholding. Yet, group-level analysis can veil individual differences and\nthresholding may overlook weaker but biologically critical brain connections.\nAdditionally, machine learning-based AD prediction largely focuses on\nunivariate outcomes, such as disease status. Here, we introduce explainable\ngraph-theoretical machine learning (XGML), a framework employing kernel density\nestimation and dynamic time warping to construct individual metabolic brain\ngraphs that capture the distance between pair-wise brain regions and identify\nsubgraphs most predictive of multivariate AD-related outcomes. Using FDG-PET\ndata from the Alzheimer's Disease Neuroimaging Initiative, XGML builds\nmetabolic brain graphs and uncovers subgraphs predictive of eight AD-related\ncognitive scores in new subjects. XGML shows robust performance, particularly\nfor predicting scores measuring learning, memory, language, praxis, and\norientation, such as CDRSB ($r = 0.74$), ADAS11 ($r = 0.73$), and ADAS13 ($r =\n0.71$). Moreover, XGML unveils key edges jointly but differentially predictive\nof several AD-related outcomes; they may serve as potential network biomarkers\nfor assessing overall cognitive decline. Together, we show the promise of\ngraph-theoretical machine learning in biomarker discovery and disease\nprediction and its potential to improve our understanding of network neural\nmechanisms underlying AD.\n","authors":["Narmina Baghirova","Duy-Thanh Vũ","Duy-Cat Can","Christelle Schneuwly Diaz","Julien Bodlet","Guillaume Blanc","Georgi Hrusanov","Bernard Ries","Oliver Y. Chén"],"pdf_url":"https://arxiv.org/pdf/2503.16286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18688v3","updated":"2025-03-20T16:07:09Z","published":"2024-11-27T19:00:10Z","title":"Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via\n  Inference-Time Alignment","summary":"  With the widespread deployment of Multimodal Large Language Models (MLLMs)\nfor visual-reasoning tasks, improving their safety has become crucial. Recent\nresearch indicates that despite training-time safety alignment, these models\nremain vulnerable to jailbreak attacks. In this work, we first highlight an\nimportant safety gap to describe that alignment achieved solely through safety\ntraining may be insufficient against jailbreak attacks. To address this\nvulnerability, we propose Immune, an inference-time defense framework that\nleverages a safe reward model through controlled decoding to defend against\njailbreak attacks. Additionally, we provide a mathematical characterization of\nImmune, offering insights on why it improves safety against jailbreaks.\nExtensive evaluations on diverse jailbreak benchmarks using recent MLLMs reveal\nthat Immune effectively enhances model safety while preserving the model's\noriginal capabilities. For instance, against text-based jailbreak attacks on\nLLaVA-1.6, Immune reduces the attack success rate by 57.82% and 16.78% compared\nto the base MLLM and state-of-the-art defense strategy, respectively.\n","authors":["Soumya Suvra Ghosal","Souradip Chakraborty","Vaibhav Singh","Tianrui Guan","Mengdi Wang","Ahmad Beirami","Furong Huang","Alvaro Velasquez","Dinesh Manocha","Amrit Singh Bedi"],"pdf_url":"https://arxiv.org/pdf/2411.18688v3.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.16278v1","updated":"2025-03-20T16:07:04Z","published":"2025-03-20T16:07:04Z","title":"Uni-3DAR: Unified 3D Generation and Understanding via Autoregression on\n  Compressed Spatial Tokens","summary":"  Recent advancements in large language models and their multi-modal extensions\nhave demonstrated the effectiveness of unifying generation and understanding\nthrough autoregressive next-token prediction. However, despite the critical\nrole of 3D structural generation and understanding ({3D GU}) in AI for science,\nthese tasks have largely evolved independently, with autoregressive methods\nremaining underexplored. To bridge this gap, we introduce Uni-3DAR, a unified\nframework that seamlessly integrates {3D GU} tasks via autoregressive\nprediction. At its core, Uni-3DAR employs a novel hierarchical tokenization\nthat compresses 3D space using an octree, leveraging the inherent sparsity of\n3D structures. It then applies an additional tokenization for fine-grained\nstructural details, capturing key attributes such as atom types and precise\nspatial coordinates in microscopic 3D structures. We further propose two\noptimizations to enhance efficiency and effectiveness. The first is a two-level\nsubtree compression strategy, which reduces the octree token sequence by up to\n8x. The second is a masked next-token prediction mechanism tailored for\ndynamically varying token positions, significantly boosting model performance.\nBy combining these strategies, Uni-3DAR successfully unifies diverse {3D GU}\ntasks within a single autoregressive framework. Extensive experiments across\nmultiple microscopic {3D GU} tasks, including molecules, proteins, polymers,\nand crystals, validate its effectiveness and versatility. Notably, Uni-3DAR\nsurpasses previous state-of-the-art diffusion models by a substantial margin,\nachieving up to 256\\% relative improvement while delivering inference speeds up\nto 21.8x faster. The code is publicly available at\nhttps://github.com/dptech-corp/Uni-3DAR.\n","authors":["Shuqi Lu","Haowei Lin","Lin Yao","Zhifeng Gao","Xiaohong Ji","Weinan E","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2503.16278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16271v1","updated":"2025-03-20T16:03:39Z","published":"2025-03-20T16:03:39Z","title":"Rethinking Robustness in Machine Learning: A Posterior Agreement\n  Approach","summary":"  The robustness of algorithms against covariate shifts is a fundamental\nproblem with critical implications for the deployment of machine learning\nalgorithms in the real world. Current evaluation methods predominantly match\nthe robustness definition to that of standard generalization, relying on\nstandard metrics like accuracy-based scores, which, while designed for\nperformance assessment, lack a theoretical foundation encompassing their\napplication in estimating robustness to distribution shifts. In this work, we\nset the desiderata for a robustness metric, and we propose a novel principled\nframework for the robustness assessment problem that directly follows the\nPosterior Agreement (PA) theory of model validation. Specifically, we extend\nthe PA framework to the covariate shift setting by proposing a PA metric for\nrobustness evaluation in supervised classification tasks. We assess the\nsoundness of our metric in controlled environments and through an empirical\nrobustness analysis in two different covariate shift scenarios: adversarial\nlearning and domain generalization. We illustrate the suitability of PA by\nevaluating several models under different nature and magnitudes of shift, and\nproportion of affected observations. The results show that the PA metric\nprovides a sensible and consistent analysis of the vulnerabilities in learning\nalgorithms, even in the presence of few perturbed observations.\n","authors":["João Borges S. Carvalho","Alessandro Torcinovich","Victor Jimenez Rodriguez","Antonio E. Cinà","Carlos Cotrini","Lea Schönherr","Joachim M. Buhmann"],"pdf_url":"https://arxiv.org/pdf/2503.16271v1.pdf","comment":"Preprint submitted to TMLR. 29 pages, 13 figures"},{"id":"http://arxiv.org/abs/2305.11311v2","updated":"2025-03-20T15:59:24Z","published":"2023-05-18T21:22:23Z","title":"BELLA: Black box model Explanations by Local Linear Approximations","summary":"  Understanding the decision-making process of black-box models has become not\njust a legal requirement, but also an additional way to assess their\nperformance. However, the state of the art post-hoc explanation approaches for\nregression models rely on synthetic data generation, which introduces\nuncertainty and can hurt the reliability of the explanations. Furthermore, they\ntend to produce explanations that apply to only very few data points. In this\npaper, we present BELLA, a deterministic model-agnostic post-hoc approach for\nexplaining the individual predictions of regression black-box models. BELLA\nprovides explanations in the form of a linear model trained in the feature\nspace. BELLA maximizes the size of the neighborhood to which the linear model\napplies so that the explanations are accurate, simple, general, and robust.\nBELLA can produce both factual and counterfactual explanations.\n","authors":["Nedeljko Radulovic","Albert Bifet","Fabian Suchanek"],"pdf_url":"https://arxiv.org/pdf/2305.11311v2.pdf","comment":"19 pages,3 figures, submitted to TMLR journal"},{"id":"http://arxiv.org/abs/2503.10428v2","updated":"2025-03-20T15:57:34Z","published":"2025-03-13T14:50:33Z","title":"Langevin Monte-Carlo Provably Learns Depth Two Neural Nets at Any Size\n  and Data","summary":"  In this work, we will establish that the Langevin Monte-Carlo algorithm can\nlearn depth-2 neural nets of any size and for any data and we give\nnon-asymptotic convergence rates for it. We achieve this via showing that under\nTotal Variation distance and q-Renyi divergence, the iterates of Langevin Monte\nCarlo converge to the Gibbs distribution of Frobenius norm regularized losses\nfor any of these nets, when using smooth activations and in both classification\nand regression settings. Most critically, the amount of regularization needed\nfor our results is independent of the size of the net. This result combines\nseveral recent observations, like our previous papers showing that two-layer\nneural loss functions can always be regularized by a certain constant amount\nsuch that they satisfy the Villani conditions, and thus their Gibbs measures\nsatisfy a Poincare inequality.\n","authors":["Dibyakanti Kumar","Samyak Jha","Anirbit Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2503.10428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12225v2","updated":"2025-03-20T15:52:29Z","published":"2025-02-17T15:14:10Z","title":"Subjective Logic Encodings","summary":"  Many existing approaches for learning from labeled data assume the existence\nof gold-standard labels. According to these approaches, inter-annotator\ndisagreement is seen as noise to be removed, either through refinement of\nannotation guidelines, label adjudication, or label filtering. However,\nannotator disagreement can rarely be totally eradicated, especially on more\nsubjective tasks such as sentiment analysis or hate speech detection where\ndisagreement is natural. Therefore, a new approach to learning from labeled\ndata, called data perspectivism, seeks to leverage inter-annotator disagreement\nto learn models that stay true to the inherent uncertainty of the task by\ntreating annotations as opinions of the annotators, rather than gold-standard\nfacts. Despite this conceptual grounding, existing methods under data\nperspectivism are limited to using disagreement as the sole source of\nannotation uncertainty. To expand the possibilities of data perspectivism, we\nintroduce Subjective Logic Encodings (SLEs), a flexible framework for\nconstructing classification targets that explicitly encodes annotations as\nopinions of the annotators. Based on Subjective Logic Theory, SLEs encode\nlabels as Dirichlet distributions and provide principled methods for encoding\nand aggregating various types of annotation uncertainty -- annotator\nconfidence, reliability, and disagreement -- into the targets. We show that\nSLEs are a generalization of other types of label encodings as well as how to\nestimate models to predict SLEs using a distribution matching objective.\n","authors":["Jake Vasilakes","Chrysoula Zerva","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2502.12225v2.pdf","comment":"We make our code publicly available at\n  https://github.com/jvasilakes/SLEncodings"},{"id":"http://arxiv.org/abs/2502.02393v2","updated":"2025-03-20T15:52:20Z","published":"2025-02-04T15:14:01Z","title":"Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention\n  Transformers","summary":"  Chain-of-thought reasoning and scratchpads have emerged as critical tools for\nenhancing the computational capabilities of transformers. While theoretical\nresults show that polynomial-length scratchpads can extend transformers'\nexpressivity from $TC^0$ to $PTIME$, their required length remains poorly\nunderstood. Empirical evidence even suggests that transformers need scratchpads\neven for many problems in $TC^0$, such as Parity or Multiplication, challenging\noptimistic bounds derived from circuit complexity. In this work, we initiate\nthe study of systematic lower bounds for the number of CoT steps across\ndifferent algorithmic problems, in the hard-attention regime. We study a\nvariety of algorithmic problems, and provide bounds that are tight up to\nlogarithmic factors. Overall, these results contribute to emerging\nunderstanding of the power and limitations of chain-of-thought reasoning.\n","authors":["Alireza Amiri","Xinting Huang","Mark Rofin","Michael Hahn"],"pdf_url":"https://arxiv.org/pdf/2502.02393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20081v3","updated":"2025-03-20T15:51:46Z","published":"2024-10-26T05:18:48Z","title":"emg2qwerty: A Large Dataset with Baselines for Touch Typing using\n  Surface Electromyography","summary":"  Surface electromyography (sEMG) non-invasively measures signals generated by\nmuscle activity with sufficient sensitivity to detect individual spinal neurons\nand richness to identify dozens of gestures and their nuances. Wearable\nwrist-based sEMG sensors have the potential to offer low friction, subtle,\ninformation rich, always available human-computer inputs. To this end, we\nintroduce emg2qwerty, a large-scale dataset of non-invasive electromyographic\nsignals recorded at the wrists while touch typing on a QWERTY keyboard,\ntogether with ground-truth annotations and reproducible baselines. With 1,135\nsessions spanning 108 users and 346 hours of recording, this is the largest\nsuch public dataset to date. These data demonstrate non-trivial, but well\ndefined hierarchical relationships both in terms of the generative process,\nfrom neurons to muscles and muscle combinations, as well as in terms of domain\nshift across users and user sessions. Applying standard modeling techniques\nfrom the closely related field of Automatic Speech Recognition (ASR), we show\nstrong baseline performance on predicting key-presses using sEMG signals alone.\nWe believe the richness of this task and dataset will facilitate progress in\nseveral problems of interest to both the machine learning and neuroscientific\ncommunities. Dataset and code can be accessed at\nhttps://github.com/facebookresearch/emg2qwerty.\n","authors":["Viswanath Sivakumar","Jeffrey Seely","Alan Du","Sean R Bittner","Adam Berenzweig","Anuoluwapo Bolarinwa","Alexandre Gramfort","Michael I Mandel"],"pdf_url":"https://arxiv.org/pdf/2410.20081v3.pdf","comment":"Published at NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2503.16251v1","updated":"2025-03-20T15:46:03Z","published":"2025-03-20T15:46:03Z","title":"RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning\n  by Balancing Privacy, Fairness and Utility in Autonomous Vehicles","summary":"  Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches.\n","authors":["Dawood Wasif","Terrence J. Moore","Jin-Hee Cho"],"pdf_url":"https://arxiv.org/pdf/2503.16251v1.pdf","comment":"Submitted to PETS 2025 (under review)"},{"id":"http://arxiv.org/abs/2503.16247v1","updated":"2025-03-20T15:43:14Z","published":"2025-03-20T15:43:14Z","title":"OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution\n  Detection","summary":"  The growing reliance on Artificial Intelligence (AI) in critical domains such\nas healthcare demands robust mechanisms to ensure the trustworthiness of these\nsystems, especially when faced with unexpected or anomalous inputs. This paper\nintroduces the Open Medical Imaging Benchmarks for Out-Of-Distribution\nDetection (OpenMIBOOD), a comprehensive framework for evaluating\nout-of-distribution (OOD) detection methods specifically in medical imaging\ncontexts. OpenMIBOOD includes three benchmarks from diverse medical domains,\nencompassing 14 datasets divided into covariate-shifted in-distribution,\nnear-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these\nbenchmarks, providing a standardized reference to advance the development and\nfair comparison of OOD detection methods. Results reveal that findings from\nbroad-scale OOD benchmarks in natural image domains do not translate to medical\napplications, underscoring the critical need for such benchmarks in the medical\nfield. By mitigating the risk of exposing AI models to inputs outside their\ntraining distribution, OpenMIBOOD aims to support the advancement of reliable\nand trustworthy AI systems in healthcare. The repository is available at\nhttps://github.com/remic-othr/OpenMIBOOD.\n","authors":["Max Gutbrod","David Rauber","Danilo Weber Nunes","Christoph Palm"],"pdf_url":"https://arxiv.org/pdf/2503.16247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16240v1","updated":"2025-03-20T15:37:39Z","published":"2025-03-20T15:37:39Z","title":"Machine learning identifies nullclines in oscillatory dynamical systems","summary":"  We introduce CLINE (Computational Learning and Identification of Nullclines),\na neural network-based method that uncovers the hidden structure of nullclines\nfrom oscillatory time series data. Unlike traditional approaches aiming at\ndirect prediction of system dynamics, CLINE identifies static geometric\nfeatures of the phase space that encode the (non)linear relationships between\nstate variables. It overcomes challenges such as multiple time scales and\nstrong nonlinearities while producing interpretable results convertible into\nsymbolic differential equations. We validate CLINE on various oscillatory\nsystems, showcasing its effectiveness.\n","authors":["Bartosz Prokop","Jimmy Billen","Nikita Frolov","Lendert Gelens"],"pdf_url":"https://arxiv.org/pdf/2503.16240v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.19482v3","updated":"2025-03-20T15:35:56Z","published":"2024-10-25T11:37:04Z","title":"Measuring memorization in language models via probabilistic extraction","summary":"  Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.\n","authors":["Jamie Hayes","Marika Swanberg","Harsh Chaudhari","Itay Yona","Ilia Shumailov","Milad Nasr","Christopher A. Choquette-Choo","Katherine Lee","A. Feder Cooper"],"pdf_url":"https://arxiv.org/pdf/2410.19482v3.pdf","comment":"NAACL 25"},{"id":"http://arxiv.org/abs/2410.07066v3","updated":"2025-03-20T15:32:05Z","published":"2024-10-09T17:11:22Z","title":"A Gentle Introduction and Tutorial on Deep Generative Models in\n  Transportation Research","summary":"  Deep Generative Models (DGMs) have rapidly advanced in recent years, becoming\nessential tools in various fields due to their ability to learn complex data\ndistributions and generate synthetic data. Their importance in transportation\nresearch is increasingly recognized, particularly for applications like traffic\ndata generation, prediction, and feature extraction. This paper offers a\ncomprehensive introduction and tutorial on DGMs, with a focus on their\napplications in transportation. It begins with an overview of generative\nmodels, followed by detailed explanations of fundamental models, a systematic\nreview of the literature, and practical tutorial code to aid implementation.\nThe paper also discusses current challenges and opportunities, highlighting how\nthese models can be effectively utilized and further developed in\ntransportation research. This paper serves as a valuable reference, guiding\nresearchers and practitioners from foundational knowledge to advanced\napplications of DGMs in transportation research.\n","authors":["Seongjin Choi","Zhixiong Jin","Seung Woo Ham","Jiwon Kim","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2410.07066v3.pdf","comment":"64 pages, 21 figures, 4 tables"},{"id":"http://arxiv.org/abs/2503.07645v2","updated":"2025-03-20T15:31:27Z","published":"2025-03-06T04:47:37Z","title":"BicliqueEncoder: An Efficient Method for Link Prediction in Bipartite\n  Networks using Formal Concept Analysis and Transformer Encoder","summary":"  We propose a novel and efficient method for link prediction in bipartite\nnetworks, using \\textit{formal concept analysis} (FCA) and the Transformer\nencoder. Link prediction in bipartite networks finds practical applications in\nvarious domains such as product recommendation in online sales, and prediction\nof chemical-disease interaction in medical science. Since for link prediction,\nthe topological structure of a network contains valuable information, many\napproaches focus on extracting structural features and then utilizing them for\nlink prediction. Bi-cliques, as a type of structural feature of bipartite\ngraphs, can be utilized for link prediction. Although several link prediction\nmethods utilizing bi-cliques have been proposed and perform well in rather\nsmall datasets, all of them face challenges with scalability when dealing with\nlarge datasets since they demand substantial computational resources. This\nlimits the practical utility of these approaches in real-world applications. To\novercome the limitation, we introduce a novel approach employing iceberg\nconcept lattices and the Transformer encoder. Our method requires fewer\ncomputational resources, making it suitable for large-scale datasets while\nmaintaining high prediction performance. We conduct experiments on five large\nreal-world datasets that exceed the capacity of previous bi-clique-based\napproaches to demonstrate the efficacy of our method. Additionally, we perform\nsupplementary experiments on five small datasets to compare with the previous\nbi-clique-based methods for bipartite link prediction and demonstrate that our\nmethod is more efficient than the previous ones.\n","authors":["Hongyuan Yang","Siqi Peng","Akihiro Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2503.07645v2.pdf","comment":"33 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.16233v1","updated":"2025-03-20T15:31:01Z","published":"2025-03-20T15:31:01Z","title":"Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated\n  Learning: A Step Towards Responsible AI","summary":"  Federated Learning (FL) enables collaborative machine learning while\npreserving data privacy but struggles to balance privacy preservation (PP) and\nfairness. Techniques like Differential Privacy (DP), Homomorphic Encryption\n(HE), and Secure Multi-Party Computation (SMC) protect sensitive data but\nintroduce trade-offs. DP enhances privacy but can disproportionately impact\nunderrepresented groups, while HE and SMC mitigate fairness concerns at the\ncost of computational overhead. This work explores the privacy-fairness\ntrade-offs in FL under IID (Independent and Identically Distributed) and\nnon-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse\ndatasets. Our findings highlight context-dependent trade-offs and offer\nguidelines for designing FL systems that uphold responsible AI principles,\nensuring fairness, privacy, and equitable real-world applications.\n","authors":["Dawood Wasif","Dian Chen","Sindhuja Madabushi","Nithin Alluru","Terrence J. Moore","Jin-Hee Cho"],"pdf_url":"https://arxiv.org/pdf/2503.16233v1.pdf","comment":"Submitted to IJCAI 2025 (under review)"},{"id":"http://arxiv.org/abs/2409.20089v2","updated":"2025-03-20T15:28:18Z","published":"2024-09-30T08:41:39Z","title":"Robust LLM safeguarding via refusal feature adversarial training","summary":"  Large language models (LLMs) are vulnerable to adversarial attacks that can\nelicit harmful responses. Defending against such attacks remains challenging\ndue to the opacity of jailbreaking mechanisms and the high computational cost\nof training LLMs robustly. We demonstrate that adversarial attacks share a\nuniversal mechanism for circumventing LLM safeguards that works by ablating a\ndimension in the residual stream embedding space called the refusal feature. We\nfurther show that the operation of refusal feature ablation (RFA) approximates\nthe worst-case perturbation of offsetting model safety. Based on these\nfindings, we propose Refusal Feature Adversarial Training (ReFAT), a novel\nalgorithm that efficiently performs LLM adversarial training by simulating the\neffect of input-level attacks via RFA. Experiment results show that ReFAT\nsignificantly improves the robustness of three popular LLMs against a wide\nrange of adversarial attacks, with considerably less computational overhead\ncompared to existing adversarial training methods.\n","authors":["Lei Yu","Virginie Do","Karen Hambardzumyan","Nicola Cancedda"],"pdf_url":"https://arxiv.org/pdf/2409.20089v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16227v1","updated":"2025-03-20T15:22:39Z","published":"2025-03-20T15:22:39Z","title":"Flight Testing an Optionally Piloted Aircraft: a Case Study on Trust\n  Dynamics in Human-Autonomy Teaming","summary":"  This paper examines how trust is formed, maintained, or diminished over time\nin the context of human-autonomy teaming with an optionally piloted aircraft.\nWhereas traditional factor-based trust models offer a static representation of\nhuman confidence in technology, here we discuss how variations in the\nunderlying factors lead to variations in trust, trust thresholds, and human\nbehaviours. Over 200 hours of flight test data collected over a multi-year test\ncampaign from 2021 to 2023 were reviewed. The\ndispositional-situational-learned, process-performance-purpose, and IMPACTS\nhomeostasis trust models are applied to illuminate trust trends during nominal\nautonomous flight operations. The results offer promising directions for future\nstudies on trust dynamics and design-for-trust in human-autonomy teaming.\n","authors":["Jeremy C. -H. Wang","Ming Hou","David Dunwoody","Marko Ilievski","Justin Tomasi","Edward Chao","Carl Pigeon"],"pdf_url":"https://arxiv.org/pdf/2503.16227v1.pdf","comment":"IEEE International Conference on Human-Machine Systems 2025,\n  keywords: trust, human factors, aviation, safety-critical, human-autonomy\n  teaming"},{"id":"http://arxiv.org/abs/2502.12756v3","updated":"2025-03-20T15:18:36Z","published":"2025-02-18T11:18:17Z","title":"Navigating Demand Uncertainty in Container Shipping: Deep Reinforcement\n  Learning for Enabling Adaptive and Feasible Master Stowage Planning","summary":"  Reinforcement learning (RL) has shown promise in solving various\ncombinatorial optimization problems. However, conventional RL faces challenges\nwhen dealing with real-world constraints, especially when action space\nfeasibility is explicit and dependent on the corresponding state or trajectory.\nIn this work, we focus on using RL in container shipping, often considered the\ncornerstone of global trade, by dealing with the critical challenge of master\nstowage planning. The main objective is to maximize cargo revenue and minimize\noperational costs while navigating demand uncertainty and various complex\noperational constraints, namely vessel capacity and stability, which must be\ndynamically updated along the vessel's voyage. To address this problem, we\nimplement a deep reinforcement learning framework with feasibility projection\nto solve the master stowage planning problem (MPP) under demand uncertainty.\nThe experimental results show that our architecture efficiently finds adaptive,\nfeasible solutions for this multi-stage stochastic optimization problem,\noutperforming traditional mixed-integer programming and RL with feasibility\nregularization. Our AI-driven decision-support policy enables adaptive and\nfeasible planning under uncertainty, optimizing operational efficiency and\ncapacity utilization while contributing to sustainable and resilient global\nsupply chains.\n","authors":["Jaike van Twiller","Yossiri Adulyasak","Erick Delage","Djordje Grbic","Rune Møller Jensen"],"pdf_url":"https://arxiv.org/pdf/2502.12756v3.pdf","comment":"This paper is currently under review for IJCAI 2025"},{"id":"http://arxiv.org/abs/2503.16219v1","updated":"2025-03-20T15:13:23Z","published":"2025-03-20T15:13:23Z","title":"Reinforcement Learning for Reasoning in Small LLMs: What Works and What\n  Doesn't","summary":"  Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs.\n","authors":["Quy-Anh Dang","Chris Ngo"],"pdf_url":"https://arxiv.org/pdf/2503.16219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16207v1","updated":"2025-03-20T14:54:19Z","published":"2025-03-20T14:54:19Z","title":"Neural Variable-Order Fractional Differential Equation Networks","summary":"  Neural differential equation models have garnered significant attention in\nrecent years for their effectiveness in machine learning applications.Among\nthese, fractional differential equations (FDEs) have emerged as a promising\ntool due to their ability to capture memory-dependent dynamics, which are often\nchallenging to model with traditional integer-order approaches.While existing\nmodels have primarily focused on constant-order fractional derivatives,\nvariable-order fractional operators offer a more flexible and expressive\nframework for modeling complex memory patterns. In this work, we introduce the\nNeural Variable-Order Fractional Differential Equation network (NvoFDE), a\nnovel neural network framework that integrates variable-order fractional\nderivatives with learnable neural networks.Our framework allows for the\nmodeling of adaptive derivative orders dependent on hidden features, capturing\nmore complex feature-updating dynamics and providing enhanced flexibility. We\nconduct extensive experiments across multiple graph datasets to validate the\neffectiveness of our approach.Our results demonstrate that NvoFDE outperforms\ntraditional constant-order fractional and integer models across a range of\ntasks, showcasing its superior adaptability and performance.\n","authors":["Wenjun Cui","Qiyu Kang","Xuhao Li","Kai Zhao","Wee Peng Tay","Weihua Deng","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2503.16207v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2503.16206v1","updated":"2025-03-20T14:51:04Z","published":"2025-03-20T14:51:04Z","title":"Interpretable Neural Causal Models with TRAM-DAGs","summary":"  The ultimate goal of most scientific studies is to understand the underlying\ncausal mechanism between the involved variables. Structural causal models\n(SCMs) are widely used to represent such causal mechanisms. Given an SCM,\ncausal queries on all three levels of Pearl's causal hierarchy can be answered:\n$L_1$ observational, $L_2$ interventional, and $L_3$ counterfactual. An\nessential aspect of modeling the SCM is to model the dependency of each\nvariable on its causal parents. Traditionally this is done by parametric\nstatistical models, such as linear or logistic regression models. This allows\nto handle all kinds of data types and fit interpretable models but bears the\nrisk of introducing a bias. More recently neural causal models came up using\nneural networks (NNs) to model the causal relationships, allowing the\nestimation of nearly any underlying functional form without bias. However,\ncurrent neural causal models are generally restricted to continuous variables\nand do not yield an interpretable form of the causal relationships.\nTransformation models range from simple statistical regressions to complex\nnetworks and can handle continuous, ordinal, and binary data. Here, we propose\nto use TRAMs to model the functional relationships in SCMs allowing us to\nbridge the gap between interpretability and flexibility in causal modeling. We\ncall this method TRAM-DAG and assume currently that the underlying directed\nacyclic graph is known. For the fully observed case, we benchmark TRAM-DAGs\nagainst state-of-the-art statistical and NN-based causal models. We show that\nTRAM-DAGs are interpretable but also achieve equal or superior performance in\nqueries ranging from $L_1$ to $L_3$ in the causal hierarchy. For the continuous\ncase, TRAM-DAGs allow for counterfactual queries for three common causal\nstructures, including unobserved confounding.\n","authors":["Beate Sick","Oliver Dürr"],"pdf_url":"https://arxiv.org/pdf/2503.16206v1.pdf","comment":"Accepted at the CLeaR 2025 Conference"},{"id":"http://arxiv.org/abs/2411.19146v4","updated":"2025-03-20T14:50:04Z","published":"2024-11-28T13:45:42Z","title":"Puzzle: Distillation-Based NAS for Inference-Optimized LLMs","summary":"  Large language models (LLMs) offer remarkable capabilities, yet their high\ninference costs restrict wider adoption. While increasing parameter counts\nimproves accuracy, it also broadens the gap between state-of-the-art\ncapabilities and practical deployability. We present Puzzle, a hardware-aware\nframework that accelerates the inference of LLMs while preserving their\ncapabilities. Using neural architecture search (NAS) at a large-scale, Puzzle\noptimizes models with tens of billions of parameters. Our approach utilizes\nblockwise local knowledge distillation (BLD) for parallel architecture\nexploration and employs mixed-integer programming for precise constraint\noptimization.\n  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct\n(Nemotron-51B), a publicly available model derived from Llama-3.1-70B-Instruct.\nNemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single\nNVIDIA H100 GPU while retaining 98.4% of the original model's benchmark\naccuracies. Notably, it is the most accurate model supporting single H100 GPU\ninference with large batch sizes, despite training on only 45B tokens, far\nfewer than the 15T used to train Llama-70B. Lastly, we derive\nLlama-3.3-Nemotron-49B-Super-Base to demonstrate Puzzle can retain long-context\nand that lightweight alignment on these derived models allows them to surpass\nthe parent model in specific capabilities. Our work establishes that powerful\nLLM models can be optimized for efficient deployment with only negligible loss\nin quality, underscoring that inference performance, not parameter count alone,\nshould guide model selection.\n","authors":["Akhiad Bercovich","Tomer Ronen","Talor Abramovich","Nir Ailon","Nave Assaf","Mohammad Dabbah","Ido Galil","Amnon Geifman","Yonatan Geifman","Izhak Golan","Netanel Haber","Ehud Karpas","Roi Koren","Itay Levy","Pavlo Molchanov","Shahar Mor","Zach Moshe","Najeeb Nabwani","Omri Puny","Ran Rubin","Itamar Schen","Ido Shahaf","Oren Tropp","Omer Ullman Argov","Ran Zilberstein","Ran El-Yaniv"],"pdf_url":"https://arxiv.org/pdf/2411.19146v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16199v1","updated":"2025-03-20T14:45:55Z","published":"2025-03-20T14:45:55Z","title":"Deferring Concept Bottleneck Models: Learning to Defer Interventions to\n  Inaccurate Experts","summary":"  Concept Bottleneck Models (CBMs) are machine learning models that improve\ninterpretability by grounding their predictions on human-understandable\nconcepts, allowing for targeted interventions in their decision-making process.\nHowever, when intervened on, CBMs assume the availability of humans that can\nidentify the need to intervene and always provide correct interventions. Both\nassumptions are unrealistic and impractical, considering labor costs and human\nerror-proneness. In contrast, Learning to Defer (L2D) extends supervised\nlearning by allowing machine learning models to identify cases where a human is\nmore likely to be correct than the model, thus leading to deferring systems\nwith improved performance. In this work, we gain inspiration from L2D and\npropose Deferring CBMs (DCBMs), a novel framework that allows CBMs to learn\nwhen an intervention is needed. To this end, we model DCBMs as a composition of\ndeferring systems and derive a consistent L2D loss to train them. Moreover, by\nrelying on a CBM architecture, DCBMs can explain why defer occurs on the final\ntask. Our results show that DCBMs achieve high predictive performance and\ninterpretability at the cost of deferring more to humans.\n","authors":["Andrea Pugnana","Riccardo Massidda","Francesco Giannini","Pietro Barbiero","Mateo Espinosa Zarlenga","Roberto Pellungrini","Gabriele Dominici","Fosca Giannotti","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2503.16199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16195v1","updated":"2025-03-20T14:42:11Z","published":"2025-03-20T14:42:11Z","title":"VP-NTK: Exploring the Benefits of Visual Prompting in Differentially\n  Private Data Synthesis","summary":"  Differentially private (DP) synthetic data has become the de facto standard\nfor releasing sensitive data. However, many DP generative models suffer from\nthe low utility of synthetic data, especially for high-resolution images. On\nthe other hand, one of the emerging techniques in parameter efficient\nfine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing\nmodels to be reused for the purpose of adapting to subsequent downstream tasks.\nIn this work, we explore such a phenomenon in constructing captivating\ngenerative models with DP constraints. We show that VP in conjunction with\nDP-NTK, a DP generator that exploits the power of the neural tangent kernel\n(NTK) in training DP generative models, achieves a significant performance\nboost, particularly for high-resolution image datasets, with accuracy improving\nfrom 0.644$\\pm$0.044 to 0.769. Lastly, we perform ablation studies on the\neffect of different parameters that influence the overall performance of\nVP-NTK. Our work demonstrates a promising step forward in improving the utility\nof DP synthetic data, particularly for high-resolution images.\n","authors":["Chia-Yi Hsu","Jia-You Chen","Yu-Lin Tsai","Chih-Hsun Lin","Pin-Yu Chen","Chia-Mu Yu","Chun-Ying Huang"],"pdf_url":"https://arxiv.org/pdf/2503.16195v1.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2503.16192v1","updated":"2025-03-20T14:39:21Z","published":"2025-03-20T14:39:21Z","title":"Nonparametric Bellman Mappings for Value Iteration in Distributed\n  Reinforcement Learning","summary":"  This paper introduces novel Bellman mappings (B-Maps) for value iteration\n(VI) in distributed reinforcement learning (DRL), where multiple agents operate\nover a network without a centralized fusion node. Each agent constructs its own\nnonparametric B-Map for VI while communicating only with direct neighbors to\nachieve consensus. These B-Maps operate on Q-functions represented in a\nreproducing kernel Hilbert space, enabling a nonparametric formulation that\nallows for flexible, agent-specific basis function design. Unlike existing DRL\nmethods that restrict information exchange to Q-function estimates, the\nproposed framework also enables agents to share basis information in the form\nof covariance matrices, capturing additional structural details. A theoretical\nanalysis establishes linear convergence rates for both Q-function and\ncovariance-matrix estimates toward their consensus values. The optimal learning\nrates for consensus-based updates are dictated by the ratio of the smallest\npositive eigenvalue to the largest one of the network's Laplacian matrix.\nFurthermore, each nodal Q-function estimate is shown to lie very close to the\nfixed point of a centralized nonparametric B-Map, effectively allowing the\nproposed DRL design to approximate the performance of a centralized fusion\ncenter. Numerical experiments on two well-known control problems demonstrate\nthe superior performance of the proposed nonparametric B-Maps compared to prior\nmethods. Notably, the results reveal a counter-intuitive finding: although the\nproposed approach involves greater information exchange -- specifically through\nthe sharing of covariance matrices -- it achieves the desired performance with\nlower cumulative communication cost than existing DRL schemes, highlighting the\ncrucial role of basis information in accelerating the learning process.\n","authors":["Yuki Akiyama","Konstantinos Slavakis"],"pdf_url":"https://arxiv.org/pdf/2503.16192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16191v1","updated":"2025-03-20T14:39:11Z","published":"2025-03-20T14:39:11Z","title":"Large Language Models for Water Distribution Systems Modeling and\n  Decision-Making","summary":"  The design, operations, and management of water distribution systems (WDS)\ninvolve complex mathematical models. These models are continually improving due\nto computational advancements, leading to better decision-making and more\nefficient WDS management. However, the significant time and effort required for\nmodeling, programming, and analyzing results remain substantial challenges.\nAnother issue is the professional burden, which confines the interaction with\nmodels, databases, and other sophisticated tools to a small group of experts,\nthereby causing non-technical stakeholders to depend on these experts or make\ndecisions without modeling support. Furthermore, explaining model results is\nchallenging even for experts, as it is often unclear which conditions cause the\nmodel to reach a certain state or recommend a specific policy. The recent\nadvancements in Large Language Models (LLMs) open doors for a new stage in\nhuman-model interaction. This study proposes a framework of plain language\ninteractions with hydraulic and water quality models based on LLM-EPANET\narchitecture. This framework is tested with increasing levels of complexity of\nqueries to study the ability of LLMs to interact with WDS models, run complex\nsimulations, and report simulation results. The performance of the proposed\nframework is evaluated across several categories of queries and hyper-parameter\nconfigurations, demonstrating its potential to enhance decision-making\nprocesses in WDS management.\n","authors":["Yinon Goldshtein","Gal Perelman","Assaf Schuster","Avi Ostfeld"],"pdf_url":"https://arxiv.org/pdf/2503.16191v1.pdf","comment":"Accepted to EWRI Congress 2025"},{"id":"http://arxiv.org/abs/2503.16187v1","updated":"2025-03-20T14:37:40Z","published":"2025-03-20T14:37:40Z","title":"Manifold learning in metric spaces","summary":"  Laplacian-based methods are popular for dimensionality reduction of data\nlying in $\\mathbb{R}^N$. Several theoretical results for these algorithms\ndepend on the fact that the Euclidean distance approximates the geodesic\ndistance on the underlying submanifold which the data are assumed to lie on.\nHowever, for some applications, other metrics, such as the Wasserstein\ndistance, may provide a more appropriate notion of distance than the Euclidean\ndistance. We provide a framework that generalizes the problem of manifold\nlearning to metric spaces and study when a metric satisfies sufficient\nconditions for the pointwise convergence of the graph Laplacian.\n","authors":["Liane Xu","Amit Singer"],"pdf_url":"https://arxiv.org/pdf/2503.16187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16183v1","updated":"2025-03-20T14:34:03Z","published":"2025-03-20T14:34:03Z","title":"Variance-Aware Noisy Training: Hardening DNNs against Unstable Analog\n  Computations","summary":"  The disparity between the computational demands of deep learning and the\ncapabilities of compute hardware is expanding drastically. Although deep\nlearning achieves remarkable performance in countless tasks, its escalating\nrequirements for computational power and energy consumption surpass the\nsustainable limits of even specialized neural processing units, including the\nApple Neural Engine and NVIDIA TensorCores. This challenge is intensified by\nthe slowdown in CMOS scaling.\n  Analog computing presents a promising alternative, offering substantial\nimprovements in energy efficiency by directly manipulating physical quantities\nsuch as current, voltage, charge, or photons. However, it is inherently\nvulnerable to manufacturing variations, nonlinearities, and noise, leading to\ndegraded prediction accuracy. One of the most effective techniques for\nenhancing robustness, Noisy Training, introduces noise during the training\nphase to reinforce the model against disturbances encountered during inference.\nAlthough highly effective, its performance degrades in real-world environments\nwhere noise characteristics fluctuate due to external factors such as\ntemperature variations and temporal drift.\n  This study underscores the necessity of Noisy Training while revealing its\nfundamental limitations in the presence of dynamic noise. To address these\nchallenges, we propose Variance-Aware Noisy Training, a novel approach that\nmitigates performance degradation by incorporating noise schedules which\nemulate the evolving noise conditions encountered during inference. Our method\nsubstantially improves model robustness, without training overhead. We\ndemonstrate a significant increase in robustness, from 72.3\\% with conventional\nNoisy Training to 97.3\\% with Variance-Aware Noisy Training on CIFAR-10 and\nfrom 38.5\\% to 89.9\\% on Tiny ImageNet.\n","authors":["Xiao Wang","Hendrik Borras","Bernhard Klein","Holger Fröning"],"pdf_url":"https://arxiv.org/pdf/2503.16183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10361v5","updated":"2025-03-20T14:27:22Z","published":"2023-05-17T16:38:11Z","title":"Human Choice Prediction in Language-based Persuasion Games:\n  Simulation-based Off-Policy Evaluation","summary":"  Recent advances in Large Language Models (LLMs) have spurred interest in\ndesigning LLM-based agents for tasks that involve interaction with human and\nartificial agents. This paper addresses a key aspect in the design of such\nagents: predicting human decisions in off-policy evaluation (OPE). We focus on\nlanguage-based persuasion games, where an expert aims to influence the\ndecision-maker through verbal messages. In our OPE framework, the prediction\nmodel is trained on human interaction data collected from encounters with one\nset of expert agents, and its performance is evaluated on interactions with a\ndifferent set of experts. Using a dedicated application, we collected a dataset\nof 87K decisions from humans playing a repeated decision-making game with\nartificial agents. To enhance off-policy performance, we propose a simulation\ntechnique involving interactions across the entire agent space and simulated\ndecision-makers. Our learning strategy yields significant OPE gains, e.g.,\nimproving prediction accuracy in the top 15% challenging cases by 7.1%. Our\ncode and the large dataset we collected and generated are submitted as\nsupplementary material and publicly available in our GitHub repository:\nhttps://github.com/eilamshapira/HumanChoicePrediction\n","authors":["Eilam Shapira","Omer Madmon","Reut Apel","Moshe Tennenholtz","Roi Reichart"],"pdf_url":"https://arxiv.org/pdf/2305.10361v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16179v1","updated":"2025-03-20T14:24:01Z","published":"2025-03-20T14:24:01Z","title":"Narrowing Class-Wise Robustness Gaps in Adversarial Training","summary":"  Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training.\n","authors":["Fatemeh Amerehi","Patrick Healy"],"pdf_url":"https://arxiv.org/pdf/2503.16179v1.pdf","comment":"4 figures, ICLR 2025 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2312.00267v3","updated":"2025-03-20T14:23:17Z","published":"2023-12-01T00:54:02Z","title":"Sample Efficient Preference Alignment in LLMs via Active Exploration","summary":"  Preference-based feedback is important for many applications in machine\nlearning where evaluation of a reward function is not feasible. Notable recent\nexamples arise in preference alignment for large language models, including in\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO). For many applications of preference alignment, the cost of\nacquiring human feedback can be substantial. In this work, we take advantage of\nthe fact that one can often choose contexts at which to obtain human feedback\nto most efficiently identify a good policy, and formalize the setting as an\nactive contextual dueling bandit problem. We propose an active exploration\nalgorithm to efficiently select the data and provide theoretical proof that it\nhas a polynomial worst-case regret bound. We extend the setting and methodology\nfor practical use in preference alignment of large language models. We provide\ntwo extensions, an online and an offline approach. Our method outperforms the\nbaselines with limited samples of human preferences on several language models\nand four real-world datasets including two new datasets that we contribute to\nthe literature.\n","authors":["Viraj Mehta","Syrine Belakaria","Vikramjeet Das","Ojash Neopane","Yijia Dai","Ilija Bogunovic","Barbara Engelhardt","Stefano Ermon","Jeff Schneider","Willie Neiswanger"],"pdf_url":"https://arxiv.org/pdf/2312.00267v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15451v2","updated":"2025-03-20T14:10:27Z","published":"2025-02-21T13:25:00Z","title":"Binary-Integer-Programming Based Algorithm for Expert Load Balancing in\n  Mixture-of-Experts Models","summary":"  For pre-training of MoE (Mixture-of-Experts) models, one of the main issues\nis unbalanced expert loads, which may cause routing collapse or increased\ncomputational overhead. Existing methods contain the Loss-Controlled method and\nthe Loss-Free method, where both the unbalanced degrees at first several\ntraining steps are still high and decrease slowly. In this work, we propose\nBIP-Based Balancing, an expert load balancing algorithm based on binary integer\nprogramming (BIP). The algorithm maintains an additional vector q on each MoE\nlayer that can help change the top-K order of s by solving a binary integer\nprogramming with very small time costs. We implement the algorithm on two MoE\nlanguage models: 16-expert (0.3B) and 64-expert (1.1B). The experimental\nresults show that on both models comparing with the Loss-Controlled method and\nthe Loss-Free method, our algorithm trains models with the lowest perplexities,\nwhile saves at least 13% of pre-training time compared with the Loss-Controlled\nmethod. Within our current knowledge, this is the first routing algorithm that\nachieves maintaining load balance status on every expert in every MoE layer\nfrom the first step to the last step during the whole pre-training process,\nwhile the trained MoE models also perform well. The code material of this work\nis available at https://github.com/sunyuanLLM/bip_routing_algorithm.\n","authors":["Yuan Sun"],"pdf_url":"https://arxiv.org/pdf/2502.15451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13570v2","updated":"2025-03-20T14:09:08Z","published":"2025-02-19T09:22:48Z","title":"An Efficient Permutation-Based Kernel Two-Sample Test","summary":"  Two-sample hypothesis testing-determining whether two sets of data are drawn\nfrom the same distribution-is a fundamental problem in statistics and machine\nlearning with broad scientific applications. In the context of nonparametric\ntesting, maximum mean discrepancy (MMD) has gained popularity as a test\nstatistic due to its flexibility and strong theoretical foundations. However,\nits use in large-scale scenarios is plagued by high computational costs. In\nthis work, we use a Nystr\\\"om approximation of the MMD to design a\ncomputationally efficient and practical testing algorithm while preserving\nstatistical guarantees. Our main result is a finite-sample bound on the power\nof the proposed test for distributions that are sufficiently separated with\nrespect to the MMD. The derived separation rate matches the known minimax\noptimal rate in this setting. We support our findings with a series of\nnumerical experiments, emphasizing realistic scientific data.\n","authors":["Antoine Chatalic","Marco Letizia","Nicolas Schreuder","Lorenzo Rosasco"],"pdf_url":"https://arxiv.org/pdf/2502.13570v2.pdf","comment":"23 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.06825v2","updated":"2025-03-20T14:07:59Z","published":"2025-02-05T11:26:32Z","title":"RLOMM: An Efficient and Robust Online Map Matching Framework with\n  Reinforcement Learning","summary":"  Online map matching is a fundamental problem in location-based services,\naiming to incrementally match trajectory data step-by-step onto a road network.\nHowever, existing methods fail to meet the needs for efficiency, robustness,\nand accuracy required by large-scale online applications, making this task\nstill challenging. This paper introduces a novel framework that achieves high\naccuracy and efficient matching while ensuring robustness in handling diverse\nscenarios. To improve efficiency, we begin by modeling the online map matching\nproblem as an Online Markov Decision Process (OMDP) based on its inherent\ncharacteristics. This approach helps efficiently merge historical and real-time\ndata, reducing unnecessary calculations. Next, to enhance robustness, we design\na reinforcement learning method, enabling robust handling of real-time data\nfrom dynamically changing environments. In particular, we propose a novel model\nlearning process and a comprehensive reward function, allowing the model to\nmake reasonable current matches from a future-oriented perspective, and to\ncontinuously update and optimize during the decision-making process based on\nfeedback. Lastly, to address the heterogeneity between trajectories and roads,\nwe design distinct graph structures, facilitating efficient representation\nlearning through graph and recurrent neural networks. To further align\ntrajectory and road data, we introduce contrastive learning to decrease their\ndistance in the latent space, thereby promoting effective integration of the\ntwo. Extensive evaluations on three real-world datasets confirm that our method\nsignificantly outperforms existing state-of-the-art solutions in terms of\naccuracy, efficiency and robustness.\n","authors":["Minxiao Chen","Haitao Yuan","Nan Jiang","Zhihan Zheng","Sai Wu","Ao Zhou","Shangguang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06825v2.pdf","comment":"Accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2503.16159v1","updated":"2025-03-20T13:57:33Z","published":"2025-03-20T13:57:33Z","title":"Neural Combinatorial Optimization for Real-World Routing","summary":"  Vehicle Routing Problems (VRPs) are a class of NP-hard problems ubiquitous in\nseveral real-world logistics scenarios that pose significant challenges for\noptimization. Neural Combinatorial Optimization (NCO) has emerged as a\npromising alternative to classical approaches, as it can learn fast heuristics\nto solve VRPs. However, most research works in NCO for VRPs focus on simplified\nsettings, which do not account for asymmetric distances and travel durations\nthat cannot be derived by simple Euclidean distances and unrealistic data\ndistributions, hindering real-world deployment. This work introduces RRNCO\n(Real Routing NCO) to bridge the gap of NCO between synthetic and real-world\nVRPs in the critical aspects of both data and modeling. First, we introduce a\nnew, openly available dataset with real-world data containing a diverse dataset\nof locations, distances, and duration matrices from 100 cities, considering\nrealistic settings with actual routing distances and durations obtained from\nOpen Source Routing Machine (OSRM). Second, we propose a novel approach that\nefficiently processes both node and edge features through contextual gating,\nenabling the construction of more informed node embedding, and we finally\nincorporate an Adaptation Attention Free Module (AAFM) with neural adaptive\nbias mechanisms that effectively integrates not only distance matrices but also\nangular relationships between nodes, allowing our model to capture rich\nstructural information. RRNCO achieves state-of-the-art results in real-world\nVRPs among NCO methods. We make our dataset and code publicly available at\nhttps://github.com/ai4co/real-routing-nco.\n","authors":["Jiwoo Son","Zhikai Zhao","Federico Berto","Chuanbo Hua","Changhyun Kwon","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2503.16159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04004v2","updated":"2025-03-20T13:53:48Z","published":"2025-01-07T18:59:58Z","title":"LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes","summary":"  LiDAR data pretraining offers a promising approach to leveraging large-scale,\nreadily available datasets for enhanced data utilization. However, existing\nmethods predominantly focus on sparse voxel representation, overlooking the\ncomplementary attributes provided by other LiDAR representations. In this work,\nwe propose LiMoE, a framework that integrates the Mixture of Experts (MoE)\nparadigm into LiDAR data representation learning to synergistically combine\nmultiple representations, such as range images, sparse voxels, and raw points.\nOur approach consists of three stages: i) Image-to-LiDAR Pretraining, which\ntransfers prior knowledge from images to point clouds across different\nrepresentations; ii) Contrastive Mixture Learning (CML), which uses MoE to\nadaptively activate relevant attributes from each representation and distills\nthese mixed features into a unified 3D network; iii) Semantic Mixture\nSupervision (SMS), which combines semantic logits from multiple representations\nto boost downstream segmentation performance. Extensive experiments across\neleven large-scale LiDAR datasets demonstrate our effectiveness and\nsuperiority. The code has been made publicly accessible.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Liang Pan","Ziwei Liu","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2501.04004v2.pdf","comment":"CVPR 2025; 27 pages, 17 figures, 10 tables; Project Page at\n  https://ldkong.com/LiMoE"},{"id":"http://arxiv.org/abs/2410.14420v2","updated":"2025-03-20T13:52:32Z","published":"2024-10-18T12:33:10Z","title":"Asymptotic non-linear shrinkage and eigenvector overlap for weighted\n  sample covariance","summary":"  We compute asymptotic non-linear shrinkage formulas for covariance and\nprecision matrix estimators for weighted sample covariances, and the joint\nsample-population eigenvector overlap distribution, in the spirit of Ledoit and\nP\\'ech\\'e. We detail explicitly the formulas for exponentially-weighted sample\ncovariances. We propose an algorithm to numerically compute those formulas.\nExperimentally, we show the performance of the asymptotic non-linear shrinkage\nestimators. Finally, we test the robustness of the theory to a heavy-tailed\ndistributions.\n","authors":["Benoit Oriol"],"pdf_url":"https://arxiv.org/pdf/2410.14420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01701v2","updated":"2025-03-20T13:44:19Z","published":"2023-02-03T12:55:08Z","title":"Assessment of Spatio-Temporal Predictors in the Presence of Missing and\n  Heterogeneous Data","summary":"  Deep learning approaches achieve outstanding predictive performance in\nmodeling modern data, despite the increasing complexity and scale. However,\nevaluating the quality of predictive models becomes more challenging, as\ntraditional statistical assumptions often no longer hold. In particular,\nspatio-temporal data exhibit dependencies across both time and space, often\ninvolving nonlinear dynamics, non-stationarities, and missing observations. As\na result, advanced predictors such as spatio-temporal graph neural networks\nrequire novel evaluation methodologies. This paper introduces a residual\ncorrelation analysis framework designed to assess the optimality of\nspatio-temporal predictive neural models, particularly in scenarios with\nincomplete and heterogeneous data. By leveraging the principle that residual\ncorrelation indicates information not captured by the model, this framework\nserves as a powerful tool to identify and localize regions in space and time\nwhere model performance can be improved. A key advantage of the proposed\napproach is its ability to operate under minimal assumptions, enabling robust\nevaluation of deep learning models applied to multivariate time series, even in\nthe presence of missing and heterogeneous data. The methodology employs\ntailored spatio-temporal graphs to encode sparse spatial and temporal\ndependencies within the data and utilizes asymptotically distribution-free\nsummary statistics to pinpoint time intervals and spatial regions where the\nmodel underperforms. The effectiveness of the proposed residual analysis is\ndemonstrated through validation on both synthetic and real-world scenarios\ninvolving state-of-the-art predictive models.\n","authors":["Daniele Zambon","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2302.01701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08460v2","updated":"2025-03-20T13:29:36Z","published":"2024-12-11T15:25:38Z","title":"Federated Learning for Traffic Flow Prediction with Synthetic Data\n  Augmentation","summary":"  Deep-learning based traffic prediction models require vast amounts of data to\nlearn embedded spatial and temporal dependencies. The inherent privacy and\ncommercial sensitivity of such data has encouraged a shift towards\ndecentralised data-driven methods, such as Federated Learning (FL). Under a\ntraditional Machine Learning paradigm, traffic flow prediction models can\ncapture spatial and temporal relationships within centralised data. In reality,\ntraffic data is likely distributed across separate data silos owned by multiple\nstakeholders. In this work, a cross-silo FL setting is motivated to facilitate\nstakeholder collaboration for optimal traffic flow prediction applications.\nThis work introduces an FL framework, referred to as FedTPS, to generate\nsynthetic data to augment each client's local dataset by training a\ndiffusion-based trajectory generation model through FL. The proposed framework\nis evaluated on a large-scale real world ride-sharing dataset using various FL\nmethods and Traffic Flow Prediction models, including a novel prediction model\nwe introduce, which leverages Temporal and Graph Attention mechanisms to learn\nthe Spatio-Temporal dependencies embedded within regional traffic flow data.\nExperimental results show that FedTPS outperforms multiple other FL baselines\nwith respect to global model performance.\n","authors":["Fermin Orozco","Pedro Porto Buarque de Gusmão","Hongkai Wen","Johan Wahlström","Man Luo"],"pdf_url":"https://arxiv.org/pdf/2412.08460v2.pdf","comment":"11 pages, 7 figures, 6 tables, ACM format"},{"id":"http://arxiv.org/abs/2503.16123v1","updated":"2025-03-20T13:11:44Z","published":"2025-03-20T13:11:44Z","title":"Distributed Learning over Arbitrary Topology: Linear Speed-Up with\n  Polynomial Transient Time","summary":"  We study a distributed learning problem in which $n$ agents, each with\npotentially heterogeneous local data, collaboratively minimize the sum of their\nlocal cost functions via peer-to-peer communication. We propose a novel\nalgorithm, Spanning Tree Push-Pull (STPP), which employs two spanning trees\nextracted from a general communication graph to distribute both model\nparameters and stochastic gradients. Unlike prior approaches that rely heavily\non spectral gap properties, STPP leverages a more flexible topological\ncharacterization, enabling robust information flow and efficient updates.\nTheoretically, we prove that STPP achieves linear speedup and polynomial\ntransient iteration complexity, up to $O(n^7)$ for smooth nonconvex objectives\nand $\\tilde{O}(n^3)$ for smooth strongly convex objectives, under arbitrary\nnetwork topologies. Moreover, compared with the existing methods, STPP achieves\nfaster convergence rates on sparse and non-regular topologies (e.g., directed\nring) and reduces communication overhead on dense networks (e.g., static\nexponential graph). These results significantly advance the state of the art,\nespecially when $n$ is large. Numerical experiments further demonstrate the\nstrong performance of STPP and confirm the practical relevance of its\ntheoretical convergence rates across various common graph architectures. Our\ncode is available at\nhttps://anonymous.4open.science/r/SpanningTreePushPull-5D3E.\n","authors":["Runze You","Shi Pu"],"pdf_url":"https://arxiv.org/pdf/2503.16123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16117v1","updated":"2025-03-20T13:04:43Z","published":"2025-03-20T13:04:43Z","title":"Improving Discriminator Guidance in Diffusion Models","summary":"  Discriminator Guidance has become a popular method for efficiently refining\npre-trained Score-Matching Diffusion models. However, in this paper, we\ndemonstrate that the standard implementation of this technique does not\nnecessarily lead to a distribution closer to the real data distribution.\nSpecifically, we show that training the discriminator using Cross-Entropy loss,\nas commonly done, can in fact increase the Kullback-Leibler divergence between\nthe model and target distributions, particularly when the discriminator\noverfits. To address this, we propose a theoretically sound training objective\nfor discriminator guidance that properly minimizes the KL divergence. We\nanalyze its properties and demonstrate empirically across multiple datasets\nthat our proposed method consistently improves over the conventional method by\nproducing samples of higher quality.\n","authors":["Alexandre Verine","Mehdi Inane","Florian Le Bronnec","Benjamin Negrevergne","Yann Chevaleyre"],"pdf_url":"https://arxiv.org/pdf/2503.16117v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16502v3","updated":"2025-03-20T12:57:03Z","published":"2024-09-24T23:18:32Z","title":"GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for\n  Improved Visual Localization","summary":"  Although various visual localization approaches exist, such as scene\ncoordinate regression and camera pose regression, these methods often struggle\nwith optimization complexity or limited accuracy. To address these challenges,\nwe explore the use of novel view synthesis techniques, particularly 3D Gaussian\nSplatting (3DGS), which enables the compact encoding of both 3D geometry and\nscene appearance. We propose a two-stage procedure that integrates dense and\nrobust keypoint descriptors from the lightweight XFeat feature extractor into\n3DGS, enhancing performance in both indoor and outdoor environments. The coarse\npose estimates are directly obtained via 2D-3D correspondences between the 3DGS\nrepresentation and query image descriptors. In the second stage, the initial\npose estimate is refined by minimizing the rendering-based photometric warp\nloss. Benchmarking on widely used indoor and outdoor datasets demonstrates\nimprovements over recent neural rendering-based localization methods, such as\nNeRFMatch and PNeRFLoc.\n","authors":["Gennady Sidorov","Malik Mohrat","Denis Gridusov","Ruslan Rakhimov","Sergey Kolyubin"],"pdf_url":"https://arxiv.org/pdf/2409.16502v3.pdf","comment":"Project website at https://gsplatloc.github.io/"},{"id":"http://arxiv.org/abs/2503.16107v1","updated":"2025-03-20T12:51:37Z","published":"2025-03-20T12:51:37Z","title":"Learn to Bid as a Price-Maker Wind Power Producer","summary":"  Wind power producers (WPPs) participating in short-term power markets face\nsignificant imbalance costs due to their non-dispatchable and variable\nproduction. While some WPPs have a large enough market share to influence\nprices with their bidding decisions, existing optimal bidding methods rarely\naccount for this aspect. Price-maker approaches typically model bidding as a\nbilevel optimization problem, but these methods require complex market models,\nestimating other participants' actions, and are computationally demanding. To\naddress these challenges, we propose an online learning algorithm that\nleverages contextual information to optimize WPP bids in the price-maker\nsetting. We formulate the strategic bidding problem as a contextual multi-armed\nbandit, ensuring provable regret minimization. The algorithm's performance is\nevaluated against various benchmark strategies using a numerical simulation of\nthe German day-ahead and real-time markets.\n","authors":["Shobhit Singhal","Marta Fochesato","Liviu Aolaritei","Florian Dörfler"],"pdf_url":"https://arxiv.org/pdf/2503.16107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.01729v5","updated":"2025-03-20T12:32:40Z","published":"2021-10-04T22:01:01Z","title":"Stochastic tensor space feature theory with applications to robust\n  machine learning","summary":"  In this paper we develop a Multilevel Orthogonal Subspace (MOS)\nKarhunen-Loeve feature theory based on stochastic tensor spaces, for the\nconstruction of robust machine learning features. Training data is treated as\ninstances of a random field within a relevant Bochner space. Our key\nobservation is that separate machine learning classes can reside predominantly\nin mostly distinct subspaces. Using the Karhunen-Loeve expansion and a\nhierarchical expansion of the first (nominal) class, a MOS is constructed to\ndetect anomalous signal components, treating the second class as an outlier of\nthe first. The projection coefficients of the input data into these subspaces\nare then used to train a Machine Learning (ML) classifier. These coefficients\nbecome new features from which much clearer separation surfaces can arise for\nthe underlying classes. Tests in the blood plasma dataset (Alzheimer's Disease\nNeuroimaging Initiative) show dramatic increases in accuracy. This is in\ncontrast to popular ML methods such as Gradient Boosting, RUS Boost, Random\nForest and (Convolutional) Neural Networks.\n","authors":["Julio Enrique Castrillon-Candas","Dingning Liu","Sicheng Yang","Xiaoling Zhang","Mark Kon"],"pdf_url":"https://arxiv.org/pdf/2110.01729v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16091v1","updated":"2025-03-20T12:32:35Z","published":"2025-03-20T12:32:35Z","title":"AIMI: Leveraging Future Knowledge and Personalization in Sparse Event\n  Forecasting for Treatment Adherence","summary":"  Adherence to prescribed treatments is crucial for individuals with chronic\nconditions to avoid costly or adverse health outcomes. For certain patient\ngroups, intensive lifestyle interventions are vital for enhancing medication\nadherence. Accurate forecasting of treatment adherence can open pathways to\ndeveloping an on-demand intervention tool, enabling timely and personalized\nsupport. With the increasing popularity of smartphones and wearables, it is now\neasier than ever to develop and deploy smart activity monitoring systems.\nHowever, effective forecasting systems for treatment adherence based on\nwearable sensors are still not widely available. We close this gap by proposing\nAdherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI\nis a knowledge-guided adherence forecasting system that leverages smartphone\nsensors and previous medication history to estimate the likelihood of\nforgetting to take a prescribed medication. A user study was conducted with 27\nparticipants who took daily medications to manage their cardiovascular\ndiseases. We designed and developed CNN and LSTM-based forecasting models with\nvarious combinations of input features and found that LSTM models can forecast\nmedication adherence with an accuracy of 0.932 and an F-1 score of 0.936.\nMoreover, through a series of ablation studies involving convolutional and\nrecurrent neural network architectures, we demonstrate that leveraging known\nknowledge about future and personalized training enhances the accuracy of\nmedication adherence forecasting. Code available:\nhttps://github.com/ab9mamun/AIMI.\n","authors":["Abdullah Mamun","Diane J. Cook","Hassan Ghasemzadeh"],"pdf_url":"https://arxiv.org/pdf/2503.16091v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.17178v2","updated":"2025-03-20T12:30:17Z","published":"2023-10-26T06:05:12Z","title":"Relational Object-Centric Actor-Critic","summary":"  The advances in unsupervised object-centric representation learning have\nsignificantly improved its application to downstream tasks. Recent works\nhighlight that disentangled object representations can aid policy learning in\nimage-based, object-centric reinforcement learning tasks. This paper proposes a\nnovel object-centric reinforcement learning algorithm that integrates\nactor-critic and model-based approaches by incorporating an object-centric\nworld model within the critic. The world model captures the environment's\ndata-generating process by predicting the next state and reward given the\ncurrent state-action pair, where actions are interventions in the environment.\nIn model-based reinforcement learning, world model learning can be interpreted\nas a causal induction problem, where the agent must learn the causal\nrelationships underlying the environment's dynamics. We evaluate our method in\na simulated 3D robotic environment and a 2D environment with compositional\nstructure. As baselines, we compare against object-centric, model-free\nactor-critic algorithms and a state-of-the-art monolithic model-based\nalgorithm. While the baselines show comparable performance in easier tasks, our\napproach outperforms them in more challenging scenarios with a large number of\nobjects or more complex dynamics.\n","authors":["Leonid Ugadiarov","Vitaliy Vorobyov","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2310.17178v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16086v1","updated":"2025-03-20T12:28:31Z","published":"2025-03-20T12:28:31Z","title":"Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly","summary":"  Ensuring food safety and quality is critical in the food processing industry,\nwhere the detection of contaminants remains a persistent challenge. This study\npresents an automated solution for detecting foreign objects on pork belly meat\nusing hyperspectral imaging (HSI). A hyperspectral camera was used to capture\ndata across various bands in the near-infrared (NIR) spectrum (900-1700 nm),\nenabling accurate identification of contaminants that are often undetectable\nthrough traditional visual inspection methods. The proposed solution combines\npre-processing techniques with a segmentation approach based on a lightweight\nVision Transformer (ViT) to distinguish contaminants from meat, fat, and\nconveyor belt materials. The adopted strategy demonstrates high detection\naccuracy and training efficiency, while also addressing key industrial\nchallenges such as inherent noise, temperature variations, and spectral\nsimilarity between contaminants and pork belly. Experimental results validate\nthe effectiveness of hyperspectral imaging in enhancing food safety,\nhighlighting its potential for broad real-time applications in automated\nquality control processes.\n","authors":["Gabriela Ghimpeteanu","Hayat Rajani","Josep Quintana","Rafael Garcia"],"pdf_url":"https://arxiv.org/pdf/2503.16086v1.pdf","comment":"Article under review by Computers in Industry, Elsevier"},{"id":"http://arxiv.org/abs/2503.16081v1","updated":"2025-03-20T12:22:18Z","published":"2025-03-20T12:22:18Z","title":"OThink-MR1: Stimulating multimodal generalized reasoning capabilities\n  through dynamic reinforcement learning","summary":"  Multimodal Language Models have gained significant traction for their ability\nto process diverse input data types and generate coherent, contextually\nrelevant outputs across various applications. While supervised fine-tuning\n(SFT) has been the predominant approach to enhance MLLM capabilities in\ntask-specific optimization, it often falls short in fostering crucial\ngeneralized reasoning abilities. Despite the potential of reinforcement\nlearning (RL) to address these limitations, it faces two issues: (1) its\ngeneralized capabilities in multimodal tasks remain underexplored. (2) its\ntraining constraints such as constant Kullback-Leibler or clamp strategy easily\nlead to suboptimal bottleneck. To adress these issues, we introduce OThink-MR1,\na framework that extends RL to MLLMs, enabling them to achieve deeper\nunderstanding and reasoning across multimodal tasks. We design a dynamic\nKullback-Leibler strategy that significantly enhances RL performance,\nsurpassing SFT in same-task evaluations. Also, we are the first to reveal that\nRL exhibits remarkable cross-task generalization capabilities, which shows that\nmodels post-trained with RL on one multimodal task can be effectively\ntransfered to another tasks. Finally, extensive experiments demonstrate the\ngreat reasoning ability of our proposed OThink-MR1.\n","authors":["Zhiyuan Liu","Yuting Zhang","Feng Liu","Changwang Zhang","Ying Sun","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2503.16081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10941v2","updated":"2025-03-20T12:18:18Z","published":"2024-01-17T18:06:17Z","title":"Crowd-PrefRL: Preference-Based Reward Learning from Crowds","summary":"  Preference-based reinforcement learning (RL) provides a framework to train AI\nagents using human feedback through preferences over pairs of behaviors,\nenabling agents to learn desired behaviors when it is difficult to specify a\nnumerical reward function. While this paradigm leverages human feedback, it\ntypically treats the feedback as given by a single human user. However,\ndifferent users may desire multiple AI behaviors and modes of interaction.\nMeanwhile, incorporating preference feedback from crowds (i.e. ensembles of\nusers) in a robust manner remains a challenge, and the problem of training RL\nagents using feedback from multiple human users remains understudied. In this\nwork, we introduce a conceptual framework, Crowd-PrefRL, that integrates\npreference-based RL approaches with techniques from unsupervised crowdsourcing\nto enable training of autonomous system behaviors from crowdsourced feedback.\nWe show preliminary results suggesting that Crowd-PrefRL can learn reward\nfunctions and agent policies from preference feedback provided by crowds of\nunknown expertise and reliability. We also show that in most cases, agents\ntrained with Crowd-PrefRL outperform agents trained with majority-vote\npreferences or preferences from any individual user, especially when the spread\nof user error rates among the crowd is large. Results further suggest that our\nmethod can identify the presence of minority viewpoints within the crowd in an\nunsupervised manner.\n","authors":["David Chhan","Ellen Novoseller","Vernon J. Lawhern"],"pdf_url":"https://arxiv.org/pdf/2401.10941v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16072v1","updated":"2025-03-20T12:09:01Z","published":"2025-03-20T12:09:01Z","title":"Redefining Toxicity: An Objective and Context-Aware Approach for\n  Stress-Level-Based Detection","summary":"  The fundamental problem of toxicity detection lies in the fact that the term\n\"toxicity\" is ill-defined. Such uncertainty causes researchers to rely on\nsubjective and vague data during model training, which leads to non-robust and\ninaccurate results, following the 'garbage in - garbage out' paradigm. This\nstudy introduces a novel, objective, and context-aware framework for toxicity\ndetection, leveraging stress levels as a key determinant of toxicity. We\npropose new definition, metric and training approach as a parts of our\nframework and demonstrate it's effectiveness using a dataset we collected.\n","authors":["Sergey Berezin","Reza Farahbakhsh","Noel Crespi"],"pdf_url":"https://arxiv.org/pdf/2503.16072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11624v4","updated":"2025-03-20T12:06:17Z","published":"2024-06-17T15:07:55Z","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","summary":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","authors":["Omer Sahin Tas","Royden Wagner"],"pdf_url":"https://arxiv.org/pdf/2406.11624v4.pdf","comment":"ICLR 2025 camera-ready. Our implementation is available at\n  \\href{https://github.com/kit-mrt/future-motion}{this https URL}"},{"id":"http://arxiv.org/abs/2406.03146v2","updated":"2025-03-20T12:04:41Z","published":"2024-06-05T11:01:42Z","title":"Tiny models from tiny data: Textual and null-text inversion for few-shot\n  distillation","summary":"  Few-shot learning deals with problems such as image classification using very\nfew training examples. Recent vision foundation models show excellent few-shot\ntransfer abilities, but are large and slow at inference. Using knowledge\ndistillation, the capabilities of high-performing but slow models can be\ntransferred to tiny, efficient models. However, common distillation methods\nrequire a large set of unlabeled data, which is not available in the few-shot\nsetting. To overcome this lack of data, there has been a recent interest in\nusing synthetic data. We expand on this line of research by presenting a novel\ndiffusion model inversion technique (TINT) combining the diversity of textual\ninversion with the specificity of null-text inversion. Using this method in a\nfew-shot distillation pipeline leads to state-of-the-art accuracy among small\nstudent models on popular benchmarks, while being significantly faster than\nprior work. Popular few-shot benchmarks involve evaluation over a large number\nof episodes, which is computationally cumbersome for methods involving\nsynthetic data generation. We also present a theoretical analysis on how the\naccuracy estimator variance depends on the number of episodes and query\nexamples, and use these results to lower the computational effort required for\nmethod evaluation. Finally, to further motivate the use of generative models in\nfew-shot distillation, we demonstrate that our method outperforms training on\nreal data mined from the dataset used in the original diffusion model training.\nSource code is available at https://github.com/pixwse/tiny2.\n","authors":["Erik Landolsi","Fredrik Kahl"],"pdf_url":"https://arxiv.org/pdf/2406.03146v2.pdf","comment":"24 pages (13 main pages + references and appendix)"},{"id":"http://arxiv.org/abs/2502.17984v2","updated":"2025-03-20T12:02:58Z","published":"2025-02-25T08:53:02Z","title":"Generalized Decision Focused Learning under Imprecise\n  Uncertainty--Theoretical Study","summary":"  Decision Focused Learning has emerged as a critical paradigm for integrating\nmachine learning with downstream optimisation. Despite its promise, existing\nmethodologies predominantly rely on probabilistic models and focus narrowly on\ntask objectives, overlooking the nuanced challenges posed by epistemic\nuncertainty, non-probabilistic modelling approaches, and the integration of\nuncertainty into optimisation constraints. This paper bridges these gaps by\nintroducing innovative frameworks: (i) a non-probabilistic lens for epistemic\nuncertainty representation, leveraging intervals (the least informative\nuncertainty model), Contamination (hybrid model), and probability boxes (the\nmost informative uncertainty model); (ii) methodologies to incorporate\nuncertainty into constraints, expanding Decision-Focused Learning's utility in\nconstrained environments; (iii) the adoption of Imprecise Decision Theory for\nambiguity-rich decision-making contexts; and (iv) strategies for addressing\nsparse data challenges. Empirical evaluations on benchmark optimisation\nproblems demonstrate the efficacy of these approaches in improving decision\nquality and robustness and dealing with said gaps.\n","authors":["Keivan Shariatmadar","Neil Yorke-Smith","Ahmad Osman","Fabio Cuzzolin","Hans Hallez","David Moens"],"pdf_url":"https://arxiv.org/pdf/2502.17984v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2401.10288v2","updated":"2025-03-20T12:01:44Z","published":"2024-01-17T03:57:36Z","title":"Self-supervised New Activity Detection in Sensor-based Smart\n  Environments","summary":"  With the rapid advancement of ubiquitous computing technology, human activity\nanalysis based on time series data from a diverse range of sensors enables the\ndelivery of more intelligent services. Despite the importance of exploring new\nactivities in real-world scenarios, existing human activity recognition studies\ngenerally rely on predefined known activities and often overlook detecting new\npatterns (novelties) that have not been previously observed during training.\nNovelty detection in human activities becomes even more challenging due to (1)\ndiversity of patterns within the same known activity, (2) shared patterns\nbetween known and new activities, and (3) differences in sensor properties of\neach activity dataset. We introduce CLAN, a two-tower model that leverages\nContrastive Learning with diverse data Augmentation for New activity detection\nin sensor-based environments. CLAN simultaneously and explicitly utilizes\nmultiple types of strongly shifted data as negative samples in contrastive\nlearning, effectively learning invariant representations that adapt to various\npattern variations within the same activity. To enhance the ability to\ndistinguish between known and new activities that share common features, CLAN\nincorporates both time and frequency domains, enabling the learning of\nmulti-faceted discriminative representations. Additionally, we design an\nautomatic selection mechanism of data augmentation methods tailored to each\ndataset's properties, generating appropriate positive and negative pairs for\ncontrastive learning. Comprehensive experiments on real-world datasets show\nthat CLAN achieves a 9.24% improvement in AUROC compared to the best-performing\nbaseline model.\n","authors":["Hyunju Kim","Dongman Lee"],"pdf_url":"https://arxiv.org/pdf/2401.10288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01667v2","updated":"2025-03-20T12:01:38Z","published":"2024-11-03T19:45:15Z","title":"GraphXForm: Graph transformer for computer-aided molecular design","summary":"  Generative deep learning has become pivotal in molecular design for drug\ndiscovery, materials science, and chemical engineering. A widely used paradigm\nis to pretrain neural networks on string representations of molecules and\nfine-tune them using reinforcement learning on specific objectives. However,\nstring-based models face challenges in ensuring chemical validity and enforcing\nstructural constraints like the presence of specific substructures. We propose\nto instead combine graph-based molecular representations, which can naturally\nensure chemical validity, with transformer architectures, which are highly\nexpressive and capable of modeling long-range dependencies between atoms. Our\napproach iteratively modifies a molecular graph by adding atoms and bonds,\nwhich ensures chemical validity and facilitates the incorporation of structural\nconstraints. We present GraphXForm, a decoder-only graph transformer\narchitecture, which is pretrained on existing compounds and then fine-tuned\nusing a new training algorithm that combines elements of the deep cross-entropy\nmethod and self-improvement learning. We evaluate GraphXForm on various drug\ndesign tasks, demonstrating superior objective scores compared to\nstate-of-the-art molecular design approaches. Furthermore, we apply GraphXForm\nto two solvent design tasks for liquid-liquid extraction, again outperforming\nalternative methods while flexibly enforcing structural constraints or\ninitiating design from existing molecular structures.\n","authors":["Jonathan Pirnay","Jan G. Rittig","Alexander B. Wolf","Martin Grohe","Jakob Burger","Alexander Mitsos","Dominik G. Grimm"],"pdf_url":"https://arxiv.org/pdf/2411.01667v2.pdf","comment":"Published in Digital Discovery, 2025"},{"id":"http://arxiv.org/abs/2404.15305v2","updated":"2025-03-20T11:56:18Z","published":"2024-03-29T08:48:07Z","title":"SelfReplay: Adapting Self-Supervised Sensory Models via Adaptive\n  Meta-Task Replay","summary":"  Self-supervised learning has emerged as a method for utilizing massive\nunlabeled data for pre-training models, providing an effective feature\nextractor for various mobile sensing applications. However, when deployed to\nend-users, these models encounter significant domain shifts attributed to user\ndiversity. We investigate the performance degradation that occurs when\nself-supervised models are fine-tuned in heterogeneous domains. To address the\nissue, we propose SelfReplay, a few-shot domain adaptation framework for\npersonalizing self-supervised models. SelfReplay proposes self-supervised\nmeta-learning for initial model pre-training, followed by a user-side model\nadaptation by replaying the self-supervision with user-specific data. This\nallows models to adjust their pre-trained representations to the user with only\na few samples. Evaluation with four benchmarks demonstrates that SelfReplay\noutperforms existing baselines by an average F1-score of 8.8%p. Our on-device\ncomputational overhead analysis on a commodity off-the-shelf (COTS) smartphone\nshows that SelfReplay completes adaptation within an unobtrusive latency (in\nthree minutes) with only a 9.54% memory consumption, demonstrating the\ncomputational efficiency of the proposed method.\n","authors":["Hyungjun Yoon","Jaehyun Kwak","Biniyam Aschalew Tolera","Gaole Dai","Mo Li","Taesik Gong","Kimin Lee","Sung-Ju Lee"],"pdf_url":"https://arxiv.org/pdf/2404.15305v2.pdf","comment":"Accepted to the 23rd ACM Conference on Embedded Networked Sensor\n  Systems (ACM SenSys 2025)"},{"id":"http://arxiv.org/abs/2411.19772v3","updated":"2025-03-20T11:55:30Z","published":"2024-11-29T15:18:06Z","title":"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos","summary":"  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n","authors":["Tiantian Geng","Jinrui Zhang","Qingni Wang","Teng Wang","Jinming Duan","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.19772v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.16057v1","updated":"2025-03-20T11:45:08Z","published":"2025-03-20T11:45:08Z","title":"Expert Race: A Flexible Routing Strategy for Scaling Diffusion\n  Transformer with Mixture of Experts","summary":"  Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties.\n","authors":["Yike Yuan","Ziyu Wang","Zihao Huang","Defa Zhu","Xun Zhou","Jingyi Yu","Qiyang Min"],"pdf_url":"https://arxiv.org/pdf/2503.16057v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01478v5","updated":"2025-03-20T11:28:41Z","published":"2025-03-03T12:37:34Z","title":"SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction","summary":"  Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.\n","authors":["Lu Dai","Yijie Xu","Jinhui Ye","Hao Liu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.01478v5.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2503.11249v2","updated":"2025-03-20T11:04:51Z","published":"2025-03-14T10:00:13Z","title":"Spherical Tree-Sliced Wasserstein Distance","summary":"  Sliced Optimal Transport (OT) simplifies the OT problem in high-dimensional\nspaces by projecting supports of input measures onto one-dimensional lines and\nthen exploiting the closed-form expression of the univariate OT to reduce the\ncomputational burden of OT. Recently, the Tree-Sliced method has been\nintroduced to replace these lines with more intricate structures, known as tree\nsystems. This approach enhances the ability to capture topological information\nof integration domains in Sliced OT while maintaining low computational cost.\nInspired by this approach, in this paper, we present an adaptation of tree\nsystems on OT problems for measures supported on a sphere. As a counterpart to\nthe Radon transform variant on tree systems, we propose a novel spherical Radon\ntransform with a new integration domain called spherical trees. By leveraging\nthis transform and exploiting the spherical tree structures, we derive\nclosed-form expressions for OT problems on the sphere. Consequently, we obtain\nan efficient metric for measures on the sphere, named Spherical Tree-Sliced\nWasserstein (STSW) distance. We provide an extensive theoretical analysis to\ndemonstrate the topology of spherical trees and the well-definedness and\ninjectivity of our Radon transform variant, which leads to an orthogonally\ninvariant distance between spherical measures. Finally, we conduct a wide range\nof numerical experiments, including gradient flows and self-supervised\nlearning, to assess the performance of our proposed metric, comparing it to\nrecent benchmarks.\n","authors":["Viet-Hoang Tran","Thanh T. Chu","Khoi N. M. Nguyen","Trang Pham","Tam Le","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.11249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20580v3","updated":"2025-03-20T11:00:14Z","published":"2025-02-27T22:45:41Z","title":"Training Large Neural Networks With Low-Dimensional Error Feedback","summary":"  Training deep neural networks typically relies on backpropagating high\ndimensional error signals a computationally intensive process with little\nevidence supporting its implementation in the brain. However, since most tasks\ninvolve low-dimensional outputs, we propose that low-dimensional error signals\nmay suffice for effective learning. To test this hypothesis, we introduce a\nnovel local learning rule based on Feedback Alignment that leverages indirect,\nlow-dimensional error feedback to train large networks. Our method decouples\nthe backward pass from the forward pass, enabling precise control over error\nsignal dimensionality while maintaining high-dimensional representations. We\nbegin with a detailed theoretical derivation for linear networks, which forms\nthe foundation of our learning framework, and extend our approach to nonlinear,\nconvolutional, and transformer architectures. Remarkably, we demonstrate that\neven minimal error dimensionality on the order of the task dimensionality can\nachieve performance matching that of traditional backpropagation. Furthermore,\nour rule enables efficient training of convolutional networks, which have\npreviously been resistant to Feedback Alignment methods, with minimal error.\nThis breakthrough not only paves the way toward more biologically accurate\nmodels of learning but also challenges the conventional reliance on\nhigh-dimensional gradient signals in neural network training. Our findings\nsuggest that low-dimensional error signals can be as effective as\nhigh-dimensional ones, prompting a reevaluation of gradient-based learning in\nhigh-dimensional systems. Ultimately, our work offers a fresh perspective on\nneural network optimization and contributes to understanding learning\nmechanisms in both artificial and biological systems.\n","authors":["Maher Hanut","Jonathan Kadmon"],"pdf_url":"https://arxiv.org/pdf/2502.20580v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.07726v2","updated":"2025-03-20T10:47:07Z","published":"2024-08-14T14:18:47Z","title":"Development of a graph neural network surrogate for travel demand\n  modelling","summary":"  As urban environments grow, the modelling of transportation systems becomes\nincreasingly complex. This paper advances the field of travel demand modelling\nby introducing advanced Graph Neural Network (GNN) architectures as surrogate\nmodels, addressing key limitations of previous approaches. Building on prior\nwork with Graph Convolutional Networks (GCNs), we introduce GATv3, a new Graph\nAttention Network (GAT) variant that mitigates over-smoothing through residual\nconnections, enabling deeper and more expressive architectures. Additionally,\nwe propose a fine-grained classification framework that improves predictive\nstability while achieving numerical precision comparable to regression,\noffering a more interpretable and efficient alternative. To enhance model\nperformance, we develop a synthetic data generation strategy, which expands the\naugmented training dataset without overfitting. Our experiments demonstrate\nthat GATv3 significantly improves classification performance, while the GCN\nmodel shows unexpected dominance in fine-grained classification when\nsupplemented with additional training data. The results highlight the\nadvantages of fine-grained classification over regression for travel demand\nmodelling tasks and reveal new challenges in extending GAT-based architectures\nto complex transport scenarios. Notably, GATv3 appears well-suited for\nclassification-based transportation applications, such as section control and\ncongestion warning systems, which require a higher degree of differentiation\namong neighboring links. These findings contribute to refining GNN-based\nsurrogates, offering new possibilities for applying GATv3 and fine-grained\nclassification in broader transportation challenges.\n","authors":["Nikita Makarov","Santhanakrishnan Narayanan","Constantinos Antoniou"],"pdf_url":"https://arxiv.org/pdf/2408.07726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16138v2","updated":"2025-03-20T10:28:31Z","published":"2024-10-21T16:04:50Z","title":"Theoretical Insights into Line Graph Transformation on Graph Learning","summary":"  Line graph transformation has been widely studied in graph theory, where each\nnode in a line graph corresponds to an edge in the original graph. This has\ninspired a series of graph neural networks (GNNs) applied to transformed line\ngraphs, which have proven effective in various graph representation learning\ntasks. However, there is limited theoretical study on how line graph\ntransformation affects the expressivity of GNN models. In this study, we focus\non two types of graphs known to be challenging to the Weisfeiler-Leman (WL)\ntests: Cai-F\\\"urer-Immerman (CFI) graphs and strongly regular graphs, and show\nthat applying line graph transformation helps exclude these challenging graph\nproperties, thus potentially assist WL tests in distinguishing these graphs. We\nempirically validate our findings by conducting a series of experiments that\ncompare the accuracy and efficiency of graph isomorphism tests and GNNs on both\nline-transformed and original graphs across these graph structure types.\n","authors":["Fan Yang","Xingyue Huang"],"pdf_url":"https://arxiv.org/pdf/2410.16138v2.pdf","comment":"21 pages, code available at\n  https://github.com/lukeyf/graphs-and-lines"},{"id":"http://arxiv.org/abs/2503.16010v1","updated":"2025-03-20T10:24:14Z","published":"2025-03-20T10:24:14Z","title":"Patch-based learning of adaptive Total Variation parameter maps for\n  blind image denoising","summary":"  We consider a patch-based learning approach defined in terms of neural\nnetworks to estimate spatially adaptive regularisation parameter maps for image\ndenoising with weighted Total Variation and test it to situations when the\nnoise distribution is unknown. As an example, we consider situations where\nnoise could be either Gaussian or Poisson and perform preliminary model\nselection by a standard binary classification network. Then, we define a\npatch-based approach where at each image pixel an optimal weighting between TV\nregularisation and the corresponding data fidelity is learned in a supervised\nway using reference natural image patches upon optimisation of SSIM and in a\nsliding window fashion. Extensive numerical results are reported for both noise\nmodels, showing significant improvement w.r.t. results obtained by means of\noptimal scalar regularisation.\n","authors":["Claudio Fantasia","Luca Calatroni","Xavier Descombes","Rim Rekik"],"pdf_url":"https://arxiv.org/pdf/2503.16010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14538v2","updated":"2025-03-20T10:20:22Z","published":"2025-03-17T14:08:35Z","title":"Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal\n  Approach Combining Imaging and Clinical Data","summary":"  Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings.\n","authors":["Ananya Ganapthy","Praveen Shastry","Naveen Kumarasami","Anandakumar D","Keerthana R","Mounigasri M","Varshinipriya M","Kishore Prasath Venkatesh","Bargava Subramanian","Kalyan Sivasailam"],"pdf_url":"https://arxiv.org/pdf/2503.14538v2.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.00422v3","updated":"2025-03-20T10:08:31Z","published":"2023-12-31T08:22:51Z","title":"Interpreting the Curse of Dimensionality from Distance Concentration and\n  Manifold Effect","summary":"  The characteristics of data like distribution and heterogeneity, become more\ncomplex and counterintuitive as dimensionality increases. This phenomenon is\nknown as curse of dimensionality, where common patterns and relationships\n(e.g., internal pattern and boundary pattern) that hold in low-dimensional\nspace may be invalid in higher-dimensional space. It leads to a decreasing\nperformance for the regression, classification, or clustering models or\nalgorithms. Curse of dimensionality can be attributed to many causes. In this\npaper, we first summarize the potential challenges associated with manipulating\nhigh-dimensional data, and explains the possible causes for the failure of\nregression, classification, or clustering tasks. Subsequently, we delve into\ntwo major causes of the curse of dimensionality, distance concentration, and\nmanifold effect, by performing theoretical and empirical analyses. The results\ndemonstrate that, as the dimensionality increases, nearest neighbor search\n(NNS) using three classical distance measurements, Minkowski distance,\nChebyshev distance, and cosine distance, becomes meaningless. Meanwhile, the\ndata incorporates more redundant features, and the variance contribution of\nprincipal component analysis (PCA) is skewed towards a few dimensions.\n","authors":["Dehua Peng","Zhipeng Gui","Huayi Wu"],"pdf_url":"https://arxiv.org/pdf/2401.00422v3.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2210.04527v5","updated":"2025-03-20T10:04:47Z","published":"2022-10-10T09:52:02Z","title":"A policy gradient approach for Finite Horizon Constrained Markov\n  Decision Processes","summary":"  The infinite horizon setting is widely adopted for problems of reinforcement\nlearning (RL). These invariably result in stationary policies that are optimal.\nIn many situations, finite horizon control problems are of interest and for\nsuch problems, the optimal policies are time-varying in general. Another\nsetting that has become popular in recent times is of Constrained Reinforcement\nLearning, where the agent maximizes its rewards while it also aims to satisfy\nsome given constraint criteria. However, this setting has only been studied in\nthe context of infinite horizon MDPs where stationary policies are optimal. We\npresent an algorithm for constrained RL in the Finite Horizon Setting where the\nhorizon terminates after a fixed (finite) time. We use function approximation\nin our algorithm which is essential when the state and action spaces are large\nor continuous and use the policy gradient method to find the optimal policy.\nThe optimal policy that we obtain depends on the stage and so is non-stationary\nin general. To the best of our knowledge, our paper presents the first policy\ngradient algorithm for the finite horizon setting with constraints. We show the\nconvergence of our algorithm to a constrained optimal policy. We also compare\nand analyze the performance of our algorithm through experiments and show that\nour algorithm performs better than some other well known algorithms.\n","authors":["Soumyajit Guin","Shalabh Bhatnagar"],"pdf_url":"https://arxiv.org/pdf/2210.04527v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18532v2","updated":"2025-03-20T09:58:49Z","published":"2025-01-30T17:58:36Z","title":"Differentially Private Steering for Large Language Model Alignment","summary":"  Aligning Large Language Models (LLMs) with human values and away from\nundesirable behaviors (such as hallucination) has become increasingly\nimportant. Recently, steering LLMs towards a desired behavior via activation\nediting has emerged as an effective method to mitigate harmful generations at\ninference-time. Activation editing modifies LLM representations by preserving\ninformation from positive demonstrations (e.g., truthful) and minimising\ninformation from negative demonstrations (e.g., hallucinations). When these\ndemonstrations come from a private dataset, the aligned LLM may leak private\ninformation contained in those private samples. In this work, we present the\nfirst study of aligning LLM behavior with private datasets. Our work proposes\nthe Private Steering for LLM Alignment (PSA) algorithm to edit LLM activations\nwith differential privacy (DP) guarantees. We conduct extensive experiments on\nseven different benchmarks with open-source LLMs of different sizes (0.5B to\n7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that\nPSA achieves DP guarantees for LLM alignment with minimal loss in performance,\nincluding alignment metrics, open-ended text generation quality, and\ngeneral-purpose reasoning. We also develop the first Membership Inference\nAttack (MIA) for evaluating and auditing the empirical privacy for the problem\nof LLM steering via activation editing. Our experiments support the theoretical\nguarantees by showing improved guarantees for our PSA algorithm compared to\nseveral existing non-private techniques.\n","authors":["Anmol Goel","Yaxi Hu","Iryna Gurevych","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2501.18532v2.pdf","comment":"ICLR 2025 Camera Ready; Code: https://github.com/UKPLab/iclr2025-psa"},{"id":"http://arxiv.org/abs/2411.03884v3","updated":"2025-03-20T09:46:11Z","published":"2024-11-06T13:00:34Z","title":"Polynomial Composition Activations: Unleashing the Dynamics of Large\n  Language Models","summary":"  Transformers have found extensive applications across various domains due to\nthe powerful fitting capabilities. This success can be partially attributed to\ntheir inherent nonlinearity. Thus, in addition to the ReLU function employed in\nthe original transformer architecture, researchers have explored alternative\nmodules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment\nrepresentational capacity. In this paper, we propose a novel category of\npolynomial composition activations (PolyCom), designed to optimize the dynamics\nof transformers. Theoretically, we provide a comprehensive mathematical\nanalysis of PolyCom, highlighting its enhanced expressivity and efficacy\nrelative to other activation functions. Notably, we demonstrate that networks\nincorporating PolyCom achieve the $\\textbf{optimal approximation rate}$,\nindicating that PolyCom networks require minimal parameters to approximate\ngeneral smooth functions in Sobolev spaces. We conduct empirical experiments on\nthe pre-training configurations of large language models (LLMs), including both\ndense and sparse architectures. By substituting conventional activation\nfunctions with PolyCom, we enable LLMs to capture higher-order interactions\nwithin the data, thus improving performance metrics in terms of accuracy and\nconvergence rates. Extensive experimental results demonstrate the effectiveness\nof our method, showing substantial improvements over other activation\nfunctions. Code is available at https://github.com/BryceZhuo/PolyCom.\n","authors":["Zhijian Zhuo","Ya Wang","Yutao Zeng","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2411.03884v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.15983v1","updated":"2025-03-20T09:30:35Z","published":"2025-03-20T09:30:35Z","title":"InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based\n  Transformer","summary":"  This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks.\n","authors":["Tony Zhang","Rickard Brännvall"],"pdf_url":"https://arxiv.org/pdf/2503.15983v1.pdf","comment":"7 pages, 2 tables"},{"id":"http://arxiv.org/abs/2407.07719v3","updated":"2025-03-20T09:21:43Z","published":"2024-06-17T13:09:25Z","title":"Model-based learning for multi-antenna multi-frequency\n  location-to-channel mapping","summary":"  Years of study of the propagation channel showed a close relation between a\nlocation and the associated communication channel response. The use of a neural\nnetwork to learn the location-to-channel mapping can therefore be envisioned.\nThe Implicit Neural Representation (INR) literature showed that classical\nneural architecture are biased towards learning low-frequency content, making\nthe location-to-channel mapping learning a non-trivial problem. Indeed, it is\nwell known that this mapping is a function rapidly varying with the location,\non the order of the wavelength. This paper leverages the model-based machine\nlearning paradigm to derive a problem-specific neural architecture from a\npropagation channel model. The resulting architecture efficiently overcomes the\nspectral-bias issue. It only learns low-frequency sparse correction terms\nactivating a dictionary of high-frequency components. The proposed architecture\nis evaluated against classical INR architectures on realistic synthetic data,\nshowing much better accuracy. Its mapping learning performance is explained\nbased on the approximated channel model, highlighting the explainability of the\nmodel-based machine learning paradigm.\n","authors":["Baptiste Chatelier","Vincent Corlay","Matthieu Crussière","Luc Le Magoarou"],"pdf_url":"https://arxiv.org/pdf/2407.07719v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.15144v3","updated":"2025-03-20T09:20:09Z","published":"2024-11-06T09:14:26Z","title":"Physically Parameterized Differentiable MUSIC for DoA Estimation with\n  Uncalibrated Arrays","summary":"  Direction of arrival (DoA) estimation is a common sensing problem in radar,\nsonar, audio, and wireless communication systems. It has gained renewed\nimportance with the advent of the integrated sensing and communication\nparadigm. To fully exploit the potential of such sensing systems, it is crucial\nto take into account potential hardware impairments that can negatively impact\nthe obtained performance. This study introduces a joint DoA estimation and\nhardware impairment learning scheme following a model-based approach.\nSpecifically, a differentiable version of the multiple signal classification\n(MUSIC) algorithm is derived, allowing efficient learning of the considered\nimpairments. The proposed approach supports both supervised and unsupervised\nlearning strategies, showcasing its practical potential. Simulation results\nindicate that the proposed method successfully learns significant inaccuracies\nin both antenna locations and complex gains. Additionally, the proposed method\noutperforms the classical MUSIC algorithm in the DoA estimation task.\n","authors":["Baptiste Chatelier","José Miguel Mateos-Ramos","Vincent Corlay","Christian Häger","Matthieu Crussière","Henk Wymeersch","Luc Le Magoarou"],"pdf_url":"https://arxiv.org/pdf/2411.15144v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15972v1","updated":"2025-03-20T09:16:02Z","published":"2025-03-20T09:16:02Z","title":"TVineSynth: A Truncated C-Vine Copula Generator of Synthetic Tabular\n  Data to Balance Privacy and Utility","summary":"  We propose TVineSynth, a vine copula based synthetic tabular data generator,\nwhich is designed to balance privacy and utility, using the vine tree structure\nand its truncation to do the trade-off. Contrary to synthetic data generators\nthat achieve DP by globally adding noise, TVineSynth performs a controlled\napproximation of the estimated data generating distribution, so that it does\nnot suffer from poor utility of the resulting synthetic data for downstream\nprediction tasks. TVineSynth introduces a targeted bias into the vine copula\nmodel that, combined with the specific tree structure of the vine, causes the\nmodel to zero out privacy-leaking dependencies while relying on those that are\nbeneficial for utility. Privacy is here measured with membership (MIA) and\nattribute inference attacks (AIA). Further, we theoretically justify how the\nconstruction of TVineSynth ensures AIA privacy under a natural privacy measure\nfor continuous sensitive attributes. When compared to competitor models, with\nand without DP, on simulated and on real-world data, TVineSynth achieves a\nsuperior privacy-utility balance.\n","authors":["Elisabeth Griesbauer","Claudia Czado","Arnoldo Frigessi","Ingrid Hobæk Haff"],"pdf_url":"https://arxiv.org/pdf/2503.15972v1.pdf","comment":"Accepted at the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2025)"},{"id":"http://arxiv.org/abs/2407.01033v2","updated":"2025-03-20T09:05:26Z","published":"2024-07-01T07:33:00Z","title":"Neural Networks Trained by Weight Permutation are Universal\n  Approximators","summary":"  The universal approximation property is fundamental to the success of neural\nnetworks, and has traditionally been achieved by training networks without any\nconstraints on their parameters. However, recent experimental research proposed\na novel permutation-based training method, which exhibited a desired\nclassification performance without modifying the exact weight values. In this\npaper, we provide a theoretical guarantee of this permutation training method\nby proving its ability to guide a ReLU network to approximate one-dimensional\ncontinuous functions. Our numerical results further validate this method's\nefficiency in regression tasks with various initializations. The notable\nobservations during weight permutation suggest that permutation training can\nprovide an innovative tool for describing network learning behavior.\n","authors":["Yongqiang Cai","Gaohang Chen","Zhonghua Qiao"],"pdf_url":"https://arxiv.org/pdf/2407.01033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15962v1","updated":"2025-03-20T08:59:25Z","published":"2025-03-20T08:59:25Z","title":"Information maximization for a broad variety of multi-armed bandit games","summary":"  Information and free-energy maximization are physics principles that provide\ngeneral rules for an agent to optimize actions in line with specific goals and\npolicies. These principles are the building blocks for designing\ndecision-making policies capable of efficient performance with only partial\ninformation. Notably, the information maximization principle has shown\nremarkable success in the classical bandit problem and has recently been shown\nto yield optimal algorithms for Gaussian and sub-Gaussian reward distributions.\nThis article explores a broad extension of physics-based approaches to more\ncomplex and structured bandit problems. To this end, we cover three distinct\ntypes of bandit problems, where information maximization is adapted and leads\nto strong performance. Since the main challenge of information maximization\nlies in avoiding over-exploration, we highlight how information is tailored at\nvarious levels to mitigate this issue, paving the way for more efficient and\nrobust decision-making strategies.\n","authors":["Alex Barbier-Chebbah","Christian L. Vestergaard","Jean-Baptiste Masson"],"pdf_url":"https://arxiv.org/pdf/2503.15962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01924v2","updated":"2025-03-20T08:49:42Z","published":"2025-03-02T12:07:00Z","title":"TAET: Two-Stage Adversarial Equalization Training on Long-Tailed\n  Distributions","summary":"  Adversarial robustness is a critical challenge in deploying deep neural\nnetworks for real-world applications. While adversarial training is a widely\nrecognized defense strategy, most existing studies focus on balanced datasets,\noverlooking the prevalence of long-tailed distributions in real-world data,\nwhich significantly complicates robustness. This paper provides a comprehensive\nanalysis of adversarial training under long-tailed distributions and identifies\nlimitations in the current state-of-the-art method, AT-BSL, in achieving robust\nperformance under such conditions. To address these challenges, we propose a\nnovel training framework, TAET, which integrates an initial stabilization phase\nfollowed by a stratified equalization adversarial training phase. Additionally,\nprior work on long-tailed robustness has largely ignored the crucial evaluation\nmetric of balanced accuracy. To bridge this gap, we introduce the concept of\nbalanced robustness, a comprehensive metric tailored for assessing robustness\nunder long-tailed distributions. Extensive experiments demonstrate that our\nmethod surpasses existing advanced defenses, achieving significant improvements\nin both memory and computational efficiency. This work represents a substantial\nadvancement in addressing robustness challenges in real-world applications. Our\ncode is available at:\nhttps://github.com/BuhuiOK/TAET-Two-Stage-Adversarial-Equalization-Training-on-Long-Tailed-Distributions.\n","authors":["Wang YuHang","Junkang Guo","Aolei Liu","Kaihao Wang","Zaitong Wu","Zhenyu Liu","Wenfei Yin","Jian Liu"],"pdf_url":"https://arxiv.org/pdf/2503.01924v2.pdf","comment":"Text: 8 pages of main content, 5 pages of appendices have been\n  accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2501.15615v2","updated":"2025-03-20T08:46:37Z","published":"2025-01-26T17:46:40Z","title":"Deterministic Reservoir Computing for Chaotic Time Series Prediction","summary":"  Reservoir Computing was shown in recent years to be useful as efficient to\nlearn networks in the field of time series tasks. Their randomized\ninitialization, a computational benefit, results in drawbacks in theoretical\nanalysis of large random graphs, because of which deterministic variations are\nan still open field of research. Building upon Next-Gen Reservoir Computing and\nthe Temporal Convolution Derived Reservoir Computing, we propose a\ndeterministic alternative to the higher-dimensional mapping therein, TCRC-LM\nand TCRC-CM, utilizing the parametrized but deterministic Logistic mapping and\nChebyshev maps. To further enhance the predictive capabilities in the task of\ntime series forecasting, we propose the novel utilization of the Lobachevsky\nfunction as non-linear activation function.\n  As a result, we observe a new, fully deterministic network being able to\noutperform TCRCs and classical Reservoir Computing in the form of the prominent\nEcho State Networks by up to $99.99\\%$ for the non-chaotic time series and\n$87.13\\%$ for the chaotic ones.\n","authors":["Johannes Viehweg","Constanze Poll","Patrick Mäder"],"pdf_url":"https://arxiv.org/pdf/2501.15615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13180v2","updated":"2025-03-20T08:41:33Z","published":"2025-03-17T13:54:27Z","title":"GC-Fed: Gradient Centralized Federated Learning with Partial Client\n  Participation","summary":"  Federated Learning (FL) enables privacy-preserving multi-source information\nfusion (MSIF) but is challenged by client drift in highly heterogeneous data\nsettings. Many existing drift-mitigation strategies rely on reference-based\ntechniques--such as gradient adjustments or proximal loss--that use historical\nsnapshots (e.g., past gradients or previous global models) as reference points.\nWhen only a subset of clients participates in each training round, these\nhistorical references may not accurately capture the overall data distribution,\nleading to unstable training. In contrast, our proposed Gradient Centralized\nFederated Learning (GC-Fed) employs a hyperplane as a historically independent\nreference point to guide local training and enhance inter-client alignment.\nGC-Fed comprises two complementary components: Local GC, which centralizes\ngradients during local training, and Global GC, which centralizes updates\nduring server aggregation. In our hybrid design, Local GC is applied to\nfeature-extraction layers to harmonize client contributions, while Global GC\nrefines classifier layers to stabilize round-wise performance. Theoretical\nanalysis and extensive experiments on benchmark FL tasks demonstrate that\nGC-Fed effectively mitigates client drift and achieves up to a 20% improvement\nin accuracy under heterogeneous and partial participation conditions.\n","authors":["Jungwon Seo","Ferhat Ozgur Catak","Chunming Rong","Kibeom Hong","Minhoe Kim"],"pdf_url":"https://arxiv.org/pdf/2503.13180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.09988v2","updated":"2025-03-20T08:40:48Z","published":"2025-03-13T02:55:06Z","title":"Label Unbalance in High-frequency Trading","summary":"  In financial trading, return prediction is one of the foundation for a\nsuccessful trading system. By the fast development of the deep learning in\nvarious areas such as graphical processing, natural language, it has also\ndemonstrate significant edge in handling with financial data. While the success\nof the deep learning relies on huge amount of labeled sample, labeling each\ntime/event as profitable or unprofitable, under the transaction cost,\nespecially in the high-frequency trading world, suffers from serious label\nimbalance issue.In this paper, we adopts rigurious end-to-end deep learning\nframework with comprehensive label imbalance adjustment methods and succeed in\npredicting in high-frequency return in the Chinese future market. The code for\nour method is publicly available at\nhttps://github.com/RS2002/Label-Unbalance-in-High-Frequency-Trading .\n","authors":["Zijian Zhao","Xuming Zhang","Jiayu Wen","Mingwen Liu","Xiaoteng Ma"],"pdf_url":"https://arxiv.org/pdf/2503.09988v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2503.15946v1","updated":"2025-03-20T08:38:58Z","published":"2025-03-20T08:38:58Z","title":"Multivariate Time Series Anomaly Detection in Industry 5.0","summary":"  Industry5.0 environments present a critical need for effective anomaly\ndetection methods that can indicate equipment malfunctions, process\ninefficiencies, or potential safety hazards. The ever-increasing sensorization\nof manufacturing lines makes processes more observable, but also poses the\nchallenge of continuously analyzing vast amounts of multivariate time series\ndata. These challenges include data quality since data may contain noise, be\nunlabeled or even mislabeled. A promising approach consists of combining an\nembedding model with other Machine Learning algorithms to enhance the overall\nperformance in detecting anomalies. Moreover, representing time series as\nvectors brings many advantages like higher flexibility and improved ability to\ncapture complex temporal dependencies. We tested our solution in a real\nindustrial use case, using data collected from a Bonfiglioli plant. The results\ndemonstrate that, unlike traditional reconstruction-based autoencoders, which\noften struggle in the presence of sporadic noise, our embedding-based framework\nmaintains high performance across various noise conditions.\n","authors":["Lorenzo Colombi","Michela Vespa","Nicolas Belletti","Matteo Brina","Simon Dahdal","Filippo Tabanelli","Elena Bellodi","Mauro Tortonesi","Cesare Stefanelli","Massimiliano Vignoli"],"pdf_url":"https://arxiv.org/pdf/2503.15946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00101v3","updated":"2025-03-20T08:26:21Z","published":"2024-08-27T12:07:09Z","title":"NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals","summary":"  Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm.\n","authors":["Wei-Bang Jiang","Yansen Wang","Bao-Liang Lu","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2409.00101v3.pdf","comment":"The Thirteenth International Conference on Learning Representations"},{"id":"http://arxiv.org/abs/2503.08738v3","updated":"2025-03-20T08:23:40Z","published":"2025-03-11T06:30:49Z","title":"Shedding Light in Task Decomposition in Program Synthesis: The Driving\n  Force of the Synthesizer Model","summary":"  Task decomposition is a fundamental mechanism in program synthesis, enabling\ncomplex problems to be broken down into manageable subtasks. ExeDec, a\nstate-of-the-art program synthesis framework, employs this approach by\ncombining a Subgoal Model for decomposition and a Synthesizer Model for program\ngeneration to facilitate compositional generalization. In this work, we develop\nREGISM, an adaptation of ExeDec that removes decomposition guidance and relies\nsolely on iterative execution-driven synthesis. By comparing these two\nexemplary approaches-ExeDec, which leverages task decomposition, and REGISM,\nwhich does not-we investigate the interplay between task decomposition and\nprogram generation. Our findings indicate that ExeDec exhibits significant\nadvantages in length generalization and concept composition tasks, likely due\nto its explicit decomposition strategies. At the same time, REGISM frequently\nmatches or surpasses ExeDec's performance across various scenarios, with its\nsolutions often aligning more closely with ground truth decompositions. These\nobservations highlight the importance of repeated execution-guided synthesis in\ndriving task-solving performance, even within frameworks that incorporate\nexplicit decomposition strategies. Our analysis suggests that task\ndecomposition approaches like ExeDec hold significant potential for advancing\nprogram synthesis, though further work is needed to clarify when and why these\nstrategies are most effective.\n","authors":["Janis Zenkner","Tobias Sesterhenn","Christian Bartelt"],"pdf_url":"https://arxiv.org/pdf/2503.08738v3.pdf","comment":"Accepted at ICLR 2025 Workshop Deep Learning for Code"},{"id":"http://arxiv.org/abs/2503.15928v1","updated":"2025-03-20T08:08:17Z","published":"2025-03-20T08:08:17Z","title":"Sample-Efficient Bayesian Transfer Learning for Online Machine Parameter\n  Optimization","summary":"  Correctly setting the parameters of a production machine is essential to\nimprove product quality, increase efficiency, and reduce production costs while\nalso supporting sustainability goals. Identifying optimal parameters involves\nan iterative process of producing an object and evaluating its quality.\nMinimizing the number of iterations is, therefore, desirable to reduce the\ncosts associated with unsuccessful attempts. This work introduces a method to\noptimize the machine parameters in the system itself using a \\ac{BO} algorithm.\nBy leveraging existing machine data, we use a transfer learning approach in\norder to identify an optimum with minimal iterations, resulting in a\ncost-effective transfer learning algorithm. We validate our approach on a laser\nmachine for cutting sheet metal in the real world.\n","authors":["Philipp Wagner","Tobias Nagel","Philipp Leube","Marco F. Huber"],"pdf_url":"https://arxiv.org/pdf/2503.15928v1.pdf","comment":"Accepted in IEEE Conference on Artificial Intelligence, 2025"},{"id":"http://arxiv.org/abs/2503.15918v1","updated":"2025-03-20T07:52:19Z","published":"2025-03-20T07:52:19Z","title":"Denoising-based Contractive Imitation Learning","summary":"  A fundamental challenge in imitation learning is the \\emph{covariate shift}\nproblem. Existing methods to mitigate covariate shift often require additional\nexpert interactions, access to environment dynamics, or complex adversarial\ntraining, which may not be practical in real-world applications. In this paper,\nwe propose a simple yet effective method (DeCIL) to mitigate covariate shift by\nincorporating a denoising mechanism that enhances the contraction properties of\nthe state transition mapping. Our approach involves training two neural\nnetworks: a dynamics model ( f ) that predicts the next state from the current\nstate, and a joint state-action denoising policy network ( d ) that refines\nthis state prediction via denoising and outputs the corresponding action. We\nprovide theoretical analysis showing that the denoising network acts as a local\ncontraction mapping, reducing the error propagation of the state transition and\nimproving stability. Our method is straightforward to implement and can be\neasily integrated with existing imitation learning frameworks without requiring\nadditional expert data or complex modifications to the training procedure.\nEmpirical results demonstrate that our approach effectively improves success\nrate of various imitation learning tasks under noise perturbation.\n","authors":["Macheng Shen","Jishen Peng","Zefang Huang"],"pdf_url":"https://arxiv.org/pdf/2503.15918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15786v2","updated":"2025-03-20T07:50:44Z","published":"2024-07-22T16:46:33Z","title":"LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement\n  Learning","summary":"  Recent advances in reinforcement learning (RL) have predominantly leveraged\nneural network policies for decision-making, yet these models often lack\ninterpretability, posing challenges for stakeholder comprehension and trust.\nConcept bottleneck models offer an interpretable alternative by integrating\nhuman-understandable concepts into policies. However, prior work assumes that\nconcept annotations are readily available during training. For RL, this\nrequirement poses a significant limitation: it necessitates continuous\nreal-time concept annotation, which either places an impractical burden on\nhuman annotators or incurs substantial costs in API queries and inference time\nwhen employing automated labeling methods. To overcome this limitation, we\nintroduce a novel training scheme that enables RL agents to efficiently learn a\nconcept-based policy by only querying annotators to label a small set of data.\nOur algorithm, LICORICE, involves three main contributions: interleaving\nconcept learning and RL training, using an ensemble to actively select\ninformative data points for labeling, and decorrelating the concept data. We\nshow how LICORICE reduces human labeling efforts to 500 or fewer concept labels\nin three environments, and 5000 or fewer in two more complex environments, all\nat no cost to performance. We also explore the use of VLMs as automated concept\nannotators, finding them effective in some cases but imperfect in others. Our\nwork significantly reduces the annotation burden for interpretable RL, making\nit more practical for real-world applications that necessitate transparency.\n","authors":["Zhuorui Ye","Stephanie Milani","Geoffrey J. Gordon","Fei Fang"],"pdf_url":"https://arxiv.org/pdf/2407.15786v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2311.07595v2","updated":"2025-03-20T07:42:28Z","published":"2023-11-10T10:21:09Z","title":"A Diagnosis and Treatment of Liver Diseases: Integrating Batch\n  Processing, Rule-Based Event Detection and Explainable Artificial\n  Intelligence","summary":"  Liver diseases pose a significant global health burden, impacting many\nindividuals and having substantial economic and social consequences. Rising\nliver problems are considered a fatal disease in many countries, such as Egypt\nand Moldova. This study aims to develop a diagnosis and treatment model for\nliver disease using Basic Formal Ontology (BFO), Patient Clinical Data (PCD)\nontology, and detection rules derived from a decision tree algorithm. For the\ndevelopment of the ontology, the National Viral Hepatitis Control Program\n(NVHCP) guidelines were used, which made the ontology more accurate and\nreliable. The Apache Jena framework uses batch processing to detect events\nbased on these rules. Based on the event detected, queries can be directly\nprocessed using SPARQL. We convert these Decision Tree (DT) and medical\nguidelines-based rules into Semantic Web Rule Language (SWRL) to operationalize\nthe ontology. Using this SWRL in the ontology to predict different types of\nliver disease with the help of the Pellet and Drools inference engines in\nProtege Tools, a total of 615 records were taken from different liver diseases.\nAfter inferring the rules, the result can be generated for the patient\naccording to the rules, and other patient-related details, along with different\nprecautionary suggestions, can be obtained based on these results. These rules\ncan make suggestions more accurate with the help of Explainable Artificial\nIntelligence (XAI) with open API-based suggestions. When the patient has\nprescribed a medical test, the model accommodates this result using optical\ncharacter recognition (OCR), and the same process applies when the patient has\nprescribed a further medical suggestion according to the test report. These\nmodels combine to form a comprehensive Decision Support System (DSS) for the\ndiagnosis of liver disease.\n","authors":["Ritesh Chandra","Sadhana Tiwari","Satyam Rastogi","Sonali Agarwal"],"pdf_url":"https://arxiv.org/pdf/2311.07595v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14640v2","updated":"2025-03-20T07:32:22Z","published":"2024-10-18T17:41:19Z","title":"HR-Bandit: Human-AI Collaborated Linear Recourse Bandit","summary":"  Human doctors frequently recommend actionable recourses that allow patients\nto modify their conditions to access more effective treatments. Inspired by\nsuch healthcare scenarios, we propose the Recourse Linear UCB\n($\\textsf{RLinUCB}$) algorithm, which optimizes both action selection and\nfeature modifications by balancing exploration and exploitation. We further\nextend this to the Human-AI Linear Recourse Bandit ($\\textsf{HR-Bandit}$),\nwhich integrates human expertise to enhance performance. $\\textsf{HR-Bandit}$\noffers three key guarantees: (i) a warm-start guarantee for improved initial\nperformance, (ii) a human-effort guarantee to minimize required human\ninteractions, and (iii) a robustness guarantee that ensures sublinear regret\neven when human decisions are suboptimal. Empirical results, including a\nhealthcare case study, validate its superior performance against existing\nbenchmarks.\n","authors":["Junyu Cao","Ruijiang Gao","Esmaeil Keyvanshokooh"],"pdf_url":"https://arxiv.org/pdf/2410.14640v2.pdf","comment":"18 pages, AISTATS 25"},{"id":"http://arxiv.org/abs/2402.09488v2","updated":"2025-03-20T07:27:08Z","published":"2024-02-14T09:07:00Z","title":"Intelligent Agricultural Greenhouse Control System Based on Internet of\n  Things and Machine Learning","summary":"  This study endeavors to conceptualize and execute a sophisticated\nagricultural greenhouse control system grounded in the amalgamation of the\nInternet of Things (IoT) and machine learning. Through meticulous monitoring of\nintrinsic environmental parameters within the greenhouse and the integration of\nmachine learning algorithms, the conditions within the greenhouse are aptly\nmodulated. The envisaged outcome is an enhancement in crop growth efficiency\nand yield, accompanied by a reduction in resource wastage. In the backdrop of\nescalating global population figures and the escalating exigencies of climate\nchange, agriculture confronts unprecedented challenges. Conventional\nagricultural paradigms have proven inadequate in addressing the imperatives of\nfood safety and production efficiency. Against this backdrop, greenhouse\nagriculture emerges as a viable solution, proffering a controlled milieu for\ncrop cultivation to augment yields, refine quality, and diminish reliance on\nnatural resources [b1]. Nevertheless, greenhouse agriculture contends with a\ngamut of challenges. Traditional greenhouse management strategies, often\ngrounded in experiential knowledge and predefined rules, lack targeted\npersonalized regulation, thereby resulting in resource inefficiencies. The\nexigencies of real-time monitoring and precise control of the greenhouse's\ninternal environment gain paramount importance with the burgeoning scale of\nagriculture. To redress this challenge, the study introduces IoT technology and\nmachine learning algorithms into greenhouse agriculture, aspiring to institute\nan intelligent agricultural greenhouse control system conducive to augmenting\nthe efficiency and sustainability of agricultural production.\n","authors":["Cangqing Wang","Jiangchuan Gong"],"pdf_url":"https://arxiv.org/pdf/2402.09488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19942v2","updated":"2025-03-20T07:24:19Z","published":"2024-11-29T18:58:17Z","title":"FreeCloth: Free-form Generation Enhances Challenging Clothed Human\n  Modeling","summary":"  Achieving realistic animated human avatars requires accurate modeling of\npose-dependent clothing deformations. Existing learning-based methods heavily\nrely on the Linear Blend Skinning (LBS) of minimally-clothed human models like\nSMPL to model deformation. However, they struggle to handle loose clothing,\nsuch as long dresses, where the canonicalization process becomes ill-defined\nwhen the clothing is far from the body, leading to disjointed and fragmented\nresults. To overcome this limitation, we propose FreeCloth, a novel hybrid\nframework to model challenging clothed humans. Our core idea is to use\ndedicated strategies to model different regions, depending on whether they are\nclose to or distant from the body. Specifically, we segment the human body into\nthree categories: unclothed, deformed, and generated. We simply replicate\nunclothed regions that require no deformation. For deformed regions close to\nthe body, we leverage LBS to handle the deformation. As for the generated\nregions, which correspond to loose clothing areas, we introduce a novel\nfree-form, part-aware generator to model them, as they are less affected by\nmovements. This free-form generation paradigm brings enhanced flexibility and\nexpressiveness to our hybrid framework, enabling it to capture the intricate\ngeometric details of challenging loose clothing, such as skirts and dresses.\nExperimental results on the benchmark dataset featuring loose clothing\ndemonstrate that FreeCloth achieves state-of-the-art performance with superior\nvisual fidelity and realism, particularly in the most challenging cases.\n","authors":["Hang Ye","Xiaoxuan Ma","Hai Ci","Wentao Zhu","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2411.19942v2.pdf","comment":"23 pages, 26 figures"},{"id":"http://arxiv.org/abs/2411.16154v2","updated":"2025-03-20T07:05:27Z","published":"2024-11-25T07:26:22Z","title":"DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders","summary":"  Self-supervised learning (SSL) is pervasively exploited in training\nhigh-quality upstream encoders with a large amount of unlabeled data. However,\nit is found to be susceptible to backdoor attacks merely via polluting a small\nportion of training data. The victim encoders associate triggered inputs with\ntarget embeddings, e.g., mapping a triggered cat image to an airplane\nembedding, such that the downstream tasks inherit unintended behaviors when the\ntrigger is activated. Emerging backdoor attacks have shown great threats across\ndifferent SSL paradigms such as contrastive learning and CLIP, yet limited\nresearch is devoted to defending against such attacks, and existing defenses\nfall short in detecting advanced stealthy backdoors. To address the\nlimitations, we propose a novel detection mechanism, DeDe, which detects the\nactivation of backdoor mappings caused by triggered inputs on victim encoders.\nSpecifically, DeDe trains a decoder for any given SSL encoder using an\nauxiliary dataset (which can be out-of-distribution or even slightly poisoned),\nso that for any triggered input that misleads the encoder into the target\nembedding, the decoder generates an output image significantly different from\nthe input. DeDe leverages the discrepancy between the input and the decoded\noutput to identify potential backdoor misbehavior during inference. We\nempirically evaluate DeDe on both contrastive learning and CLIP models against\nvarious types of backdoor attacks. Our results demonstrate promising detection\neffectiveness over various advanced attacks and superior performance compared\nover state-of-the-art detection methods.\n","authors":["Sizai Hou","Songze Li","Duanyi Yao"],"pdf_url":"https://arxiv.org/pdf/2411.16154v2.pdf","comment":"To appear on CVPR 2025"},{"id":"http://arxiv.org/abs/2503.15902v1","updated":"2025-03-20T07:03:13Z","published":"2025-03-20T07:03:13Z","title":"On the Limits of Applying Graph Transformers for Brain Connectome\n  Classification","summary":"  Brain connectomes offer detailed maps of neural connections within the brain.\nRecent studies have proposed novel connectome graph datasets and attempted to\nimprove connectome classification by using graph deep learning. With recent\nadvances demonstrating transformers' ability to model intricate relationships\nand outperform in various domains, this work explores their performance on the\nnovel NeuroGraph benchmark datasets and synthetic variants derived from\nprobabilistically removing edges to simulate noisy data. Our findings suggest\nthat graph transformers offer no major advantage over traditional GNNs on this\ndataset. Furthermore, both traditional and transformer GNN models maintain\naccuracy even with all edges removed, suggesting that the dataset's graph\nstructures may not significantly impact predictions. We propose further\nassessing NeuroGraph as a brain connectome benchmark, emphasizing the need for\nwell-curated datasets and improved preprocessing strategies to obtain\nmeaningful edge connections.\n","authors":["Jose Lara-Rangel","Clare Heinbaugh"],"pdf_url":"https://arxiv.org/pdf/2503.15902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16819v4","updated":"2025-03-20T07:02:30Z","published":"2024-11-25T16:41:45Z","title":"Pathways on the Image Manifold: Image Editing via Video Generation","summary":"  Recent advances in image editing, driven by image diffusion models, have\nshown remarkable progress. However, significant challenges remain, as these\nmodels often struggle to follow complex edit instructions accurately and\nfrequently compromise fidelity by altering key elements of the original image.\nSimultaneously, video generation has made remarkable strides, with models that\neffectively function as consistent and continuous world simulators. In this\npaper, we propose merging these two fields by utilizing image-to-video models\nfor image editing. We reformulate image editing as a temporal process, using\npretrained video models to create smooth transitions from the original image to\nthe desired edit. This approach traverses the image manifold continuously,\nensuring consistent edits while preserving the original image's key aspects.\nOur approach achieves state-of-the-art results on text-based image editing,\ndemonstrating significant improvements in both edit accuracy and image\npreservation. Visit our project page at\nhttps://rotsteinnoam.github.io/Frame2Frame.\n","authors":["Noam Rotstein","Gal Yona","Daniel Silver","Roy Velich","David Bensaïd","Ron Kimmel"],"pdf_url":"https://arxiv.org/pdf/2411.16819v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15901v1","updated":"2025-03-20T07:00:01Z","published":"2025-03-20T07:00:01Z","title":"A multi-model approach using XAI and anomaly detection to predict\n  asteroid hazards","summary":"  The potential for catastrophic collision makes near-Earth asteroids (NEAs) a\nserious concern. Planetary defense depends on accurately classifying\npotentially hazardous asteroids (PHAs), however the complexity of the data\nhampers conventional techniques. This work offers a sophisticated method for\naccurately predicting hazards by combining machine learning, deep learning,\nexplainable AI (XAI), and anomaly detection. Our approach extracts essential\nparameters like size, velocity, and trajectory from historical and real-time\nasteroid data. A hybrid algorithm improves prediction accuracy by combining\nseveral cutting-edge models. A forecasting module predicts future asteroid\nbehavior, and Monte Carlo simulations evaluate the likelihood of collisions.\nTimely mitigation is made possible by a real-time alarm system that notifies\nworldwide monitoring stations. This technique enhances planetary defense\nefforts by combining real-time alarms with sophisticated predictive modeling.\n","authors":["Amit Kumar Mondal","Nafisha Aslam","Prasenjit Maji","Hemanta Kumar Mondal"],"pdf_url":"https://arxiv.org/pdf/2503.15901v1.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.18639v3","updated":"2025-03-20T06:55:44Z","published":"2024-10-24T10:58:17Z","title":"Diffusion Attribution Score: Evaluating Training Data Influence in\n  Diffusion Model","summary":"  As diffusion models become increasingly popular, the misuse of copyrighted\nand private images has emerged as a major concern. One promising solution to\nmitigate this issue is identifying the contribution of specific training\nsamples in generative models, a process known as data attribution. Existing\ndata attribution methods for diffusion models typically quantify the\ncontribution of a training sample by evaluating the change in diffusion loss\nwhen the sample is included or excluded from the training process. However, we\nargue that the direct usage of diffusion loss cannot represent such a\ncontribution accurately due to the calculation of diffusion loss. Specifically,\nthese approaches measure the divergence between predicted and ground truth\ndistributions, which leads to an indirect comparison between the predicted\ndistributions and cannot represent the variances between model behaviors. To\naddress these issues, we aim to measure the direct comparison between predicted\ndistributions with an attribution score to analyse the training sample\nimportance, which is achieved by Diffusion Attribution Score (\\textit{DAS}).\nUnderpinned by rigorous theoretical analysis, we elucidate the effectiveness of\nDAS. Additionally, we explore strategies to accelerate DAS calculations,\nfacilitating its application to large-scale diffusion models. Our extensive\nexperiments across various datasets and diffusion models demonstrate that DAS\nsignificantly surpasses previous benchmarks in terms of the linear\ndata-modelling score, establishing new state-of-the-art performance. Code is\navailable at \\hyperlink{here}{https://github.com/Jinxu-Lin/DAS}.\n","authors":["Jinxu Lin","Linwei Tao","Minjing Dong","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2410.18639v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15897v1","updated":"2025-03-20T06:49:33Z","published":"2025-03-20T06:49:33Z","title":"Learning 3D Scene Analogies with Neural Contextual Scene Maps","summary":"  Understanding scene contexts is crucial for machines to perform tasks and\nadapt prior knowledge in unseen or noisy 3D environments. As data-driven\nlearning is intractable to comprehensively encapsulate diverse ranges of\nlayouts and open spaces, we propose teaching machines to identify relational\ncommonalities in 3D spaces. Instead of focusing on point-wise or object-wise\nrepresentations, we introduce 3D scene analogies, which are smooth maps between\n3D scene regions that align spatial relationships. Unlike well-studied single\ninstance-level maps, these scene-level maps smoothly link large scene regions,\npotentially enabling unique applications in trajectory transfer in AR/VR, long\ndemonstration transfer for imitation learning, and context-aware object\nrearrangement. To find 3D scene analogies, we propose neural contextual scene\nmaps, which extract descriptor fields summarizing semantic and geometric\ncontexts, and holistically align them in a coarse-to-fine manner for map\nestimation. This approach reduces reliance on individual feature points, making\nit robust to input noise or shape variations. Experiments demonstrate the\neffectiveness of our approach in identifying scene analogies and transferring\ntrajectories or object placements in diverse indoor scenes, indicating its\npotential for robotics and AR/VR applications.\n","authors":["Junho Kim","Gwangtak Bae","Eun Sun Lee","Young Min Kim"],"pdf_url":"https://arxiv.org/pdf/2503.15897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08748v2","updated":"2025-03-20T06:40:17Z","published":"2025-03-11T10:50:07Z","title":"Mirror Descent and Novel Exponentiated Gradient Algorithms Using\n  Trace-Form Entropies and Deformed Logarithms","summary":"  In this paper we propose and investigate a wide class of Mirror Descent\nupdates (MD) and associated novel Generalized Exponentiated Gradient (GEG)\nalgorithms by exploiting various trace-form entropies and associated deformed\nlogarithms and their inverses - deformed (generalized) exponential functions.\nThe proposed algorithms can be considered as extension of entropic MD and\ngeneralization of multiplicative updates. In the literature, there exist\nnowadays over fifty mathematically well defined generalized entropies, so\nimpossible to exploit all of them in one research paper. So we focus on a few\nselected most popular entropies and associated logarithms like the Tsallis,\nKaniadakis and Sharma-Taneja-Mittal and some of their extension like Tempesta\nor Kaniadakis-Scarfone entropies. The shape and properties of the deformed\nlogarithms and their inverses are tuned by one or more hyperparameters. By\nlearning these hyperparameters, we can adapt to distribution of training data,\nwhich can be designed to the specific geometry of the optimization problem,\nleading to potentially faster convergence and better performance. The using\ngeneralized entropies and associated deformed logarithms in the Bregman\ndivergence, used as a regularization term, provides some new insight into\nexponentiated gradient descent updates.\n","authors":["Andrzej Cichocki","Toshihisa Tanaka","Sergio Cruces"],"pdf_url":"https://arxiv.org/pdf/2503.08748v2.pdf","comment":"22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2411.19930v2","updated":"2025-03-20T06:35:22Z","published":"2024-11-29T18:42:28Z","title":"On Domain-Specific Post-Training for Multimodal Large Language Models","summary":"  Adapting general multimodal large language models (MLLMs) to specific\ndomains, such as scientific and industrial fields, is highly significant in\npromoting their practical applications. This paper systematically investigates\ndomain adaptation of MLLMs through post-training, focusing on data synthesis,\ntraining pipelines, and task evaluation. (1) Data Synthesis: Using only\nopen-source models, we develop a generate-then-filter pipeline that curates\ndiverse visual instruction tasks based on domain-specific image-caption pairs.\nThe resulting data surpass the data synthesized by manual rules or strong\nclosed-source models (e.g., GPT-4V) in enhancing domain-specific performance.\n(2) Training Pipeline: While the two-stage training--initially on image-caption\npairs followed by visual instruction tasks--is commonly adopted for developing\ngeneral MLLMs, we apply a single-stage training pipeline to enhance task\ndiversity for domain-specific post-training. (3) Task Evaluation: We conduct\nextensive experiments in high-impact domains such as biomedicine, food, and\nremote sensing, by post-training a variety of MLLMs and then evaluating MLLM\nperformance on various domain-specific tasks. Furthermore, we fully open-source\nour models, code, and data to encourage future research in this area.\n","authors":["Daixuan Cheng","Shaohan Huang","Ziyu Zhu","Xintong Zhang","Wayne Xin Zhao","Zhongzhi Luan","Bo Dai","Zhenliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.19930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15890v1","updated":"2025-03-20T06:27:35Z","published":"2025-03-20T06:27:35Z","title":"Time After Time: Deep-Q Effect Estimation for Interventions on When and\n  What to do","summary":"  Problems in fields such as healthcare, robotics, and finance requires\nreasoning about the value both of what decision or action to take and when to\ntake it. The prevailing hope is that artificial intelligence will support such\ndecisions by estimating the causal effect of policies such as how to treat\npatients or how to allocate resources over time. However, existing methods for\nestimating the effect of a policy struggle with \\emph{irregular time}. They\neither discretize time, or disregard the effect of timing policies. We present\na new deep-Q algorithm that estimates the effect of both when and what to do\ncalled Earliest Disagreement Q-Evaluation (EDQ). EDQ makes use of recursion for\nthe Q-function that is compatible with flexible sequence models, such as\ntransformers. EDQ provides accurate estimates under standard assumptions. We\nvalidate the approach through experiments on survival time and tumor growth\ntasks.\n","authors":["Yoav Wald","Mark Goldstein","Yonathan Efroni","Wouter A. C. van Amsterdam","Rajesh Ranganath"],"pdf_url":"https://arxiv.org/pdf/2503.15890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15889v1","updated":"2025-03-20T06:27:09Z","published":"2025-03-20T06:27:09Z","title":"LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized\n  Test-Time Adaptation on Edge Devices","summary":"  While there are many advantages to deploying machine learning models on edge\ndevices, the resource constraints of mobile platforms, the dynamic nature of\nthe environment, and differences between the distribution of training versus\nin-the-wild data make such deployments challenging. Current test-time\nadaptation methods are often memory-intensive and not designed to be\nquantization-compatible or deployed on low-resource devices. To address these\nchallenges, we present LeanTTA, a novel backpropagation-free and stateless\nframework for quantized test-time adaptation tailored to edge devices. Our\napproach minimizes computational costs by dynamically updating normalization\nstatistics without backpropagation, which frees LeanTTA from the common pitfall\nof relying on large batches and historical data, making our method robust to\nrealistic deployment scenarios. Our approach is the first to enable further\ncomputational gains by combining partial adaptation with quantized module\nfusion. We validate our framework across sensor modalities, demonstrating\nsignificant improvements over state-of-the-art TTA methods, including a 15.7%\nerror reduction, peak memory usage of only 11.2MB for ResNet18, and fast\nadaptation within an order-of-magnitude of normal inference speeds on-device.\nLeanTTA provides a robust solution for achieving the right trade offs between\naccuracy and system efficiency in edge deployments, addressing the unique\nchallenges posed by limited data and varied operational conditions.\n","authors":["Cynthia Dong","Hong Jia","Young D. Kwon","Georgios Rizos","Cecilia Mascolo"],"pdf_url":"https://arxiv.org/pdf/2503.15889v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.15886v1","updated":"2025-03-20T06:20:13Z","published":"2025-03-20T06:20:13Z","title":"Enhancing Zero-Shot Image Recognition in Vision-Language Models through\n  Human-like Concept Guidance","summary":"  In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods.\n","authors":["Hui Liu","Wenya Wang","Kecheng Chen","Jie Liu","Yibing Liu","Tiexin Qin","Peisong He","Xinghao Jiang","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2503.15886v1.pdf","comment":"21 pages, 7 figures 7 tables"},{"id":"http://arxiv.org/abs/2503.15880v1","updated":"2025-03-20T06:05:36Z","published":"2025-03-20T06:05:36Z","title":"InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced\n  Preference Optimization","summary":"  Direct Preference Optimization (DPO) optimizes language models to align with\nhuman preferences. Utilizing on-policy samples, generated directly by the\npolicy model, typically results in better performance due to its distribution\nconsistency with the model compared to off-policy samples. This paper\nidentifies the quality of candidate preference samples as another critical\nfactor. While the quality of on-policy data is inherently constrained by the\ncapabilities of the policy model, off-policy data, which can be derived from\ndiverse sources, offers greater potential for quality despite experiencing\ndistribution shifts. However, current research mostly relies on on-policy data\nand neglects the value of off-policy data in terms of data quality, due to the\nchallenge posed by distribution shift. In this paper, we propose InCo-DPO, an\nefficient method for synthesizing preference data by integrating on-policy and\noff-policy data, allowing dynamic adjustments to balance distribution shifts\nand data quality, thus finding an optimal trade-off. Consequently, InCo-DPO\novercomes the limitations of distribution shifts in off-policy data and the\nquality constraints of on-policy data. We evaluated InCo-DPO with the\nAlpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate\nthat our approach not only outperforms both on-policy and off-policy data but\nalso achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the\nvanilla DPO using Gemma-2 model.\n","authors":["Yunan Wang","Jijie Li","Bo-Wen Zhang","Liangdong Wang","Guang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.15880v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15870v1","updated":"2025-03-20T05:48:48Z","published":"2025-03-20T05:48:48Z","title":"FedSAF: A Federated Learning Framework for Enhanced Gastric Cancer\n  Detection and Privacy Preservation","summary":"  Gastric cancer is one of the most commonly diagnosed cancers and has a high\nmortality rate. Due to limited medical resources, developing machine learning\nmodels for gastric cancer recognition provides an efficient solution for\nmedical institutions. However, such models typically require large sample sizes\nfor training and testing, which can challenge patient privacy. Federated\nlearning offers an effective alternative by enabling model training across\nmultiple institutions without sharing sensitive patient data. This paper\naddresses the limited sample size of publicly available gastric cancer data\nwith a modified data processing method. This paper introduces FedSAF, a novel\nfederated learning algorithm designed to improve the performance of existing\nmethods, particularly in non-independent and identically distributed (non-IID)\ndata scenarios. FedSAF incorporates attention-based message passing and the\nFisher Information Matrix to enhance model accuracy, while a model splitting\nfunction reduces computation and transmission costs. Hyperparameter tuning and\nablation studies demonstrate the effectiveness of this new algorithm, showing\nimprovements in test accuracy on gastric cancer datasets, with FedSAF\noutperforming existing federated learning methods like FedAMP, FedAvg, and\nFedProx. The framework's robustness and generalization ability were further\nvalidated across additional datasets (SEED, BOT, FashionMNIST, and CIFAR-10),\nachieving high performance in diverse environments.\n","authors":["Yuxin Miao","Xinyuan Yang","Hongda Fan","Yichun Li","Yishu Hong","Xiechen Guo","Ali Braytee","Weidong Huang","Ali Anaissi"],"pdf_url":"https://arxiv.org/pdf/2503.15870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15865v1","updated":"2025-03-20T05:36:33Z","published":"2025-03-20T05:36:33Z","title":"Active management of battery degradation in wireless sensor network\n  using deep reinforcement learning for group battery replacement","summary":"  Wireless sensor networks (WSNs) have become a promising solution for\nstructural health monitoring (SHM), especially in hard-to-reach or remote\nlocations. Battery-powered WSNs offer various advantages over wired systems,\nhowever limited battery life has always been one of the biggest obstacles in\npractical use of the WSNs, regardless of energy harvesting methods. While\nvarious methods have been studied for battery health management, existing\nmethods exclusively aim to extend lifetime of individual batteries, lacking a\nsystem level view. A consequence of applying such methods is that batteries in\na WSN tend to fail at different times, posing significant difficulty on\nplanning and scheduling of battery replacement trip. This study investigate a\ndeep reinforcement learning (DRL) method for active battery degradation\nmanagement by optimizing duty cycle of WSNs at the system level. This active\nmanagement strategy effectively reduces earlier failure of battery individuals\nwhich enable group replacement without sacrificing WSN performances. A\nsimulated environment based on a real-world WSN setup was developed to train a\nDRL agent and learn optimal duty cycle strategies. The performance of the\nstrategy was validated in a long-term setup with various network sizes,\ndemonstrating its efficiency and scalability.\n","authors":["Jong-Hyun Jeonga","Hongki Jo","Qiang Zhou","Tahsin Afroz Hoque Nishat","Lang Wu"],"pdf_url":"https://arxiv.org/pdf/2503.15865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18921v2","updated":"2025-03-20T05:23:42Z","published":"2024-07-09T13:47:05Z","title":"Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey","summary":"  On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.\n","authors":["Guanqiao Qu","Qiyuan Chen","Wei Wei","Zheng Lin","Xianhao Chen","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2407.18921v2.pdf","comment":"42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials"},{"id":"http://arxiv.org/abs/2503.15853v1","updated":"2025-03-20T05:15:36Z","published":"2025-03-20T05:15:36Z","title":"Network Embedding Exploration Tool (NEExT)","summary":"  Many real-world and artificial systems and processes can be represented as\ngraphs. Some examples of such systems include social networks, financial\ntransactions, supply chains, and molecular structures. In many of these cases,\none needs to consider a collection of graphs, rather than a single network.\nThis could be a collection of distinct but related graphs, such as different\nprotein structures or graphs resulting from dynamic processes on the same\nnetwork. Examples of the latter include the evolution of social networks,\ncommunity-induced graphs, or ego-nets around various nodes. A significant\nchallenge commonly encountered is the absence of ground-truth labels for graphs\nor nodes, necessitating the use of unsupervised techniques to analyze such\nsystems. Moreover, even when ground-truth labels are available, many existing\ngraph machine learning methods depend on complex deep learning models,\ncomplicating model explainability and interpretability. To address some of\nthese challenges, we have introduced NEExT (Network Embedding Exploration Tool)\nfor embedding collections of graphs via user-defined node features. The\nadvantages of the framework are twofold: (i) the ability to easily define your\nown interpretable node-based features in view of the task at hand, and (ii)\nfast embedding of graphs provided by the Vectorizers library. In this paper, we\ndemonstrate the usefulness of NEExT on collections of synthetic and real-world\ngraphs. For supervised tasks, we demonstrate that performance in graph\nclassification tasks could be achieved similarly to other state-of-the-art\ntechniques while maintaining model interpretability. Furthermore, our framework\ncan also be used to generate high-quality embeddings in an unsupervised way,\nwhere target variables are not available.\n","authors":["Ashkan Dehghan","Paweł Prałat","François Théberge"],"pdf_url":"https://arxiv.org/pdf/2503.15853v1.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.05655v2","updated":"2025-03-20T05:10:58Z","published":"2024-10-08T03:10:55Z","title":"Efficient Policy Evaluation with Safety Constraint for Reinforcement\n  Learning","summary":"  In reinforcement learning, classic on-policy evaluation methods often suffer\nfrom high variance and require massive online data to attain the desired\naccuracy. Previous studies attempt to reduce evaluation variance by searching\nfor or designing proper behavior policies to collect data. However, these\napproaches ignore the safety of such behavior policies -- the designed behavior\npolicies have no safety guarantee and may lead to severe damage during online\nexecutions. In this paper, to address the challenge of reducing variance while\nensuring safety simultaneously, we propose an optimal variance-minimizing\nbehavior policy under safety constraints. Theoretically, while ensuring safety\nconstraints, our evaluation method is unbiased and has lower variance than\non-policy evaluation. Empirically, our method is the only existing method to\nachieve both substantial variance reduction and safety constraint satisfaction.\nFurthermore, we show our method is even superior to previous methods in both\nvariance reduction and execution safety.\n","authors":["Claire Chen","Shuze Daniel Liu","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05655v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2410.02226"},{"id":"http://arxiv.org/abs/2410.07627v2","updated":"2025-03-20T05:08:24Z","published":"2024-10-10T05:43:07Z","title":"Automatic Curriculum Expert Iteration for Reliable LLM Reasoning","summary":"  Hallucinations (i.e., generating plausible but inaccurate content) and\nlaziness (i.e. excessive refusals or defaulting to \"I don't know\") persist as\nmajor challenges in LLM reasoning. Current efforts to reduce hallucinations\nprimarily focus on factual errors in knowledge-grounded tasks, often neglecting\nhallucinations related to faulty reasoning. Meanwhile, some approaches render\nLLMs overly conservative, limiting their problem-solving capabilities. To\nmitigate hallucination and laziness in reasoning tasks, we propose Automatic\nCurriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align\nresponses to the model's capabilities--assertively answering within its limits\nand declining when tasks exceed them. In our method, Expert Iteration explores\nthe reasoning trajectories near the LLM policy, guiding incorrect paths back on\ntrack to reduce compounding errors and improve robustness; it also promotes\nappropriate \"I don't know\" responses after sufficient reasoning attempts. The\ncurriculum automatically adjusts rewards, incentivizing extended reasoning\nbefore acknowledging incapability, thereby pushing the limits of LLM reasoning\nand aligning its behaviour with these limits. We compare Auto-CEI with various\nSOTA baselines across logical reasoning, mathematics, and planning tasks, where\nAuto-CEI achieves superior alignment by effectively balancing assertiveness and\nconservativeness. The code is available at\nhttps://github.com/SalesforceAIResearch/Auto-CEI .\n","authors":["Zirui Zhao","Hanze Dong","Amrita Saha","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.07627v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2503.07667v2","updated":"2025-03-20T05:05:56Z","published":"2025-03-09T01:45:05Z","title":"CLIMB: Data Foundations for Large Scale Multimodal Clinical Foundation\n  Models","summary":"  Recent advances in clinical AI have enabled remarkable progress across many\nclinical domains. However, existing benchmarks and models are primarily limited\nto a small set of modalities and tasks, which hinders the development of\nlarge-scale multimodal methods that can make holistic assessments of patient\nhealth and well-being. To bridge this gap, we introduce Clinical Large-Scale\nIntegrative Multimodal Benchmark (CLIMB), a comprehensive clinical benchmark\nunifying diverse clinical data across imaging, language, temporal, and graph\nmodalities. CLIMB comprises 4.51 million patient samples totaling 19.01\nterabytes distributed across 2D imaging, 3D video, time series, graphs, and\nmultimodal data. Through extensive empirical evaluation, we demonstrate that\nmultitask pretraining significantly improves performance on understudied\ndomains, achieving up to 29% improvement in ultrasound and 23% in ECG analysis\nover single-task learning. Pretraining on CLIMB also effectively improves\nmodels' generalization capability to new tasks, and strong unimodal encoder\nperformance translates well to multimodal performance when paired with\ntask-appropriate fusion strategies. Our findings provide a foundation for new\narchitecture designs and pretraining strategies to advance clinical AI\nresearch. Code is released at https://github.com/DDVD233/climb.\n","authors":["Wei Dai","Peilin Chen","Malinda Lu","Daniel Li","Haowen Wei","Hejie Cui","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2503.07667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02226v2","updated":"2025-03-20T05:00:23Z","published":"2024-10-03T05:47:55Z","title":"Doubly Optimal Policy Evaluation for Reinforcement Learning","summary":"  Policy evaluation estimates the performance of a policy by (1) collecting\ndata from the environment and (2) processing raw data into a meaningful\nestimate. Due to the sequential nature of reinforcement learning, any improper\ndata-collecting policy or data-processing method substantially deteriorates the\nvariance of evaluation results over long time steps. Thus, policy evaluation\noften suffers from large variance and requires massive data to achieve the\ndesired accuracy. In this work, we design an optimal combination of\ndata-collecting policy and data-processing baseline. Theoretically, we prove\nour doubly optimal policy evaluation method is unbiased and guaranteed to have\nlower variance than previously best-performing methods. Empirically, compared\nwith previous works, we show our method reduces variance substantially and\nachieves superior empirical performance.\n","authors":["Shuze Daniel Liu","Claire Chen","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.02226v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15845v1","updated":"2025-03-20T04:58:50Z","published":"2025-03-20T04:58:50Z","title":"Network-wide Freeway Traffic Estimation Using Sparse Sensor Data: A\n  Dirichlet Graph Auto-Encoder Approach","summary":"  Network-wide Traffic State Estimation (TSE), which aims to infer a complete\nimage of network traffic states with sparsely deployed sensors, plays a vital\nrole in intelligent transportation systems. With the development of data-driven\nmethods, traffic dynamics modeling has advanced significantly. However, TSE\nposes fundamental challenges for data-driven approaches, since historical\npatterns cannot be learned locally at sensor-free segments. Although inductive\ngraph learning shows promise in estimating states at locations without sensor,\nexisting methods typically handle unobserved locations by filling them with\nzeros, introducing bias to the sensitive graph message propagation. The\nrecently proposed Dirichlet Energy-based Feature Propagation (DEFP) method\nachieves State-Of-The-Art (SOTA) performance in unobserved node classification\nby eliminating the need for zero-filling. However, applying it to TSE faces\nthree key challenges: inability to handle directed traffic networks, strong\nassumptions in traffic spatial correlation modeling, and overlooks distinct\npropagation rules of different patterns (e.g., congestion and free flow). We\npropose DGAE, a novel inductive graph representation model that addresses these\nchallenges through theoretically derived DEFP for Directed graph (DEFP4D),\nenhanced spatial representation learning via DEFP4D-guided latent space\nencoding, and physics-guided propagation mechanisms that separately handles\ncongested and free-flow patterns. Experiments on three traffic datasets\ndemonstrate that DGAE outperforms existing SOTA methods and exhibits strong\ncross-city transferability. Furthermore, DEFP4D can serve as a standalone\nlightweight solution, showing superior performance under extremely sparse\nsensor conditions.\n","authors":["Qishen Zhou","Yifan Zhang","Michail A. Makridis","Anastasios Kouvelas","Yibing Wang","Simon Hu"],"pdf_url":"https://arxiv.org/pdf/2503.15845v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2503.15842v1","updated":"2025-03-20T04:49:40Z","published":"2025-03-20T04:49:40Z","title":"FedAWA: Adaptive Optimization of Aggregation Weights in Federated\n  Learning Using Client Vectors","summary":"  Federated Learning (FL) has emerged as a promising framework for distributed\nmachine learning, enabling collaborative model training without sharing local\ndata, thereby preserving privacy and enhancing security. However, data\nheterogeneity resulting from differences across user behaviors, preferences,\nand device characteristics poses a significant challenge for federated\nlearning. Most previous works overlook the adjustment of aggregation weights,\nrelying solely on dataset size for weight assignment, which often leads to\nunstable convergence and reduced model performance. Recently, several studies\nhave sought to refine aggregation strategies by incorporating dataset\ncharacteristics and model alignment. However, adaptively adjusting aggregation\nweights while ensuring data security-without requiring additional proxy\ndata-remains a significant challenge. In this work, we propose Federated\nlearning with Adaptive Weight Aggregation (FedAWA), a novel method that\nadaptively adjusts aggregation weights based on client vectors during the\nlearning process. The client vector captures the direction of model updates,\nreflecting local data variations, and is used to optimize the aggregation\nweight without requiring additional datasets or violating privacy. By assigning\nhigher aggregation weights to local models whose updates align closely with the\nglobal optimization direction, FedAWA enhances the stability and generalization\nof the global model. Extensive experiments under diverse scenarios demonstrate\nthe superiority of our method, providing a promising solution to the challenges\nof data heterogeneity in federated learning.\n","authors":["Changlong Shi","He Zhao","Bingjie Zhang","Mingyuan Zhou","Dandan Guo","Yi Chang"],"pdf_url":"https://arxiv.org/pdf/2503.15842v1.pdf","comment":"Accepted in CVPR 2025"},{"id":"http://arxiv.org/abs/2406.11624v4","updated":"2025-03-20T12:06:17Z","published":"2024-06-17T15:07:55Z","title":"Words in Motion: Extracting Interpretable Control Vectors for Motion\n  Transformers","summary":"  Transformer-based models generate hidden states that are difficult to\ninterpret. In this work, we analyze hidden states and modify them at inference,\nwith a focus on motion forecasting. We use linear probing to analyze whether\ninterpretable features are embedded in hidden states. Our experiments reveal\nhigh probing accuracy, indicating latent space regularities with functionally\nimportant directions. Building on this, we use the directions between hidden\nstates with opposing features to fit control vectors. At inference, we add our\ncontrol vectors to hidden states and evaluate their impact on predictions.\nRemarkably, such modifications preserve the feasibility of predictions. We\nfurther refine our control vectors using sparse autoencoders (SAEs). This leads\nto more linear changes in predictions when scaling control vectors. Our\napproach enables mechanistic interpretation as well as zero-shot generalization\nto unseen dataset characteristics with negligible computational overhead.\n","authors":["Omer Sahin Tas","Royden Wagner"],"pdf_url":"https://arxiv.org/pdf/2406.11624v4.pdf","comment":"ICLR 2025 camera-ready. Our implementation is available at\n  github.com/kit-mrt/future-motion"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.16421v1","updated":"2025-03-20T17:59:42Z","published":"2025-03-20T17:59:42Z","title":"MagicMotion: Controllable Video Generation with Dense-to-Sparse\n  Trajectory Guidance","summary":"  Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site.\n","authors":["Quanhao Li","Zhen Xing","Rui Wang","Hui Zhang","Qi Dai","Zuxuan Wu"],"pdf_url":"https://arxiv.org/pdf/2503.16421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16406v1","updated":"2025-03-20T17:56:20Z","published":"2025-03-20T17:56:20Z","title":"VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness","summary":"  Recent large-scale text-to-image diffusion models generate photorealistic\nimages but often struggle to accurately depict interactions between humans and\nobjects due to their limited ability to differentiate various interaction\nwords. In this work, we propose VerbDiff to address the challenge of capturing\nnuanced interactions within text-to-image diffusion models. VerbDiff is a novel\ntext-to-image generation model that weakens the bias between interaction words\nand objects, enhancing the understanding of interactions. Specifically, we\ndisentangle various interaction words from frequency-based anchor words and\nleverage localized interaction regions from generated images to help the model\nbetter capture semantics in distinctive words without extra conditions. Our\napproach enables the model to accurately understand the intended interaction\nbetween humans and objects, producing high-quality images with accurate\ninteractions aligned with specified verbs. Extensive experiments on the\nHICO-DET dataset demonstrate the effectiveness of our method compared to\nprevious approaches.\n","authors":["SeungJu Cha","Kwanyoung Lee","Ye-Chan Kim","Hyunwoo Oh","Dong-Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2503.16406v1.pdf","comment":"Accepted at CVPR 2025, code :\n  https://github.com/SeungJuCha/VerbDiff.git"},{"id":"http://arxiv.org/abs/2503.16264v1","updated":"2025-03-20T15:57:25Z","published":"2025-03-20T15:57:25Z","title":"Do image and video quality metrics model low-level human vision?","summary":"  Image and video quality metrics, such as SSIM, LPIPS, and VMAF, are aimed to\npredict the perceived quality of the evaluated content and are often claimed to\nbe \"perceptual\". Yet, few metrics directly model human visual perception, and\nmost rely on hand-crafted formulas or training datasets to achieve alignment\nwith perceptual data. In this paper, we propose a set of tests for\nfull-reference quality metrics that examine their ability to model several\naspects of low-level human vision: contrast sensitivity, contrast masking, and\ncontrast matching. The tests are meant to provide additional scrutiny for newly\nproposed metrics. We use our tests to analyze 33 existing image and video\nquality metrics and find their strengths and weaknesses, such as the ability of\nLPIPS and MS-SSIM to predict contrast masking and poor performance of VMAF in\nthis task. We further find that the popular SSIM metric overemphasizes\ndifferences in high spatial frequencies, but its multi-scale counterpart,\nMS-SSIM, addresses this shortcoming. Such findings cannot be easily made using\nexisting evaluation protocols.\n","authors":["Dounia Hammou","Yancheng Cai","Pavan Madhusudanarao","Christos G. Bampis","Rafał K. Mantiuk"],"pdf_url":"https://arxiv.org/pdf/2503.16264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.16112v1","updated":"2025-03-20T13:00:36Z","published":"2025-03-20T13:00:36Z","title":"PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming","summary":"  Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).\n","authors":["Liming Liu","Jiangkai Wu","Haoyang Wang","Peiheng Wang","Xinggong Zhang","Zongming Guo"],"pdf_url":"https://arxiv.org/pdf/2503.16112v1.pdf","comment":"7 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.16064v1","updated":"2025-03-20T11:56:27Z","published":"2025-03-20T11:56:27Z","title":"PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for\n  Adaptive Hashing Retrieval","summary":"  Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash.\n","authors":["Qiang Zou","Shuli Cheng","Jiayi Chen"],"pdf_url":"https://arxiv.org/pdf/2503.16064v1.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2411.19772v3","updated":"2025-03-20T11:55:30Z","published":"2024-11-29T15:18:06Z","title":"LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware\n  Omni-Modal Perception of Long Videos","summary":"  Despite impressive advancements in video understanding, most efforts remain\nlimited to coarse-grained or visual-only video tasks. However, real-world\nvideos encompass omni-modal information (vision, audio, and speech) with a\nseries of events forming a cohesive storyline. The lack of multi-modal video\ndata with fine-grained event annotations and the high cost of manual labeling\nare major obstacles to comprehensive omni-modality video perception. To address\nthis gap, we propose an automatic pipeline consisting of high-quality\nmulti-modal video filtering, semantically coherent omni-modal event boundary\ndetection, and cross-modal correlation-aware event captioning. In this way, we\npresent LongVALE, the first-ever Vision-Audio-Language Event understanding\nbenchmark comprising 105K omni-modal events with precise temporal boundaries\nand detailed relation-aware captions within 8.4K high-quality long videos.\nFurther, we build a baseline that leverages LongVALE to enable video large\nlanguage models (LLMs) for omni-modality fine-grained temporal video\nunderstanding for the first time. Extensive experiments demonstrate the\neffectiveness and great potential of LongVALE in advancing comprehensive\nmulti-modal video understanding.\n","authors":["Tiantian Geng","Jinrui Zhang","Qingni Wang","Teng Wang","Jinming Duan","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2411.19772v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2312.08195v2","updated":"2025-03-20T04:15:45Z","published":"2023-12-13T14:59:49Z","title":"Image is All You Need to Empower Large-scale Diffusion Models for\n  In-Domain Generation","summary":"  In-domain generation aims to perform a variety of tasks within a specific\ndomain, such as unconditional generation, text-to-image, image editing, 3D\ngeneration, and more. Early research typically required training specialized\ngenerators for each unique task and domain, often relying on fully-labeled\ndata. Motivated by the powerful generative capabilities and broad applications\nof diffusion models, we are driven to explore leveraging label-free data to\nempower these models for in-domain generation. Fine-tuning a pre-trained\ngenerative model on domain data is an intuitive but challenging way and often\nrequires complex manual hyper-parameter adjustments since the limited diversity\nof the training data can easily disrupt the model's original generative\ncapabilities. To address this challenge, we propose a guidance-decoupled prior\npreservation mechanism to achieve high generative quality and controllability\nby image-only data, inspired by preserving the pre-trained model from a\ndenoising guidance perspective. We decouple domain-related guidance from the\nconditional guidance used in classifier-free guidance mechanisms to preserve\nopen-world control guidance and unconditional guidance from the pre-trained\nmodel. We further propose an efficient domain knowledge learning technique to\ntrain an additional text-free UNet copy to predict domain guidance. Besides, we\ntheoretically illustrate a multi-guidance in-domain generation pipeline for a\nvariety of generative tasks, leveraging multiple guidances from distinct\ndiffusion models and conditions. Extensive experiments demonstrate the\nsuperiority of our method in domain-specific synthesis and its compatibility\nwith various diffusion-based control methods and applications.\n","authors":["Pu Cao","Feng Zhou","Lu Yang","Tianrui Huang","Qing Song"],"pdf_url":"https://arxiv.org/pdf/2312.08195v2.pdf","comment":"Accepted to CVPR2025. Code is available at\n  https://github.com/PRIV-Creation/In-domain-Generation-Diffusion"}]}}